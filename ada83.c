//                                                                                                  //
//                                  A D A 8 3   C O M P I L E R                                    //
//                                                                                                  //
//              An Ada 1983 Compiler Targeting LLVM Intermediate Representation                     //
//                                                                                                  //
//  §1.   Foundations         Includes, typedefs, target constants, ctype wrappers               //
//  §2.   Measurement         Bit/byte morphisms, LLVM type selection, range checks              //
//  §3.   Memory              Arena allocator for the compilation session                        //
//  §4.   Text                String slices, hashing, edit distance                              //
//  §5.   Provenance          Source locations and diagnostic reporting                          //
//  §6.   Arithmetic          Big integers, big reals, exact rationals                           //
//  §7.   Lexical Analysis    Token kinds, lexer state, scanning functions                       //
//  §8.   Syntax              Node kinds, the syntax tree, node lists                            //
//  §9.   Parsing             Recursive descent for the full Ada 83 grammar                      //
//  §10.  Types               The Ada type lattice, Type_Info, classification                    //
//  §11.  Names               Symbol table, scopes, overload resolution                         //
//  §12.  Semantics           Name resolution, type checking, constant folding                   //
//  §13.  Code Generation     LLVM IR emission for every Ada construct                          //
//  §14.  Library Management  ALI files, checksums, dependency tracking                         //
//  §15.  Elaboration         Dependency ordering for multi-unit programs                        //
//  §16.  Generics            Macro-style instantiation of generic units                         //
//  §17.  File Loading        Include-path search, source file I/O                               //
//  §18.  Vector Paths        SIMD-accelerated scanning on x86-64 and ARM64                     //
//  §19.  Driver              Command-line parsing and top-level orchestration                    //
//                                                                                                  //


#include <ctype.h>
#include <dirent.h>
#include <errno.h>
#include <fcntl.h>
#include <iso646.h>
#include <limits.h>
#include <math.h>
#include <stdarg.h>
#include <stdbool.h>
#include <stdint.h>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <strings.h>
#include <pthread.h>
#include <sys/stat.h>
#include <sys/wait.h>
#include <unistd.h>

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §1.  FOUNDATIONS — Includes, typedefs, target constants, ctype wrappers
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Ada's numeric model demands integers wider than 64 bits; GCC/Clang __int128 gives us
// native 128-bit registers on 64-bit targets.  All widths, bounds, and capacities for the
// compiler's subsystems are defined here so that every translation unit sees consistent
// constants.  Safe ctype wrappers cast to unsigned char before calling the C library.
//

// -- Extended Integer Types
// Ada's numeric model demands integers wider than 64 bits; GCC/Clang __int128 gives us
// native 128-bit registers on 64-bit targets.
typedef __int128          int128_t;
typedef unsigned __int128 uint128_t;

// -- Target Data Model
// 64-bit LP64 host throughout; sizes in Type_Info are bytes, bit widths only in IR.
enum { Bits_Per_Unit = 8 };

typedef enum {
  Width_1      = 1,   Width_8     = 8,   Width_16     = 16,
  Width_32     = 32,  Width_64    = 64,  Width_128    = 128,
  Width_Ptr    = 64,  Width_Float = 32,  Width_Double = 64
} Bit_Width;

typedef enum {
  Ada_Short_Short_Integer_Bits    = Width_8,
  Ada_Short_Integer_Bits          = Width_16,
  Ada_Integer_Bits                = Width_32,
  Ada_Long_Integer_Bits           = Width_64,
  Ada_Long_Long_Integer_Bits      = Width_64,
  Ada_Long_Long_Long_Integer_Bits = Width_128
} Ada_Integer_Width;

enum {
  Default_Size_Bits   = Ada_Integer_Bits,
  Default_Size_Bytes  = Ada_Integer_Bits / Bits_Per_Unit,
  Default_Align_Bytes = Default_Size_Bytes
};

// -- Fat-Pointer Layout
// Unconstrained array parameters carry { data, bounds } -- 16 bytes on 64-bit targets.
#define FAT_PTR_TYPE         "{ ptr, ptr }"
#define FAT_PTR_ALLOC_SIZE   16
#define STRING_BOUND_TYPE    "i32"
#define STRING_BOUND_WIDTH   32
#define STRING_BOUNDS_STRUCT "{ i32, i32 }"
#define STRING_BOUNDS_ALLOC  8

// -- IEEE Floating-Point Model
#define IEEE_FLOAT_DIGITS      6
#define IEEE_DOUBLE_DIGITS     15
#define IEEE_FLOAT_MANTISSA    24
#define IEEE_DOUBLE_MANTISSA   53
#define IEEE_FLOAT_EMAX        128
#define IEEE_DOUBLE_EMAX       1024
#define IEEE_FLOAT_EMIN        (-125)
#define IEEE_DOUBLE_EMIN       (-1021)
#define IEEE_MACHINE_RADIX     2
#define IEEE_DOUBLE_MIN_NORMAL 2.2250738585072014e-308
#define IEEE_FLOAT_MIN_NORMAL  1.1754943508222875e-38
#define LOG2_OF_10             3.321928094887362

// -- Subsystem Capacities
enum { Default_Chunk_Size = 1 << 24 }; // Arena chunk: 16 MiB
#define SYMBOL_TABLE_SIZE   1024       // Hash buckets (§11)
#define MAX_INTERPRETATIONS 64         // Overload ceiling
#define ALI_VERSION         "Ada83 1.0 built " __DATE__ " " __TIME__
#define ELAB_MAX_VERTICES   512        // Elaboration graph (§15)
#define ELAB_MAX_EDGES      2048
#define ELAB_MAX_COMPONENTS 256

// -- Build-in-Place Formal Names
#define BIP_ALLOC_NAME  "__BIPalloc"
#define BIP_ACCESS_NAME "__BIPaccess"
#define BIP_MASTER_NAME "__BIPmaster"
#define BIP_CHAIN_NAME  "__BIPchain"
#define BIP_FINAL_NAME  "__BIPfinal"

// -- Code Generator Capacities
#define TEMP_TYPE_CAPACITY 4096 // Ring buffer for temp types
#define EXC_REF_CAPACITY   512  // Exception references per unit
#define MAX_AGG_DIMS       8    // Array aggregate dimensions
#define MAX_DISC_CACHE     16   // Discriminant value cache depth

// -- Runtime Check Flags
// Each bit controls a check category suppressible via pragma Suppress (RM 11.5).
#define CHK_RANGE        ((uint32_t)1)
#define CHK_OVERFLOW     ((uint32_t)2)
#define CHK_INDEX        ((uint32_t)4)
#define CHK_LENGTH       ((uint32_t)8)
#define CHK_DIVISION     ((uint32_t)16)
#define CHK_ACCESS       ((uint32_t)32)
#define CHK_DISCRIMINANT ((uint32_t)64)
#define CHK_ELABORATION  ((uint32_t)128)
#define CHK_STORAGE      ((uint32_t)256)
#define CHK_ALL          ((uint32_t)0xFFFFFFFF)

// -- Platform Detection
#if defined(__x86_64__) || defined(_M_X64)
  #define SIMD_X86_64  1
#elif defined(__aarch64__) || defined(_M_ARM64)
  #define SIMD_ARM64   1
#else
  #define SIMD_GENERIC 1
#endif

// -- Safe Character Classification
// Wrappers cast to unsigned char before calling ctype.  The C library functions
// take int and require unsigned char to avoid undefined behaviour on platforms
// where plain char is signed.
static inline int  Is_Alpha  (char ch) { return isalpha  ((unsigned char) ch); }
static inline int  Is_Digit  (char ch) { return isdigit  ((unsigned char) ch); }
static inline int  Is_Xdigit (char ch) { return isxdigit ((unsigned char) ch); }
static inline int  Is_Space  (char ch) { return isspace  ((unsigned char) ch); }
static inline char To_Lower  (char ch) { return (char) tolower ((unsigned char) ch); }

// Fast identifier-character lookup table per Ada LRM §2.3.  A character is an
// identifier constituent when it is an ASCII letter, digit, or underscore, or
// a Latin-1 letter in 0xC0..0xD6, 0xD8..0xF6, 0xF8..0xFF (excluding the
// multiplication sign 0xD7 and division sign 0xF7).
static const uint8_t Id_Char_Table[256] = {

  // ASCII letters
  ['A']=1,['B']=1,['C']=1,['D']=1,['E']=1,['F']=1,['G']=1,['H']=1,
  ['I']=1,['J']=1,['K']=1,['L']=1,['M']=1,['N']=1,['O']=1,['P']=1,
  ['Q']=1,['R']=1,['S']=1,['T']=1,['U']=1,['V']=1,['W']=1,['X']=1,
  ['Y']=1,['Z']=1,
  ['a']=1,['b']=1,['c']=1,['d']=1,['e']=1,['f']=1,['g']=1,['h']=1,
  ['i']=1,['j']=1,['k']=1,['l']=1,['m']=1,['n']=1,['o']=1,['p']=1,
  ['q']=1,['r']=1,['s']=1,['t']=1,['u']=1,['v']=1,['w']=1,['x']=1,
  ['y']=1,['z']=1,

  // Digits and underscore
  ['0']=1,['1']=1,['2']=1,['3']=1,['4']=1,['5']=1,['6']=1,['7']=1,
  ['8']=1,['9']=1,['_']=1,

  // Latin-1 uppercase letters: À–Ö (0xC0–0xD6)
  [0xC0]=1,[0xC1]=1,[0xC2]=1,[0xC3]=1,[0xC4]=1,[0xC5]=1,[0xC6]=1,[0xC7]=1,
  [0xC8]=1,[0xC9]=1,[0xCA]=1,[0xCB]=1,[0xCC]=1,[0xCD]=1,[0xCE]=1,[0xCF]=1,
  [0xD0]=1,[0xD1]=1,[0xD2]=1,[0xD3]=1,[0xD4]=1,[0xD5]=1,[0xD6]=1,
  // 0xD7 = × (multiplication sign) — NOT an identifier character

  // Latin-1: Ø–ß (0xD8–0xDF)
  [0xD8]=1,[0xD9]=1,[0xDA]=1,[0xDB]=1,[0xDC]=1,[0xDD]=1,[0xDE]=1,[0xDF]=1,

  // Latin-1 lowercase letters: à–ö (0xE0–0xF6)
  [0xE0]=1,[0xE1]=1,[0xE2]=1,[0xE3]=1,[0xE4]=1,[0xE5]=1,[0xE6]=1,[0xE7]=1,
  [0xE8]=1,[0xE9]=1,[0xEA]=1,[0xEB]=1,[0xEC]=1,[0xED]=1,[0xEE]=1,[0xEF]=1,
  [0xF0]=1,[0xF1]=1,[0xF2]=1,[0xF3]=1,[0xF4]=1,[0xF5]=1,[0xF6]=1,
  // 0xF7 = ÷ (division sign) — NOT an identifier character

  // Latin-1 remaining lowercase: ø–ÿ (0xF8–0xFF)
  [0xF8]=1,[0xF9]=1,[0xFA]=1,[0xFB]=1,[0xFC]=1,[0xFD]=1,[0xFE]=1,[0xFF]=1
};
#define Is_Id_Char(ch) (Id_Char_Table[(uint8_t)(ch)])

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §2.  MEASUREMENT — Bit/byte morphisms, LLVM type selection, range checks
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Size morphisms convert between the byte world of Type_Info and the bit world of LLVM IR.
// Llvm_Int_Type and Llvm_Float_Type map a bit width to the smallest LLVM type that holds it.
// Range predicates determine representation width for integer and modular types.
//

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §2.1  Bit/Byte Conversions — Size morphisms
//
// To_Bits   : bytes -> bits   (multiplicative, total — never truncates)
// To_Bytes  : bits  -> bytes  (ceiling division — rounds up to next whole byte)
// Byte_Align: bits  -> bits   (round up to a byte boundary)
// Align_To  : round size up to the nearest alignment boundary
// ─────────────────────────────────────────────────────────────────────────────────────────────────
static inline uint64_t To_Bits    (uint64_t bytes) { return bytes * Bits_Per_Unit; }
static inline uint64_t To_Bytes   (uint64_t bits)  { return (bits + Bits_Per_Unit - 1) / Bits_Per_Unit; }
static inline uint64_t Byte_Align (uint64_t bits)  { return To_Bits (To_Bytes (bits)); }
static inline size_t   Align_To   (size_t size, size_t alignment) {
  return alignment ? ((size + alignment - 1) & ~(alignment - 1)) : size;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §2.2  LLVM Type Selection — Width-to-type morphisms
//
// Given a bit width, return the smallest LLVM integer or float type that can
// hold that width.
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Return the smallest LLVM integer type (i1 through i128) for `bits' bits.
static inline const char *Llvm_Int_Type (uint32_t bits) {
  return bits <= 1   ? "i1"   : bits <= 8   ? "i8"   : bits <= 16  ? "i16" :
         bits <= 32  ? "i32"  : bits <= 64  ? "i64"  : "i128";
}
// Return "float" for single-precision widths, "double" otherwise.
static inline const char *Llvm_Float_Type (uint32_t bits) {
  return bits <= Width_Float ? "float" : "double";
}
// Return true when the LLVM type string denotes a thin pointer ("ptr").
static inline bool Llvm_Type_Is_Pointer (const char *llvm_type) {
  return llvm_type
    and llvm_type[0] == 'p' and llvm_type[1] == 't'
    and llvm_type[2] == 'r' and llvm_type[3] == '\0';
}
// Return true when the LLVM type string denotes the fat-pointer struct
// "{ ptr, ptr }" used for unconstrained array parameters.
static inline bool Llvm_Type_Is_Fat_Pointer (const char *llvm_type) {
  return llvm_type and strcmp (llvm_type, "{ ptr, ptr }") == 0;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §2.3  Range Predicates — Determining representation width
//
// Compute the minimum number of bits needed to represent the integer range
// [lo .. hi].  All bounds are int128_t / uint128_t so the full Ada integer
// range is representable.
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Return true when [lo..hi] fits within the signed range of `bits' bits.
static inline bool Fits_In_Signed (int128_t lo, int128_t hi, uint32_t bits) {
  if (bits >= 128) return true;
  if (bits >= 64)  return lo >= (int128_t) INT64_MIN and hi <= (int128_t) INT64_MAX;
  int128_t range_min = -((int128_t) 1 << (bits - 1));
  int128_t range_max =  ((int128_t) 1 << (bits - 1)) - 1;
  return lo >= range_min and hi <= range_max;
}
// Return true when [lo..hi] fits within the unsigned range of `bits' bits.
static inline bool Fits_In_Unsigned (int128_t lo, int128_t hi, uint32_t bits) {
  if (lo < 0) return false;
  if (bits >= 128) return true;
  if (bits >= 64)  return (uint128_t) hi <= UINT64_MAX;
  return (uint128_t) hi < ((uint128_t) 1 << bits);
}
// Return the smallest standard LLVM integer width (8, 16, 32, 64, or 128)
// that can represent every value in the range [lo .. hi].
static inline uint32_t Bits_For_Range (int128_t lo, int128_t hi) {
  if (lo >= 0) {
    uint128_t upper = (uint128_t) hi;
    return upper < 256                   ? Width_8   :
           upper < 65536                 ? Width_16  :
           upper < (uint128_t) 1 << 32   ? Width_32  :
           upper <= UINT64_MAX           ? Width_64  : Width_128;
  }
  return Fits_In_Signed (lo, hi, 8)  ? Width_8  :
         Fits_In_Signed (lo, hi, 16) ? Width_16 :
         Fits_In_Signed (lo, hi, 32) ? Width_32 :
         Fits_In_Signed (lo, hi, 64) ? Width_64 : Width_128;
}
// Return the smallest standard LLVM integer width for a modular type whose
// range is 0 .. modulus-1.
static inline uint32_t Bits_For_Modulus (uint128_t modulus) {
  if (modulus == 0) return 0;
  uint128_t max_value = modulus - 1;
  return max_value < 256                   ? Width_8  :
         max_value < 65536                 ? Width_16 :
         max_value < (uint128_t) 1 << 32   ? Width_32 :
         max_value <= UINT64_MAX           ? Width_64 : Width_128;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §3.  MEMORY — Arena allocator for the compilation session
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// A simple bump allocator used for AST nodes, interned strings, and other objects whose
// lifetime spans the entire compilation.  All memory is freed in one shot at the end via
// Arena_Free_All.  Chunks are 16 MiB by default; oversized requests get their own chunk.
// All pointers are 16-byte aligned.
//

typedef struct Arena_Chunk Arena_Chunk;
struct Arena_Chunk {
  Arena_Chunk *previous; // Singly-linked list of chunks
  char        *base;     // First usable byte
  char        *current;  // Bump pointer
  char        *end;      // One past last usable byte
};

typedef struct {
  Arena_Chunk *head;       // Most recently allocated chunk in the singly-linked list
  size_t       chunk_size; // Default byte size for new chunk allocations (16 MiB)
} Memory_Arena;

extern Memory_Arena Global_Arena;

void *Arena_Allocate (size_t size); // Zero-filled, 16-byte aligned
void  Arena_Free_All (void);        // Release every chunk at end of main

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §4.  TEXT — Non-owning string views
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// A String_Slice is a (pointer, length) pair borrowed from the source buffer or the arena.
// It avoids strlen() calls and allows substring views without allocation.  Ada identifiers
// are case-insensitive (RM 2.3), so comparison and hashing fold to lower case.
//

// No null terminator required.  Comparison and hashing fold to lower case (Ada RM 2.3).
typedef struct {
  const char *data;   // Pointer into the source buffer or arena (not null-terminated)
  uint32_t    length; // Number of bytes in the slice
} String_Slice;

#define S(lit) ((String_Slice){ .data = (lit), .length = sizeof(lit) - 1 })

extern const String_Slice Empty_Slice;

// Wrap a null-terminated C string as a non-owning slice.
String_Slice Slice_From_Cstring      (const char  *source);
// Arena-allocate a copy of `slice' so it outlives the source buffer.
String_Slice Slice_Duplicate         (String_Slice slice);
// Byte-exact equality test (case-sensitive).
bool         Slice_Equal             (String_Slice left,  String_Slice right);
// Case-insensitive equality following Ada RM 2.3 identifier equivalence.
bool         Slice_Equal_Ignore_Case (String_Slice left,  String_Slice right);
// FNV-1a hash folded to lower case for use in hash tables.
uint64_t     Slice_Hash              (String_Slice slice);
// Levenshtein edit distance for spelling-correction suggestions.
int          Edit_Distance           (String_Slice left,  String_Slice right);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §5.  PROVENANCE — Anchoring diagnostics to source text
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Every AST node, token, and symbol carries a Source_Location so that error messages can
// point the programmer at the exact file, line, and column where the problem was detected.
// Errors are accumulated rather than triggering an immediate abort, so the compiler can
// report multiple issues in a single invocation.
//

// accumulated; Error_Count is checked after each phase.  Fatal_Error calls exit(1).
typedef struct {
  const char *filename; // Path of the source file (interned, never freed)
  uint32_t    line;     // One-based line number within the file
  uint32_t    column;   // One-based column number within the line
} Source_Location;

extern const Source_Location No_Location;
extern int                   Error_Count;

// Emit a diagnostic message and increment Error_Count without stopping.
void Report_Error (Source_Location location, const char *format, ...);

// Emit a diagnostic message and terminate the compiler immediately.
__attribute__((noreturn))
void Fatal_Error  (Source_Location location, const char *format, ...);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §6.  ARITHMETIC — Big integers, big reals, exact rationals
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Three levels of exact arithmetic for Ada numeric literals and constant folding:
//   Big_Integer  — Arbitrary-precision integers as little-endian 64-bit limb arrays.
//   Big_Real     — Significand (Big_Integer) paired with a power-of-ten exponent.
//   Rational     — Exact quotient of two Big_Integers, always reduced by GCD.
// All storage is arena-allocated; no explicit deallocation.  Ada numeric literals can
// exceed the 64-bit range (e.g. mod 2**128), so arbitrary precision is not optional.
//

//   Big_Integer  Arbitrary-precision integers as little-endian 64-bit limb arrays.
//   Big_Real     Significand (Big_Integer) paired with a power-of-ten exponent.
//   Rational     Exact quotient of two Big_Integers, always reduced by GCD.

// -- Big_Integer
typedef struct {
  uint64_t *limbs;       // Little-endian 64-bit digits
  uint32_t  count;       // Active limbs
  uint32_t  capacity;    // Allocated slots
  bool      is_negative; // Sign flag; magnitude always positive
} Big_Integer;

// Arena-allocate a zero-initialised Big_Integer with room for `capacity' limbs.
Big_Integer *Big_Integer_New             (uint32_t     capacity);
// Return an independent deep copy of `source'.
Big_Integer *Big_Integer_Clone           (const Big_Integer *source);
// Return a fresh Big_Integer whose value is 1.
Big_Integer *Big_Integer_One             (void);
// Grow the limb array if fewer than `needed' slots are allocated.
void         Big_Integer_Ensure_Capacity (Big_Integer *integer, uint32_t needed);
// Strip leading zero limbs so that `count' reflects the true magnitude.
void         Big_Integer_Normalize       (Big_Integer *integer);
// In-place: integer = integer * factor + addend  (schoolbook single-limb step).
void         Big_Integer_Mul_Add_Small   (Big_Integer *integer,
                                          uint64_t     factor,
                                          uint64_t     addend);

// Three-way comparison: returns -1, 0, or +1.
int          Big_Integer_Compare         (const Big_Integer  *left,
                                          const Big_Integer  *right);
// Return a new Big_Integer equal to left + right (signs handled internally).
Big_Integer *Big_Integer_Add             (const Big_Integer  *left,
                                          const Big_Integer  *right);
// Schoolbook O(n*m) multiplication; result is arena-allocated.
Big_Integer *Big_Integer_Multiply        (const Big_Integer  *left,
                                          const Big_Integer  *right);
// Euclid's algorithm; both operands are treated as non-negative.
Big_Integer *Big_Integer_GCD             (const Big_Integer  *left,
                                          const Big_Integer  *right);
// Simultaneous division and remainder (Knuth Algorithm D).
void         Big_Integer_Div_Rem         (const Big_Integer  *dividend,
                                          const Big_Integer  *divisor,
                                          Big_Integer       **quotient,
                                          Big_Integer       **remainder);

// Parse a decimal digit string using SIMD-accelerated 8-digit groups.
Big_Integer *Big_Integer_From_Decimal_SIMD (const char        *text);
// If the magnitude fits in a signed 64-bit word, write it to *out and return true.
bool         Big_Integer_Fits_Int64        (const Big_Integer  *integer, int64_t   *out);
// Widen to uint128_t; return false on overflow.
bool         Big_Integer_To_Uint128        (const Big_Integer  *integer, uint128_t *out);
// Widen to int128_t; return false on overflow.
bool         Big_Integer_To_Int128         (const Big_Integer  *integer, int128_t  *out);

// -- Big_Real
// A real number represented as significand * 10^exponent.  The significand is an
// arbitrary-precision integer, so no precision is lost during literal parsing.
typedef struct {
  Big_Integer *significand; // The integer part of the scientific notation
  int32_t      exponent;    // Power-of-ten scaling factor (may be negative)
} Big_Real;

// Arena-allocate a zero-valued Big_Real.
Big_Real *Big_Real_New         (void);
// Parse a decimal literal (with optional exponent) into a Big_Real.
Big_Real *Big_Real_From_String (const char    *text);
// Best-effort conversion to IEEE double; may lose precision.
double    Big_Real_To_Double   (const Big_Real *real);
// Return true when the value is exactly representable as a double.
bool      Big_Real_Fits_Double (const Big_Real *real);
// Three-way comparison; aligns exponents before comparing significands.
int       Big_Real_Compare     (const Big_Real *left,     const Big_Real *right);
// Multiply by 10^scale (shifts the exponent without touching the significand).
Big_Real *Big_Real_Scale       (const Big_Real *real,     int32_t         scale);
// Exact integer division of the significand; the exponent is unchanged.
Big_Real *Big_Real_Divide_Int  (const Big_Real *dividend, int64_t         divisor);
// Format as an LLVM hexadecimal floating-point constant into `buffer'.
void      Big_Real_To_Hex      (const Big_Real *real,
                                char           *buffer,
                                size_t          buffer_size);
// Add or subtract two Big_Reals after aligning their exponents.
Big_Real *Big_Real_Add_Sub     (const Big_Real *left,
                                const Big_Real *right,
                                bool            subtract);
// Multiply significands and sum exponents.
Big_Real *Big_Real_Multiply    (const Big_Real *left,
                                const Big_Real *right);

// -- Rational
// An exact quotient p/q with the invariant that gcd(|p|,|q|) = 1 and q > 0.
// Used for fixed-point delta arithmetic and static expression folding.
typedef struct {
  Big_Integer *numerator;   // Signed numerator (the sign lives here)
  Big_Integer *denominator; // Positive denominator, always >= 1
} Rational;

// Divide both sides by their GCD and normalise the sign.
Rational Rational_Reduce        (Big_Integer    *numer, Big_Integer    *denom);
// Convert a Big_Real to a rational by expanding the power-of-ten exponent.
Rational Rational_From_Big_Real (const Big_Real *real);
// Construct the rational value/1.
Rational Rational_From_Int      (int64_t         value);
// Exact rational addition: a/b + c/d = (ad + bc) / bd, then reduce.
Rational Rational_Add           (Rational         left, Rational        right);
// Exact rational subtraction.
Rational Rational_Sub           (Rational         left, Rational        right);
// Exact rational multiplication: (a*c) / (b*d), then reduce.
Rational Rational_Mul           (Rational         left, Rational        right);
// Exact rational division: (a*d) / (b*c), then reduce.
Rational Rational_Div           (Rational         left, Rational        right);
// Raise a rational to a non-negative integer power by repeated squaring.
Rational Rational_Pow           (Rational         base, int             exponent);
// Three-way comparison: returns -1, 0, or +1.
int      Rational_Compare       (Rational         left, Rational        right);
// Best-effort conversion to IEEE double; may lose precision.
double   Rational_To_Double     (Rational         rational);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §7.  LEXICAL ANALYSIS — Token kinds, lexer state, scanning functions
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// The scanner converts a flat character buffer into a stream of typed tokens.  Token_Kind
// enumerates every lexeme in the Ada 83 grammar: identifiers, numeric and string literals,
// delimiters, operator symbols, and the sixty-three reserved words of RM 2.9.  The lexer
// maintains a sliding cursor with one character of look-ahead; SIMD fast-paths accelerate
// whitespace skipping and identifier scanning on x86-64 and ARM64.
//

// Token_Kind enumerates every lexeme in the Ada 83 grammar: identifiers, numeric and
// string literals, delimiters, operator symbols, and the sixty-three reserved words of RM 2.9.
typedef enum {
  TK_EOF = 0,
  TK_ERROR,

  // Literals
  TK_IDENTIFIER, TK_INTEGER,   TK_REAL,      TK_CHARACTER, TK_STRING,

  // Delimiters
  TK_LPAREN,     TK_RPAREN,    TK_LBRACKET,  TK_RBRACKET,
  TK_COMMA,      TK_DOT,       TK_SEMICOLON, TK_COLON,     TK_TICK,

  // Compound delimiters
  TK_ASSIGN,     TK_ARROW,     TK_DOTDOT,    TK_LSHIFT,
  TK_RSHIFT,     TK_BOX,       TK_BAR,

  // Operators
  TK_EQ,         TK_NE,        TK_LT,        TK_LE,
  TK_GT,         TK_GE,        TK_PLUS,      TK_MINUS,
  TK_STAR,       TK_SLASH,     TK_AMPERSAND, TK_EXPON,

  // Reserved words -- the sixty-three of Ada 83
  TK_ABORT,     TK_ABS,       TK_ACCEPT,    TK_ACCESS,    TK_ALL,
  TK_AND,       TK_AND_THEN,  TK_ARRAY,     TK_AT,        TK_BEGIN,
  TK_BODY,      TK_CASE,      TK_CONSTANT,  TK_DECLARE,   TK_DELAY,
  TK_DELTA,     TK_DIGITS,    TK_DO,        TK_ELSE,      TK_ELSIF,
  TK_END,       TK_ENTRY,     TK_EXCEPTION, TK_EXIT,      TK_FOR,
  TK_FUNCTION,  TK_GENERIC,   TK_GOTO,      TK_IF,        TK_IN,
  TK_IS,        TK_LIMITED,   TK_LOOP,      TK_MOD,       TK_NEW,
  TK_NOT,       TK_NULL,      TK_OF,        TK_OR,        TK_OR_ELSE,
  TK_OTHERS,    TK_OUT,       TK_PACKAGE,   TK_PRAGMA,    TK_PRIVATE,
  TK_PROCEDURE, TK_RAISE,     TK_RANGE,     TK_RECORD,    TK_REM,
  TK_RENAMES,   TK_RETURN,    TK_REVERSE,   TK_SELECT,    TK_SEPARATE,
  TK_SUBTYPE,   TK_TASK,      TK_TERMINATE, TK_THEN,      TK_TYPE,
  TK_USE,       TK_WHEN,      TK_WHILE,     TK_WITH,      TK_XOR,

  TK_COUNT
} Token_Kind;

// Token kind names for diagnostics and error messages.
static const char *Token_Name[TK_COUNT] = {
  [TK_EOF]        = "<eof>",      [TK_ERROR]      = "<error>",    [TK_IDENTIFIER] = "identifier",
  [TK_INTEGER]    = "integer",    [TK_REAL]       = "real",       [TK_CHARACTER]  = "character",
  [TK_STRING]     = "string",

  [TK_LPAREN]     = "(",  [TK_RPAREN]     = ")",  [TK_LBRACKET]   = "[",  [TK_RBRACKET]   = "]",
  [TK_COMMA]      = ",",  [TK_DOT]        = ".",  [TK_SEMICOLON]  = ";",  [TK_COLON]      = ":",
  [TK_TICK]       = "'",  [TK_ASSIGN]     = ":=", [TK_ARROW]      = "=>", [TK_DOTDOT]     = "..",
  [TK_LSHIFT]     = "<<", [TK_RSHIFT]     = ">>", [TK_BOX]        = "<>", [TK_BAR]        = "|",
  [TK_EQ]         = "=",  [TK_NE]         = "/=", [TK_LT]         = "<",  [TK_LE]         = "<=",
  [TK_GT]         = ">",  [TK_GE]         = ">=", [TK_PLUS]       = "+",  [TK_MINUS]      = "-",
  [TK_STAR]       = "*",  [TK_SLASH]      = "/",  [TK_AMPERSAND]  = "&",  [TK_EXPON]      = "**",

  [TK_ABORT]      = "ABORT",      [TK_ABS]        = "ABS",        [TK_ACCEPT]     = "ACCEPT",
  [TK_ACCESS]     = "ACCESS",     [TK_ALL]        = "ALL",        [TK_AND]        = "AND",
  [TK_AND_THEN]   = "AND THEN",   [TK_ARRAY]      = "ARRAY",      [TK_AT]         = "AT",
  [TK_BEGIN]      = "BEGIN",      [TK_BODY]       = "BODY",       [TK_CASE]       = "CASE",
  [TK_CONSTANT]   = "CONSTANT",   [TK_DECLARE]    = "DECLARE",    [TK_DELAY]      = "DELAY",
  [TK_DELTA]      = "DELTA",      [TK_DIGITS]     = "DIGITS",     [TK_DO]         = "DO",
  [TK_ELSE]       = "ELSE",       [TK_ELSIF]      = "ELSIF",      [TK_END]        = "END",
  [TK_ENTRY]      = "ENTRY",      [TK_EXCEPTION]  = "EXCEPTION",  [TK_EXIT]       = "EXIT",
  [TK_FOR]        = "FOR",        [TK_FUNCTION]   = "FUNCTION",   [TK_GENERIC]    = "GENERIC",
  [TK_GOTO]       = "GOTO",       [TK_IF]         = "IF",         [TK_IN]         = "IN",
  [TK_IS]         = "IS",         [TK_LIMITED]    = "LIMITED",    [TK_LOOP]       = "LOOP",
  [TK_MOD]        = "MOD",        [TK_NEW]        = "NEW",        [TK_NOT]        = "NOT",
  [TK_NULL]       = "NULL",       [TK_OF]         = "OF",         [TK_OR]         = "OR",
  [TK_OR_ELSE]    = "OR ELSE",    [TK_OTHERS]     = "OTHERS",     [TK_OUT]        = "OUT",
  [TK_PACKAGE]    = "PACKAGE",    [TK_PRAGMA]     = "PRAGMA",     [TK_PRIVATE]    = "PRIVATE",
  [TK_PROCEDURE]  = "PROCEDURE",  [TK_RAISE]      = "RAISE",      [TK_RANGE]      = "RANGE",
  [TK_RECORD]     = "RECORD",     [TK_REM]        = "REM",        [TK_RENAMES]    = "RENAMES",
  [TK_RETURN]     = "RETURN",     [TK_REVERSE]    = "REVERSE",    [TK_SELECT]     = "SELECT",
  [TK_SEPARATE]   = "SEPARATE",   [TK_SUBTYPE]    = "SUBTYPE",    [TK_TASK]       = "TASK",
  [TK_TERMINATE]  = "TERMINATE",  [TK_THEN]       = "THEN",       [TK_TYPE]       = "TYPE",
  [TK_USE]        = "USE",        [TK_WHEN]       = "WHEN",       [TK_WHILE]      = "WHILE",
  [TK_WITH]       = "WITH",       [TK_XOR]        = "XOR"
};

// Keyword lookup table — sorted alphabetically; linear scan is adequate
// for the sixty-three reserved words of Ada 83.
static struct { String_Slice name; Token_Kind kind; } Keywords[] = {
  {S("abort")    , TK_ABORT    }, {S("abs")      , TK_ABS      }, {S("accept")   , TK_ACCEPT   },
  {S("access")   , TK_ACCESS   }, {S("all")      , TK_ALL      }, {S("and")      , TK_AND      },
  {S("array")    , TK_ARRAY    }, {S("at")       , TK_AT       }, {S("begin")    , TK_BEGIN    },
  {S("body")     , TK_BODY     }, {S("case")     , TK_CASE     }, {S("constant") , TK_CONSTANT },
  {S("declare")  , TK_DECLARE  }, {S("delay")    , TK_DELAY    }, {S("delta")    , TK_DELTA    },
  {S("digits")   , TK_DIGITS   }, {S("do")       , TK_DO       }, {S("else")     , TK_ELSE     },
  {S("elsif")    , TK_ELSIF    }, {S("end")      , TK_END      }, {S("entry")    , TK_ENTRY    },
  {S("exception"), TK_EXCEPTION}, {S("exit")     , TK_EXIT     }, {S("for")      , TK_FOR      },
  {S("function") , TK_FUNCTION }, {S("generic")  , TK_GENERIC  }, {S("goto")     , TK_GOTO     },
  {S("if")       , TK_IF       }, {S("in")       , TK_IN       }, {S("is")       , TK_IS       },
  {S("limited")  , TK_LIMITED  }, {S("loop")     , TK_LOOP     }, {S("mod")      , TK_MOD      },
  {S("new")      , TK_NEW      }, {S("not")      , TK_NOT      }, {S("null")     , TK_NULL     },
  {S("of")       , TK_OF       }, {S("or")       , TK_OR       }, {S("others")   , TK_OTHERS   },
  {S("out")      , TK_OUT      }, {S("package")  , TK_PACKAGE  }, {S("pragma")   , TK_PRAGMA   },
  {S("private")  , TK_PRIVATE  }, {S("procedure"), TK_PROCEDURE}, {S("raise")    , TK_RAISE    },
  {S("range")    , TK_RANGE    }, {S("record")   , TK_RECORD   }, {S("rem")      , TK_REM      },
  {S("renames")  , TK_RENAMES  }, {S("return")   , TK_RETURN   }, {S("reverse")  , TK_REVERSE  },
  {S("select")   , TK_SELECT   }, {S("separate") , TK_SEPARATE }, {S("subtype")  , TK_SUBTYPE  },
  {S("task")     , TK_TASK     }, {S("terminate"), TK_TERMINATE}, {S("then")     , TK_THEN     },
  {S("type")     , TK_TYPE     }, {S("use")      , TK_USE      }, {S("when")     , TK_WHEN     },
  {S("while")    , TK_WHILE    }, {S("with")     , TK_WITH     }, {S("xor")      , TK_XOR      },
  {{NULL, 0}, TK_EOF}
};

// Look up a name in the keyword table; return TK_IDENTIFIER if not a reserved word.
static inline Token_Kind Lookup_Keyword (String_Slice name) {
  for (int i = 0; Keywords[i].name.data; i++)
    if (Slice_Equal_Ignore_Case (name, Keywords[i].name))
      return Keywords[i].kind;
  return TK_IDENTIFIER;
}

// -- Token Record
// A single lexeme produced by the scanner.  The two anonymous unions overlay
// machine-width and arbitrary-precision values so that each token is compact.
typedef struct {
  Token_Kind      kind;          // Discriminant identifying the token class
  Source_Location location;      // Where in the source file this token began
  String_Slice    text;          // Raw source text of the lexeme
  union { int64_t      integer_value; // Machine-width value for integer literals
          double       float_value; };// Machine-width value for real literals
  union { Big_Integer *big_integer;   // Arbitrary-precision integer (overflow path)
          Big_Real    *big_real; };    // Arbitrary-precision real (overflow path)
} Token;

// Construct a zero-valued token with the given kind, location, and text.
static inline Token Make_Token (Token_Kind      kind,
                                Source_Location location,
                                String_Slice    text) {
  return (Token){
    .kind = kind, .location = location, .text = text,
    .integer_value = 0, .big_integer = NULL
  };
}

// -- Lexer State
// The lexer maintains a sliding cursor over the source buffer.  The prev_token_kind
// field is needed to distinguish the character literal tick (') from the attribute
// tick in contexts like X'First vs. 'A'.
typedef struct {
  const char *source_start;    // First byte of the source buffer
  const char *current;         // Current scan position (bump pointer)
  const char *source_end;      // One past the last byte of the source buffer
  const char *filename;        // Source file name for error reporting
  uint32_t    line;            // Current one-based line number
  uint32_t    column;          // Current one-based column number
  Token_Kind  prev_token_kind; // Kind of the most recently returned token
} Lexer;

// Construct a fresh lexer over the given source buffer, positioned at line 1, column 1.
static inline Lexer Lexer_New (const char *source, size_t length, const char *filename) {
  return (Lexer){
    .source_start    = source,
    .current         = source,
    .source_end      = source + length,
    .filename        = filename,
    .line            = 1,
    .column          = 1,
    .prev_token_kind = TK_EOF
  };
}
// Return the character `offset' bytes ahead of the cursor without consuming it.
// Returns '\0' when `offset' would read past the end of the source buffer.
static inline char Lexer_Peek (const Lexer *lex, size_t offset) {
  return lex->current + offset < lex->source_end ? lex->current[offset] : '\0';
}
// Consume and return the current character, advancing line/column tracking.
// Returns '\0' when the source buffer has been exhausted.
static inline char Lexer_Advance (Lexer *lex) {
  if (lex->current >= lex->source_end) return '\0';
  char ch = *lex->current++;
  if (ch == '\n') { lex->line++; lex->column = 1; }
  else lex->column++;
  return ch;
}
// Skip whitespace and Ada comments (-- to end of line) using SIMD fast paths.
void  Lexer_Skip_Whitespace_And_Comments (Lexer *lex);
// Produce the next token from the input stream.
Token Lexer_Next_Token                   (Lexer *lex);

// Scan an Ada identifier or reserved word.
Token Scan_Identifier        (Lexer *lex);
// Scan a numeric literal (integer or real, with optional base and exponent).
Token Scan_Number            (Lexer *lex);
// Scan a single character literal delimited by tick marks.
Token Scan_Character_Literal (Lexer *lex);
// Scan a string literal delimited by double quotes (handles "" escapes).
Token Scan_String_Literal    (Lexer *lex);
// Return the integer value of a hex digit character, or -1 if not a digit.
int   Digit_Value            (char   ch);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §8.  SYNTAX — Node kinds, the syntax tree, node lists
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// The AST uses a tagged-union design: each Syntax_Node carries a Node_Kind tag and a
// union payload specific to that kind.  The tree is a forest — one root per compilation
// unit, with shared subtrees within a unit where the grammar allows (e.g. subtype marks
// referenced from multiple declarations).  All nodes are arena-allocated and never
// individually freed.
//

// maps to a Node_Kind; the Syntax_Node record carries kind, location, type annotation,
// symbol link, and a payload union discriminated by the kind tag.

// -- Forward Declarations
typedef struct Syntax_Node Syntax_Node;
typedef struct Type_Info   Type_Info;
typedef struct Symbol      Symbol;
typedef struct Scope       Scope;

// -- Node List
// A growable array of Syntax_Node pointers.  Used everywhere a grammar rule
// produces a sequence (statement lists, declaration lists, parameter lists, etc.).
typedef struct {
  Syntax_Node **items;    // Arena-allocated pointer array
  uint32_t      count;    // Number of nodes currently stored
  uint32_t      capacity; // Allocated slots before a resize is needed
} Node_List;

// Append `node' to the list, growing the backing array if necessary.
void Node_List_Push (Node_List *list, Syntax_Node *node);

// -- Node Kinds
typedef enum {

  // Literals and primaries
  NK_INTEGER,          NK_REAL,             NK_STRING,           NK_CHARACTER,
  NK_NULL,             NK_OTHERS,           NK_IDENTIFIER,       NK_SELECTED,
  NK_ATTRIBUTE,        NK_QUALIFIED,

  // Expressions
  NK_BINARY_OP,        NK_UNARY_OP,         NK_AGGREGATE,        NK_ALLOCATOR,
  NK_APPLY,            // Unified: call, index, slice, conversion
  NK_RANGE,
  NK_ASSOCIATION,      // name => value

  // Type definitions
  NK_SUBTYPE_INDICATION,     NK_RANGE_CONSTRAINT,
  NK_INDEX_CONSTRAINT,       NK_DISCRIMINANT_CONSTRAINT,
  NK_DIGITS_CONSTRAINT,      NK_DELTA_CONSTRAINT,
  NK_ARRAY_TYPE,       NK_RECORD_TYPE,      NK_ACCESS_TYPE,      NK_DERIVED_TYPE,
  NK_ENUMERATION_TYPE, NK_INTEGER_TYPE,     NK_REAL_TYPE,
  NK_COMPONENT_DECL,   NK_VARIANT_PART,     NK_VARIANT,
  NK_DISCRIMINANT_SPEC,

  // Statements
  NK_ASSIGNMENT,       NK_CALL_STMT,        NK_RETURN,           NK_IF,         NK_CASE,
  NK_LOOP,             NK_BLOCK,            NK_EXIT,             NK_GOTO,       NK_RAISE,
  NK_NULL_STMT,        NK_LABEL,            NK_ACCEPT,           NK_SELECT,
  NK_DELAY,            NK_ABORT,            NK_CODE,

  // Declarations
  NK_OBJECT_DECL,      NK_TYPE_DECL,        NK_SUBTYPE_DECL,     NK_EXCEPTION_DECL,
  NK_PROCEDURE_SPEC,   NK_FUNCTION_SPEC,
  NK_PROCEDURE_BODY,   NK_FUNCTION_BODY,
  NK_PACKAGE_SPEC,     NK_PACKAGE_BODY,     NK_TASK_SPEC,        NK_TASK_BODY,
  NK_ENTRY_DECL,       NK_SUBPROGRAM_RENAMING,
  NK_PACKAGE_RENAMING, NK_EXCEPTION_RENAMING,
  NK_GENERIC_DECL,     NK_GENERIC_INST,
  NK_PARAM_SPEC,       NK_USE_CLAUSE,       NK_WITH_CLAUSE,      NK_PRAGMA,
  NK_REPRESENTATION_CLAUSE, NK_EXCEPTION_HANDLER,
  NK_CONTEXT_CLAUSE,        NK_COMPILATION_UNIT,

  // Generic formals
  NK_GENERIC_TYPE_PARAM,     NK_GENERIC_OBJECT_PARAM,
  NK_GENERIC_SUBPROGRAM_PARAM,

  NK_COUNT
} Node_Kind;

// -- Parameter Mode Kind  (extracted from NK_PARAM_SPEC payload)
typedef enum { MODE_IN, MODE_OUT, MODE_IN_OUT } Parameter_Mode_Kind;

// -- Generic Formal Type Definition Kind  (extracted from NK_GENERIC_TYPE_PARAM payload)
typedef enum {
  GEN_DEF_PRIVATE = 0, GEN_DEF_LIMITED_PRIVATE,
  GEN_DEF_DISCRETE,    GEN_DEF_INTEGER,
  GEN_DEF_FLOAT,       GEN_DEF_FIXED,
  GEN_DEF_ARRAY,       GEN_DEF_ACCESS,
  GEN_DEF_DERIVED
} Generic_Def_Kind;

// -- Generic Object Mode Kind  (extracted from NK_GENERIC_OBJECT_PARAM payload)
typedef enum { GEN_MODE_IN = 0, GEN_MODE_OUT, GEN_MODE_IN_OUT } Generic_Mode_Kind;

// -- Syntax Node Record
// The payload union is discriminated by `kind'.  Each variant is written as in an Ada
// variant record: one field per line, with a trailing comment describing its role.
struct Syntax_Node {
  Node_Kind       kind;     // Discriminant tag for the payload union
  Source_Location location; // Where this construct appeared in source
  Type_Info      *type;     // Set by semantic analysis, NULL before
  Symbol         *symbol;   // Set by name resolution, NULL before

  union {

    //  when NK_INTEGER =>
    struct {
      int64_t      value;     // Machine-width literal value
      Big_Integer *big_value; // Arbitrary-precision overflow
    } integer_lit;

    //  when NK_REAL =>
    struct {
      double    value;     // Machine-width float value
      Big_Real *big_value; // Arbitrary-precision overflow
    } real_lit;

    //  when NK_STRING | NK_CHARACTER | NK_IDENTIFIER =>
    struct {
      String_Slice text; // Raw source text
    } string_val;

    //  when NK_SELECTED =>
    struct {
      Syntax_Node *prefix;   // The dotted prefix expression
      String_Slice selector; // The selected component name
    } selected;

    //  when NK_ATTRIBUTE =>
    struct {
      Syntax_Node *prefix;    // The prefix before the tick
      String_Slice name;      // Attribute designator
      Node_List    arguments; // Optional attribute arguments
    } attribute;

    //  when NK_QUALIFIED =>
    struct {
      Syntax_Node *subtype_mark; // Qualifying subtype
      Syntax_Node *expression;   // Qualified expression
    } qualified;

    //  when NK_BINARY_OP =>
    struct {
      Token_Kind   op;    // Operator token kind
      Syntax_Node *left;  // Left operand
      Syntax_Node *right; // Right operand
    } binary;

    //  when NK_UNARY_OP =>
    struct {
      Token_Kind   op;      // Operator token kind
      Syntax_Node *operand; // Sole operand
    } unary;

    //  when NK_AGGREGATE =>
    struct {
      Node_List items;            // Component associations
      bool      is_named;         // True if using named notation
      bool      is_parenthesized; // True if parenthesized
    } aggregate;

    //  when NK_ALLOCATOR =>
    struct {
      Syntax_Node *subtype_mark; // Allocated subtype
      Syntax_Node *expression;   // Initializer or NULL
    } allocator;

    //  when NK_APPLY =>
    struct {
      Syntax_Node *prefix;    // Called / indexed / sliced / converted name
      Node_List    arguments; // Actual parameter list
    } apply;

    //  when NK_RANGE =>
    struct {
      Syntax_Node *low;  // Low bound expression
      Syntax_Node *high; // High bound expression
    } range;

    //  when NK_ASSOCIATION =>
    struct {
      Node_List    choices;    // Discrete choices or names
      Syntax_Node *expression; // Associated value
    } association;

    //  when NK_SUBTYPE_INDICATION =>
    struct {
      Syntax_Node *subtype_mark; // Named subtype
      Syntax_Node *constraint;   // Optional constraint or NULL
    } subtype_ind;

    //  when NK_INDEX_CONSTRAINT =>
    struct {
      Node_List ranges; // Discrete range per dimension
    } index_constraint;

    //  when NK_RANGE_CONSTRAINT =>
    struct {
      Syntax_Node *range; // The constraining range
    } range_constraint;

    //  when NK_DISCRIMINANT_CONSTRAINT =>
    struct {
      Node_List associations; // Discriminant value associations
    } discriminant_constraint;

    //  when NK_DIGITS_CONSTRAINT =>
    struct {
      Syntax_Node *digits_expr; // Digits expression
      Syntax_Node *range;       // Optional range or NULL
    } digits_constraint;

    //  when NK_DELTA_CONSTRAINT =>
    struct {
      Syntax_Node *delta_expr; // Delta expression
      Syntax_Node *range;      // Optional range or NULL
    } delta_constraint;

    //  when NK_ARRAY_TYPE =>
    struct {
      Node_List    indices;        // Index subtype definitions
      Syntax_Node *component_type; // Element subtype indication
      bool         is_constrained; // True for constrained arrays
    } array_type;

    //  when NK_RECORD_TYPE =>
    struct {
      Node_List    discriminants; // Discriminant specifications
      Node_List    components;    // Component declarations
      Syntax_Node *variant_part;  // Variant part or NULL
      bool         is_null;       // True for null record
    } record_type;

    //  when NK_ACCESS_TYPE =>
    struct {
      Syntax_Node *designated;  // Designated subtype
      bool         is_constant; // Access-to-constant
    } access_type;

    //  when NK_DERIVED_TYPE =>
    struct {
      Syntax_Node *parent_type; // Parent subtype indication
      Syntax_Node *constraint;  // Optional constraint or NULL
    } derived_type;

    //  when NK_ENUMERATION_TYPE =>
    struct {
      Node_List literals; // Enumeration literal names
    } enum_type;

    //  when NK_INTEGER_TYPE =>
    struct {
      Syntax_Node *range;      // Range constraint
      uint128_t    modulus;    // Modular type modulus
      bool         is_modular; // True for mod types
    } integer_type;

    //  when NK_REAL_TYPE =>
    struct {
      Syntax_Node *precision; // Digits or delta expression
      Syntax_Node *range;     // Optional range or NULL
      Syntax_Node *delta;     // Delta for fixed-point
    } real_type;

    //  when NK_COMPONENT_DECL =>
    struct {
      Node_List    names;          // Defining identifiers
      Syntax_Node *component_type; // Component subtype indication
      Syntax_Node *init;           // Default expression or NULL
    } component;

    //  when NK_VARIANT_PART =>
    struct {
      String_Slice discriminant; // Discriminant name
      Node_List    variants;     // Variant alternatives
    } variant_part;

    //  when NK_VARIANT =>
    struct {
      Node_List    choices;      // Discrete choice list
      Node_List    components;   // Component declarations
      Syntax_Node *variant_part; // Nested variant part or NULL
    } variant;

    //  when NK_DISCRIMINANT_SPEC =>
    struct {
      Node_List    names;        // Discriminant names
      Syntax_Node *disc_type;    // Discriminant subtype
      Syntax_Node *default_expr; // Default value or NULL
    } discriminant;

    //  when NK_ASSIGNMENT =>
    struct {
      Syntax_Node *target; // Left-hand side name
      Syntax_Node *value;  // Right-hand side expression
    } assignment;

    //  when NK_RETURN =>
    struct {
      Syntax_Node *expression; // Return value or NULL
    } return_stmt;

    //  when NK_IF =>
    struct {
      Syntax_Node *condition;   // Boolean condition
      Node_List    then_stmts;  // Then-part statements
      Node_List    elsif_parts; // Elsif clauses
      Node_List    else_stmts;  // Else-part statements
    } if_stmt;

    //  when NK_CASE =>
    struct {
      Syntax_Node *expression;   // Selecting expression
      Node_List    alternatives; // Case alternatives
    } case_stmt;

    //  when NK_LOOP =>
    struct {
      String_Slice  label;            // Optional loop label
      Symbol       *label_symbol;     // Resolved label symbol
      Syntax_Node  *iteration_scheme; // For/while or NULL
      Node_List     statements;       // Loop body
      bool          is_reverse;       // True for reverse iteration
    } loop_stmt;

    //  when NK_BLOCK =>
    struct {
      String_Slice label;        // Optional block label
      Symbol      *label_symbol; // Resolved label symbol
      Node_List    declarations; // Declarative part
      Node_List    statements;   // Statement sequence
      Node_List    handlers;     // Exception handlers
    } block_stmt;

    //  when NK_EXIT =>
    struct {
      String_Slice  loop_name; // Target loop name or empty
      Syntax_Node  *condition; // When condition or NULL
      Symbol       *target;    // Resolved loop symbol
    } exit_stmt;

    //  when NK_GOTO =>
    struct {
      String_Slice name;   // Target label name
      Symbol      *target; // Resolved label symbol
    } goto_stmt;

    //  when NK_LABEL =>
    struct {
      String_Slice  name;      // Label identifier
      Syntax_Node  *statement; // Labelled statement
      Symbol       *symbol;    // Label symbol
    } label_node;

    //  when NK_RAISE =>
    struct {
      Syntax_Node *exception_name; // Exception name or NULL for reraise
    } raise_stmt;

    //  when NK_ACCEPT =>
    struct {
      String_Slice  entry_name; // Accepted entry name
      Syntax_Node  *index;      // Entry family index or NULL
      Node_List     parameters; // Formal parameters
      Node_List     statements; // Accept body
      Symbol       *entry_sym;  // Resolved entry symbol
    } accept_stmt;

    //  when NK_SELECT =>
    struct {
      Node_List    alternatives; // Select alternatives
      Syntax_Node *else_part;    // Else part or NULL
    } select_stmt;

    //  when NK_DELAY =>
    struct {
      Syntax_Node *expression; // Duration expression
    } delay_stmt;

    //  when NK_ABORT =>
    struct {
      Node_List task_names; // Task objects to abort
    } abort_stmt;

    //  when NK_OBJECT_DECL =>
    struct {
      Node_List    names;       // Defining identifiers
      Syntax_Node *object_type; // Object subtype indication
      Syntax_Node *init;        // Initial value or NULL
      bool         is_constant; // True for constant declarations
      bool         is_aliased;  // True for aliased objects
      bool         is_rename;   // True for renaming declarations
    } object_decl;

    //  when NK_TYPE_DECL =>
    struct {
      String_Slice  name;          // Type name
      Node_List     discriminants; // Known discriminant part
      Syntax_Node  *definition;    // Type definition
      bool          is_limited;    // Limited type
      bool          is_private;    // Private type
    } type_decl;

    //  when NK_EXCEPTION_DECL =>
    struct {
      Node_List    names;   // Exception identifiers
      Syntax_Node *renamed; // Renamed exception or NULL
    } exception_decl;

    //  when NK_PROCEDURE_SPEC | NK_FUNCTION_SPEC =>
    struct {
      String_Slice  name;        // Subprogram name
      Node_List     parameters;  // Formal parameter list
      Syntax_Node  *return_type; // Return type or NULL
      Syntax_Node  *renamed;     // Renamed entity or NULL
    } subprogram_spec;

    //  when NK_PROCEDURE_BODY | NK_FUNCTION_BODY =>
    struct {
      Syntax_Node *specification;  // The subprogram spec
      Node_List    declarations;   // Declarative part
      Node_List    statements;     // Body statements
      Node_List    handlers;       // Exception handlers
      bool         is_separate;    // Is a subunit stub
      bool         code_generated; // Already emitted
    } subprogram_body;

    //  when NK_PACKAGE_SPEC =>
    struct {
      String_Slice name;          // Package name
      Node_List    visible_decls; // Visible part declarations
      Node_List    private_decls; // Private part declarations
    } package_spec;

    //  when NK_PACKAGE_BODY =>
    struct {
      String_Slice name;         // Package name
      Node_List    declarations; // Body declarations
      Node_List    statements;   // Body statements
      Node_List    handlers;     // Exception handlers
      bool         is_separate;  // Is a subunit stub
    } package_body;

    //  when NK_PACKAGE_RENAMING =>
    struct {
      String_Slice  new_name; // New package name
      Syntax_Node  *old_name; // Renamed package name
    } package_renaming;

    //  when NK_TASK_SPEC =>
    struct {
      String_Slice name;    // Task name
      Node_List    entries; // Entry declarations
      bool         is_type; // True for task type
    } task_spec;

    //  when NK_TASK_BODY =>
    struct {
      String_Slice name;         // Task name
      Node_List    declarations; // Body declarations
      Node_List    statements;   // Body statements
      Node_List    handlers;     // Exception handlers
      bool         is_separate;  // Is a subunit stub
    } task_body;

    //  when NK_ENTRY_DECL =>
    struct {
      String_Slice name;              // Entry name
      Node_List    parameters;        // Formal parameters
      Node_List    index_constraints; // Family index constraints
    } entry_decl;

    //  when NK_PARAM_SPEC =>
    struct {
      Node_List    names;        // Parameter identifiers
      Syntax_Node *param_type;   // Parameter subtype indication
      Syntax_Node *default_expr; // Default value or NULL
      Parameter_Mode_Kind mode;
    } param_spec;

    //  when NK_GENERIC_DECL =>
    struct {
      Node_List    formals; // Generic formal parameters
      Syntax_Node *unit;    // The generic unit declaration
    } generic_decl;

    //  when NK_GENERIC_INST =>
    struct {
      Syntax_Node *generic_name;  // Name of generic template
      Node_List    actuals;       // Actual parameter list
      String_Slice instance_name; // Instance defining name
      Token_Kind   unit_kind;     // TK_PACKAGE / TK_PROCEDURE / TK_FUNCTION
    } generic_inst;

    //  when NK_GENERIC_TYPE_PARAM =>
    struct {
      String_Slice     name;
      Generic_Def_Kind def_kind;       // Form of the generic formal type
      Syntax_Node     *def_detail;     // Type definition detail
      Node_List        discriminants;  // Discriminant part
    } generic_type_param;

    //  when NK_GENERIC_OBJECT_PARAM =>
    struct {
      Node_List        names;        // Object parameter names
      Syntax_Node     *object_type;  // Object subtype indication
      Syntax_Node     *default_expr; // Default value or NULL
      Generic_Mode_Kind mode;
    } generic_object_param;

    //  when NK_GENERIC_SUBPROGRAM_PARAM =>
    struct {
      String_Slice  name;         // Formal subprogram name
      Node_List     parameters;   // Formal parameters
      Syntax_Node  *return_type;  // Return type or NULL
      Syntax_Node  *default_name; // Default subprogram or NULL
      bool          is_function;  // True for generic function
      bool          default_box;  // True for <>
    } generic_subprog_param;

    //  when NK_USE_CLAUSE =>
    struct {
      Node_List names; // Used package names
    } use_clause;

    //  when NK_PRAGMA =>
    struct {
      String_Slice name;      // Pragma identifier
      Node_List    arguments; // Pragma arguments
    } pragma_node;

    //  when NK_EXCEPTION_HANDLER =>
    struct {
      Node_List exceptions; // Handled exception choices
      Node_List statements; // Handler statements
    } handler;

    //  when NK_REPRESENTATION_CLAUSE =>
    struct {
      Syntax_Node *entity_name;       // Entity being represented
      String_Slice attribute;         // Representation attribute
      Syntax_Node *expression;        // Representation expression
      Node_List    component_clauses; // Record rep component clauses
      bool         is_record_rep;     // True for record rep clause
      bool         is_enum_rep;       // True for enum rep clause
    } rep_clause;

    //  when NK_CONTEXT_CLAUSE =>
    struct {
      Node_List with_clauses; // With clauses
      Node_List use_clauses;  // Use clauses
    } context;

    //  when NK_COMPILATION_UNIT =>
    struct {
      Syntax_Node *context;         // Context clause
      Syntax_Node *unit;            // The library unit
      Syntax_Node *separate_parent; // Separate parent or NULL
    } compilation_unit;

  };
};

Syntax_Node *Node_New (Node_Kind kind, Source_Location location);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.  PARSING — Recursive descent for the full Ada 83 grammar
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Recursive descent mirrors the grammar, making the grammar itself the invariant.
//
// Key design decisions:
//   1. UNIFIED APPLY NODE — All X(...) forms parse as NK_APPLY.  Semantic analysis
//      later distinguishes calls, indexing, slicing, and type conversions.
//   2. UNIFIED ASSOCIATION PARSING — One helper handles positional, named, and choice
//      associations used in aggregates, calls, and generic actuals.
//   3. UNIFIED POSTFIX CHAIN — One loop handles .selector, 'attribute, and (args).
//


typedef struct {
  Lexer      lexer;          // The underlying lexer driving token production
  Token      current_token;  // One-token look-ahead for LL(1) decisions
  Token      previous_token; // The most recently consumed token
  bool       had_error;      // True once any syntax error has been reported
  bool       panic_mode;     // True while skipping tokens to resynchronise
  uint32_t   last_line;      // Line of the previous token (for progress check)
  uint32_t   last_column;    // Column of the previous token (for progress check)
  Token_Kind last_kind;      // Kind of the previous token (for progress check)
} Parser;

typedef enum {
  PREC_NONE = 0,  PREC_LOGICAL,        PREC_RELATIONAL,
  PREC_ADDITIVE,  PREC_MULTIPLICATIVE, PREC_EXPONENTIAL,
  PREC_UNARY,     PREC_PRIMARY
} Precedence;

// Construct a parser over the given source buffer, priming the one-token look-ahead.
Parser          Parser_New              (const char *source, size_t length, const char *filename);
// Return true when the current token matches `kind' without consuming it.
bool            Parser_At               (Parser *p, Token_Kind kind);
// Return true when the current token matches either k1 or k2.
bool            Parser_At_Any           (Parser *p, Token_Kind k1, Token_Kind k2);
// Peek one token ahead and return true if it matches `kind'.
bool            Parser_Peek_At          (Parser *p, Token_Kind kind);
// Consume the current token and return it; refill the look-ahead.
Token           Parser_Advance          (Parser *p);
// If the current token matches `kind', consume it and return true.
bool            Parser_Match            (Parser *p, Token_Kind kind);
// Consume the current token if it matches `kind'; otherwise report an error.
bool            Parser_Expect           (Parser *p, Token_Kind kind);
// Return the source location of the current token.
Source_Location Parser_Location         (Parser *p);
// Expect and consume an identifier, returning its text.
String_Slice    Parser_Identifier       (Parser *p);
// Report a syntax error at the previous token's location.
void            Parser_Error            (Parser *p, const char *message);
// Report a syntax error at the current token's location.
void            Parser_Error_At_Current (Parser *p, const char *expected);
// Skip tokens until a statement or declaration boundary is found.
void            Parser_Synchronize      (Parser *p);
// Guard against infinite loops: return false if the parser has not advanced.
bool            Parser_Check_Progress   (Parser *p);
void            Parser_Check_End_Name   (Parser *p, String_Slice expected);
Precedence      Get_Infix_Precedence    (Token_Kind kind);
bool            Is_Right_Associative    (Token_Kind kind);

Syntax_Node *Parse_Expression              (Parser *p);
Syntax_Node *Parse_Expression_Precedence   (Parser *p, Precedence min_prec);
Syntax_Node *Parse_Unary                   (Parser *p);
Syntax_Node *Parse_Primary                 (Parser *p);
Syntax_Node *Parse_Name                    (Parser *p);
Syntax_Node *Parse_Simple_Name             (Parser *p);
Syntax_Node *Parse_Choice                  (Parser *p);
Syntax_Node *Parse_Range                   (Parser *p);
void         Parse_Association_List        (Parser *p, Node_List *list);
void         Parse_Parameter_List          (Parser *p, Node_List *params);

Syntax_Node *Parse_Subtype_Indication      (Parser *p);
Syntax_Node *Parse_Type_Definition         (Parser *p);
Syntax_Node *Parse_Enumeration_Type        (Parser *p);
Syntax_Node *Parse_Array_Type              (Parser *p);
Syntax_Node *Parse_Record_Type             (Parser *p);
Syntax_Node *Parse_Access_Type             (Parser *p);
Syntax_Node *Parse_Derived_Type            (Parser *p);
Syntax_Node *Parse_Discrete_Range          (Parser *p);
Syntax_Node *Parse_Variant_Part            (Parser *p);
Syntax_Node *Parse_Discriminant_Part       (Parser *p);

Syntax_Node *Parse_Statement               (Parser *p);
void         Parse_Statement_Sequence      (Parser *p, Node_List *list);
Syntax_Node *Parse_Assignment_Or_Call      (Parser *p);
Syntax_Node *Parse_Return_Statement        (Parser *p);
Syntax_Node *Parse_If_Statement            (Parser *p);
Syntax_Node *Parse_Case_Statement          (Parser *p);
Syntax_Node *Parse_Loop_Statement          (Parser *p, String_Slice label);
Syntax_Node *Parse_Block_Statement         (Parser *p, String_Slice label);
Syntax_Node *Parse_Exit_Statement          (Parser *p);
Syntax_Node *Parse_Goto_Statement          (Parser *p);
Syntax_Node *Parse_Raise_Statement         (Parser *p);
Syntax_Node *Parse_Delay_Statement         (Parser *p);
Syntax_Node *Parse_Abort_Statement         (Parser *p);
Syntax_Node *Parse_Accept_Statement        (Parser *p);
Syntax_Node *Parse_Select_Statement        (Parser *p);
Syntax_Node *Parse_Pragma                  (Parser *p);

Syntax_Node *Parse_Declaration             (Parser *p);
void         Parse_Declarative_Part        (Parser *p, Node_List *list);
Syntax_Node *Parse_Object_Declaration      (Parser *p);
Syntax_Node *Parse_Type_Declaration        (Parser *p);
Syntax_Node *Parse_Subtype_Declaration     (Parser *p);
Syntax_Node *Parse_Procedure_Specification (Parser *p);
Syntax_Node *Parse_Function_Specification  (Parser *p);
Syntax_Node *Parse_Subprogram_Body         (Parser *p, Syntax_Node *spec);
Syntax_Node *Parse_Package_Specification   (Parser *p);
Syntax_Node *Parse_Package_Body            (Parser *p);
void         Parse_Generic_Formal_Part     (Parser *p, Node_List *formals);
Syntax_Node *Parse_Generic_Declaration     (Parser *p);
Syntax_Node *Parse_Use_Clause              (Parser *p);
Syntax_Node *Parse_With_Clause             (Parser *p);
Syntax_Node *Parse_Representation_Clause   (Parser *p);
Syntax_Node *Parse_Context_Clause          (Parser *p);
Syntax_Node *Parse_Compilation_Unit        (Parser *p);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §10.  TYPES — The Ada type lattice, Type_Info, classification
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Ada's types form a lattice rooted at the universal types.  Every type in the program
// is represented by a Type_Info descriptor carrying kind, scalar bounds, composite
// structure, representation size, and derivation chains.  A type combines name, range,
// and representation as three orthogonal concerns.
//
// INVARIANT: All sizes in Type_Info are stored in BYTES, not bits.
//


typedef enum {
  TYPE_UNKNOWN = 0,
  TYPE_BOOLEAN,          TYPE_CHARACTER,       TYPE_INTEGER,
  TYPE_MODULAR,          TYPE_ENUMERATION,     TYPE_FLOAT,
  TYPE_FIXED,            TYPE_ARRAY,           TYPE_RECORD,
  TYPE_STRING,           TYPE_ACCESS,          TYPE_UNIVERSAL_INTEGER,
  TYPE_UNIVERSAL_REAL,   TYPE_TASK,            TYPE_SUBPROGRAM,
  TYPE_PRIVATE,          TYPE_LIMITED_PRIVATE, TYPE_INCOMPLETE,
  TYPE_PACKAGE,
  TYPE_COUNT
} Type_Kind;

typedef enum {
  BOUND_NONE, BOUND_INTEGER, BOUND_FLOAT, BOUND_EXPR
} Bound_Kind;

// A scalar bound that may be a compile-time integer, a compile-time float, or
// a deferred expression to be evaluated at elaboration time.
typedef struct {
  Bound_Kind kind;      // Which variant of the union is active
  union {
    int128_t     int_value;   // Compile-time integer bound
    double       float_value; // Compile-time floating-point bound
    Syntax_Node *expr;        // Deferred expression (elaboration-time)
  };
  uint32_t cached_temp; // LLVM temp register holding the evaluated bound, or 0
} Type_Bound;

// Describes one alternative of a record variant part (RM 3.7.3).
typedef struct {
  int64_t  disc_value_low;  // Low bound of the discriminant range for this variant
  int64_t  disc_value_high; // High bound of the discriminant range for this variant
  bool     is_others;       // True when this variant covers the `others' choice
  uint32_t first_component; // Index of the first component belonging to this variant
  uint32_t component_count; // Number of components in this variant
  uint32_t variant_size;    // Byte size of this variant's component area
} Variant_Info;

// Describes one field of a record type, whether it is a discriminant, a
// fixed component, or a component inside a variant alternative.
typedef struct {
  String_Slice  name;             // Field name as it appears in source
  Type_Info    *component_type;   // Type of this field
  uint32_t      byte_offset;      // Offset from record base in bytes
  uint32_t      bit_offset;       // Additional bit offset (for packed records)
  uint32_t      bit_size;         // Bit size when packed; 0 means use component_type->size
  Syntax_Node  *default_expr;     // Default initial value expression, or NULL
  bool          is_discriminant;  // True if this field is a discriminant
  int32_t       variant_index;    // Index into Type_Info.record.variants, or -1
} Component_Info;

// Describes one dimension of an array type.
typedef struct {
  Type_Info *index_type; // The discrete type governing this dimension
  Type_Bound low_bound;  // Lower bound of the index range
  Type_Bound high_bound; // Upper bound of the index range
} Index_Info;

struct Type_Info {
  Type_Kind    kind;               // Discriminant of the type union below
  String_Slice name;               // User-visible type name
  Symbol      *defining_symbol;    // Back-link to the symbol that declared this type
  uint32_t     size;               // Object size in bytes (never bits)
  uint32_t     alignment;          // Required alignment in bytes
  uint32_t     specified_bit_size; // Explicit 'Size clause value, or 0 when unset
  Type_Bound   low_bound;          // Scalar lower bound (unused for composites)
  Type_Bound   high_bound;         // Scalar upper bound (unused for composites)
  uint128_t    modulus;            // Modulus for TYPE_MODULAR; 0 otherwise
  Type_Info   *base_type;          // Nearest anonymous base type, or self
  Type_Info   *parent_type;        // Parent in a derived-type chain, or NULL

  union {

    //  when TYPE_ARRAY | TYPE_STRING =>
    struct {
      Index_Info *indices;        // Per-dimension index type and bounds
      uint32_t    index_count;    // Number of dimensions
      Type_Info  *element_type;   // Component type of the array
      bool        is_constrained; // True for constrained (bounds known statically)
    } array;

    //  when TYPE_RECORD =>
    struct {
      Component_Info *components;               // Flat list of all record fields
      uint32_t        component_count;          // Length of the components array
      uint32_t        discriminant_count;       // How many are discriminants
      bool            has_discriminants;         // True when a discriminant part exists
      bool            all_defaults;              // True if every discriminant has a default
      bool            is_constrained;            // True for a constrained record subtype
      Variant_Info   *variants;                  // Variant alternatives, or NULL
      uint32_t        variant_count;             // Number of variant alternatives
      uint32_t        variant_offset;            // Byte offset where the variant area begins
      uint32_t        max_variant_size;          // Largest variant alternative in bytes
      Syntax_Node    *variant_part_node;         // AST node of the variant part, or NULL
      int64_t        *disc_constraint_values;    // Static discriminant constraint values
      Syntax_Node   **disc_constraint_exprs;     // Dynamic discriminant constraint exprs
      uint32_t       *disc_constraint_preeval;   // Pre-evaluated temps for constraints
      bool            has_disc_constraints;       // True when a discriminant constraint exists
    } record;

    //  when TYPE_ACCESS =>
    struct {
      Type_Info *designated_type;    // The type that the pointer designates
      bool       is_access_constant; // True for access-to-constant
    } access;

    //  when TYPE_ENUMERATION | TYPE_BOOLEAN | TYPE_CHARACTER =>
    struct {
      String_Slice *literals;      // Names of the enumeration literals in order
      uint32_t      literal_count; // Number of literals
      int64_t      *rep_values;    // Representation values from an enum rep clause
    } enumeration;

    //  when TYPE_FIXED =>
    struct { double delta; double small; int scale; } fixed;
    //  when TYPE_FLOAT =>
    struct { int digits; }                            flt;
  };

  uint32_t    suppressed_checks;  // Bitmask of CHK_* flags suppressed by pragma
  bool        is_packed;          // True when pragma Pack has been applied
  bool        is_limited;         // True for limited types (no assignment or equality)
  bool        is_frozen;          // True once the representation has been finalised
  int64_t     storage_size;       // 'Storage_Size attribute value, or -1 if unset
  const char *equality_func_name; // Mangled name of the compiler-generated "=" function
  uint32_t    rt_global_id;       // Runtime type identifier for exception dispatch
};

extern Type_Info *Frozen_Composite_Types[256];
extern uint32_t   Frozen_Composite_Count;
extern Symbol    *Exception_Symbols[256];
extern uint32_t   Exception_Symbol_Count;

Type_Info *Type_New    (Type_Kind kind, String_Slice name);
void       Freeze_Type (Type_Info *type_info);

bool Type_Is_Scalar               (const Type_Info *t);
bool Type_Is_Discrete             (const Type_Info *t);
bool Type_Is_Numeric              (const Type_Info *t);
bool Type_Is_Real                 (const Type_Info *t);
bool Type_Is_Float_Representation (const Type_Info *t);
bool Type_Is_Array_Like           (const Type_Info *t);
bool Type_Is_Composite            (const Type_Info *t);
bool Type_Is_Access               (const Type_Info *t);
bool Type_Is_Record               (const Type_Info *t);
bool Type_Is_Task                 (const Type_Info *t);
bool Type_Is_Float                (const Type_Info *t);
bool Type_Is_Fixed_Point          (const Type_Info *t);
bool Type_Is_Private              (const Type_Info *t);
bool Type_Is_Limited              (const Type_Info *t);
bool Type_Is_Integer_Like         (const Type_Info *t);
bool Type_Is_Unsigned             (const Type_Info *t);
bool Type_Is_Enumeration          (const Type_Info *t);
bool Type_Is_Boolean              (const Type_Info *t);
bool Type_Is_Character            (const Type_Info *t);
bool Type_Is_String               (const Type_Info *t);
bool Type_Is_Universal_Integer    (const Type_Info *t);
bool Type_Is_Universal_Real       (const Type_Info *t);
bool Type_Is_Universal            (const Type_Info *t);
bool Type_Is_Unconstrained_Array  (const Type_Info *t);
bool Type_Is_Constrained_Array    (const Type_Info *t);
bool Type_Is_Bool_Array           (const Type_Info *t);
bool Type_Has_Dynamic_Bounds      (const Type_Info *t);
bool Type_Needs_Fat_Pointer       (const Type_Info *t);
bool Type_Needs_Fat_Pointer_Load  (const Type_Info *t);

Type_Info *Type_Base (Type_Info *t);
Type_Info *Type_Root (Type_Info *t);

int128_t Type_Bound_Value                 (Type_Bound  bound);
bool     Type_Bound_Is_Compile_Time_Known (Type_Bound  b);
bool     Type_Bound_Is_Set                (Type_Bound  b);
double   Type_Bound_Float_Value           (Type_Bound  b);
int128_t Array_Element_Count              (Type_Info  *t);
int128_t Array_Low_Bound                  (Type_Info  *t);

const char *Type_To_Llvm           (Type_Info       *t);
const char *Type_To_Llvm_Sig       (Type_Info       *t);
const char *Array_Bound_Llvm_Type  (const Type_Info *t);
const char *Bounds_Type_For        (const char      *bound_type);
const char *Float_Llvm_Type_Of     (const Type_Info *t);
int         Bounds_Alloc_Size      (const char      *bound_type);
bool        Float_Is_Single        (const Type_Info *t);
int         Float_Effective_Digits (const Type_Info *t);

void Float_Model_Parameters (const Type_Info *t,
                              int64_t         *out_mantissa,
                              int64_t         *out_emax);

bool        Expression_Is_Slice             (const Syntax_Node *node);
bool        Expression_Is_Boolean           (Syntax_Node       *node);
bool        Expression_Is_Float             (Syntax_Node       *node);
const char *Expression_Llvm_Type            (Syntax_Node       *node);
bool        Expression_Produces_Fat_Pointer (const Syntax_Node *node,
                                             const Type_Info   *type);

bool Types_Same_Named           (Type_Info *t1,   Type_Info *t2);
bool Subprogram_Is_Primitive_Of (Symbol    *sub,  Type_Info *type);
void Create_Derived_Operation   (Symbol    *sub,
                                 Type_Info *derived_type,
                                 Type_Info *parent_type,
                                 Symbol    *type_sym);
void Derive_Subprograms         (Type_Info *derived_type,
                                 Type_Info *parent_type,
                                 Symbol    *type_sym);

// Maps generic formal type names to their actual types during instantiation.
typedef struct {
  uint32_t count;             // Number of active mappings
  struct {
    String_Slice formal_name; // Name of the generic formal type parameter
    Type_Info   *actual_type; // Actual type supplied at instantiation
  } mappings[32];             // Fixed-size array of formal-to-actual pairs
} Generic_Type_Map;

extern Generic_Type_Map g_generic_type_map;

void       Set_Generic_Type_Map        (Symbol    *inst);
Type_Info *Resolve_Generic_Actual_Type (Type_Info *type);
bool       Check_Is_Suppressed         (Type_Info *type, Symbol *sym, uint32_t check_bit);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §11.  NAMES — Symbol table, scopes, overload resolution
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Every named entity — variable, constant, type, subprogram, package, exception, loop,
// label — is represented by a Symbol.  Symbols live in Scopes chained outward from
// inner to enclosing.  The symbol table implements Ada's visibility and overloading
// rules: hierarchical scopes, use-clause visibility, and multi-meaning overloads.
// We use a hash table with chaining and a scope stack for nested contexts.
//
// Overload resolution is a two-pass process (RM 8.6):
//   1. Bottom-up: collect all possible interpretations of each identifier.
//   2. Top-down: given context type expectations, select the unique valid one.
//

// label -- is represented by a Symbol.  Symbols live in Scopes chained outward from

typedef enum {
  SYMBOL_UNKNOWN = 0,
  SYMBOL_VARIABLE,    SYMBOL_CONSTANT,     SYMBOL_TYPE,
  SYMBOL_SUBTYPE,     SYMBOL_PROCEDURE,    SYMBOL_FUNCTION,
  SYMBOL_PARAMETER,   SYMBOL_PACKAGE,      SYMBOL_EXCEPTION,
  SYMBOL_LABEL,       SYMBOL_LOOP,         SYMBOL_ENTRY,
  SYMBOL_COMPONENT,   SYMBOL_DISCRIMINANT, SYMBOL_LITERAL,
  SYMBOL_GENERIC,     SYMBOL_GENERIC_INSTANCE,
  SYMBOL_COUNT
} Symbol_Kind;

typedef enum { PARAM_IN = 0, PARAM_OUT, PARAM_IN_OUT } Parameter_Mode;

// Describes one formal parameter of a subprogram (RM 6.1).
typedef struct {
  String_Slice    name;          // Parameter name as it appears in the spec
  Type_Info      *param_type;    // Subtype of the formal
  Parameter_Mode  mode;          // IN, OUT, or IN OUT
  Syntax_Node    *default_value; // Default expression, or NULL
  struct Symbol  *param_sym;     // Back-link to the Symbol for this parameter
} Parameter_Info;

// Return true when a parameter of this mode is passed by reference.
bool Param_Is_By_Reference (Parameter_Mode mode);

// -- Symbol Visibility Level  (extracted from Symbol record)
typedef enum {
  VIS_HIDDEN              = 0,
  VIS_IMMEDIATELY_VISIBLE = 1,
  VIS_USE_VISIBLE         = 2,
  VIS_DIRECTLY_VISIBLE    = 3
} Visibility_Level;

// -- Calling Convention  (extracted from Symbol record)
typedef enum {
  CONVENTION_ADA = 0,    CONVENTION_C,        CONVENTION_STDCALL,
  CONVENTION_INTRINSIC,  CONVENTION_ASSEMBLER
} Convention_Kind;

struct Symbol {
  // -- Identity
  Symbol_Kind      kind;             // What kind of entity this symbol represents
  String_Slice     name;             // The Ada identifier for this entity
  Source_Location  location;         // Where this entity was declared
  Type_Info       *type;             // Type of the entity (variable, constant, param, etc.)
  Scope           *defining_scope;   // The scope in which this symbol was introduced
  Symbol          *parent;           // Enclosing package or subprogram symbol, or NULL
  Symbol          *next_overload;    // Next homonym in the overload chain
  Symbol          *next_in_bucket;   // Next symbol in the hash-bucket collision chain
  Visibility_Level visibility;       // Current visibility (hidden, use, direct)

  // -- Declaration link
  Syntax_Node    *declaration;       // AST node that declared this entity

  // -- Subprogram profile
  Parameter_Info *parameters;        // Array of formal parameter descriptors
  uint32_t        parameter_count;   // Length of the parameters array
  Type_Info      *return_type;       // Return type for functions; NULL for procedures

  // -- Package exports
  Symbol        **exported;          // Symbols made visible by a package specification
  uint32_t        exported_count;    // Number of exported symbols

  // -- Addressing
  uint32_t        unique_id;         // Compiler-unique serial number for mangling
  uint32_t        nesting_level;     // Static nesting depth for uplevel access
  int64_t         frame_offset;      // Byte offset within the enclosing stack frame
  Scope          *scope;             // Body scope owned by this symbol (pkg/subp)

  // -- Import / export / convention
  bool            is_inline;         // Pragma Inline applied
  bool            is_imported;       // Pragma Import applied
  bool            is_exported;       // Pragma Export applied
  String_Slice    external_name;     // External linker name from pragma Import/Export
  String_Slice    link_name;         // Link-time name override
  Convention_Kind convention;        // Calling convention (Ada, C, Stdcall, etc.)
  uint32_t        suppressed_checks; // Bitmask of CHK_* suppressed by pragma Suppress
  bool            is_unreferenced;   // Pragma Unreferenced applied

  // -- Code generation bookkeeping
  bool            extern_emitted;         // LLVM declare already written
  bool            body_emitted;           // LLVM define already written
  bool            is_named_number;        // This is a number declaration (RM 3.2)
  bool            is_overloaded;          // More than one meaning in scope
  bool            body_claimed;           // Body already associated with a spec
  bool            is_predefined;          // Standard-library predefined entity
  bool            needs_address_marker;   // Needs a @__address_marker global
  bool            is_identity_function;   // Derived identity "=" operator
  uint32_t        disc_agg_temp;          // Temp register for discriminant aggregate
  bool            is_disc_constrained;    // Constrained by a discriminant constraint
  bool            needs_fat_ptr_storage;  // Requires fat-pointer alloca

  // -- Derived operations
  Symbol         *parent_operation;  // Original primitive inherited by derivation
  Type_Info      *derived_from_type; // Parent type from which this op was derived

  // -- Labels, loops, entries
  uint32_t        llvm_label_id;      // LLVM label for goto targets
  uint32_t        loop_exit_label_id; // LLVM label for exit-loop targets
  uint32_t        entry_index;        // Index in the task entry family
  Syntax_Node    *renamed_object;     // Renamed entity expression, or NULL

  // -- Generic instantiation state
  Syntax_Node    *generic_formals;         // AST of the generic formal part
  Syntax_Node    *generic_unit;            // AST of the generic unit declaration
  Syntax_Node    *generic_body;            // AST of the generic body (found later)
  Symbol         *generic_template;        // Back-link to the generic template symbol
  Symbol         *instantiated_subprogram; // Expanded subprogram from instantiation
  struct {
    String_Slice  formal_name;       // Name of the generic formal parameter
    Type_Info    *actual_type;       // Actual type supplied at instantiation
    Symbol       *actual_subprogram; // Actual subprogram supplied at instantiation
    Syntax_Node  *actual_expr;       // Actual expression (for object formals)
    Token_Kind    builtin_operator;  // Intrinsic operator kind, or TK_EOF
  } *generic_actuals;                // Array of formal-to-actual mappings
  uint32_t        generic_actual_count; // Length of the generic_actuals array
  Syntax_Node    *expanded_spec;     // Macro-expanded specification AST
  Syntax_Node    *expanded_body;     // Macro-expanded body AST
};

// A scope is a hash table of symbols chained outward from inner to enclosing.
struct Scope {
  Symbol   *buckets[SYMBOL_TABLE_SIZE]; // Hash-bucket heads for O(1) name lookup
  Scope    *parent;            // Enclosing scope (NULL for the global scope)
  Symbol   *owner;             // Package or subprogram that owns this scope
  uint32_t  nesting_level;     // Static nesting depth (0 = global)
  Symbol  **symbols;           // Flat array of all symbols in declaration order
  uint32_t  symbol_count;      // Number of symbols currently in the scope
  uint32_t  symbol_capacity;   // Allocated slots in the symbols array
  int64_t   frame_size;        // Total byte size of the local stack frame
  Symbol  **frame_vars;        // Subset of symbols that occupy frame slots
  uint32_t  frame_var_count;   // Number of frame variables
  uint32_t  frame_var_capacity;// Allocated slots in the frame_vars array
};

// The Symbol_Manager owns the scope stack and caches handles to the predefined
// types so that every part of the compiler can reach Standard.Boolean, etc.
typedef struct {
  Scope     *current_scope;         // Top of the scope stack
  Scope     *global_scope;          // The outermost (Standard) scope
  Type_Info *type_boolean;          // Standard.Boolean
  Type_Info *type_integer;          // Standard.Integer
  Type_Info *type_float;            // Standard.Float
  Type_Info *type_character;        // Standard.Character
  Type_Info *type_string;           // Standard.String
  Type_Info *type_duration;         // Duration (for delay statements)
  Type_Info *type_universal_integer;// Universal_Integer (numeric class)
  Type_Info *type_universal_real;   // Universal_Real (numeric class)
  Type_Info *type_address;          // System.Address
  uint32_t   next_unique_id;        // Monotonic counter for mangled names
} Symbol_Manager;

extern Symbol_Manager *sm;

Scope   *Scope_New                         (Scope  *parent);
void     Symbol_Manager_Push_Scope         (Symbol *owner);
void     Symbol_Manager_Pop_Scope          (void);
void     Symbol_Manager_Push_Existing_Scope(Scope  *scope);
uint32_t Symbol_Hash_Name                  (String_Slice name);

Symbol *Symbol_New          (Symbol_Kind kind, String_Slice name, Source_Location location);
void    Symbol_Add          (Symbol *sym);
Symbol *Symbol_Find         (String_Slice name);
Symbol *Symbol_Find_By_Type (String_Slice name, Type_Info *expected_type);

void Symbol_Manager_Init_Predefined (void);
void Symbol_Manager_Init            (void);

// Captures the types and names of actual arguments at a call site for
// overload resolution.
typedef struct {
  Type_Info   **types; // Array of resolved argument types
  uint32_t      count; // Number of actual arguments
  String_Slice *names; // Named-association formal names, or NULL for positional
} Argument_Info;

typedef struct {
  Symbol    *nam;          // Candidate symbol
  Type_Info *typ;          // Result type
  Type_Info *opnd_typ;     // Operand type (for operator resolution)
  bool       is_universal; // From a universal context
  uint32_t   scope_depth;  // Distance from use to declaration
} Interpretation;

typedef struct {
  Interpretation items[MAX_INTERPRETATIONS];
  uint32_t       count;
} Interp_List;

bool Type_Covers             (Type_Info    *expected, Type_Info    *actual);
bool Arguments_Match_Profile (Symbol       *sym,      Argument_Info *args);
void Collect_Interpretations (String_Slice  name,     Interp_List   *list);
void Filter_By_Arguments     (Interp_List  *interps,  Argument_Info *args);
bool Symbol_Hides            (Symbol       *sym1,     Symbol        *sym2);

int32_t Score_Interpretation    (Interpretation *interp,
                                 Type_Info      *context_type,
                                 Argument_Info  *args);
Symbol *Disambiguate            (Interp_List    *interps,
                                 Type_Info      *context_type,
                                 Argument_Info  *args);
Symbol *Resolve_Overloaded_Call (String_Slice    name,
                                 Argument_Info  *args,
                                 Type_Info      *context_type);

String_Slice Operator_Name (Token_Kind op);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §12.  SEMANTICS — Name resolution, type checking, constant folding
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// The semantic pass walks the syntax tree to resolve identifiers, check type
// compatibility, fold static expressions, and freeze type representations.
// A permissive parser gives the type checker material to work with.
//
// Semantic analysis performs:
//   - Name resolution: bind identifiers to symbols
//   - Type checking: verify type compatibility of operations
//   - Overload resolution: select correct subprogram
//   - Constraint checking: verify bounds, indices, etc.
//


Type_Info *Resolve_Expression   (Syntax_Node *node);
Type_Info *Resolve_Identifier   (Syntax_Node *node);
Type_Info *Resolve_Selected     (Syntax_Node *node);
Type_Info *Resolve_Binary_Op    (Syntax_Node *node);
Type_Info *Resolve_Apply        (Syntax_Node *node);
bool       Resolve_Char_As_Enum (Syntax_Node *char_node, Type_Info *enum_type);

void Resolve_Statement           (Syntax_Node *node);
void Resolve_Declaration         (Syntax_Node *node);
void Resolve_Declaration_List    (Node_List   *list);
void Resolve_Statement_List      (Node_List   *list);
void Resolve_Compilation_Unit    (Syntax_Node *node);
void Freeze_Declaration_List     (Node_List   *list);
void Populate_Package_Exports    (Symbol      *pkg_sym,  Syntax_Node *pkg_spec);
void Preregister_Labels          (Node_List   *list);
void Install_Declaration_Symbols (Node_List   *decls);

bool        Is_Integer_Expr     (Syntax_Node *node);
double      Eval_Const_Numeric  (Syntax_Node *node);
bool        Eval_Const_Rational (Syntax_Node *node, Rational *out);
const char *I128_Decimal        (int128_t     value);
const char *U128_Decimal        (uint128_t    value);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §13.  CODE GENERATION — LLVM IR emission for every Ada construct
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// The code generator walks the resolved syntax tree and emits LLVM IR as plain text to
// a FILE*.  The AST is semantic and the IR is operational, with translation bridging
// the gap.
//
// Key principles:
//   1. Operate at native type width; convert only at explicit Ada type conversions and
//      LLVM intrinsic boundaries (memcpy length, alloc size must be i64).
//   2. All pointer types use opaque 'ptr' (LLVM 15+).
//   3. Static links for nested subprogram access.
//   4. Fat pointers { ptr, ptr } for unconstrained arrays (data + bounds).
//
// The Code_Generator record holds all mutable state for one compilation unit:
// temporaries, labels, globals, string constants, deferred bodies, and exception state.
//
// LLVM IR is an SSA (Static Single Assignment) form where every %N temporary is defined
// exactly once.  Emit_Temp() allocates a fresh temporary number; the Emit() function
// writes IR text.  Labels partition code into basic blocks — straight-line sequences
// ending in a terminator (br, ret, switch, unreachable).  Emit_Label_Here() begins a
// new block; Emit_Branch_If_Needed() closes the current one if it isn't already
// terminated.
//


typedef struct {
  // -- Output stream and counters
  FILE        *output;                // Destination file for LLVM IR text
  uint32_t     temp_id;               // Next %N temporary register number
  uint32_t     label_id;              // Next label.N basic-block label number
  uint32_t     global_id;             // Next @global.N global variable number
  uint32_t     string_id;             // Next @.str.N string constant number

  // -- Current context
  Symbol      *current_function;      // The subprogram currently being emitted
  uint32_t     current_nesting_level; // Static nesting depth of current_function
  Symbol      *current_instance;      // Active generic instance, or NULL

  // -- Control flow
  uint32_t     loop_exit_label;       // Target label for the nearest enclosing loop exit
  uint32_t     loop_continue_label;   // Target label for the nearest enclosing loop continue
  bool         has_return;            // True if current path ends with a return
  bool         block_terminated;      // True if the current basic block is already terminated
  bool         header_emitted;        // True once the LLVM module header has been written

  // -- Deferred work
  Symbol      *main_candidate;        // The "main" procedure symbol, or NULL
  Syntax_Node *deferred_bodies[64];   // Subprogram bodies to emit after the current unit
  uint32_t     deferred_count;        // Number of deferred bodies queued

  // -- Nested subprograms (static chain)
  Symbol      *enclosing_function;    // Immediately enclosing function for uplevel access
  bool         is_nested;             // True while emitting a nested subprogram

  // -- Exception handling state
  uint32_t     exception_handler_label;// Label of the current exception handler entry
  uint32_t     exception_jmp_buf;      // Temp holding the setjmp buffer
  bool         in_exception_region;    // True inside a begin..exception..end

  // -- String constant accumulator
  char        *string_const_buffer;   // Growing buffer for LLVM string constant data
  size_t       string_const_size;     // Bytes currently in the buffer
  size_t       string_const_capacity; // Allocated capacity of the buffer

  // -- Address markers
  Symbol      *address_markers[256];  // Symbols needing @__address_marker globals
  uint32_t     address_marker_count;  // Number of address markers registered

  // -- Duplicate emission guard
  uint32_t     emitted_func_ids[1024];// Unique IDs of already-emitted functions
  uint32_t     emitted_func_count;    // Number of entries in emitted_func_ids

  // -- Task support
  bool         in_task_body;          // True while emitting a task body
  Symbol      *elab_funcs[64];        // Elaboration functions to call from main
  uint32_t     elab_func_count;       // Number of registered elaboration functions

  // -- Temp-type ring buffer
  uint32_t     temp_type_keys[TEMP_TYPE_CAPACITY];      // Temp ID -> ring key
  const char  *temp_types[TEMP_TYPE_CAPACITY];           // Temp ID -> LLVM type string
  uint8_t      temp_is_fat_alloca[TEMP_TYPE_CAPACITY];   // Temp ID -> fat-alloca flag

  // -- Exception reference table
  char        *exc_refs[EXC_REF_CAPACITY]; // Mangled names of referenced exceptions
  uint32_t     exc_ref_count;              // Number of exception references

  // -- Miscellaneous
  bool         needs_trim_helpers;         // True when trim-string helpers are needed
  uint32_t     rt_type_counter;            // Counter for runtime type descriptors
  uint32_t     in_agg_component;           // Nesting depth inside aggregate generation
  uint32_t     inner_agg_bnd_lo[MAX_AGG_DIMS]; // Low-bound temps for inner dimensions
  uint32_t     inner_agg_bnd_hi[MAX_AGG_DIMS]; // High-bound temps for inner dimensions
  int          inner_agg_bnd_n;            // Number of inner aggregate bound pairs
  uint32_t     disc_cache[MAX_DISC_CACHE]; // Cached discriminant value temps
  uint32_t     disc_cache_count;           // Number of entries in the disc cache
  Type_Info   *disc_cache_type;            // Record type the disc cache belongs to
} Code_Generator;

extern Code_Generator *cg;

void Code_Generator_Init (FILE *output);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1  Core IR Emission Primitives
//
// LLVM IR is an SSA (Static Single Assignment) intermediate representation.  Every value
// lives in a numbered "temporary" written %1, %2, ... and is defined exactly once.
// Emit_Temp() hands out the next number; the Emit() variadic function writes a line of
// IR text to the output stream.
//
// Control flow is expressed as "basic blocks" — straight-line instruction sequences ending
// in exactly one terminator (br, ret, switch, unreachable).  Emit_Label_Here() opens a
// new block; Emit_Branch_If_Needed() closes the current one with a branch if it is not
// already terminated.  This two-function protocol prevents the common bug of emitting
// instructions after a terminator (which LLVM rejects).
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Allocate a fresh SSA temporary number (%N).  Numbers are never reused within a function.
uint32_t    Emit_Temp             (void);
// Allocate a fresh basic-block label number (label.N).
uint32_t    Emit_Label            (void);
// Associate an LLVM type string (e.g. "i32", "ptr") with a temporary for later queries.
void        Temp_Set_Type         (uint32_t temp_id, const char *llvm_type);
// Retrieve the LLVM type previously associated with a temporary.
const char *Temp_Get_Type         (uint32_t temp_id);
// Mark a temporary as holding the address of a fat-pointer alloca (not the fat value itself).
void        Temp_Mark_Fat_Alloca  (uint32_t temp_id);
// Return true if this temporary was marked as a fat-pointer alloca.
bool        Temp_Is_Fat_Alloca    (uint32_t temp_id);

// Write a formatted line of LLVM IR text to the output stream (like fprintf to cg->output).
void Emit                   (const char *format, ...);
// Emit an LLVM debug-location comment anchoring the next instruction to source.
void Emit_Location          (Source_Location location);
// Emit a basic-block label, closing the previous block if it has no terminator.
void Emit_Label_Here        (uint32_t label);
// If the current block lacks a terminator, emit an unconditional "br label %N".
void Emit_Branch_If_Needed  (uint32_t label);
// Append formatted text to the string-constant accumulator (for @.str.N globals).
void Emit_String_Const      (const char *format, ...);
// Append a single character to the string-constant accumulator.
void Emit_String_Const_Char (char ch);
// Emit a floating-point constant using LLVM's hexadecimal literal syntax.
void Emit_Float_Constant    (uint32_t    result,
                             const char *float_type,
                             double      value,
                             const char *comment);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1.1  LLVM Type Helpers
//
// LLVM's type system is simpler than Ada's: integers are iN, floats are "float"/"double",
// and all pointers are the opaque type "ptr".  These helpers translate between Ada's rich
// type model and LLVM's flat one, choosing the correct comparison predicate, arithmetic
// width, or bounds struct layout.
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Return the LLVM integer type used for string bounds (e.g. "i32").
const char *String_Bound_Type    (void);
// Return the LLVM struct type for a pair of string bounds (e.g. "{ i32, i32 }").
const char *String_Bounds_Struct (void);
// Return the LLVM integer type used for general-purpose integer arithmetic.
const char *Integer_Arith_Type   (void);
// Return the wider of two LLVM integer types (e.g. wider of "i16" and "i32" is "i32").
const char *Wider_Int_Type       (const char *left, const char *right);
// Return the LLVM fcmp predicate string ("oeq", "olt", ...) for a floating-point comparison.
const char *Float_Cmp_Predicate  (int op);
// Return the LLVM icmp predicate ("eq", "slt", "ult", ...) for an integer comparison.
const char *Int_Cmp_Predicate    (int op, bool is_unsigned);
// Return the byte size of the string bounds struct.
int         String_Bounds_Alloc  (void);
// Return the bit width of an LLVM type string (e.g. "i32" -> 32, "ptr" -> 64).
int         Type_Bits            (const char *llvm_type);
// Return true if the LLVM type string denotes a floating-point type ("float" or "double").
bool        Is_Float_Type        (const char *llvm_type);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1.2  Name Mangling and Symbol References
//
// LLVM IR uses textual identifiers for globals (@name) and locals (%name).  Ada's dotted
// names (Package.Subprogram) and overloaded identifiers must be encoded into unique,
// linker-safe strings.  The mangling scheme prefixes each component with its byte length
// (like Itanium C++ mangling) so that "Ada.Text_IO.Put" becomes "_ada__3ada8text_io3put".
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Return true if the symbol has global (module-level) linkage rather than local scope.
bool   Symbol_Is_Global   (Symbol *sym);
// Mangle one name component into buf[pos..pos+max]; return the new write position.
size_t Mangle_Slice_Into  (char         *buf,
                            size_t        pos,
                            size_t        max,
                            String_Slice  name);
// Mangle a full symbol (with parent chain) into buf; return the new write position.
size_t Mangle_Into_Buffer (char   *buf,
                            size_t  pos,
                            size_t  max,
                            Symbol *sym);

// Return the mangled linker name for a symbol as an arena-allocated slice.
String_Slice Symbol_Mangle_Name    (Symbol      *sym);
// Return a qualified mangled name by joining parent and child components.
String_Slice Mangle_Qualified_Name (String_Slice parent, String_Slice name);
// Emit "@mangled_name" (the define/declare name) for a symbol.
void         Emit_Symbol_Name      (Symbol *sym);
// Emit "@mangled_name" as a reference (call target or global address).
void         Emit_Symbol_Ref       (Symbol *sym);
// Emit the alloca or global declaration that provides storage for a symbol.
void         Emit_Symbol_Storage   (Symbol *sym);
// Emit a reference to an exception's runtime type descriptor global.
void         Emit_Exception_Ref    (Symbol *exc);
// Find a locally-instantiated copy of a generic template symbol.
Symbol      *Find_Instance_Local   (const Symbol *template_sym);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1.3  Static Chain (Nested Subprogram Access)
//
// Ada allows nested subprograms to reference variables declared in enclosing scopes.  LLVM
// has no built-in closure mechanism, so the compiler passes a hidden "static chain" pointer
// as an extra argument.  Each nesting level adds one pointer indirection; the depth is
// computed at compile time from the symbol table's nesting_level fields.
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Walk the scope chain to find the nearest enclosing subprogram symbol.
Symbol  *Find_Enclosing_Subprogram      (Symbol *sym);
// Return true if this subprogram needs a static-chain parameter (has uplevel references).
bool     Subprogram_Needs_Static_Chain  (Symbol *sym);
// Return true if accessing this symbol requires traversing a static chain.
bool     Is_Uplevel_Access              (const Symbol *sym);
// Return the number of static-chain hops needed to reach a nested procedure's frame.
int      Nested_Frame_Depth             (Symbol *proc);
// Precompute the static-chain argument for a nested call (returns a temp holding the pointer).
uint32_t Precompute_Nested_Frame_Arg    (Symbol *proc);
// Emit the static-chain argument in a call instruction; return true if one was emitted.
bool     Emit_Nested_Frame_Arg          (Symbol *proc, uint32_t precomp);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.2  Runtime Checks and Type Conversions
//
// Ada requires runtime checks for range violations, overflow, division by zero,
// null access, index bounds, array length mismatches, and discriminant mismatches.
// Each check category can be suppressed via pragma Suppress (RM 11.5).  These
// functions emit LLVM IR that compares a value against bounds and branches to
// an exception-raising block on failure.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
void Emit_Raise_Exception  (const char *exc_name, const char *comment);
void Emit_Check_With_Raise (uint32_t    cond,
                             bool        raise_on_true,
                             const char *comment);
void Emit_Range_Check_With_Raise (uint32_t    val,
                                   int64_t     lo_val,
                                   int64_t     hi_val,
                                   const char *type,
                                   const char *comment);
uint32_t Emit_Overflow_Checked_Op (uint32_t    left,
                                    uint32_t    right,
                                    const char *op,
                                    const char *llvm_type,
                                    Type_Info  *result_type);
void Emit_Division_Check (uint32_t    divisor,
                           const char *type,
                           Type_Info  *t);
void Emit_Signed_Division_Overflow_Check (uint32_t    dividend,
                                           uint32_t    divisor,
                                           const char *llvm_type,
                                           Type_Info  *type);

uint32_t Emit_Convert           (uint32_t src, const char *from, const char *to);
uint32_t Emit_Convert_Ext       (uint32_t    src,
                                  const char *src_type,
                                  const char *dst_type,
                                  bool        is_unsigned);
uint32_t Emit_Coerce            (uint32_t temp, const char *desired_type);
uint32_t Emit_Coerce_Default_Int(uint32_t temp, const char *desired_type);

uint32_t Emit_Index_Check (uint32_t    index,
                            uint32_t    low_bound,
                            uint32_t    high_bound,
                            const char *index_type,
                            Type_Info  *array_type);
void Emit_Length_Check (uint32_t    src_length,
                         uint32_t    dst_length,
                         const char *len_type,
                         Type_Info  *array_type);
void Emit_Access_Check       (uint32_t ptr_val, Type_Info *acc_type);
void Emit_Discriminant_Check (uint32_t    actual,
                               uint32_t    expected,
                               const char *disc_type,
                               Type_Info  *record_type);
uint32_t Emit_Constraint_Check (uint32_t   val,
                                 Type_Info *target,
                                 Type_Info *source);
uint32_t Emit_Constraint_Check_With_Type (uint32_t    val,
                                          Type_Info  *target,
                                          Type_Info  *source,
                                          const char *actual_val_type);
void Emit_Subtype_Constraint_Compat_Check (Type_Info *subtype);

uint32_t Emit_Widen_For_Intrinsic (uint32_t val, const char *from_type);
uint32_t Emit_Extend_To_I64       (uint32_t val, const char *from);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.3  Fat-Pointer Operations
//
// Ada unconstrained arrays (e.g. String) carry their bounds at runtime.  In LLVM IR
// this is modelled as a "fat pointer" — a { ptr, ptr } pair where the first element
// points to the data and the second to a { i32, i32 } bounds struct holding the low
// and high index values.  This design lets slices, function parameters, and allocator
// results share a uniform representation without knowing the array size at compile time.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
uint32_t Emit_Fat_Pointer          (uint32_t    data_ptr,
                                    int128_t    low,
                                    int128_t    high,
                                    const char *bt);
uint32_t Emit_Fat_Pointer_Dynamic  (uint32_t    data_ptr,
                                    uint32_t    lo,
                                    uint32_t    hi,
                                    const char *bt);
uint32_t Emit_Fat_Pointer_Heap     (uint32_t    data_ptr,
                                    uint32_t    lo,
                                    uint32_t    hi,
                                    const char *bt);
uint32_t Emit_Fat_Pointer_MultiDim (uint32_t    data_ptr,
                                    uint32_t   *lo,
                                    uint32_t   *hi,
                                    uint32_t    ndims,
                                    const char *bt);
uint32_t Emit_Fat_Pointer_Null     (const char *bt);
uint32_t Fat_Ptr_As_Value          (uint32_t fat_ptr);

uint32_t Emit_Fat_Pointer_Data     (uint32_t fat_ptr, const char *bt);
uint32_t Emit_Fat_Pointer_Bound    (uint32_t    fat_ptr,
                                    const char *bt,
                                    uint32_t    field_index);
uint32_t Emit_Fat_Pointer_Low_Dim  (uint32_t fat_ptr, const char *bt, uint32_t dim);
uint32_t Emit_Fat_Pointer_High_Dim (uint32_t fat_ptr, const char *bt, uint32_t dim);
uint32_t Emit_Fat_Pointer_Length   (uint32_t fat_ptr, const char *bt);
uint32_t Emit_Fat_Pointer_Length_Dim(uint32_t fat_ptr, const char *bt, uint32_t dim);
uint32_t Emit_Fat_Pointer_Compare  (uint32_t    left_fat,
                                    uint32_t    right_fat,
                                    const char *bt);

uint32_t Emit_Load_Fat_Pointer           (Symbol   *sym, const char *bt);
uint32_t Emit_Load_Fat_Pointer_From_Temp (uint32_t  ptr, const char *bt);
void     Emit_Store_Fat_Pointer_To_Symbol     (uint32_t    fat,
                                               Symbol     *sym,
                                               const char *bt);
void     Emit_Store_Fat_Pointer_Fields_To_Temp(uint32_t    data,
                                               uint32_t    lo,
                                               uint32_t    hi,
                                               uint32_t    dest,
                                               const char *bt);
void     Emit_Fat_Pointer_Copy_To_Name (uint32_t    fat_ptr,
                                        Symbol     *dst,
                                        const char *bt);
void     Emit_Fat_To_Array_Memcpy      (uint32_t   fat_val,
                                        uint32_t   dest_ptr,
                                        Type_Info *t);

uint32_t Emit_Alloc_Bounds_Struct  (uint32_t lo, uint32_t hi, const char *bt);
uint32_t Emit_Alloc_Bounds_MultiDim(uint32_t   *lo,
                                    uint32_t   *hi,
                                    uint32_t    ndims,
                                    const char *bt);
uint32_t Emit_Heap_Bounds_Struct   (uint32_t lo, uint32_t hi, const char *bt);

void Emit_Fat_Pointer_Insertvalue_Named  (const char *prefix,
                                          const char *data_expr,
                                          const char *low_expr,
                                          const char *high_expr,
                                          const char *bt);
void Emit_Fat_Pointer_Extractvalue_Named (const char *src_name,
                                          const char *data_name,
                                          const char *low_name,
                                          const char *high_name,
                                          const char *bt);
void Emit_Widen_Named_For_Intrinsic   (const char *src, const char *dst, const char *bt);
void Emit_Narrow_Named_From_Intrinsic (const char *src, const char *dst, const char *bt);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.4  Bound and Length Emission
//
// Array dimensions carry compile-time or runtime bounds.  These helpers emit IR to
// extract bounds from Type_Info (when statically known) or from fat-pointer structs
// (when dynamic), compute lengths as high - low + 1, and clamp to zero for null ranges.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Pair of LLVM temporaries holding the low and high bounds of an array
// dimension, together with the LLVM integer type they are expressed in.
typedef struct {
  uint32_t    low_temp;   // LLVM temp register for the low bound
  uint32_t    high_temp;  // LLVM temp register for the high bound
  const char *bound_type; // LLVM integer type string (e.g. "i32")
} Bound_Temps;

uint32_t    Emit_Bound_Value          (Type_Bound *bound);
uint32_t    Emit_Bound_Value_Typed    (Type_Bound *bound, const char **out_type);
uint32_t    Emit_Single_Bound         (Type_Bound *bound, const char *ty);
Bound_Temps Emit_Bounds               (Type_Info  *type,  uint32_t dim);
Bound_Temps Emit_Bounds_From_Fat      (uint32_t    fat,   const char *bt);
Bound_Temps Emit_Bounds_From_Fat_Dim  (uint32_t    fat,   const char *bt, uint32_t dim);
uint32_t    Emit_Length_From_Bounds   (uint32_t    lo,    uint32_t hi, const char *bt);
uint32_t    Emit_Length_Clamped       (uint32_t    lo,    uint32_t hi, const char *bt);
uint32_t    Emit_Static_Int           (int128_t    value, const char *ty);
uint32_t    Emit_Type_Bound           (Type_Bound *bound, const char *ty);
uint32_t    Emit_Min_Value            (uint32_t    left,  uint32_t right, const char *ty);
uint32_t    Emit_Memcmp_Eq            (uint32_t    left_ptr,
                                       uint32_t    right_ptr,
                                       uint32_t    byte_size_temp,
                                       int64_t     byte_size_static,
                                       bool        is_dynamic);
uint32_t    Emit_Array_Lex_Compare    (uint32_t    left_ptr,
                                       uint32_t    right_ptr,
                                       uint32_t    elem_size,
                                       const char *bt);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.5  Exception Handling
//
// Ada exceptions use a setjmp/longjmp model: entering a begin..exception..end block
// calls setjmp to save the machine state, then dispatches to the matching handler on
// longjmp.  The Exception_Setup struct captures the four LLVM temporaries/labels
// emitted at block entry.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
// The four LLVM labels / temps emitted when entering a begin..exception..end block.
typedef struct {
  uint32_t handler_frame;  // Stack alloca for the exception handler frame
  uint32_t jmp_buf;        // setjmp buffer temp
  uint32_t normal_label;   // Label for the non-exceptional continuation
  uint32_t handler_label;  // Label for the exception dispatch entry
} Exception_Setup;

Exception_Setup Emit_Exception_Handler_Setup (void);
uint32_t        Emit_Current_Exception_Id    (void);
void            Generate_Exception_Dispatch  (Node_List *handlers,
                                              uint32_t   exc_id,
                                              uint32_t   end_label);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.6  Expression Generation
//
// Each Generate_* function lowers one AST node kind to LLVM IR.  The return value
// is a uint32_t temporary number (%N) holding the result.  Expressions are compiled
// bottom-up: leaves (literals, identifiers) produce a single load or constant;
// compound nodes (binary ops, calls, aggregates) recursively generate their children
// and then emit the combining instruction.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
uint32_t Generate_Expression        (Syntax_Node *node);
uint32_t Generate_Lvalue            (Syntax_Node *node);
uint32_t Generate_Integer_Literal   (Syntax_Node *node);
uint32_t Generate_Real_Literal      (Syntax_Node *node);
uint32_t Generate_String_Literal    (Syntax_Node *node);
uint32_t Generate_Identifier        (Syntax_Node *node);
uint32_t Generate_Binary_Op         (Syntax_Node *node);
uint32_t Generate_Unary_Op          (Syntax_Node *node);
uint32_t Generate_Apply             (Syntax_Node *node);
uint32_t Generate_Selected          (Syntax_Node *node);
uint32_t Generate_Attribute         (Syntax_Node *node);
uint32_t Generate_Qualified         (Syntax_Node *node);
uint32_t Generate_Allocator         (Syntax_Node *node);
uint32_t Generate_Aggregate         (Syntax_Node *node);
uint32_t Generate_Composite_Address (Syntax_Node *node);
uint32_t Generate_Bound_Value       (Type_Bound   bound, const char *ty);
uint32_t Generate_Array_Equality    (uint32_t    left_ptr,
                                     uint32_t    right_ptr,
                                     Type_Info  *t);
uint32_t Generate_Record_Equality   (uint32_t    left_ptr,
                                     uint32_t    right_ptr,
                                     Type_Info  *t);
uint32_t Normalize_To_Fat_Pointer   (Syntax_Node *expr,
                                     uint32_t     raw,
                                     Type_Info   *type,
                                     const char  *bt);
uint32_t Wrap_Constrained_As_Fat    (Syntax_Node *expr,
                                     Type_Info   *type,
                                     const char  *bt);
uint32_t Convert_Real_To_Fixed      (uint32_t val, double small, const char *fix_type);

bool     Aggregate_Produces_Fat_Pointer (const Type_Info *t);
void     Ensure_Runtime_Type_Globals    (Type_Info *t);
uint32_t Emit_Bool_Array_Binop          (uint32_t    left,
                                         uint32_t    right,
                                         Type_Info  *result_type,
                                         const char *ir_op);
uint32_t Emit_Bool_Array_Not            (uint32_t operand, Type_Info *result_type);
uint32_t Get_Dimension_Index            (Syntax_Node *arg);

void     Emit_Float_Type_Limit   (uint32_t     t,
                                   Type_Info   *type,
                                   bool         is_low,
                                   String_Slice attr);
uint32_t Emit_Bound_Attribute    (uint32_t     t,
                                   Type_Info   *prefix_type,
                                   Symbol      *prefix_sym,
                                   Syntax_Node *prefix_expr,
                                   bool         needs_runtime_bounds,
                                   uint32_t     dim,
                                   bool         is_low,
                                   String_Slice attr);
uint32_t Emit_Disc_Constraint_Value (Type_Info  *type_info,
                                      uint32_t    disc_index,
                                      const char *disc_type);
void Emit_Nested_Disc_Checks (Type_Info *parent_type);
void Emit_Comp_Disc_Check    (uint32_t   ptr, Type_Info *comp_ti);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.7  Statement Generation
//
// Statements produce side effects rather than values, so they return void.  Control
// flow (if/case/loop) is lowered to LLVM basic blocks connected by br/switch terminators.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
void Generate_Statement        (Syntax_Node *node);
void Generate_Statement_List   (Node_List   *list);
void Generate_Assignment       (Syntax_Node *node);
void Generate_If_Statement     (Syntax_Node *node);
void Generate_Loop_Statement   (Syntax_Node *node);
void Generate_For_Loop         (Syntax_Node *node);
void Generate_Return_Statement (Syntax_Node *node);
void Generate_Case_Statement   (Syntax_Node *node);
void Generate_Block_Statement  (Syntax_Node *node);
void Generate_Raise_Statement  (Syntax_Node *node);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.8  Declaration Generation
//
// Declarations allocate storage (alloca for locals, @global for globals), emit
// initialisation code, and register subprogram definitions.  Deferred bodies are
// queued for later emission to handle forward references.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
void         Generate_Declaration            (Syntax_Node *node);
void         Generate_Declaration_List       (Node_List   *list);
void         Generate_Object_Declaration     (Syntax_Node *node);
void         Generate_Subprogram_Body        (Syntax_Node *node);
void         Generate_Task_Body              (Syntax_Node *node);
void         Emit_Extern_Subprogram          (Symbol *sym);
void         Emit_Function_Header            (Symbol *sym, bool is_nested);
void         Process_Deferred_Bodies         (uint32_t saved_deferred_count);
void         Generate_Generic_Instance_Body  (Symbol      *inst_sym,
                                              Syntax_Node *template_body);
bool         Is_Builtin_Function             (String_Slice name);
bool         Has_Nested_Subprograms          (Node_List *declarations, Node_List *statements);
bool         Has_Nested_In_Statements        (Node_List *statements);
Syntax_Node *Find_Homograph_Body             (Symbol      **exports,
                                              uint32_t      idx,
                                              String_Slice  name,
                                              Node_List    *body_decls);
void         Emit_Task_Function_Name         (Symbol *task_sym, String_Slice fallback_name);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.9  Aggregate Helpers
//
// Ada aggregates (both array and record) are compiled by classifying the associations
// (positional vs. named vs. others), allocating storage, and emitting per-element stores.
// Multi-dimensional array aggregates and discriminated record aggregates require careful
// index arithmetic and variant-part selection.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
typedef struct {
  uint32_t     n_positional; // Number of positional associations
  bool         has_named;    // True if any named associations are present
  bool         has_others;   // True if an `others' association is present
  Syntax_Node *others_expr;  // The expression from the `others' association, or NULL
} Agg_Class;

// A single discriminant allocation entry: symbol plus its LLVM temp.
typedef struct { Symbol *sym; uint32_t temp; }              Disc_Alloc_Entry;
// Array of discriminant allocation entries for aggregate generation.
typedef struct { Disc_Alloc_Entry *entries; uint32_t count; } Disc_Alloc_Info;

Agg_Class Agg_Classify        (Syntax_Node *node);
bool      Bound_Pair_Overflows (Type_Bound low, Type_Bound high);
uint32_t  Agg_Resolve_Elem    (Syntax_Node *expr,
                                bool         multidim,
                                bool         elem_is_composite,
                                Type_Info   *agg_type,
                                const char  *elem_type,
                                Type_Info   *elem_ti);
void      Agg_Store_At_Static (uint32_t    base,
                                uint32_t    val,
                                int128_t    idx,
                                const char *elem_type,
                                uint32_t    elem_size,
                                bool        is_composite);
void      Agg_Store_At_Dynamic(uint32_t    base,
                                uint32_t    val,
                                uint32_t    arr_idx,
                                const char *idx_type,
                                const char *elem_type,
                                uint32_t    elem_size,
                                uint32_t    rt_row_size,
                                bool        is_composite);
bool      Agg_Elem_Is_Composite  (Type_Info *elem, bool multidim);
uint32_t  Agg_Comp_Byte_Size     (Type_Info *ti,   Syntax_Node *src);
void      Agg_Rec_Store          (uint32_t        val,
                                   uint32_t        dest_ptr,
                                   Component_Info *comp,
                                   Syntax_Node    *src_expr);
void      Agg_Rec_Disc_Post      (uint32_t         val,
                                   Component_Info  *comp,
                                   uint32_t         disc_ordinal,
                                   Type_Info       *agg_type,
                                   Disc_Alloc_Info *da_info);

uint32_t Disc_Ordinal_Before             (Type_Info *type_info, uint32_t comp_index);
int32_t  Find_Record_Component           (Type_Info *record_type, String_Slice name);
bool     Is_Others_Choice                (Syntax_Node *choice);
bool     Is_Static_Int_Node              (Syntax_Node *n);
int128_t Static_Int_Value                (Syntax_Node *n);
void     Desugar_Aggregate_Range_Choices (Syntax_Node *agg);
void     Emit_Inner_Consistency_Track    (uint32_t   *inner_trk_lo,
                                          uint32_t   *inner_trk_hi,
                                          uint32_t    inner_trk_first,
                                          uint32_t    inner_trk_mm,
                                          int         n_inner_dims,
                                          const char *bt);
void     Collect_Disc_Symbols_In_Expr    (Syntax_Node *node,
                                          Symbol     **found,
                                          uint32_t    *count,
                                          uint32_t     max);

void Generate_Type_Equality_Function (Type_Info *t);
void Generate_Implicit_Operators     (void);
void Generate_Exception_Globals      (void);
void Generate_Extern_Declarations    (Syntax_Node *node);
void Generate_Compilation_Unit       (Syntax_Node *node);

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.10  Build-in-Place Protocol
//
// Functions returning limited types (records with tasks, controlled components, or
// explicit limited declarations) cannot be copied.  Instead the caller passes extra
// hidden formals (BIPalloc, BIPaccess, BIPmaster, BIPchain, BIPfinal) that tell the
// callee where to construct the result directly.  This avoids illegal copies of
// limited objects per RM 7.5.
// ─────────────────────────────────────────────────────────────────────────────────────────────────
typedef enum {
  BIP_ALLOC_UNSPECIFIED = 0, BIP_ALLOC_CALLER      = 1,
  BIP_ALLOC_SECONDARY   = 2, BIP_ALLOC_GLOBAL_HEAP = 3,
  BIP_ALLOC_USER_POOL   = 4
} BIP_Alloc_Form;

typedef enum {
  BIP_FORMAL_ALLOC_FORM,   BIP_FORMAL_STORAGE_POOL,
  BIP_FORMAL_FINALIZATION, BIP_FORMAL_TASK_MASTER,
  BIP_FORMAL_ACTIVATION,   BIP_FORMAL_OBJECT_ACCESS
} BIP_Formal_Kind;

// Context for a build-in-place function call: describes where and how the
// result object is allocated.
typedef struct {
  Symbol        *func;                // The function returning a limited type
  Type_Info     *result_type;         // The limited result type
  BIP_Alloc_Form alloc_form;          // Who allocates the result storage
  uint32_t       dest_ptr;            // LLVM temp holding the destination pointer
  bool           needs_finalization;  // True if the result needs finalisation
  bool           has_tasks;           // True if the result contains task components
} BIP_Context;

// Per-function state tracking during BIP function code generation.
typedef struct {
  bool     is_bip_function;    // True while emitting a BIP function body
  uint32_t bip_alloc_param;    // Temp for the BIPalloc extra formal
  uint32_t bip_access_param;   // Temp for the BIPaccess extra formal
  uint32_t bip_master_param;   // Temp for the BIPmaster extra formal
  uint32_t bip_chain_param;    // Temp for the BIPchain extra formal
  bool     has_task_components; // True if the result type contains tasks
} BIP_Function_State;

extern BIP_Function_State g_bip_state;

bool BIP_Is_Explicitly_Limited        (const Type_Info *t);
bool BIP_Is_Task_Type                 (const Type_Info *t);
bool BIP_Record_Has_Limited_Component (const Type_Info *t);
bool BIP_Is_Limited_Type              (const Type_Info *t);
bool BIP_Is_BIP_Function              (const Symbol    *func);
bool BIP_Type_Has_Task_Component      (const Type_Info *t);
bool BIP_Needs_Alloc_Form             (const Symbol    *func);

uint32_t       BIP_Extra_Formal_Count   (const Symbol *func);
BIP_Alloc_Form BIP_Determine_Alloc_Form (bool is_allocator,
                                          bool in_return_stmt,
                                          bool has_target);
void           BIP_Begin_Function       (const Symbol *func);
bool           BIP_In_BIP_Function      (void);
void           BIP_End_Function         (void);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §14.  LIBRARY MANAGEMENT — ALI files, checksums, dependency tracking
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// ALI (Ada Library Information) files record dependencies, checksums, and exported
// symbols per compilation unit.  When a package is WITH'd, the compiler first checks
// the ALI cache to avoid re-parsing unchanged sources.  CRC32 checksums detect stale
// dependencies; exported symbols are reinstalled into the symbol table from ALI data.
//


// One compilation unit described in an ALI file.
typedef struct {
  String_Slice unit_name;       // Fully qualified Ada unit name
  String_Slice source_name;     // Filename of the source
  uint32_t     source_checksum; // CRC32 of the source text
  bool         is_body;         // True for a package body, false for a spec
  bool         is_generic;      // True for a generic unit
  bool         is_preelaborate; // Pragma Preelaborate applies
  bool         is_pure;         // Pragma Pure applies
  bool         has_elaboration; // Unit has elaboration-time code
} Unit_Info;

// One WITH dependency recorded in an ALI file.
typedef struct {
  String_Slice name;        // Withed unit name
  String_Slice source_file; // Source filename of the withed unit
  String_Slice ali_file;    // ALI filename of the withed unit
  bool         is_limited;  // True for a limited-with clause
  bool         elaborate;   // Pragma Elaborate applies
  bool         elaborate_all;// Pragma Elaborate_All applies
} With_Info;

// Source file dependency entry in an ALI file.
typedef struct {
  String_Slice source_file; // Filename of the depended-upon source
  uint32_t     timestamp;   // Modification timestamp at compile time
  uint32_t     checksum;    // CRC32 of the source text
} Dependency_Info;

// One exported symbol in an ALI file.
typedef struct {
  String_Slice name;         // Ada name of the exported entity
  String_Slice mangled_name; // Linker-level mangled name
  char         kind;         // 'V' variable, 'F' function, 'T' type, etc.
  uint32_t     line;         // Source line where the entity is declared
  String_Slice type_name;    // Ada name of the entity's type
  String_Slice llvm_type;    // LLVM IR type string
  uint32_t     param_count;  // Number of parameters (for subprograms)
} Export_Info;

typedef struct {
  Unit_Info       units[8];
  uint32_t        unit_count;
  With_Info       withs[64];
  uint32_t        with_count;
  Dependency_Info deps[128];
  uint32_t        dep_count;
  Export_Info     exports[256];
  uint32_t        export_count;
} ALI_Info;

// An exported symbol as read back from an ALI file (heap-allocated strings).
typedef struct {
  char     kind;         // Entity kind character ('V', 'F', 'T', etc.)
  char    *name;         // Ada name (heap-allocated)
  char    *mangled_name; // Linker-level name (heap-allocated)
  char    *llvm_type;    // LLVM IR type string (heap-allocated)
  uint32_t line;         // Source line number
  char    *type_name;    // Ada type name (heap-allocated)
  uint32_t param_count;  // Number of parameters (for subprograms)
} ALI_Export;

// Cached summary of one previously read ALI file, used to avoid re-parsing.
typedef struct ALI_Cache_Entry_Forward {
  char       *unit_name;      // Fully qualified Ada unit name
  char       *source_file;    // Source filename
  char       *ali_file;       // ALI filename on disk
  uint32_t    checksum;       // CRC32 of the source at compile time
  bool        is_spec;        // True for a specification, false for a body
  bool        is_generic;     // True for a generic unit
  bool        is_preelaborate;// Pragma Preelaborate applies
  bool        is_pure;        // Pragma Pure applies
  bool        loaded;         // True once symbols have been installed
  char       *withs[64];      // Names of units this unit depends on
  uint32_t    with_count;     // Number of with dependencies
  ALI_Export  exports[256];   // Exported symbols
  uint32_t    export_count;   // Number of exported symbols
} ALI_Cache_Entry;

extern ALI_Cache_Entry ALI_Cache[256];
extern uint32_t        ALI_Cache_Count;

uint32_t CRC32_Update (uint32_t crc, const void *data, size_t length);

// Map an Ada type name to its LLVM IR type string via the symbol table.
// Falls back to the default integer type if the name is not found.
String_Slice LLVM_Type_Basic (String_Slice ada_type);

void ALI_Collect_Exports (ALI_Info *ali, Syntax_Node *unit);
void ALI_Collect_Withs   (ALI_Info *ali, Syntax_Node *ctx);
void ALI_Collect_Unit    (ALI_Info    *ali,
                           Syntax_Node *cu,
                           const char  *source,
                           size_t       source_size);

void             ALI_Write (FILE *out, ALI_Info *ali);
ALI_Cache_Entry *ALI_Read  (const char *ali_path);

char *ALI_Find          (String_Slice     unit_name);
bool  Try_Load_From_ALI (String_Slice     name);
void  ALI_Load_Symbols  (ALI_Cache_Entry *entry);

void Generate_ALI_File (const char   *output_path,
                         Syntax_Node **units,
                         int           unit_count,
                         const char   *source,
                         size_t        source_size);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §15.  ELABORATION — Dependency ordering for multi-unit programs
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Library-level packages must be elaborated in dependency order before the main
// procedure runs.  This section builds the dependency graph, detects strongly-connected
// components via Tarjan's algorithm, and produces a topological ordering.  Cycles
// caused by Elaborate_All pragmas are detected and reported.
//


typedef enum { UNIT_SPEC, UNIT_BODY, UNIT_SPEC_ONLY, UNIT_BODY_ONLY } Elab_Unit_Kind;

typedef enum {
  EDGE_WITH,          EDGE_ELABORATE,
  EDGE_ELABORATE_ALL, EDGE_SPEC_BEFORE_BODY,
  EDGE_INVOCATION,    EDGE_FORCED
} Elab_Edge_Kind;

typedef enum { PREC_HIGHER, PREC_EQUAL, PREC_LOWER } Elab_Precedence;

typedef enum {
  ELAB_ORDER_OK,
  ELAB_ORDER_HAS_CYCLE,
  ELAB_ORDER_HAS_ELABORATE_ALL_CYCLE
} Elab_Order_Status;

typedef struct Elab_Vertex Elab_Vertex;
typedef struct Elab_Edge   Elab_Edge;

struct Elab_Vertex {
  uint32_t       id;              // Graph-unique vertex index
  String_Slice   name;            // Unit name
  Elab_Unit_Kind kind;            // Spec, body, or standalone
  Symbol        *symbol;          // Corresponding package symbol
  uint32_t       component_id;    // SCC component from Tarjan
  uint32_t       pending_strong;  // Unelaborated strong predecessors
  uint32_t       pending_weak;    // Unelaborated weak predecessors
  bool           in_elab_order;   // Already placed in final order
  bool           is_preelaborate; // Pragma Preelaborate
  bool           is_pure;         // Pragma Pure
  bool           has_elab_body;   // Has elaboration code
  bool           is_predefined;   // Standard library unit
  bool           is_internal;     // Compiler-generated unit
  bool           needs_elab_code; // Needs an elab call in main
  Elab_Vertex   *body_vertex;     // Paired body (from a spec)
  Elab_Vertex   *spec_vertex;     // Paired spec (from a body)
  uint32_t       first_pred_edge; // Head of predecessor edge list
  uint32_t       first_succ_edge; // Head of successor edge list
  int32_t        tarjan_index;    // Tarjan discovery index
  int32_t        tarjan_lowlink;  // Tarjan lowlink value
  bool           tarjan_on_stack; // Currently on Tarjan stack
};

struct Elab_Edge {
  uint32_t       id;             // Graph-unique edge index
  Elab_Edge_Kind kind;           // Dependency kind
  bool           is_strong;      // Strong edges block elaboration
  uint32_t       pred_vertex_id; // Source vertex
  uint32_t       succ_vertex_id; // Target vertex
  uint32_t       next_pred_edge; // Next edge in pred list
  uint32_t       next_succ_edge; // Next edge in succ list
};

typedef struct { uint64_t bits[(ELAB_MAX_VERTICES + 63) / 64]; } Elab_Vertex_Set;

// The full elaboration dependency graph.  Vertices are compilation units,
// edges are WITH / ELABORATE / ELABORATE_ALL / invocation dependencies.
typedef struct {
  Elab_Vertex  vertices[ELAB_MAX_VERTICES];               // All compilation units
  uint32_t     vertex_count;                               // Number of vertices
  Elab_Edge    edges[ELAB_MAX_EDGES];                      // All dependency edges
  uint32_t     edge_count;                                 // Number of edges
  uint32_t     component_pending_strong[ELAB_MAX_COMPONENTS]; // Strong predecessors per SCC
  uint32_t     component_pending_weak[ELAB_MAX_COMPONENTS];   // Weak predecessors per SCC
  uint32_t     component_count;                            // Number of SCCs found by Tarjan
  Elab_Vertex *order[ELAB_MAX_VERTICES];                   // Final topological elaboration order
  uint32_t     order_count;                                // Length of the order array
  bool         has_elaborate_all_cycle;                     // True if an ELABORATE_ALL cycle exists
} Elab_Graph;

// Mutable state for Tarjan's strongly-connected-components algorithm.
typedef struct {
  uint32_t stack[ELAB_MAX_VERTICES]; // DFS stack of vertex indices
  uint32_t stack_top;                // Current stack depth
  int32_t  index;                    // Next DFS discovery index
} Tarjan_State;

extern Elab_Graph g_elab_graph;
extern bool       g_elab_graph_initialized;

Elab_Graph Elab_Graph_New        (void);
void       Elab_Init             (void);
void       Elab_Find_Components  (Elab_Graph *g);
void       Elab_Pair_Specs_Bodies(Elab_Graph *g);
uint32_t   Elab_Find_Vertex      (const Elab_Graph *g,
                                   String_Slice      name,
                                   Elab_Unit_Kind    kind);
uint32_t   Elab_Add_Vertex       (Elab_Graph    *g,
                                   String_Slice   name,
                                   Elab_Unit_Kind kind,
                                   Symbol        *sym);
uint32_t   Elab_Add_Edge         (Elab_Graph    *g,
                                   uint32_t       pred_id,
                                   uint32_t       succ_id,
                                   Elab_Edge_Kind kind);

bool               Edge_Kind_Is_Strong   (Elab_Edge_Kind    k);
Elab_Vertex       *Elab_Get_Vertex       (Elab_Graph       *g, uint32_t id);
const Elab_Vertex *Elab_Get_Vertex_Const (const Elab_Graph *g, uint32_t id);

Elab_Vertex_Set Elab_Set_Empty    (void);
bool            Elab_Set_Contains (const Elab_Vertex_Set *s, uint32_t id);
void            Elab_Set_Insert   (Elab_Vertex_Set *s, uint32_t id);
void            Elab_Set_Remove   (Elab_Vertex_Set *s, uint32_t id);
uint32_t        Elab_Set_Size     (const Elab_Vertex_Set *s);

void Elab_Elaborate_Vertex (Elab_Graph      *g,
                             uint32_t         v_id,
                             Elab_Vertex_Set *elaborable,
                             Elab_Vertex_Set *waiting);
Elab_Order_Status Elab_Elaborate_Graph (Elab_Graph *g);
uint32_t          Elab_Register_Unit   (String_Slice name,
                                         bool         is_body,
                                         Symbol      *sym,
                                         bool         is_preelaborate,
                                         bool         is_pure,
                                         bool         has_elab_code);

Elab_Order_Status Elab_Compute_Order    (void);
uint32_t          Elab_Get_Order_Count  (void);
Symbol           *Elab_Get_Order_Symbol (uint32_t index);
bool              Elab_Needs_Elab_Call  (uint32_t index);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §16.  GENERICS — Macro-style instantiation of generic units
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Generic units are instantiated by macro-style expansion: the template AST is
// deep-cloned with formal-to-actual substitution, then resolved and code-generated
// as though the programmer had written the expanded text by hand.  Each instantiation
// gets its own copy of the AST, its own symbols, and its own emitted IR.
//


typedef struct {
  String_Slice  formal_name;   // Generic formal parameter name
  Type_Info    *actual_type;   // Substituted actual type
  Symbol       *actual_symbol; // Actual symbol (for subprogram formals)
  Syntax_Node  *actual_expr;   // Actual expression (for object formals)
} Generic_Mapping;

typedef struct {
  Generic_Mapping  mappings[32]; // Formal-to-actual mapping array
  uint32_t         count;        // Number of active mappings
  Symbol          *instance_sym; // The instantiation symbol
  Symbol          *template_sym; // The generic template symbol
} Instantiation_Env;

Type_Info   *Env_Lookup_Type (Instantiation_Env *env, String_Slice name);
Syntax_Node *Env_Lookup_Expr (Instantiation_Env *env, String_Slice name);

Syntax_Node *Node_Deep_Clone (Syntax_Node       *node,
                               Instantiation_Env *env,
                               int                depth);
void         Node_List_Clone (Node_List         *dst,
                               Node_List         *src,
                               Instantiation_Env *env,
                               int                depth);

void Build_Instantiation_Env (Instantiation_Env *env, Symbol *inst, Symbol *tmpl);
void Expand_Generic_Package  (Symbol *instance_sym);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §17.  FILE LOADING — Include-path search, source file I/O
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// WITH clauses name packages that must be found on disk, loaded, parsed, analysed,
// and code-generated before the withing unit can proceed.  Include_Paths lists the
// directories to search; Lookup_Path maps a unit name to a file path.  Loading_Set
// detects circular WITH dependencies by tracking which units are currently being loaded.
//


extern const char   *Include_Paths[32];
extern uint32_t      Include_Path_Count;
extern Syntax_Node  *Loaded_Package_Bodies[128];
extern int           Loaded_Body_Count;
extern String_Slice  Loaded_Body_Names[128];
extern int           Loaded_Body_Names_Count;

// Tracks which packages are currently being loaded to detect circular WITH chains.
typedef struct {
  String_Slice names[64]; // Unit names currently being loaded
  int          count;     // Number of names in the set
} Loading_Set;

extern Loading_Set Loading_Packages;

bool  Body_Already_Loaded  (String_Slice name);
void  Mark_Body_Loaded     (String_Slice name);
bool  Loading_Set_Contains (String_Slice name);
void  Loading_Set_Add      (String_Slice name);
void  Loading_Set_Remove   (String_Slice name);
char *Lookup_Path          (String_Slice name);
char *Lookup_Path_Body     (String_Slice name);
bool  Has_Precompiled_LL   (String_Slice name);

void  Load_Package_Spec (String_Slice name, char *src);
char *Read_File         (const char *path, size_t *out_size);
char *Read_File_Simple  (const char *path);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §18.  VECTOR PATHS — SIMD-accelerated scanning on x86-64 and ARM64
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// Vectorised scanning primitives for whitespace skipping, identifier recognition,
// digit scanning, and single-character search.  Three implementations are selected
// at compile time by the platform detection macros:
//   x86-64   AVX-512BW (64-byte), AVX2 (32-byte), with scalar tail
//   ARM64    NEON/ASIMD (16-byte), with scalar tail
//   Generic  Scalar fallback with unrolled loops
//


#ifdef SIMD_X86_64
  extern int Simd_Has_Avx512;
  extern int Simd_Has_Avx2;
  uint32_t   Tzcnt32 (uint32_t value);
  uint64_t   Tzcnt64 (uint64_t value);
  #ifdef __AVX2__
    uint32_t Simd_Parse_8_Digits_Avx2 (const char *digits);
    int      Simd_Parse_Digits_Avx2   (const char *cursor,
                                        const char *limit,
                                        uint64_t   *out);
  #endif
#elif defined(SIMD_ARM64)
  uint64_t Tzcnt64 (uint64_t value);
#endif

void        Simd_Detect_Features   (void);
const char *Simd_Skip_Whitespace   (const char *cursor, const char *limit);
const char *Simd_Find_Newline      (const char *cursor, const char *limit);
const char *Simd_Find_Quote        (const char *cursor, const char *limit);
const char *Simd_Find_Double_Quote (const char *cursor, const char *limit);
const char *Simd_Scan_Identifier   (const char *cursor, const char *limit);
const char *Simd_Scan_Digits       (const char *cursor, const char *limit);

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §19.  DRIVER — Command-line parsing and top-level orchestration
// ═════════════════════════════════════════════════════════════════════════════════════════════════
//
// The main driver parses command-line arguments, compiles each source file to LLVM IR
// — optionally forking a subprocess per file for parallel compilation — and returns
// an exit status.  Derive_Output_Path maps an input .adb or .ads to the .ll output.
//


typedef struct {
  const char *input_path;  // Source file to compile
  const char *output_path; // NULL means derive from input
  int         exit_status; // Zero for success, one for failure
} Compile_Job;

void  Compile_File        (const char *input_path, const char *output_path);
void  Derive_Output_Path  (const char *input, char *out, size_t out_size);
void *Compile_Worker      (void *arg);
int   main                (int argc, char *argv[]);



// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §3. MEMORY - Bump allocation for the compilation session                                         
// ═════════════════════════════════════════════════════════════════════════════════════════════════

Memory_Arena Global_Arena = { .head = NULL, .chunk_size = 0 };
void *Arena_Allocate (size_t size) {
  size = Align_To (size, 16);
  if (not Global_Arena.head or Global_Arena.head->current + size > Global_Arena.head->end) {
    size_t needed = Default_Chunk_Size;
    if (size > needed) needed = size + sizeof (Arena_Chunk);
    Arena_Chunk *fresh = malloc (sizeof (Arena_Chunk) + needed);
    if (not fresh) { fprintf (stderr, "Out of memory\n"); exit (1); }
    fresh->previous = Global_Arena.head;
    char *raw     = (char *) (fresh + 1);
    char *aligned = (char *) (((uintptr_t) raw + 15) & ~(uintptr_t) 15);
    fresh->base = fresh->current = aligned;
    fresh->end  = raw + needed;
    Global_Arena.head = fresh;
  }
  void *result = Global_Arena.head->current;
  Global_Arena.head->current += size;
  return memset (result, 0, size);
}
void Arena_Free_All (void) {
  Arena_Chunk *chunk = Global_Arena.head;
  while (chunk) {
    Arena_Chunk *previous = chunk->previous;
    free (chunk);
    chunk = previous;
  }
  Global_Arena.head = NULL;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §4. TEXT - Non-owning string views                                                               
// ═════════════════════════════════════════════════════════════════════════════════════════════════

const String_Slice Empty_Slice = { .data = NULL, .length = 0 };
String_Slice Slice_From_Cstring (const char *source) {
  return (String_Slice){
    .data   = source,
    .length = source ? (uint32_t) strlen (source) : 0
  };
}
String_Slice Slice_Duplicate (String_Slice slice) {
  if (not slice.length) return Empty_Slice;
  char *copy = Arena_Allocate (slice.length + 1);
  memcpy (copy, slice.data, slice.length);
  return (String_Slice){ .data = copy, .length = slice.length };
}
bool Slice_Equal (String_Slice left, String_Slice right) {
  if (left.length != right.length) return false;
  return memcmp (left.data, right.data, left.length) == 0;
}
bool Slice_Equal_Ignore_Case (String_Slice left, String_Slice right) {
  if (left.length != right.length) return false;
  for (uint32_t i = 0; i < left.length; i++)
    if (To_Lower (left.data[i]) != To_Lower (right.data[i])) return false;
  return true;
}

// FNV-1a hash with case folding, used for case-insensitive symbol-table lookup.  The offset
// basis and prime are the standard 64-bit FNV parameters from Fowler/Noll/Vo.
uint64_t Slice_Hash (String_Slice slice) {
  uint64_t hash = 14695981039346656037ULL;
  for (uint32_t i = 0; i < slice.length; i++)
    hash = (hash ^ (uint8_t) To_Lower (slice.data[i])) * 1099511628211ULL;
  return hash;
}

// Levenshtein edit distance for "did you mean …?" diagnostic suggestions.  The O(n*m) dynamic-
// programming table is stack-allocated; identifiers longer than 20 characters bail out early.
__attribute__ ((unused))
int Edit_Distance (String_Slice left, String_Slice right) {
  if (left.length > 20 or right.length > 20) return 100;
  int table[21][21];
  for (uint32_t i = 0; i <= left.length;  i++) table[i][0] = (int) i;
  for (uint32_t j = 0; j <= right.length; j++) table[0][j] = (int) j;
  for (uint32_t i = 1; i <= left.length; i++)
    for (uint32_t j = 1; j <= right.length; j++) {
      int cost   = To_Lower (left.data[i - 1]) != To_Lower (right.data[j - 1]);
      int delete = table[i - 1][j]     + 1;
      int insert = table[i][j - 1]     + 1;
      int subst  = table[i - 1][j - 1] + cost;
      table[i][j] = delete < insert
        ? (delete < subst ? delete : subst)
        : (insert < subst ? insert : subst);
    }
  return table[left.length][right.length];
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §5. PROVENANCE - Anchoring diagnostics to source text                                            
// ═════════════════════════════════════════════════════════════════════════════════════════════════

const Source_Location No_Location = { .filename = NULL, .line = 0, .column = 0 };

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §5.2 ERROR HANDLING - Accumulating diagnostic reports                                            
// ═════════════════════════════════════════════════════════════════════════════════════════════════

int Error_Count = 0;
void Report_Error (Source_Location location, const char *format, ...) {
  va_list args;
  va_start (args, format);
  fprintf (stderr, "%s:%u:%u: error: ",
           location.filename ? location.filename : "<unknown>",
           location.line, location.column);
  vfprintf (stderr, format, args);
  fputc ('\n', stderr);
  va_end (args);
  Error_Count++;
}
__attribute__ ((unused, noreturn))
void Fatal_Error (Source_Location location, const char *format, ...) {
  va_list args;
  va_start (args, format);
  fprintf (stderr, "%s:%u:%u: INTERNAL ERROR: ",
           location.filename ? location.filename : "<unknown>",
           location.line, location.column);
  vfprintf (stderr, format, args);
  fputc ('\n', stderr);
  va_end (args);
  exit (1);
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §6. BIG INTEGER - Arbitrary-precision integers and reals for Ada literal values                  
// ═════════════════════════════════════════════════════════════════════════════════════════════════

Big_Integer *Big_Integer_New (uint32_t capacity) {
  Big_Integer *result = Arena_Allocate (sizeof (Big_Integer));
  result->limbs    = Arena_Allocate (capacity * sizeof (uint64_t));
  result->capacity = capacity;
  return result;
}
void Big_Integer_Ensure_Capacity (Big_Integer *integer, uint32_t needed) {
  if (needed <= integer->capacity) return;
  uint32_t new_capacity = integer->capacity * 2;
  if (new_capacity < needed) new_capacity = needed;
  uint64_t *new_limbs = Arena_Allocate (new_capacity * sizeof (uint64_t));
  memcpy (new_limbs, integer->limbs, integer->count * sizeof (uint64_t));
  integer->limbs    = new_limbs;
  integer->capacity = new_capacity;
}

// Remove leading zero limbs and canonicalise the sign (zero is always non-negative).
void Big_Integer_Normalize (Big_Integer *integer) {
  while (integer->count > 0 and integer->limbs[integer->count - 1] == 0)
    integer->count--;
  if (integer->count == 0) integer->is_negative = false;
}

// Multiply the integer in place by a single-word factor and add a single-word addend.  This is
// the core loop for building a big integer from a digit string: integer = integer * base + digit.
void Big_Integer_Mul_Add_Small (Big_Integer *integer, uint64_t factor, uint64_t addend) {
  __uint128_t carry = addend;
  for (uint32_t i = 0; i < integer->count; i++) {
    carry += (__uint128_t) integer->limbs[i] * factor;
    integer->limbs[i] = (uint64_t) carry;
    carry >>= 64;
  }
  if (carry) {
    Big_Integer_Ensure_Capacity (integer, integer->count + 1);
    integer->limbs[integer->count++] = (uint64_t) carry;
  }
}

// If the big integer fits in a signed 64-bit value, store it in *out and return true.
bool Big_Integer_Fits_Int64 (const Big_Integer *integer, int64_t *out) {
  if (integer->count == 0) { *out = 0; return true; }
  if (integer->count > 1) return false;
  uint64_t magnitude = integer->limbs[0];
  if (integer->is_negative) {
    if (magnitude > (uint64_t) INT64_MAX + 1) return false;
    *out = -(int64_t) magnitude;
  } else {
    if (magnitude > (uint64_t) INT64_MAX) return false;
    *out = (int64_t) magnitude;
  }
  return true;
}

// Extract a Big_Integer as an unsigned 128-bit value.  Returns true when the magnitude fits in
// uint128_t (0 .. 2**128 - 1), i.e. the integer has at most two limbs and is non-negative.
bool Big_Integer_To_Uint128 (const Big_Integer *integer, uint128_t *out) {
  if (integer->is_negative) return false;
  if (integer->count == 0) { *out = 0; return true; }
  if (integer->count == 1) { *out = (uint128_t) integer->limbs[0]; return true; }
  if (integer->count == 2) {
    *out = ((uint128_t) integer->limbs[1] << 64) | (uint128_t) integer->limbs[0];
    return true;
  }
  return false;
}

// Extract a Big_Integer as a signed 128-bit value.  Returns true when the value fits in the
// int128_t range (-2**127 .. 2**127 - 1).
bool Big_Integer_To_Int128 (const Big_Integer *integer, int128_t *out) {
  uint128_t magnitude;
  if (integer->count == 0) { *out = 0; return true; }
  if (integer->count > 2) return false;
  if (integer->count == 1)
    magnitude = (uint128_t) integer->limbs[0];
  else
    magnitude = ((uint128_t) integer->limbs[1] << 64) | (uint128_t) integer->limbs[0];

  // The most negative 128-bit signed value is -(2**127).
  if (integer->is_negative) {
    if (magnitude > ((uint128_t) 1 << 127)) return false;
    *out = -(int128_t) magnitude;
  } else {
    if (magnitude > (((uint128_t) 1 << 127) - 1)) return false;
    *out = (int128_t) magnitude;
  }
  return true;
}

// Sign-aware comparison of two big integers.  Returns -1, 0, or +1 following the convention
// of strcmp: negative means left < right.
int Big_Integer_Compare (const Big_Integer *left, const Big_Integer *right) {
  bool left_negative  = left->is_negative  and left->count  > 0;
  bool right_negative = right->is_negative and right->count > 0;
  bool left_zero  = left->count  == 0;
  bool right_zero = right->count == 0;
  if (left_zero and right_zero) return 0;
  if (left_zero)  return right_negative ? 1 : -1;
  if (right_zero) return left_negative  ? -1 : 1;
  if (left_negative != right_negative) return left_negative ? -1 : 1;

  // Both have the same sign - compare magnitudes limb by limb from the most significant.
  int magnitude_order;
  if (left->count != right->count)
    magnitude_order = left->count > right->count ? 1 : -1;
  else {
    magnitude_order = 0;
    for (int i = (int) left->count - 1; i >= 0; i--) {
      if (left->limbs[i] != right->limbs[i]) {
        magnitude_order = left->limbs[i] > right->limbs[i] ? 1 : -1;
        break;
      }
    }
  }
  return left_negative ? -magnitude_order : magnitude_order;
}

// Sign-aware addition of two big integers.  Returns a freshly allocated result.
Big_Integer *Big_Integer_Add (const Big_Integer *left, const Big_Integer *right) {
  uint32_t max_count = (left->count > right->count ? left->count : right->count) + 1;
  Big_Integer *result = Big_Integer_New (max_count);

  // When both operands have the same sign, add the magnitudes and keep the sign.
  if (left->is_negative == right->is_negative) {
    result->is_negative = left->is_negative;
    __uint128_t carry = 0;
    for (uint32_t i = 0; i < max_count; i++) {
      __uint128_t sum = carry;
      if (i < left->count)  sum += left->limbs[i];
      if (i < right->count) sum += right->limbs[i];
      result->limbs[i] = (uint64_t) sum;
      carry = sum >> 64;
    }
    result->count = max_count;
    Big_Integer_Normalize (result);

  // Different signs: subtract the smaller magnitude from the larger.
  } else {
    const Big_Integer *larger  = left;
    const Big_Integer *smaller = right;

    // Compare magnitudes (ignoring sign) to decide which is larger.
    int magnitude_cmp = 0;
    if (left->count != right->count)
      magnitude_cmp = left->count > right->count ? 1 : -1;
    else {
      for (int i = (int) left->count - 1; i >= 0; i--) {
        if (left->limbs[i] != right->limbs[i]) {
          magnitude_cmp = left->limbs[i] > right->limbs[i] ? 1 : -1;
          break;
        }
      }
    }
    if (magnitude_cmp < 0) { larger = right; smaller = left; }
    result->is_negative = larger->is_negative;
    int64_t borrow = 0;
    for (uint32_t i = 0; i < larger->count; i++) {
      int64_t diff = (int64_t) larger->limbs[i] - borrow;
      if (i < smaller->count) diff -= (int64_t) smaller->limbs[i];
      if (diff < 0) borrow = 1;
      else borrow = 0;
      result->limbs[i] = (uint64_t) diff;
    }
    result->count = larger->count;
    Big_Integer_Normalize (result);
  }
  return result;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §6.1 BIG_REAL - Arbitrary-precision real numbers for Ada literals                                
// ═════════════════════════════════════════════════════════════════════════════════════════════════

Big_Real *Big_Real_New (void) {
  Big_Real *result = Arena_Allocate (sizeof (Big_Real));
  result->significand = Big_Integer_New (4);
  result->exponent    = 0;
  return result;
}

// Parse a real literal string into an arbitrary-precision Big_Real.  Handles forms like 3.14,
// 3.14E-10, and 3.14159_26535_89793_23846_26433_83279 (underscore-separated digit groups).
Big_Real *Big_Real_From_String (const char *text) {
  Big_Real *result = Big_Real_New ();
  result->significand->is_negative = (*text == '-');
  if (*text == '-' or *text == '+') text++;

  // Collect all significant digits, stripping the decimal point and underscores.
  char cleaned[512];
  int cleaned_length = 0;
  int decimal_position = -1;
  int digit_count      = 0;
  while (*text and *text != 'E' and *text != 'e') {
    if (*text == '.') {
      decimal_position = digit_count;
    } else if (*text >= '0' and *text <= '9') {
      if (cleaned_length < (int) sizeof (cleaned) - 1)
        cleaned[cleaned_length++] = *text;
      digit_count++;
    }
    text++;
  }
  cleaned[cleaned_length] = '\0';

  // Parse the optional exponent part (E+nnn or E-nnn).
  int explicit_exponent = 0;
  if (*text == 'E' or *text == 'e') {
    text++;
    int exponent_sign = 1;
    if (*text == '-') { exponent_sign = -1; text++; }
    else if (*text == '+') text++;
    while (*text >= '0' and *text <= '9') {
      explicit_exponent = explicit_exponent * 10 + (*text - '0');
      text++;
    }
    explicit_exponent *= exponent_sign;
  }

  // Compute the combined exponent.  If the decimal appeared at position 3 within six digits
  // (e.g. "3.14159"), the implicit exponent is 3 - 6 = -3; add the explicit exponent.
  if (decimal_position >= 0)
    result->exponent = explicit_exponent + (decimal_position - digit_count);
  else
    result->exponent = explicit_exponent;

  // Build the significand from the cleaned digit string.
  result->significand = Big_Integer_From_Decimal_SIMD (cleaned);
  return result;
}

// Approximate a Big_Real as an IEEE 754 double.  Precision loss is expected for significands
// that exceed 53 bits; the result is the nearest representable double value.
double Big_Real_To_Double (const Big_Real *real) {
  if (real->significand->count == 0) return 0.0;

  // Reconstruct the significand as a double from limbs (most-significant first).
  double value = 0.0;
  for (int i = (int) real->significand->count - 1; i >= 0; i--)
    value = value * 18446744073709551616.0 + (double) real->significand->limbs[i];
  if (real->significand->is_negative) value = -value;

  // Scale by 10**exponent.
  if (real->exponent > 0) {
    for (int i = 0; i < real->exponent; i++) value *= 10.0;
  } else if (real->exponent < 0) {
    for (int i = 0; i < -real->exponent; i++) value /= 10.0;
  }
  return value;
}

// Return true when the Big_Real can be converted to double without losing significant digits.
// A double's 53-bit mantissa holds at most 15 full decimal digits.
__attribute__ ((unused))
bool Big_Real_Fits_Double (const Big_Real *real) {
  if (real->significand->count == 0) return true;
  if (real->significand->count > 1) return false;
  return real->significand->limbs[0] < 1000000000000000ULL;
}

// Emit a Big_Real as a hexadecimal IEEE 754 double encoding (0xHHHHHHHHHHHHHHHH) suitable for
// LLVM IR float literals.  This preserves full precision unlike a %f format string.
__attribute__ ((unused))
void Big_Real_To_Hex (const Big_Real *real, char *buffer, size_t buffer_size) {
  if (not real or real->significand->count == 0) {
    snprintf (buffer, buffer_size, "0.0");
    return;
  }
  double value = Big_Real_To_Double (real);
  uint64_t bits;
  memcpy (&bits, &value, sizeof (bits));
  snprintf (buffer, buffer_size, "0x%016llX", (unsigned long long) bits);
}

// Return a deep copy of a Big_Integer, allocated from the arena.
Big_Integer *Big_Integer_Clone (const Big_Integer *source) {
  Big_Integer *clone = Big_Integer_New (source->count > 0 ? source->count : 1);
  clone->count       = source->count;
  clone->is_negative = source->is_negative;
  if (source->count > 0)
    memcpy (clone->limbs, source->limbs, source->count * sizeof (uint64_t));
  return clone;
}

// Exact comparison of two Big_Real values.  Returns -1, 0, or +1.  Both operands are normalised
// to a common exponent before the significands are compared.
__attribute__ ((unused))
int Big_Real_Compare (const Big_Real *left, const Big_Real *right) {
  if (not left or not right) return 0;
  bool left_negative  = left->significand->is_negative  and left->significand->count  > 0;
  bool right_negative = right->significand->is_negative and right->significand->count > 0;
  bool left_zero  = left->significand->count  == 0;
  bool right_zero = right->significand->count == 0;
  if (left_zero and right_zero) return 0;
  if (left_zero)  return right_negative ? 1 : -1;
  if (right_zero) return left_negative  ? -1 : 1;
  if (left_negative and not right_negative) return -1;
  if (not left_negative and right_negative) return 1;

  // Both have the same sign - normalise to the smaller exponent and compare significands.
  int32_t exponent_diff = left->exponent - right->exponent;
  Big_Integer *left_sig, *right_sig;
  if (exponent_diff == 0) {
    left_sig  = Big_Integer_Clone (left->significand);
    right_sig = Big_Integer_Clone (right->significand);
  } else if (exponent_diff > 0) {
    left_sig = Big_Integer_Clone (left->significand);
    for (int32_t i = 0; i < exponent_diff; i++)
      Big_Integer_Mul_Add_Small (left_sig, 10, 0);
    right_sig = Big_Integer_Clone (right->significand);
  } else {
    left_sig  = Big_Integer_Clone (left->significand);
    right_sig = Big_Integer_Clone (right->significand);
    for (int32_t i = 0; i < -exponent_diff; i++)
      Big_Integer_Mul_Add_Small (right_sig, 10, 0);
  }
  int ordering = Big_Integer_Compare (left_sig, right_sig);
  return left_negative ? -ordering : ordering;
}

// Exact addition or subtraction of two Big_Real values.  When subtract is true, the result is      
// left - right; otherwise left + right.  Both operands are normalised to the common (minimum)      
// exponent before the significands are combined.                                                   
//                                                                                                  
__attribute__ ((unused))
Big_Real *Big_Real_Add_Sub (const Big_Real *left, const Big_Real *right, bool subtract) {
  if (not left)  return (Big_Real *) (uintptr_t) right;
  if (not right) return (Big_Real *) (uintptr_t) left;
  Big_Real *result = Big_Real_New ();
  int32_t common_exponent = left->exponent < right->exponent ? left->exponent : right->exponent;

  // Normalise both significands to the common exponent.
  Big_Integer *left_sig = Big_Integer_Clone (left->significand);
  for (int32_t i = 0; i < left->exponent - common_exponent; i++)
    Big_Integer_Mul_Add_Small (left_sig, 10, 0);
  Big_Integer *right_sig = Big_Integer_Clone (right->significand);
  for (int32_t i = 0; i < right->exponent - common_exponent; i++)
    Big_Integer_Mul_Add_Small (right_sig, 10, 0);

  // For subtraction, negate the right operand's significand before adding.
  if (subtract) right_sig->is_negative = not right_sig->is_negative;
  result->significand = Big_Integer_Add (left_sig, right_sig);
  result->exponent    = common_exponent;
  return result;
}

// Return a copy of the Big_Real scaled by 10**scale (i.e. the exponent is adjusted).
__attribute__ ((unused))
Big_Real *Big_Real_Scale (const Big_Real *real, int32_t scale) {
  if (not real) return NULL;
  Big_Real *result = Big_Real_New ();
  result->significand = Big_Integer_New (real->significand->capacity);
  result->significand->count       = real->significand->count;
  result->significand->is_negative = real->significand->is_negative;
  memcpy (result->significand->limbs, real->significand->limbs,
          real->significand->count * sizeof (uint64_t));
  result->exponent = real->exponent + scale;
  return result;
}

// Divide a Big_Real by a machine integer (used for fixed-point delta/SMALL calculations).          
// Extra decimal digits of precision are introduced before the division so that the quotient        
// retains meaningful fractional digits.                                                            
//                                                                                                  
__attribute__ ((unused))
Big_Real *Big_Real_Divide_Int (const Big_Real *dividend, int64_t divisor) {
  if (not dividend or divisor == 0) return NULL;
  int extra_precision = 30;
  Big_Real *result = Big_Real_New ();
  result->significand = Big_Integer_New (dividend->significand->capacity + 4);

  // Copy the significand and scale it up by 10**extra_precision.
  result->significand->count       = dividend->significand->count;
  result->significand->is_negative = dividend->significand->is_negative ^ (divisor < 0);
  memcpy (result->significand->limbs, dividend->significand->limbs,
          dividend->significand->count * sizeof (uint64_t));
  for (int i = 0; i < extra_precision; i++)
    Big_Integer_Mul_Add_Small (result->significand, 10, 0);

  // Perform limb-by-limb division by |divisor|.
  uint64_t abs_divisor = divisor < 0 ? (uint64_t) -divisor : (uint64_t) divisor;
  uint64_t remainder   = 0;
  for (int i = (int) result->significand->count - 1; i >= 0; i--) {
    __uint128_t combined = ((__uint128_t) remainder << 64) | result->significand->limbs[i];
    result->significand->limbs[i] = (uint64_t) (combined / abs_divisor);
    remainder = (uint64_t) (combined % abs_divisor);
  }
  Big_Integer_Normalize (result->significand);
  result->exponent = dividend->exponent - extra_precision;
  return result;
}

// Full-precision multiplication of two Big_Real values using textbook O(n*m) arithmetic.
__attribute__ ((unused))
Big_Real *Big_Real_Multiply (const Big_Real *left, const Big_Real *right) {
  if (not left or not right) return NULL;
  Big_Real *result = Big_Real_New ();
  uint32_t product_limbs = left->significand->count + right->significand->count;
  result->significand = Big_Integer_New (product_limbs + 1);
  result->significand->count = product_limbs;
  result->significand->is_negative =
    left->significand->is_negative ^ right->significand->is_negative;
  memset (result->significand->limbs, 0, product_limbs * sizeof (uint64_t));
  for (uint32_t i = 0; i < left->significand->count; i++) {
    __uint128_t carry = 0;
    for (uint32_t j = 0; j < right->significand->count; j++) {
      __uint128_t product = (__uint128_t) left->significand->limbs[i]
                          * right->significand->limbs[j]
                          + result->significand->limbs[i + j] + carry;
      result->significand->limbs[i + j] = (uint64_t) product;
      carry = product >> 64;
    }
    if (carry and i + right->significand->count < product_limbs)
      result->significand->limbs[i + right->significand->count] = (uint64_t) carry;
  }
  Big_Integer_Normalize (result->significand);
  result->exponent = left->exponent + right->exponent;
  return result;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §6.2 Exact Rational Arithmetic for Universal Reals (RM §4.10)                                    
//                                                                                                  
// Ada requires that static universal_real expressions be evaluated exactly during compilation.     
// IEEE double cannot faithfully represent fractions like 1/3, so we carry each value as a          
// numerator/denominator pair of Big_Integers, always reduced by their GCD.                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Full-precision multiplication of two Big_Integers using textbook O(n*m) arithmetic.
Big_Integer *Big_Integer_Multiply (const Big_Integer *left, const Big_Integer *right) {
  if (left->count == 0 or right->count == 0) {
    Big_Integer *zero_result = Big_Integer_New (1);
    zero_result->count = 0;
    return zero_result;
  }
  uint32_t result_limbs = left->count + right->count;
  Big_Integer *result = Big_Integer_New (result_limbs + 1);
  result->count       = result_limbs;
  result->is_negative = left->is_negative ^ right->is_negative;
  memset (result->limbs, 0, result_limbs * sizeof (uint64_t));
  for (uint32_t i = 0; i < left->count; i++) {
    __uint128_t carry = 0;
    for (uint32_t j = 0; j < right->count; j++) {
      __uint128_t product = (__uint128_t) left->limbs[i] * right->limbs[j]
                          + result->limbs[i + j] + carry;
      result->limbs[i + j] = (uint64_t) product;
      carry = product >> 64;
    }
    if (carry) result->limbs[i + right->count] += (uint64_t) carry;
  }
  Big_Integer_Normalize (result);
  return result;
}

// Unsigned division with remainder: quotient = |dividend| / |divisor|, remainder = |dividend|      
// mod |divisor|.  Signs are ignored.  Implements Knuth's Algorithm D for multi-limb divisors       
// and a fast path for single-limb divisors.                                                        
//                                                                                                  
void Big_Integer_Div_Rem (const Big_Integer *dividend, const Big_Integer *divisor,
                                 Big_Integer **quotient_out, Big_Integer **remainder_out) {
  if (divisor->count == 0) {
    *quotient_out = *remainder_out = Big_Integer_New (1);
    return;
  }
  if (dividend->count == 0
      or (Big_Integer_Compare (dividend, divisor) != 0 and dividend->count < divisor->count)) {
    *quotient_out  = Big_Integer_New (1); (*quotient_out)->count = 0;
    *remainder_out = Big_Integer_Clone (dividend); (*remainder_out)->is_negative = false;
    return;
  }

  // Single-limb divisor: simple linear pass.
  if (divisor->count == 1) {
    uint64_t single_divisor = divisor->limbs[0];
    Big_Integer *quotient = Big_Integer_New (dividend->count);
    quotient->count = dividend->count;
    __uint128_t running_remainder = 0;
    for (int i = (int) dividend->count - 1; i >= 0; i--) {
      __uint128_t combined = (running_remainder << 64) | dividend->limbs[i];
      quotient->limbs[i]   = (uint64_t) (combined / single_divisor);
      running_remainder     = combined % single_divisor;
    }
    Big_Integer_Normalize (quotient);
    Big_Integer *remainder = Big_Integer_New (1);
    if (running_remainder) { remainder->limbs[0] = (uint64_t) running_remainder; remainder->count = 1; }
    else remainder->count = 0;
    *quotient_out = quotient; *remainder_out = remainder;
    return;
  }

  // Multi-limb: Knuth Algorithm D.  First normalise so that the top bit of the divisor's most
  // significant limb is set (this guarantees the trial-quotient estimate is within 2).
  uint32_t dividend_limbs = dividend->count;
  uint32_t divisor_limbs  = divisor->count;
  int normalise_shift = __builtin_clzll (divisor->limbs[divisor_limbs - 1]);

  // Create shifted copies of dividend (into u) and divisor (into v).
  Big_Integer *u = Big_Integer_New (dividend_limbs + 1);
  u->count = dividend_limbs + 1;
  Big_Integer *v = Big_Integer_New (divisor_limbs);
  v->count = divisor_limbs;
  if (normalise_shift > 0) {
    uint64_t carry = 0;
    for (uint32_t i = 0; i < divisor_limbs; i++) {
      v->limbs[i] = (divisor->limbs[i] << normalise_shift) | carry;
      carry = divisor->limbs[i] >> (64 - normalise_shift);
    }
    carry = 0;
    for (uint32_t i = 0; i < dividend_limbs; i++) {
      u->limbs[i] = (dividend->limbs[i] << normalise_shift) | carry;
      carry = dividend->limbs[i] >> (64 - normalise_shift);
    }
    u->limbs[dividend_limbs] = carry;
  } else {
    memcpy (u->limbs, dividend->limbs, dividend_limbs * sizeof (uint64_t));
    u->limbs[dividend_limbs] = 0;
    memcpy (v->limbs, divisor->limbs, divisor_limbs * sizeof (uint64_t));
  }
  Big_Integer *quotient = Big_Integer_New (dividend_limbs - divisor_limbs + 1);
  quotient->count = dividend_limbs - divisor_limbs + 1;
  memset (quotient->limbs, 0, quotient->count * sizeof (uint64_t));

  // Main loop: estimate each quotient digit and subtract.
  for (int j = (int) (dividend_limbs - divisor_limbs); j >= 0; j--) {
    __uint128_t numerator = ((__uint128_t) u->limbs[j + divisor_limbs] << 64)
                          | u->limbs[j + divisor_limbs - 1];
    __uint128_t q_hat = numerator / v->limbs[divisor_limbs - 1];
    __uint128_t r_hat = numerator % v->limbs[divisor_limbs - 1];

    // Refine the trial quotient using the next limb of the divisor.
    while (q_hat >= ((__uint128_t) 1 << 64)
           or (divisor_limbs >= 2
               and q_hat * v->limbs[divisor_limbs - 2]
                   > (r_hat << 64) + u->limbs[j + divisor_limbs - 2])) {
      q_hat--;
      r_hat += v->limbs[divisor_limbs - 1];
      if (r_hat >= ((__uint128_t) 1 << 64)) break;
    }

    // Multiply and subtract: u[j .. j+n] -= q_hat * v[0 .. n-1].
    __int128_t borrow = 0;
    for (uint32_t i = 0; i < divisor_limbs; i++) {
      __uint128_t product = q_hat * v->limbs[i];
      __int128_t diff = (__int128_t) u->limbs[j + i] - (uint64_t) product - borrow;
      u->limbs[j + i] = (uint64_t) diff;
      borrow = (int64_t) (product >> 64) - (int64_t) (diff >> 64);
    }
    __int128_t final_diff = (__int128_t) u->limbs[j + divisor_limbs] - borrow;
    u->limbs[j + divisor_limbs] = (uint64_t) final_diff;
    quotient->limbs[j] = (uint64_t) q_hat;

    // If the subtraction went negative, add back one copy of the divisor.
    if (final_diff < 0) {
      quotient->limbs[j]--;
      __uint128_t carry = 0;
      for (uint32_t i = 0; i < divisor_limbs; i++) {
        carry += (__uint128_t) u->limbs[j + i] + v->limbs[i];
        u->limbs[j + i] = (uint64_t) carry;
        carry >>= 64;
      }
      u->limbs[j + divisor_limbs] += (uint64_t) carry;
    }
  }
  Big_Integer_Normalize (quotient);

  // The remainder is in u, but still shifted - undo the normalisation.
  Big_Integer *remainder = Big_Integer_New (divisor_limbs);
  remainder->count = divisor_limbs;
  if (normalise_shift > 0) {
    for (uint32_t i = 0; i < divisor_limbs; i++) {
      remainder->limbs[i] = (u->limbs[i] >> normalise_shift)
        | (i + 1 < u->count ? u->limbs[i + 1] << (64 - normalise_shift) : 0);
    }
  } else {
    memcpy (remainder->limbs, u->limbs, divisor_limbs * sizeof (uint64_t));
  }
  Big_Integer_Normalize (remainder);
  *quotient_out = quotient; *remainder_out = remainder;
}

// Greatest common divisor via the Euclidean algorithm (magnitudes only, signs ignored).
Big_Integer *Big_Integer_GCD (const Big_Integer *left, const Big_Integer *right) {
  Big_Integer *current   = Big_Integer_Clone (left);  current->is_negative   = false;
  Big_Integer *remaining = Big_Integer_Clone (right); remaining->is_negative = false;
  while (remaining->count > 0) {
    Big_Integer *quotient, *modulus;
    Big_Integer_Div_Rem (current, remaining, &quotient, &modulus);
    current   = remaining;
    remaining = modulus;
  }
  return current;
}

Big_Integer *Big_Integer_One (void) {
  Big_Integer *one = Big_Integer_New (1);
  one->limbs[0] = 1; one->count = 1; one->is_negative = false;
  return one;
}

// Reduce a rational by dividing both numerator and denominator by their GCD.  The denominator
// is always made positive (the sign lives on the numerator).
Rational Rational_Reduce (Big_Integer *numer, Big_Integer *denom) {
  if (denom->is_negative) {
    numer->is_negative = not numer->is_negative;
    denom->is_negative = false;
  }
  if (numer->count == 0)
    return (Rational){ .numerator = numer, .denominator = Big_Integer_One () };
  Big_Integer *gcd = Big_Integer_GCD (numer, denom);
  if (gcd->count == 1 and gcd->limbs[0] == 1)
    return (Rational){ .numerator = numer, .denominator = denom };
  Big_Integer *reduced_numer, *numer_rem, *reduced_denom, *denom_rem;
  Big_Integer_Div_Rem (numer, gcd, &reduced_numer, &numer_rem);
  Big_Integer_Div_Rem (denom, gcd, &reduced_denom, &denom_rem);
  reduced_numer->is_negative = numer->is_negative;
  return (Rational){ .numerator = reduced_numer, .denominator = reduced_denom };
}
Rational Rational_From_Big_Real (const Big_Real *real) {
  if (not real or real->significand->count == 0)
    return (Rational){ .numerator = Big_Integer_New (1), .denominator = Big_Integer_One () };
  Big_Integer *numer = Big_Integer_Clone (real->significand);
  Big_Integer *denom = Big_Integer_One ();
  if (real->exponent > 0) {
    for (int32_t i = 0; i < real->exponent; i++)
      Big_Integer_Mul_Add_Small (numer, 10, 0);
  } else if (real->exponent < 0) {
    for (int32_t i = 0; i < -real->exponent; i++)
      Big_Integer_Mul_Add_Small (denom, 10, 0);
  }
  return Rational_Reduce (numer, denom);
}
Rational Rational_From_Int (int64_t value) {
  Big_Integer *numer = Big_Integer_New (1);
  numer->limbs[0]    = value < 0 ? (uint64_t) (-value) : (uint64_t) value;
  numer->count       = value != 0 ? 1 : 0;
  numer->is_negative = value < 0;
  return (Rational){ .numerator = numer, .denominator = Big_Integer_One () };
}

// left + right  =  (left.n * right.d + right.n * left.d) / (left.d * right.d)
Rational Rational_Add (Rational left, Rational right) {
  Big_Integer *cross_left  = Big_Integer_Multiply (left.numerator,  right.denominator);
  Big_Integer *cross_right = Big_Integer_Multiply (right.numerator, left.denominator);
  Big_Integer *numer = Big_Integer_Add (cross_left, cross_right);
  Big_Integer *denom = Big_Integer_Multiply (left.denominator, right.denominator);
  return Rational_Reduce (numer, denom);
}
Rational Rational_Sub (Rational left, Rational right) {
  Big_Integer *negated_numer = Big_Integer_Clone (right.numerator);
  negated_numer->is_negative = not negated_numer->is_negative;
  Rational negated_right = { .numerator = negated_numer, .denominator = right.denominator };
  return Rational_Add (left, negated_right);
}
Rational Rational_Mul (Rational left, Rational right) {
  Big_Integer *numer = Big_Integer_Multiply (left.numerator,   right.numerator);
  Big_Integer *denom = Big_Integer_Multiply (left.denominator, right.denominator);
  return Rational_Reduce (numer, denom);
}

// left / right  =  left.n * right.d  /  (left.d * right.n)
Rational Rational_Div (Rational left, Rational right) {
  Big_Integer *numer = Big_Integer_Multiply (left.numerator,   right.denominator);
  Big_Integer *denom = Big_Integer_Multiply (left.denominator, right.numerator);
  return Rational_Reduce (numer, denom);
}
Rational Rational_Pow (Rational base, int exponent) {
  if (exponent == 0) return Rational_From_Int (1);
  bool negative_exponent = exponent < 0;
  if (negative_exponent) exponent = -exponent;
  Rational result  = Rational_From_Int (1);
  Rational current = base;
  while (exponent > 0) {
    if (exponent & 1) result = Rational_Mul (result, current);
    current = Rational_Mul (current, current);
    exponent >>= 1;
  }

  // For a negative exponent, invert the result by swapping numerator and denominator.
  if (negative_exponent) {
    Big_Integer *swap   = result.numerator;
    result.numerator    = result.denominator;
    result.denominator  = swap;
    if (result.denominator->is_negative) {
      result.numerator->is_negative   = not result.numerator->is_negative;
      result.denominator->is_negative = false;
    }
  }
  return result;
}
int Rational_Compare (Rational left, Rational right) {
  Big_Integer *lhs = Big_Integer_Multiply (left.numerator,  right.denominator);
  Big_Integer *rhs = Big_Integer_Multiply (right.numerator, left.denominator);
  return Big_Integer_Compare (lhs, rhs);
}

// Approximate a Rational as an IEEE 754 double by converting numerator and denominator
// independently and dividing.  Precision loss is inherent for large values.
__attribute__ ((unused))
double Rational_To_Double (Rational rational) {
  double numer_double = 0.0;
  double denom_double = 0.0;
  for (int i = (int) rational.numerator->count - 1; i >= 0; i--)
    numer_double = numer_double * 18446744073709551616.0
                 + (double) rational.numerator->limbs[i];
  if (rational.numerator->is_negative) numer_double = -numer_double;
  for (int i = (int) rational.denominator->count - 1; i >= 0; i--)
    denom_double = denom_double * 18446744073709551616.0
                 + (double) rational.denominator->limbs[i];
  return numer_double / denom_double;
}

// §7–§7.2: Token_Name, Keywords, Lookup_Keyword, Lexer_New, Lexer_Peek,
// Lexer_Advance, and Make_Token are defined earlier in §7 (static inline / static const).
// Lexer_Skip_Whitespace_And_Comments remains here because it depends on SIMD functions.
void Lexer_Skip_Whitespace_And_Comments (Lexer *lex) {

  // Use SIMD to find first non-whitespace, then check for Ada comments (-- to end of line)
  for (;;) {
    const char *past_space = Simd_Skip_Whitespace (lex->current, lex->source_end);

    // Update line/column by scanning for newlines in the skipped region
    while (lex->current < past_space) {
      if (*lex->current == '\n') { lex->line++; lex->column = 1; }
      else lex->column++;
      lex->current++;
    }

    // Ada comment: -- to end of line
    if (lex->current + 1 < lex->source_end and
      lex->current[0] == '-' and lex->current[1] == '-') {

      // Use SIMD to find newline
      const char *end_comment = Simd_Find_Newline (lex->current, lex->source_end);
      lex->column += (uint32_t)(end_comment - lex->current);
      lex->current = end_comment;
    } else break;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §7.3 Scanning Functions - Each scanner consumes one token and returns it                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Token Scan_Identifier (Lexer *lex) {
  Source_Location location = { .filename = lex->filename,
                               .line     = lex->line,
                               .column   = lex->column };
  const char *start  = lex->current;
  const char *end_id = Simd_Scan_Identifier (lex->current, lex->source_end);
  lex->column       += (uint32_t)(end_id - lex->current);
  lex->current       = end_id;
  String_Slice text  = { .data = start, .length = (uint32_t)(lex->current - start) };
  Token_Kind   kind  = Lookup_Keyword (text);
  return Make_Token (kind, location, text);
}

// Parse digit value in any base up to 16; returns -1 for non-hex characters
int Digit_Value (char ch) {
  if (ch >= '0' and ch <= '9') return ch - '0';
  if (ch >= 'A' and ch <= 'F') return ch - 'A' + 10;
  if (ch >= 'a' and ch <= 'f') return ch - 'a' + 10;
  return -1;
}
Token Scan_Number (Lexer *lex) {
  Source_Location location = { .filename = lex->filename,
                               .line     = lex->line,
                               .column   = lex->column };
  const char *start        = lex->current;
  int         base         = 10;
  bool        is_real      = false;
  bool        has_exponent = false;
  bool        is_based     = false;

  // Scan integer part (possibly base specifier)
  while (Is_Digit (Lexer_Peek (lex, 0)) or Lexer_Peek (lex, 0) == '_')
    Lexer_Advance (lex);

  // Based literal: 16#FFFF# or 2#1010#
  if (Lexer_Peek (lex, 0) == '#' or
    (Lexer_Peek (lex, 0) == ':' and Is_Xdigit (Lexer_Peek (lex, 1)))) {
    is_based       = true;
    char delimiter = Lexer_Peek (lex, 0);

    // Parse base from what we've scanned so far
    char base_buf[16] = {0};
    int  index        = 0;
    for (const char *scan = start; scan < lex->current and index < 15; scan++)
      if (*scan != '_') base_buf[index++] = *scan;
    base = atoi (base_buf);
    Lexer_Advance (lex);  // consume # or :

    // Scan mantissa
    while (Is_Xdigit (Lexer_Peek (lex, 0)) or Lexer_Peek (lex, 0) == '_')
      Lexer_Advance (lex);
    if (Lexer_Peek (lex, 0) == '.') {
      is_real = true;
      Lexer_Advance (lex);
      while (Is_Xdigit (Lexer_Peek (lex, 0)) or Lexer_Peek (lex, 0) == '_')
        Lexer_Advance (lex);
    }
    if (Lexer_Peek (lex, 0) == delimiter) Lexer_Advance (lex);
    if (To_Lower (Lexer_Peek (lex, 0)) == 'e') {
      has_exponent = true;
      Lexer_Advance (lex);
      if (Lexer_Peek (lex, 0) == '+' or Lexer_Peek (lex, 0) == '-')
        Lexer_Advance (lex);
      while (Is_Digit (Lexer_Peek (lex, 0)) or Lexer_Peek (lex, 0) == '_')
        Lexer_Advance (lex);
    }

  // Decimal literal with optional fraction and exponent
  } else {
    if (Lexer_Peek (lex, 0) == '.' and
        Lexer_Peek (lex, 1) != '.' and
        not Is_Alpha (Lexer_Peek (lex, 1))) {
      is_real = true;
      Lexer_Advance (lex);
      while (Is_Digit (Lexer_Peek (lex, 0)) or Lexer_Peek (lex, 0) == '_')
        Lexer_Advance (lex);
    }
    if (To_Lower (Lexer_Peek (lex, 0)) == 'e') {
      has_exponent = true;

      // Note: exponent alone doesn't make it real - 12E1 is integer 120
      Lexer_Advance (lex);
      if (Lexer_Peek (lex, 0) == '+' or Lexer_Peek (lex, 0) == '-')
        Lexer_Advance (lex);
      while (Is_Digit (Lexer_Peek (lex, 0)) or Lexer_Peek (lex, 0) == '_')
        Lexer_Advance (lex);
    }
  }
  String_Slice text = { .data   = start,
                         .length = (uint32_t)(lex->current - start) };
  Token tok = Make_Token (is_real ? TK_REAL : TK_INTEGER, location, text);

  // Convert to value: strip underscores and base delimiters, then parse
  char clean[512];
  int  index = 0;
  for (const char *scan = start; scan < lex->current and index < 510; scan++)
    if (*scan != '_' and *scan != '#' and *scan != ':') clean[index++] = *scan;
  clean[index] = '\0';
  if (is_real) {

    // Parse into Big_Real for arbitrary precision
    if (not is_based) {
      tok.big_real    = Big_Real_From_String (clean);
      tok.float_value = Big_Real_To_Double (tok.big_real);

    // Based real - parse mantissa as integer, divide once for precision.
    // Accumulating fractional digits individually causes ULP rounding drift.
    } else {
      long double whole      = 0.0L;
      long double frac_int   = 0.0L;
      int         frac_count = 0;
      int         exponent   = 0;
      int         state      = 0;
      bool        exp_neg    = false;
      for (const char *scan = start; scan < lex->current; scan++) {
        char ch = *scan;
        if (ch == '_') continue;
        if (ch == '#' or ch == ':')                     { state++; continue; }
        if (ch == '.')                                   { state = 2; continue; }
        if (To_Lower (ch) == 'e' and state > 2)         { state = 3; continue; }
        int digit = Digit_Value (ch);
        if (state == 1 and digit >= 0 and digit < base)
          whole = whole * base + digit;
        else if (state == 2 and digit >= 0 and digit < base)
          { frac_int = frac_int * base + digit; frac_count++; }
        else if (state == 3)
          { if (ch == '-') exp_neg = true;
            else if (ch != '+' and Is_Digit (ch)) exponent = exponent * 10 + (ch - '0'); }
      }

      // value = whole + frac_int / base^frac_count
      long double divisor = 1.0L;
      for (int i = 0; i < frac_count; i++) divisor *= base;
      long double value = whole + frac_int / divisor;
      if (exp_neg) exponent = -exponent;
      for (int i = 0; i < (exponent > 0 ? exponent : -exponent); i++)
        value = exponent > 0 ? value * base : value / base;
      tok.float_value = (double)value;
      tok.big_real    = NULL;
    }
  } else {
    if (not is_based and not has_exponent) {
      tok.big_integer = Big_Integer_From_Decimal_SIMD (clean);
      int64_t narrow;
      if (Big_Integer_Fits_Int64 (tok.big_integer, &narrow))
        tok.integer_value = narrow;

    // Decimal integer with exponent (e.g., 12E1 = 120)
    } else if (not is_based and has_exponent) {
      int64_t mantissa = 0;
      int     exponent = 0;
      bool    in_exp   = false;
      bool    exp_neg  = false;
      for (int i = 0; clean[i]; i++) {
        char ch = clean[i];
        if (To_Lower (ch) == 'e') {
          in_exp = true;
        } else if (in_exp) {
          if (ch == '-') exp_neg = true;
          else if (ch == '+') continue;
          else if (Is_Digit (ch)) exponent = exponent * 10 + (ch - '0');
        } else if (Is_Digit (ch)) {
          mantissa = mantissa * 10 + (ch - '0');
        }
      }
      if (exp_neg) {
        for (int i = 0; i < exponent; i++) mantissa /= 10;
      } else {
        for (int i = 0; i < exponent; i++) mantissa *= 10;
      }
      tok.integer_value = mantissa;

    // Based integer: parse from original string (e.g., 16#E#E1 = 14*16 = 224)
    } else {
      int64_t value    = 0;
      int     exponent = 0;
      int     state    = 0;  // 0 = base, 1 = mantissa, 2 = exponent
      for (const char *scan = start; scan < lex->current; scan++) {
        char ch = *scan;
        if (ch == '_') continue;
        if (ch == '#' or ch == ':') { state++; continue; }

        // Skip base part - already parsed
        if (state == 0) {

        // Mantissa in given base
        } else if (state == 1) {
          int digit = Digit_Value (ch);
          if (digit >= 0 and digit < base) value = value * base + digit;

        // Exponent (always decimal, after second delimiter)
        } else if (state == 2) {
          if (To_Lower (ch) == 'e') continue;
          if (ch == '+') continue;
          if (Is_Digit (ch)) exponent = exponent * 10 + (ch - '0');
        }
      }
      for (int i = 0; i < exponent; i++) value *= base;
      tok.integer_value = value;
    }
  }
  return tok;
}
Token Scan_Character_Literal (Lexer *lex) {
  Source_Location location = { .filename = lex->filename,
                               .line     = lex->line,
                               .column   = lex->column };
  Lexer_Advance (lex);  // consume opening '
  char ch = Lexer_Advance (lex);
  if (Lexer_Peek (lex, 0) != '\'') {
    Report_Error (location, "unterminated character literal");
    return Make_Token (TK_ERROR, location, S (""));
  }
  Lexer_Advance (lex);  // consume closing '
  Token tok = Make_Token (TK_CHARACTER, location,
                          (String_Slice){ .data = lex->current - 3, .length = 3 });
  tok.integer_value = (unsigned char)ch;
  return tok;
}
Token Scan_String_Literal (Lexer *lex) {
  Source_Location location  = { .filename = lex->filename,
                                .line     = lex->line,
                                .column   = lex->column };
  char    delimiter = Lexer_Advance (lex);  // consume opening " or %
  size_t  capacity  = 64;
  size_t  length    = 0;
  char   *buffer    = Arena_Allocate (capacity);
  while (lex->current < lex->source_end) {
    if (*lex->current == delimiter) {

      // Doubled delimiter => literal delimiter character (Ada RM §2.6)
      if (Lexer_Peek (lex, 1) == delimiter) {
        if (length >= capacity - 1) {
          char *fresh = Arena_Allocate (capacity * 2);
          memcpy (fresh, buffer, length);
          buffer   = fresh;
          capacity *= 2;
        }
        buffer[length++] = delimiter;
        Lexer_Advance (lex);
        Lexer_Advance (lex);
      } else {
        Lexer_Advance (lex);  // consume closing delimiter
        break;
      }
    } else {
      if (length >= capacity - 1) {
        char *fresh = Arena_Allocate (capacity * 2);
        memcpy (fresh, buffer, length);
        buffer   = fresh;
        capacity *= 2;
      }
      buffer[length++] = Lexer_Advance (lex);
    }
  }
  buffer[length] = '\0';
  return Make_Token (TK_STRING, location,
                     (String_Slice){ .data = buffer, .length = (uint32_t)length });
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §7.4 Main Lexer Entry Point                                                                      
//                                                                                                  
// The lexer works as an iterator: each call to Lexer_Next_Token advances the source stream by      
// one token and returns it.  The function dispatches on the first character to select the          
// appropriate scanner, with a final switch statement handling operators and delimiters.            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Token Lexer_Next_Token (Lexer *lex) {
  Lexer_Skip_Whitespace_And_Comments (lex);
  if (lex->current >= lex->source_end)
    return Make_Token (TK_EOF,
                       (Source_Location){ .filename = lex->filename,
                                          .line     = lex->line,
                                          .column   = lex->column },
                       S (""));
  Source_Location location = { .filename = lex->filename,
                               .line     = lex->line,
                               .column   = lex->column };
  char ch = Lexer_Peek (lex, 0);

  // Identifiers and keywords
  if (Is_Alpha (ch)) return Scan_Identifier (lex);

  // Numeric literals
  if (Is_Digit (ch)) return Scan_Number (lex);

  // Character literal: 'X' where X is any graphic character.                                       
  // Special case: ''' is a character literal containing single quote.                              
  // Context: after identifier or RPAREN, '( is tick+lparen (qualified expression), not char lit.   
  //                                                                                                
  {
    char middle = Lexer_Peek (lex, 1);
    char third  = Lexer_Peek (lex, 2);
    if (ch == '\'' and middle >= ' ' and third == '\'') {
      if (middle == '(' and
        (lex->prev_token_kind == TK_IDENTIFIER or
         lex->prev_token_kind == TK_RPAREN)) {

        // Not a character literal - fall through to tick handling
      } else {
        return Scan_Character_Literal (lex);
      }
    }
  }

  // String literal - both " and % delimiters (RM §2.6, Ada 83)
  if (ch == '"' or ch == '%') return Scan_String_Literal (lex);

  // Operators and delimiters
  Lexer_Advance (lex);
  char next = Lexer_Peek (lex, 0);
  switch (ch) {
    case '(':  return Make_Token (TK_LPAREN,    location, S ("("));
    case ')':  return Make_Token (TK_RPAREN,    location, S (")"));
    case '[':  return Make_Token (TK_LBRACKET,  location, S ("["));
    case ']':  return Make_Token (TK_RBRACKET,  location, S ("]"));
    case ',':  return Make_Token (TK_COMMA,     location, S (","));
    case ';':  return Make_Token (TK_SEMICOLON, location, S (";"));
    case '&':  return Make_Token (TK_AMPERSAND, location, S ("&"));
    case '|':  return Make_Token (TK_BAR,       location, S ("|"));
    case '!':  return Make_Token (TK_BAR,       location, S ("!"));
    case '+':  return Make_Token (TK_PLUS,      location, S ("+"));
    case '-':  return Make_Token (TK_MINUS,     location, S ("-"));
    case '\'': return Make_Token (TK_TICK,      location, S ("'"));
    case '.':
      if (next == '.') { Lexer_Advance (lex); return Make_Token (TK_DOTDOT, location, S ("..")); }
      return Make_Token (TK_DOT, location, S ("."));
    case ':':
      if (next == '=') { Lexer_Advance (lex); return Make_Token (TK_ASSIGN, location, S (":=")); }
      return Make_Token (TK_COLON, location, S (":"));
    case '*':
      if (next == '*') { Lexer_Advance (lex); return Make_Token (TK_EXPON, location, S ("**")); }
      return Make_Token (TK_STAR, location, S ("*"));
    case '/':
      if (next == '=') { Lexer_Advance (lex); return Make_Token (TK_NE, location, S ("/=")); }
      return Make_Token (TK_SLASH, location, S ("/"));
    case '=':
      if (next == '>') { Lexer_Advance (lex); return Make_Token (TK_ARROW, location, S ("=>")); }
      return Make_Token (TK_EQ, location, S ("="));
    case '<':
      if (next == '=') { Lexer_Advance (lex); return Make_Token (TK_LE, location, S ("<=")); }
      if (next == '<') { Lexer_Advance (lex); return Make_Token (TK_LSHIFT, location, S ("<<")); }
      if (next == '>') { Lexer_Advance (lex); return Make_Token (TK_BOX, location, S ("<>")); }
      return Make_Token (TK_LT, location, S ("<"));
    case '>':
      if (next == '=') { Lexer_Advance (lex); return Make_Token (TK_GE, location, S (">=")); }
      if (next == '>') { Lexer_Advance (lex); return Make_Token (TK_RSHIFT, location, S (">>")); }
      return Make_Token (TK_GT, location, S (">"));
    default:
      Report_Error (location, "unexpected character '%c'", ch);
      return Make_Token (TK_ERROR, location, S (""));
  }
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §8. ABSTRACT SYNTAX TREE - Parse Tree Representation                                             
// ═════════════════════════════════════════════════════════════════════════════════════════════════


void Node_List_Push (Node_List *list, Syntax_Node *node) {
  if (list->count >= list->capacity) {
    uint32_t      new_cap   = list->capacity ? list->capacity * 2 : 8;
    Syntax_Node **new_items = Arena_Allocate (new_cap * sizeof (Syntax_Node *));
    if (list->items) memcpy (new_items, list->items, list->count * sizeof (Syntax_Node *));
    list->items    = new_items;
    list->capacity = new_cap;
  }
  list->items[list->count++] = node;
}

// Node constructor - zero-initialises the union so that all pointers start as NULL
Syntax_Node *Node_New (Node_Kind kind, Source_Location location) {
  Syntax_Node *node = Arena_Allocate (sizeof (Syntax_Node));
  memset (node, 0, sizeof (Syntax_Node));
  node->kind     = kind;
  node->location = location;
  return node;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9. PARSER - Recursive Descent with Unified Postfix Handling                                     
// ═════════════════════════════════════════════════════════════════════════════════════════════════


Parser Parser_New (const char *source, size_t length, const char *filename) {
  Parser parser         = {0};
  parser.lexer          = Lexer_New (source, length, filename);
  parser.current_token  = Lexer_Next_Token (&parser.lexer);
  return parser;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.2 Token Movement                                                                              
// ─────────────────────────────────────────────────────────────────────────────────────────────────

bool Parser_At (Parser *parser, Token_Kind kind) {
  return parser->current_token.kind == kind;
}
bool Parser_At_Any (Parser *parser, Token_Kind k1, Token_Kind k2) {
  return Parser_At (parser, k1) or Parser_At (parser, k2);
}

// Lookahead: check if the NEXT token (after current) is of the given kind
bool Parser_Peek_At (Parser *parser, Token_Kind kind) {
  Token saved_token = parser->current_token;
  Lexer saved_lexer = parser->lexer;

  // Update prev_token_kind for context-sensitive lexing during lookahead
  parser->lexer.prev_token_kind = parser->current_token.kind;
  parser->current_token         = Lexer_Next_Token (&parser->lexer);
  bool result          = parser->current_token.kind == kind;
  parser->current_token = saved_token;
  parser->lexer         = saved_lexer;
  return result;
}
Token Parser_Advance (Parser *parser) {
  parser->previous_token = parser->current_token;

  // Update lexer's prev_token_kind before getting next token (for context-sensitive lexing)
  parser->lexer.prev_token_kind = parser->current_token.kind;
  parser->current_token         = Lexer_Next_Token (&parser->lexer);

  // Handle compound keywords: AND THEN, OR ELSE
  if (parser->previous_token.kind == TK_AND and Parser_At (parser, TK_THEN)) {
    parser->previous_token.kind  = TK_AND_THEN;
    parser->lexer.prev_token_kind = TK_AND_THEN;
    parser->current_token         = Lexer_Next_Token (&parser->lexer);
  } else if (parser->previous_token.kind == TK_OR and Parser_At (parser, TK_ELSE)) {
    parser->previous_token.kind  = TK_OR_ELSE;
    parser->lexer.prev_token_kind = TK_OR_ELSE;
    parser->current_token         = Lexer_Next_Token (&parser->lexer);
  }
  return parser->previous_token;
}
bool Parser_Match (Parser *parser, Token_Kind kind) {
  if (not Parser_At (parser, kind)) return false;
  Parser_Advance (parser);
  return true;
}
Source_Location Parser_Location (Parser *parser) {
  return parser->current_token.location;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.3 Error Recovery                                                                              
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Parser_Error (Parser *parser, const char *message) {
  if (parser->panic_mode) return;
  parser->panic_mode = true;
  parser->had_error  = true;
  Report_Error (parser->current_token.location, "%s", message);
}
void Parser_Error_At_Current (Parser *parser, const char *expected) {
  if (parser->panic_mode) return;
  parser->panic_mode = true;
  parser->had_error  = true;
  Report_Error (parser->current_token.location, "expected %s, got %s",
                expected, Token_Name[parser->current_token.kind]);
}

// Synchronize to a recovery point (statement or declaration boundary)
void Parser_Synchronize (Parser *parser) {
  parser->panic_mode = false;
  while (not Parser_At (parser, TK_EOF)) {
    if (parser->previous_token.kind == TK_SEMICOLON) return;
    switch (parser->current_token.kind) {
      case TK_BEGIN: case TK_END: case TK_IF: case TK_CASE: case TK_LOOP:
      case TK_FOR: case TK_WHILE: case TK_RETURN: case TK_DECLARE:
      case TK_EXCEPTION: case TK_PROCEDURE: case TK_FUNCTION:
      case TK_PACKAGE: case TK_TASK: case TK_TYPE: case TK_SUBTYPE:
      case TK_PRAGMA: case TK_ACCEPT: case TK_SELECT:
        return;
      default:
        Parser_Advance (parser);
    }
  }
}

// Check for parser progress - detect stuck parsing loops
bool Parser_Check_Progress (Parser *parser) {
  if (parser->current_token.location.line   == parser->last_line and
      parser->current_token.location.column == parser->last_column and
      parser->current_token.kind            == parser->last_kind) {
    Parser_Advance (parser);
    return false;
  }
  parser->last_line   = parser->current_token.location.line;
  parser->last_column = parser->current_token.location.column;
  parser->last_kind   = parser->current_token.kind;
  return true;
}

// Expect a specific token; report error and return false if not found
bool Parser_Expect (Parser *parser, Token_Kind kind) {
  if (Parser_At (parser, kind)) {
    Parser_Advance (parser);
    return true;
  }
  Parser_Error_At_Current (parser, Token_Name[kind]);
  return false;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.4 Identifier Parsing                                                                          
// ─────────────────────────────────────────────────────────────────────────────────────────────────

String_Slice Parser_Identifier (Parser *parser) {
  if (not Parser_At (parser, TK_IDENTIFIER)) {
    Parser_Error_At_Current (parser, "identifier");
    return Empty_Slice;
  }
  String_Slice name = Slice_Duplicate (parser->current_token.text);
  Parser_Advance (parser);
  return name;
}

// Check that the END identifier matches the expected name (also handles operator strings)
void Parser_Check_End_Name (Parser *parser, String_Slice expected_name) {
  if (Parser_At (parser, TK_IDENTIFIER) or Parser_At (parser, TK_STRING)) {
    String_Slice end_name = parser->current_token.text;
    if (not Slice_Equal_Ignore_Case (end_name, expected_name)) {
      Report_Error (parser->current_token.location,
                    "END name does not match (expected '%.*s', got '%.*s')",
                    expected_name.length, expected_name.data,
                    end_name.length, end_name.data);
    }
    Parser_Advance (parser);
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.5 Expression Parsing - Operator Precedence                                                    
//                                                                                                  
// The grammar encodes precedence; recursion direction determines associativity.                    
//                                                                                                  
// Ada precedence (highest to lowest):                                                              
//   **                               (right-associative exponentiation)                            
//   ABS  NOT                         (unary prefix)                                                
//   *  /  MOD  REM                   (multiplying operators)                                       
//   +  -  &  (binary)  +  - (unary)  (adding operators and concatenation)                          
//   =  /=  <  <=  >  >=  IN  NOT IN (relational)                                                   
//   AND  OR  XOR  AND THEN  OR ELSE (logical, short-circuit)                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.13 Subprogram Declarations and Bodies                                                         
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.13.1 Parameter Specification                                                                  
//                                                                                                  
// IN copies in, OUT copies out, IN OUT does both.  Mode defaults to IN when omitted.               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Parse_Parameter_List (Parser *p, Node_List *params) {
  if (not Parser_Match (p, TK_LPAREN)) return;
  do {
    Source_Location location = Parser_Location (p);
    Syntax_Node   *param    = Node_New (NK_PARAM_SPEC, location);

    // Identifier list
    do {
      Syntax_Node *id = Node_New (NK_IDENTIFIER, Parser_Location (p));
      id->string_val.text = Parser_Identifier (p);
      Node_List_Push (&param->param_spec.names, id);
    } while (Parser_Match (p, TK_COMMA));
    Parser_Expect (p, TK_COLON);

    // Mode
    if (Parser_Match (p, TK_IN)) {
      if (Parser_Match (p, TK_OUT)) param->param_spec.mode = MODE_IN_OUT;
      else                           param->param_spec.mode = MODE_IN;
    } else if (Parser_Match (p, TK_OUT)) {
      param->param_spec.mode = MODE_OUT;
    } else {
      param->param_spec.mode = MODE_IN;
    }
    param->param_spec.param_type = Parse_Subtype_Indication (p);

    // Default expression
    if (Parser_Match (p, TK_ASSIGN)) {
      param->param_spec.default_expr = Parse_Expression (p);
    }
    Node_List_Push (params, param);
  } while (Parser_Match (p, TK_SEMICOLON));
  Parser_Expect (p, TK_RPAREN);
}

// Parse primary: literals, names, aggregates, allocators, parenthesized expressions
Syntax_Node *Parse_Primary (Parser *p) {
  Source_Location location = Parser_Location (p);

  // Integer literal
  if (Parser_At (p, TK_INTEGER)) {
    Syntax_Node *node          = Node_New (NK_INTEGER, location);
    node->integer_lit.value     = p->current_token.integer_value;
    node->integer_lit.big_value = p->current_token.big_integer;
    Parser_Advance (p);
    return node;
  }

  // Real literal - store both double and Big_Real for arbitrary precision
  if (Parser_At (p, TK_REAL)) {
    Syntax_Node *node         = Node_New (NK_REAL, location);
    node->real_lit.value       = p->current_token.float_value;
    node->real_lit.big_value   = p->current_token.big_real;
    Parser_Advance (p);
    return node;
  }

  // Character literal - store only the text (e.g. "'X'"), extract char value when needed.          
  // Do not set integer_lit.value here: it overlaps with string_val.text.data in the union and      
  // would corrupt the text pointer.                                                                
  //                                                                                                
  if (Parser_At (p, TK_CHARACTER)) {
    Syntax_Node *node       = Node_New (NK_CHARACTER, location);
    node->string_val.text    = Slice_Duplicate (p->current_token.text);
    Parser_Advance (p);
    return node;
  }

  // String literal - but check for operator symbol used as function name.  In Ada, "+"(X, Y)       
  // is a valid function call where "+" is the operator.  If this looks like a short operator       
  // string followed by (, fall through to Parse_Name which handles operator names.                 
  //                                                                                                
  if (Parser_At (p, TK_STRING)) {
    String_Slice text              = p->current_token.text;
    bool         is_operator_call  = (text.length <= 3) and Parser_Peek_At (p, TK_LPAREN);
    if (not is_operator_call) {
      Syntax_Node *node     = Node_New (NK_STRING, location);
      node->string_val.text  = Slice_Duplicate (text);
      Parser_Advance (p);
      return node;
    }
  }

  // NULL
  if (Parser_Match (p, TK_NULL))   return Node_New (NK_NULL, location);

  // OTHERS (in aggregates)
  if (Parser_Match (p, TK_OTHERS)) return Node_New (NK_OTHERS, location);

  // NEW allocator
  if (Parser_Match (p, TK_NEW)) {
    Syntax_Node *node    = Node_New (NK_ALLOCATOR, location);
    Syntax_Node *subtype = Parse_Subtype_Indication (p);

    // If Parse_Subtype_Indication returned a qualified expression, extract the parts
    if (subtype->kind == NK_QUALIFIED) {
      node->allocator.subtype_mark = subtype->qualified.subtype_mark;
      node->allocator.expression   = subtype->qualified.expression;
    } else {
      node->allocator.subtype_mark = subtype;
    }
    return node;
  }

  // Unary operators: NOT, ABS, +, -
  if (Parser_At_Any (p, TK_NOT, TK_ABS) or
      Parser_At_Any (p, TK_PLUS, TK_MINUS)) {
    Token_Kind   op   = p->current_token.kind;
    Parser_Advance (p);
    Syntax_Node *node  = Node_New (NK_UNARY_OP, location);
    node->unary.op      = op;
    node->unary.operand = Parse_Primary (p);
    return node;
  }

  // Parenthesized expression or aggregate
  if (Parser_Match (p, TK_LPAREN)) {
    Syntax_Node *expr = Parse_Expression (p);

    // Check for aggregate indicators
    if (Parser_At (p, TK_COMMA)  or Parser_At (p, TK_ARROW) or
        Parser_At (p, TK_BAR)    or Parser_At (p, TK_WITH)  or
        Parser_At (p, TK_DOTDOT)) {

      Syntax_Node *node = Node_New (NK_AGGREGATE, location);

      // Extension aggregate: (ancestor with components)
      if (Parser_Match (p, TK_WITH)) {
        Node_List_Push (&node->aggregate.items, expr);
        node->aggregate.is_named = true;
        Parse_Association_List (p, &node->aggregate.items);

      // First element is a range: expr .. high
      } else if (Parser_At (p, TK_DOTDOT)) {
        Syntax_Node *range = Node_New (NK_RANGE, location);
        range->range.low   = expr;
        Parser_Advance (p);  // consume ..
        range->range.high  = Parse_Expression (p);

        // Check for choice list or named association
        if (Parser_At (p, TK_BAR) or Parser_At (p, TK_ARROW)) {
          Syntax_Node *assoc = Node_New (NK_ASSOCIATION, location);
          Node_List_Push (&assoc->association.choices, range);
          while (Parser_Match (p, TK_BAR)) {
            Node_List_Push (&assoc->association.choices, Parse_Choice (p));
          }
          if (Parser_Match (p, TK_ARROW)) {
            assoc->association.expression = Parse_Expression (p);
          }
          Node_List_Push (&node->aggregate.items, assoc);
        } else {
          Node_List_Push (&node->aggregate.items, range);
        }
        if (Parser_Match (p, TK_COMMA)) {
          Parse_Association_List (p, &node->aggregate.items);
        }

      // First element is part of a choice list
      } else if (Parser_At (p, TK_BAR) or Parser_At (p, TK_ARROW)) {
        Syntax_Node *assoc = Node_New (NK_ASSOCIATION, location);
        Node_List_Push (&assoc->association.choices, expr);
        while (Parser_Match (p, TK_BAR)) {
          Node_List_Push (&assoc->association.choices, Parse_Choice (p));
        }
        if (Parser_Match (p, TK_ARROW)) {
          assoc->association.expression = Parse_Expression (p);
        }
        Node_List_Push (&node->aggregate.items, assoc);
        if (Parser_Match (p, TK_COMMA)) {
          Parse_Association_List (p, &node->aggregate.items);
        }

      // First element is positional, followed by more
      } else {
        Node_List_Push (&node->aggregate.items, expr);
        Parser_Advance (p);  // consume the comma we know is there
        Parse_Association_List (p, &node->aggregate.items);
      }
      Parser_Expect (p, TK_RPAREN);
      return node;
    }
    Parser_Expect (p, TK_RPAREN);

    // RM §4.3.2: A parenthesized aggregate ((a,b,c)) uses INDEX_SUBTYPE'FIRST for its lower
    // bound, unlike a direct sub-aggregate (a,b,c) which uses the applicable index constraint.
    if (expr->kind == NK_AGGREGATE)
      expr->aggregate.is_parenthesized = true;
    return expr;
  }

  // Name (identifier, selected, indexed, etc.)
  return Parse_Name (p);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.6 Unified Postfix Parsing                                                                     
//                                                                                                  
// Handles: .selector, 'attribute, (arguments) - in one loop.                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Name (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node;

  // Base: identifier or operator symbol
  if (Parser_At (p, TK_IDENTIFIER)) {
    node = Node_New (NK_IDENTIFIER, loc);
    node->string_val.text = Parser_Identifier (p);

  // Operator symbol as name: "+" etc
  } else if (Parser_At (p, TK_STRING)) {
    node = Node_New (NK_IDENTIFIER, loc);
    node->string_val.text = Slice_Duplicate (p->current_token.text);
    Parser_Advance (p);
  } else {
    Parser_Error_At_Current (p, "name");
    return Node_New (NK_IDENTIFIER, loc);
  }

  // Postfix chain
  for (;;) {
    Source_Location postfix_loc = Parser_Location (p);

    // .selector or .ALL
    if (Parser_Match (p, TK_DOT)) {

      // Dereference: prefix.ALL
      if (Parser_Match (p, TK_ALL)) {
        Syntax_Node *deref = Node_New (NK_UNARY_OP, postfix_loc);
        deref->unary.op = TK_ALL;
        deref->unary.operand = node;
        node = deref;

      // P.'C' - character literal as enum member (RM 4.1.3)
      } else if (Parser_At (p, TK_CHARACTER)) {
        Syntax_Node *sel = Node_New (NK_SELECTED, postfix_loc);
        sel->selected.prefix = node;
        sel->selected.selector = Slice_Duplicate (p->current_token.text);
        Parser_Advance (p);
        node = sel;

      // P."+" - operator symbol (RM 6.1)
      } else if (Parser_At (p, TK_STRING)) {
        Syntax_Node *sel = Node_New (NK_SELECTED, postfix_loc);
        sel->selected.prefix = node;
        sel->selected.selector = Slice_Duplicate (p->current_token.text);
        Parser_Advance (p);
        node = sel;

      // Selection: prefix.component
      } else {
        Syntax_Node *sel = Node_New (NK_SELECTED, postfix_loc);
        sel->selected.prefix = node;
        sel->selected.selector = Parser_Identifier (p);
        node = sel;
      }
      continue;
    }

    // 'attribute or '(qualified)
    if (Parser_Match (p, TK_TICK)) {

      // Qualified expression: Type'(Expr or Aggregate)
      if (Parser_Match (p, TK_LPAREN)) {
        Syntax_Node *qual = Node_New (NK_QUALIFIED, postfix_loc);
        qual->qualified.subtype_mark = node;

        // Parse expression or aggregate
        Syntax_Node *expr = Parse_Expression (p);
        if (Parser_At (p, TK_COMMA) or Parser_At (p, TK_ARROW) or
          Parser_At (p, TK_BAR) or Parser_At (p, TK_DOTDOT)) {

          // Aggregate
          Syntax_Node *agg = Node_New (NK_AGGREGATE, postfix_loc);

          // Range: expr .. high
          if (Parser_At (p, TK_DOTDOT)) {
            Syntax_Node *range = Node_New (NK_RANGE, postfix_loc);
            range->range.low = expr;
            Parser_Advance (p);
            range->range.high = Parse_Expression (p);
            if (Parser_At (p, TK_BAR) or Parser_At (p, TK_ARROW)) {
              Syntax_Node *assoc = Node_New (NK_ASSOCIATION, postfix_loc);
              Node_List_Push (&assoc->association.choices, range);
              while (Parser_Match (p, TK_BAR)) {
                Node_List_Push (&assoc->association.choices, Parse_Choice (p));
              }
              if (Parser_Match (p, TK_ARROW)) {
                assoc->association.expression = Parse_Expression (p);
              }
              Node_List_Push (&agg->aggregate.items, assoc);
            } else {
              Node_List_Push (&agg->aggregate.items, range);
            }
            if (Parser_Match (p, TK_COMMA)) {
              Parse_Association_List (p, &agg->aggregate.items);
            }
          } else if (Parser_At (p, TK_BAR) or Parser_At (p, TK_ARROW)) {
            Syntax_Node *assoc = Node_New (NK_ASSOCIATION, postfix_loc);
            Node_List_Push (&assoc->association.choices, expr);
            while (Parser_Match (p, TK_BAR)) {
              Node_List_Push (&assoc->association.choices, Parse_Choice (p));
            }
            if (Parser_Match (p, TK_ARROW)) {
              assoc->association.expression = Parse_Expression (p);
            }
            Node_List_Push (&agg->aggregate.items, assoc);
            if (Parser_Match (p, TK_COMMA)) {
              Parse_Association_List (p, &agg->aggregate.items);
            }
          } else {
            Node_List_Push (&agg->aggregate.items, expr);
            Parser_Advance (p);  // consume comma
            Parse_Association_List (p, &agg->aggregate.items);
          }
          qual->qualified.expression = agg;
        } else {
          qual->qualified.expression = expr;
        }
        Parser_Expect (p, TK_RPAREN);
        node = qual;

      // Attribute: prefix'Name or prefix'Name (arg)
      } else {
        Syntax_Node *attr = Node_New (NK_ATTRIBUTE, postfix_loc);
        attr->attribute.prefix = node;

        // Attribute name can be reserved word or identifier
        if (Parser_At (p, TK_IDENTIFIER)) {
          attr->attribute.name = Parser_Identifier (p);
        } else if (Parser_At (p, TK_RANGE) or Parser_At (p, TK_DIGITS) or
               Parser_At (p, TK_DELTA) or Parser_At (p, TK_ACCESS) or
               Parser_At (p, TK_MOD)) {
          attr->attribute.name = Slice_Duplicate (p->current_token.text);
          Parser_Advance (p);
        } else {
          Parser_Error_At_Current (p, "attribute name");
        }

        // Optional attribute arguments (one or more)
        if (Parser_Match (p, TK_LPAREN)) {
          Parse_Association_List (p, &attr->attribute.arguments);
          Parser_Expect (p, TK_RPAREN);
        }
        node = attr;
      }
      continue;
    }

    // (arguments) - call, index, slice, or type conversion
    if (Parser_Match (p, TK_LPAREN)) {
      Syntax_Node *apply = Node_New (NK_APPLY, postfix_loc);
      apply->apply.prefix = node;
      Parse_Association_List (p, &apply->apply.arguments);
      Parser_Expect (p, TK_RPAREN);
      node = apply;
      continue;
    }
    break;
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.6.1 Simple Name Parsing (no parentheses or attributes)                                        
//                                                                                                  
// Used for generic unit names in instantiations where we don't want                                
// parentheses interpreted as function calls.                                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Simple_Name (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node;

  // Base: identifier
  if (Parser_At (p, TK_IDENTIFIER)) {
    node = Node_New (NK_IDENTIFIER, loc);
    node->string_val.text = Parser_Identifier (p);
  } else {
    Parser_Error_At_Current (p, "identifier");
    return Node_New (NK_IDENTIFIER, loc);
  }

  // Only follow dotted selections, not parentheses or ticks
  for (;;) {
    if (Parser_Match (p, TK_DOT)) {
      Source_Location sel_loc = Parser_Location (p);
      Syntax_Node *sel = Node_New (NK_SELECTED, sel_loc);
      sel->selected.prefix = node;

      // Accept character literal ('C') or operator string ("+") after dot
      if (Parser_At (p, TK_CHARACTER) or Parser_At (p, TK_STRING)) {
        sel->selected.selector = Slice_Duplicate (p->current_token.text);
        Parser_Advance (p);
      } else {
        sel->selected.selector = Parser_Identifier (p);
      }
      node = sel;
      continue;
    }
    break;
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.7 Unified Association Parsing                                                                 
//                                                                                                  
// The same syntax serves calls, aggregates, and instantiations. The parser                         
// cannot tell them apart; semantic analysis can.                                                   
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Helper to parse a choice (expression, range, or discrete_subtype_indication)
Syntax_Node *Parse_Choice (Parser *p) {
  Source_Location loc = Parser_Location (p);

  // OTHERS choice
  if (Parser_Match (p, TK_OTHERS)) {
    return Node_New (NK_OTHERS, loc);
  }
  Syntax_Node *expr = Parse_Expression (p);

  // Check if this is a range: expr .. expr
  if (Parser_Match (p, TK_DOTDOT)) {
    Syntax_Node *range = Node_New (NK_RANGE, loc);
    range->range.low = expr;
    range->range.high = Parse_Expression (p);
    return range;
  }

  // Check for subtype_mark RANGE low..high (discrete_subtype_indication)
  // This handles index constraints like: INTEGER RANGE 1..10
  if (Parser_Match (p, TK_RANGE)) {
    Syntax_Node *ind = Node_New (NK_SUBTYPE_INDICATION, loc);
    ind->subtype_ind.subtype_mark = expr;
    Syntax_Node *constraint = Node_New (NK_RANGE_CONSTRAINT, loc);
    Syntax_Node *range_low = Parse_Expression (p);
    if (Parser_Match (p, TK_DOTDOT)) {
      Syntax_Node *range_node = Node_New (NK_RANGE, loc);
      range_node->range.low = range_low;
      range_node->range.high = Parse_Expression (p);
      constraint->range_constraint.range = range_node;

    // Just a range attribute or name
    } else {
      constraint->range_constraint.range = range_low;
    }
    ind->subtype_ind.constraint = constraint;
    return ind;
  }
  return expr;
}
void Parse_Association_List (Parser *p, Node_List *list) {
  if (Parser_At (p, TK_RPAREN)) return;  // Empty list
  do {
    Source_Location loc = Parser_Location (p);
    Syntax_Node *first = Parse_Choice (p);

    // Check for choice list with | or named association with =>
    if (Parser_At (p, TK_BAR) or Parser_At (p, TK_ARROW)) {
      Syntax_Node *assoc = Node_New (NK_ASSOCIATION, loc);
      Node_List_Push (&assoc->association.choices, first);

      // Collect additional choices
      while (Parser_Match (p, TK_BAR)) {
        Node_List_Push (&assoc->association.choices, Parse_Choice (p));
      }

      // Named association: choices => value
      if (Parser_Match (p, TK_ARROW)) {
        assoc->association.expression = Parse_Expression (p);
      }
      Node_List_Push (list, assoc);

    // Positional association
    } else {
      Node_List_Push (list, first);
    }
  } while (Parser_Match (p, TK_COMMA));
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.8 Binary Expression Parsing - Precedence Climbing                                             
//                                                                                                  
// Climbing starts at low precedence and consumes equal-or-higher before returning.                 
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Precedence Get_Infix_Precedence (Token_Kind kind) {
  switch (kind) {
    case TK_AND: case TK_OR: case TK_XOR:
    case TK_AND_THEN: case TK_OR_ELSE:
      return PREC_LOGICAL;
    case TK_EQ: case TK_NE: case TK_LT: case TK_LE:
    case TK_GT: case TK_GE: case TK_IN: case TK_NOT:
      return PREC_RELATIONAL;
    case TK_PLUS: case TK_MINUS: case TK_AMPERSAND:
      return PREC_ADDITIVE;
    case TK_STAR: case TK_SLASH: case TK_MOD: case TK_REM:
      return PREC_MULTIPLICATIVE;
    case TK_EXPON:
      return PREC_EXPONENTIAL;
    default:
      return PREC_NONE;
  }
}
bool Is_Right_Associative (Token_Kind kind) {
  return kind == TK_EXPON;
}
Syntax_Node *Parse_Unary(Parser *p) {
  Source_Location loc = Parser_Location (p);
  if (Parser_At_Any (p, TK_PLUS, TK_MINUS) or
    Parser_At_Any (p, TK_NOT, TK_ABS)) {
    Token_Kind op = p->current_token.kind;
    Parser_Advance (p);
    Syntax_Node *node = Node_New (NK_UNARY_OP, loc);
    node->unary.op = op;
    node->unary.operand = Parse_Unary (p);
    return node;
  }
  return Parse_Primary (p);
}
Syntax_Node *Parse_Expression_Precedence(Parser *p, Precedence min_prec) {
  Syntax_Node *left = Parse_Unary (p);
  for (;;) {
    Token_Kind op = p->current_token.kind;
    Precedence prec = Get_Infix_Precedence (op);
    if (prec < min_prec) break;
    Source_Location loc = Parser_Location (p);
    Parser_Advance (p);

    // After advance, check for compound keywords (AND THEN, OR ELSE)
    // that were detected in Parser_Advance
    if (p->previous_token.kind == TK_AND_THEN or p->previous_token.kind == TK_OR_ELSE) {
      op = p->previous_token.kind;
    }

    // Handle NOT IN specially - mirrors IN range handling below (RM 4.4)
    if (op == TK_NOT and Parser_At (p, TK_IN)) {
      Parser_Advance (p);
      Syntax_Node *right = Parse_Expression_Precedence (p, prec + 1);
      if (Parser_Match (p, TK_DOTDOT)) {
        Syntax_Node *range = Node_New (NK_RANGE, loc);
        range->range.low = right;
        range->range.high = Parse_Expression_Precedence (p, prec + 1);
        right = range;
      }
      Syntax_Node *node = Node_New (NK_BINARY_OP, loc);
      node->binary.op = TK_NOT;
      node->binary.left = left;
      node->binary.right = right;
      left = node;
      continue;
    }

    // Handle IN with possible range
    if (op == TK_IN) {
      Syntax_Node *right = Parse_Expression_Precedence (p, prec + 1);

      // Check for range: X in A .. B
      if (Parser_Match (p, TK_DOTDOT)) {
        Syntax_Node *range = Node_New (NK_RANGE, loc);
        range->range.low = right;
        range->range.high = Parse_Expression_Precedence (p, prec + 1);
        right = range;
      }
      Syntax_Node *node = Node_New (NK_BINARY_OP, loc);
      node->binary.op = TK_IN;
      node->binary.left = left;
      node->binary.right = right;
      left = node;
      continue;
    }

    // Standard binary operation
    Precedence next_prec = Is_Right_Associative (op) ? prec : prec + 1;
    Syntax_Node *right = Parse_Expression_Precedence (p, next_prec);
    Syntax_Node *node = Node_New (NK_BINARY_OP, loc);
    node->binary.op = op;
    node->binary.left = left;
    node->binary.right = right;
    left = node;
  }
  return left;
}
Syntax_Node *Parse_Expression (Parser *p) {
  return Parse_Expression_Precedence (p, PREC_LOGICAL);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.9 Range Parsing                                                                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Range(Parser *p) {
  Source_Location loc = Parser_Location (p);

  // BOX: <> for unconstrained
  if (Parser_Match (p, TK_BOX)) {
    return Node_New (NK_RANGE, loc);  // Empty range = unconstrained
  }
  Syntax_Node *low = Parse_Expression (p);
  if (Parser_Match (p, TK_DOTDOT)) {
    Syntax_Node *node = Node_New (NK_RANGE, loc);
    node->range.low = low;
    node->range.high = Parse_Expression (p);
    return node;
  }

  // Check for subtype_mark RANGE low..high (discrete subtype definition)
  // e.g., "INTEGER RANGE 1..10" or "STAT RANGE 1..5"
  if (Parser_Match (p, TK_RANGE)) {
    Syntax_Node *ind = Node_New (NK_SUBTYPE_INDICATION, loc);
    ind->subtype_ind.subtype_mark = low;

    // Now parse the actual range constraint
    Syntax_Node *constraint = Node_New (NK_RANGE_CONSTRAINT, loc);
    Syntax_Node *range_low = Parse_Expression (p);
    if (Parser_Match (p, TK_DOTDOT)) {
      Syntax_Node *range_node = Node_New (NK_RANGE, loc);
      range_node->range.low = range_low;
      range_node->range.high = Parse_Expression (p);
      constraint->range_constraint.range = range_node;

    // Just a range attribute or name
    } else {
      constraint->range_constraint.range = range_low;
    }
    ind->subtype_ind.constraint = constraint;
    return ind;
  }

  // Could be a subtype name used as a range
  return low;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.10 Subtype Indication Parsing                                                                 
//                                                                                                  
// A subtype is a type with a constraint that narrows the range of valid values.                    
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Subtype_Indication (Parser *p) {
  Source_Location loc = Parser_Location (p);

  // Parse_Name may consume (args) as NK_APPLY - we need to unwrap it for constraints
  Syntax_Node *name_or_apply = Parse_Name (p);

  // If Parse_Name returned NK_APPLY, the parenthesized part might be a constraint
  if (name_or_apply->kind == NK_APPLY) {
    Syntax_Node *subtype_mark = name_or_apply->apply.prefix;
    Node_List *items = &name_or_apply->apply.arguments;

    // Classify: if any item is a named association, it's a discriminant constraint
    bool is_discriminant = false;
    for (uint32_t i = 0; i < items->count; i++) {
      Syntax_Node *item = items->items[i];
      if (item->kind == NK_ASSOCIATION) {
        is_discriminant = true;
        break;
      }
    }

    // Create NK_SUBTYPE_INDICATION with appropriate constraint
    Syntax_Node *ind = Node_New (NK_SUBTYPE_INDICATION, loc);
    ind->subtype_ind.subtype_mark = subtype_mark;
    if (is_discriminant) {
      Syntax_Node *constraint = Node_New (NK_DISCRIMINANT_CONSTRAINT, loc);
      constraint->discriminant_constraint.associations = *items;
      ind->subtype_ind.constraint = constraint;
    } else {
      Syntax_Node *constraint = Node_New (NK_INDEX_CONSTRAINT, loc);
      constraint->index_constraint.ranges = *items;
      ind->subtype_ind.constraint = constraint;
    }
    return ind;
  }
  Syntax_Node *subtype_mark = name_or_apply;

  // Check for RANGE constraint
  if (Parser_Match (p, TK_RANGE)) {
    Syntax_Node *ind = Node_New (NK_SUBTYPE_INDICATION, loc);
    ind->subtype_ind.subtype_mark = subtype_mark;
    Syntax_Node *constraint = Node_New (NK_RANGE_CONSTRAINT, loc);
    constraint->range_constraint.range = Parse_Range (p);
    ind->subtype_ind.constraint = constraint;
    return ind;
  }

  // Check for DIGITS constraint (floating-point types)
  if (Parser_Match (p, TK_DIGITS)) {
    Syntax_Node *ind = Node_New (NK_SUBTYPE_INDICATION, loc);
    ind->subtype_ind.subtype_mark = subtype_mark;
    Syntax_Node *constraint = Node_New (NK_DIGITS_CONSTRAINT, loc);
    constraint->digits_constraint.digits_expr = Parse_Expression (p);

    // Optional RANGE constraint after DIGITS
    if (Parser_Match (p, TK_RANGE)) {
      constraint->digits_constraint.range = Parse_Range (p);
    }
    ind->subtype_ind.constraint = constraint;
    return ind;
  }

  // Check for DELTA constraint (fixed-point types)
  if (Parser_Match (p, TK_DELTA)) {
    Syntax_Node *ind = Node_New (NK_SUBTYPE_INDICATION, loc);
    ind->subtype_ind.subtype_mark = subtype_mark;
    Syntax_Node *constraint = Node_New (NK_DELTA_CONSTRAINT, loc);
    constraint->delta_constraint.delta_expr = Parse_Expression (p);

    // Optional RANGE constraint after DELTA
    if (Parser_Match (p, TK_RANGE)) {
      constraint->delta_constraint.range = Parse_Range (p);
    }
    ind->subtype_ind.constraint = constraint;
    return ind;
  }
  return subtype_mark;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.11 Statement Parsing                                                                          
// ═════════════════════════════════════════════════════════════════════════════════════════════════


Syntax_Node *Parse_Type_Definition (Parser *p) {
  Source_Location loc = Parser_Location (p);

  // Enumeration type
  if (Parser_At (p, TK_LPAREN)) {
    return Parse_Enumeration_Type (p);
  }

  // Array type
  if (Parser_At (p, TK_ARRAY)) {
    return Parse_Array_Type (p);
  }

  // Record type: RECORD ... END RECORD or NULL RECORD
  if (Parser_At (p, TK_RECORD)) {
    return Parse_Record_Type (p);
  }

  // Null record type: NULL RECORD
  if (Parser_Match (p, TK_NULL)) {
    Parser_Expect (p, TK_RECORD);
    Syntax_Node *node = Node_New (NK_RECORD_TYPE, loc);
    node->record_type.is_null = true;
    return node;
  }

  // Access type
  if (Parser_At (p, TK_ACCESS)) {
    return Parse_Access_Type (p);
  }

  // Derived type
  if (Parser_At (p, TK_NEW)) {
    return Parse_Derived_Type (p);
  }

  // Integer types: range, mod
  if (Parser_Match (p, TK_RANGE)) {
    Syntax_Node *node = Node_New (NK_INTEGER_TYPE, loc);
    node->integer_type.range = Parse_Range (p);
    return node;
  }
  if (Parser_Match (p, TK_MOD)) {
    Syntax_Node *node = Node_New (NK_INTEGER_TYPE, loc);
    Syntax_Node *mod_expr = Parse_Expression (p);
    node->integer_type.is_modular = true;
    node->integer_type.modulus = 0;  // Evaluated during semantic analysis
    node->integer_type.range = mod_expr;
    return node;
  }

  // Real types: digits, delta
  if (Parser_Match (p, TK_DIGITS)) {
    Syntax_Node *node = Node_New (NK_REAL_TYPE, loc);
    node->real_type.precision = Parse_Expression (p);
    if (Parser_Match (p, TK_RANGE)) {
      node->real_type.range = Parse_Range (p);
    }
    return node;
  }
  if (Parser_Match (p, TK_DELTA)) {
    Syntax_Node *node = Node_New (NK_REAL_TYPE, loc);
    node->real_type.delta = Parse_Expression (p);
    if (Parser_Match (p, TK_RANGE)) {
      node->real_type.range = Parse_Range (p);
    }
    return node;
  }
  Parser_Error (p, "expected type definition");
  return Node_New (NK_INTEGER_TYPE, loc);
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.17 Pragmas                                                                                    
// ═════════════════════════════════════════════════════════════════════════════════════════════════


Syntax_Node *Parse_Pragma (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_PRAGMA);
  Syntax_Node *node = Node_New (NK_PRAGMA, loc);
  node->pragma_node.name = Parser_Identifier (p);
  if (Parser_Match (p, TK_LPAREN)) {
    Parse_Association_List (p, &node->pragma_node.arguments);
    Parser_Expect (p, TK_RPAREN);
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.1 Simple Statements                                                                        
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Assignment_Or_Call(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *target = Parse_Name (p);
  if (Parser_Match (p, TK_ASSIGN)) {
    Syntax_Node *node = Node_New (NK_ASSIGNMENT, loc);
    node->assignment.target = target;
    node->assignment.value = Parse_Expression (p);
    return node;
  }

  // Procedure call (target is already an NK_APPLY or NK_IDENTIFIER)
  Syntax_Node *call = Node_New (NK_CALL_STMT, loc);
  call->assignment.target = target;  // Reuse field for simplicity
  return call;
}
Syntax_Node *Parse_Return_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_RETURN);
  Syntax_Node *node = Node_New (NK_RETURN, loc);
  if (not Parser_At (p, TK_SEMICOLON)) {
    node->return_stmt.expression = Parse_Expression (p);
  }
  return node;
}
Syntax_Node *Parse_Exit_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_EXIT);
  Syntax_Node *node = Node_New (NK_EXIT, loc);
  if (Parser_At (p, TK_IDENTIFIER)) {
    node->exit_stmt.loop_name = Parser_Identifier (p);
  }
  if (Parser_Match (p, TK_WHEN)) {
    node->exit_stmt.condition = Parse_Expression (p);
  }
  return node;
}
Syntax_Node *Parse_Goto_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_GOTO);
  Syntax_Node *node = Node_New (NK_GOTO, loc);
  node->goto_stmt.name = Parser_Identifier (p);
  return node;
}
Syntax_Node *Parse_Raise_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_RAISE);
  Syntax_Node *node = Node_New (NK_RAISE, loc);
  if (Parser_At (p, TK_IDENTIFIER)) {
    node->raise_stmt.exception_name = Parse_Name (p);
  }
  return node;
}
Syntax_Node *Parse_Delay_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_DELAY);
  Syntax_Node *node = Node_New (NK_DELAY, loc);
  node->delay_stmt.expression = Parse_Expression (p);
  return node;
}
Syntax_Node *Parse_Abort_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_ABORT);
  Syntax_Node *node = Node_New (NK_ABORT, loc);
  do {
    Node_List_Push (&node->abort_stmt.task_names, Parse_Name (p));
  } while (Parser_Match (p, TK_COMMA));
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.2 If Statement                                                                             
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_If_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_IF);
  Syntax_Node *node = Node_New (NK_IF, loc);
  node->if_stmt.condition = Parse_Expression (p);
  Parser_Expect (p, TK_THEN);
  Parse_Statement_Sequence (p, &node->if_stmt.then_stmts);

  // ELSIF parts
  while (Parser_At (p, TK_ELSIF)) {
    Source_Location elsif_loc = Parser_Location (p);
    Parser_Advance (p);
    Syntax_Node *elsif = Node_New (NK_IF, elsif_loc);
    elsif->if_stmt.condition = Parse_Expression (p);
    Parser_Expect (p, TK_THEN);
    Parse_Statement_Sequence (p, &elsif->if_stmt.then_stmts);
    Node_List_Push (&node->if_stmt.elsif_parts, elsif);
  }

  // ELSE part
  if (Parser_Match (p, TK_ELSE)) {
    Parse_Statement_Sequence (p, &node->if_stmt.else_stmts);
  }
  Parser_Expect (p, TK_END);
  Parser_Expect (p, TK_IF);
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.3 Case Statement                                                                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Case_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_CASE);
  Syntax_Node *node = Node_New (NK_CASE, loc);
  node->case_stmt.expression = Parse_Expression (p);
  Parser_Expect (p, TK_IS);

  // Parse alternatives
  while (Parser_At (p, TK_WHEN)) {
    Source_Location alt_loc = Parser_Location (p);
    Parser_Advance (p);
    Syntax_Node *alt = Node_New (NK_ASSOCIATION, alt_loc);

    // Parse choices - use Parse_Choice to handle ranges and OTHERS
    do {
      Node_List_Push (&alt->association.choices, Parse_Choice (p));
    } while (Parser_Match (p, TK_BAR));
    Parser_Expect (p, TK_ARROW);

    // Statements for this alternative stored as expression temporarily
    Syntax_Node *stmts = Node_New (NK_BLOCK, alt_loc);
    Parse_Statement_Sequence (p, &stmts->block_stmt.statements);
    alt->association.expression = stmts;
    Node_List_Push (&node->case_stmt.alternatives, alt);
  }
  Parser_Expect (p, TK_END);
  Parser_Expect (p, TK_CASE);
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.4 Loop Statement                                                                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Loop_Statement(Parser *p, String_Slice label) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node = Node_New (NK_LOOP, loc);
  node->loop_stmt.label = label;

  // WHILE loop
  if (Parser_Match (p, TK_WHILE)) {
    node->loop_stmt.iteration_scheme = Parse_Expression (p);
  }

  // FOR loop
  else if (Parser_Match (p, TK_FOR)) {
    Source_Location for_loc = Parser_Location (p);
    Syntax_Node *iter = Node_New (NK_BINARY_OP, for_loc);
    iter->binary.op = TK_IN;

    // Iterator identifier
    Syntax_Node *id = Node_New (NK_IDENTIFIER, for_loc);
    id->string_val.text = Parser_Identifier (p);
    iter->binary.left = id;
    Parser_Expect (p, TK_IN);
    node->loop_stmt.is_reverse = Parser_Match (p, TK_REVERSE);

    // Discrete range
    iter->binary.right = Parse_Range (p);
    node->loop_stmt.iteration_scheme = iter;
  }
  Parser_Expect (p, TK_LOOP);
  Parse_Statement_Sequence (p, &node->loop_stmt.statements);
  Parser_Expect (p, TK_END);
  Parser_Expect (p, TK_LOOP);
  if (label.data and Parser_At (p, TK_IDENTIFIER)) {
    Parser_Check_End_Name (p, label);
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.5 Block Statement                                                                          
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Block_Statement(Parser *p, String_Slice label) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node = Node_New (NK_BLOCK, loc);
  node->block_stmt.label = label;
  if (Parser_Match (p, TK_DECLARE)) {
    Parse_Declarative_Part (p, &node->block_stmt.declarations);
  }
  Parser_Expect (p, TK_BEGIN);
  Parse_Statement_Sequence (p, &node->block_stmt.statements);
  if (Parser_Match (p, TK_EXCEPTION)) {
    while (Parser_At (p, TK_WHEN)) {
      Source_Location h_loc = Parser_Location (p);
      Parser_Advance (p);
      Syntax_Node *handler = Node_New (NK_EXCEPTION_HANDLER, h_loc);

      // Exception choices
      do {
        if (Parser_Match (p, TK_OTHERS)) {
          Node_List_Push (&handler->handler.exceptions, Node_New (NK_OTHERS, h_loc));
        } else {
          Node_List_Push (&handler->handler.exceptions, Parse_Name (p));
        }
      } while (Parser_Match (p, TK_BAR));
      Parser_Expect (p, TK_ARROW);
      Parse_Statement_Sequence (p, &handler->handler.statements);
      Node_List_Push (&node->block_stmt.handlers, handler);
    }
  }
  Parser_Expect (p, TK_END);
  if (label.data and Parser_At (p, TK_IDENTIFIER)) {
    Parser_Check_End_Name (p, label);
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.6 Accept Statement                                                                         
//                                                                                                  
// ACCEPT is the server side of rendezvous where the caller blocks until accepted.                  
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Accept_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_ACCEPT);
  Syntax_Node *node = Node_New (NK_ACCEPT, loc);
  node->accept_stmt.entry_name = Parser_Identifier (p);

  // Optional index and/or parameters                                                               
  // Need to distinguish:                                                                           
  // - Entry index: (expression) like (5) or (I)                                                    
  // - Parameters: (id : type) like (X : INTEGER)                                                   
  // Lookahead to distinguish index vs parameters                                                   
  //                                                                                                
  if (Parser_At (p, TK_LPAREN)) {
    Token saved = p->current_token;
    Lexer saved_lexer = p->lexer;
    Parser_Advance (p);  // consume (
    bool is_parameter_list = false;
    if (Parser_At (p, TK_IDENTIFIER)) {
      Parser_Advance (p);  // past identifier

      // If followed by : or ,, it's a parameter list
      is_parameter_list = Parser_At (p, TK_COLON) or Parser_At (p, TK_COMMA);
    }

    // Restore and parse correctly
    p->current_token = saved;
    p->lexer = saved_lexer;

    // This is a parameter list, not an index
    if (is_parameter_list) {
      Parse_Parameter_List (p, &node->accept_stmt.parameters);

    // Parse entry index (expression)
    } else {
      Parser_Advance (p);  // consume (
      node->accept_stmt.index = Parse_Expression (p);
      Parser_Expect (p, TK_RPAREN);

      // Now check for optional parameters after index
      if (Parser_At (p, TK_LPAREN)) {
        Parse_Parameter_List (p, &node->accept_stmt.parameters);
      }
    }
  }

  // Optional body
  if (Parser_Match (p, TK_DO)) {
    Parse_Statement_Sequence (p, &node->accept_stmt.statements);
    Parser_Expect (p, TK_END);
    if (Parser_At (p, TK_IDENTIFIER)) {
      Parser_Check_End_Name (p, node->accept_stmt.entry_name);
    }
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.7 Select Statement                                                                         
//                                                                                                  
// SELECT makes a nondeterministic choice among open alternatives at runtime.                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Select_Statement(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_SELECT);
  Syntax_Node *node = Node_New (NK_SELECT, loc);

  // Parse alternatives.  Each alternative is optionally guarded:                                   
  //   [WHEN condition =>] accept_stmt ; [stmts]                                                    
  //   [WHEN condition =>] delay_stmt  ; [stmts]                                                    
  //   [WHEN condition =>] TERMINATE ;                                                              
  //   entry_call_stmt ; [stmts]           (timed/conditional)                                      
  //                                                                                                
  do {
    Source_Location alt_loc = Parser_Location (p);

    // Optional guard - parse condition then fall through to alternative
    Syntax_Node *guard = NULL;
    if (Parser_Match (p, TK_WHEN)) {
      guard = Parse_Expression (p);
      Parser_Expect (p, TK_ARROW);
    }
    if (Parser_Match (p, TK_TERMINATE)) {
      Syntax_Node *term = Node_New (NK_NULL_STMT, alt_loc);
      Node_List_Push (&node->select_stmt.alternatives, term);
      Parser_Expect (p, TK_SEMICOLON);
    } else if (Parser_Match (p, TK_DELAY)) {
      Syntax_Node *delay = Node_New (NK_DELAY, alt_loc);
      delay->delay_stmt.expression = Parse_Expression (p);
      Parser_Expect (p, TK_SEMICOLON);
      Node_List_Push (&node->select_stmt.alternatives, delay);

      // Optional statement sequence after delay
      while (not Parser_At (p, TK_OR) and not Parser_At (p, TK_ELSE) and
           not Parser_At (p, TK_END) and not Parser_At (p, TK_EOF)) {
        Syntax_Node *stmt = Parse_Statement (p);
        Node_List_Push (&node->select_stmt.alternatives, stmt);
        if (not Parser_At (p, TK_OR) and not Parser_At (p, TK_ELSE) and not Parser_At (p, TK_END)) {
          Parser_Expect (p, TK_SEMICOLON);
        }
      }
    } else if (Parser_At (p, TK_ACCEPT)) {
      Syntax_Node *accept = Parse_Accept_Statement (p);
      Node_List_Push (&node->select_stmt.alternatives, accept);
      Parser_Expect (p, TK_SEMICOLON);

      // Optional sequence of statements after accept in select
      while (not Parser_At (p, TK_OR) and not Parser_At (p, TK_ELSE) and
           not Parser_At (p, TK_END) and not Parser_At (p, TK_EOF)) {
        Syntax_Node *stmt = Parse_Statement (p);
        Node_List_Push (&accept->accept_stmt.statements, stmt);
        if (not Parser_At (p, TK_OR) and not Parser_At (p, TK_ELSE) and not Parser_At (p, TK_END)) {
          Parser_Expect (p, TK_SEMICOLON);
        }
      }

    // Entry call alternative - conditional or timed entry call
    } else if (Parser_At (p, TK_IDENTIFIER)) {
      Syntax_Node *entry_call = Parse_Statement (p);
      Node_List_Push (&node->select_stmt.alternatives, entry_call);
      Parser_Expect (p, TK_SEMICOLON);

      // Optional sequence of statements after entry call
      while (not Parser_At (p, TK_OR) and not Parser_At (p, TK_ELSE) and
           not Parser_At (p, TK_END) and not Parser_At (p, TK_EOF)) {
        Syntax_Node *stmt = Parse_Statement (p);
        Node_List_Push (&node->select_stmt.alternatives, stmt);
        if (not Parser_At (p, TK_OR) and not Parser_At (p, TK_ELSE) and not Parser_At (p, TK_END)) {
          Parser_Expect (p, TK_SEMICOLON);
        }
      }
    } else {
      break;
    }
    (void)guard;  // Guard stored in AST if needed later
  } while (Parser_Match (p, TK_OR));
  if (Parser_Match (p, TK_ELSE)) {
    node->select_stmt.else_part = Node_New (NK_BLOCK, loc);
    Parse_Statement_Sequence (p, &node->select_stmt.else_part->block_stmt.statements);
  }
  Parser_Expect (p, TK_END);
  Parser_Expect (p, TK_SELECT);
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.11.8 Statement Dispatch                                                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Statement (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Source_Location label_loc = loc;

  // Check for label(s): <<label>> or identifier:
  // Ada allows multiple labels before a statement: <<L1>> <<L2>> stmt;
  String_Slice label = Empty_Slice;

  // Handle multiple consecutive labels
  while (Parser_At (p, TK_LSHIFT) or
       (Parser_At (p, TK_IDENTIFIER) and Parser_Peek_At (p, TK_COLON))) {
    if (Parser_Match (p, TK_LSHIFT)) {
      if (label.length == 0) label_loc = loc;  // Save first label location
      label = Parser_Identifier (p);
      Parser_Expect (p, TK_RSHIFT);

    // Lookahead for "identifier :" (label) vs assignment/call
    } else if (Parser_At (p, TK_IDENTIFIER)) {
      Token saved = p->current_token;
      Lexer saved_lexer = p->lexer;
      String_Slice id = Parser_Identifier (p);

      // This is a label
      if (Parser_Match (p, TK_COLON)) {
        if (label.length == 0) label_loc = loc;  // Save first label location
        label = id;

      // Not a label - restore and let assignment/call handle it
      } else {
        p->current_token = saved;
        p->lexer = saved_lexer;
        break;
      }
    }
    loc = Parser_Location (p);  // Update location to after labels
  }
  Syntax_Node *stmt = NULL;

  // Null statement
  // Semicolon is handled by Parse_Statement_Sequence
  if (Parser_Match (p, TK_NULL)) {
    stmt = Node_New (NK_NULL_STMT, loc);
  }

  // Compound statements - loops and blocks handle labels as names
  else if (Parser_At (p, TK_LOOP) or Parser_At (p, TK_WHILE) or Parser_At (p, TK_FOR)) {
    return Parse_Loop_Statement (p, label);  // Loop keeps label as name
  }
  else if (Parser_At (p, TK_DECLARE) or Parser_At (p, TK_BEGIN)) {
    return Parse_Block_Statement (p, label);  // Block keeps label as name
  }
  else if (Parser_At (p, TK_IF)) stmt = Parse_If_Statement (p);
  else if (Parser_At (p, TK_CASE)) stmt = Parse_Case_Statement (p);
  else if (Parser_At (p, TK_ACCEPT)) stmt = Parse_Accept_Statement (p);
  else if (Parser_At (p, TK_SELECT)) stmt = Parse_Select_Statement (p);

  // Simple statements
  else if (Parser_At (p, TK_RETURN)) stmt = Parse_Return_Statement (p);
  else if (Parser_At (p, TK_EXIT)) stmt = Parse_Exit_Statement (p);
  else if (Parser_At (p, TK_GOTO)) stmt = Parse_Goto_Statement (p);
  else if (Parser_At (p, TK_RAISE)) stmt = Parse_Raise_Statement (p);
  else if (Parser_At (p, TK_DELAY)) stmt = Parse_Delay_Statement (p);
  else if (Parser_At (p, TK_ABORT)) stmt = Parse_Abort_Statement (p);

  // Pragma in statement sequence (Ada 83 RM 2.8)
  else if (Parser_At (p, TK_PRAGMA)) stmt = Parse_Pragma (p);

  // Assignment or procedure call
  else stmt = Parse_Assignment_Or_Call (p);

  // Wrap in NK_LABEL if a label was present (except for loop/block which handle labels)
  if (label.length > 0 and stmt != NULL) {
    Syntax_Node *label_node = Node_New (NK_LABEL, label_loc);
    label_node->label_node.name = label;
    label_node->label_node.statement = stmt;
    label_node->label_node.symbol = NULL;  // Set during resolution
    return label_node;
  }
  return stmt;
}
void Parse_Statement_Sequence (Parser *p, Node_List *list) {
  while (not Parser_At (p, TK_EOF) and
       not Parser_At (p, TK_END) and
       not Parser_At (p, TK_ELSE) and
       not Parser_At (p, TK_ELSIF) and
       not Parser_At (p, TK_WHEN) and
       not Parser_At (p, TK_EXCEPTION) and
       not Parser_At (p, TK_OR)) {
    if (not Parser_Check_Progress (p)) break;
    Syntax_Node *stmt = Parse_Statement (p);
    Node_List_Push (list, stmt);
    if (not Parser_At (p, TK_END) and not Parser_At (p, TK_ELSE) and
      not Parser_At (p, TK_ELSIF) and not Parser_At (p, TK_WHEN) and
      not Parser_At (p, TK_EXCEPTION) and not Parser_At (p, TK_OR)) {
      Parser_Expect (p, TK_SEMICOLON);
    }
  }
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.12 Declaration Parsing                                                                        
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.12.1 Object Declaration (variables, constants)                                                
//                                                                                                  
// Multiple names can share one type declaration but each gets its own symbol.                      
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Object_Declaration(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node = Node_New (NK_OBJECT_DECL, loc);

  // Identifier list
  do {
    Syntax_Node *id = Node_New (NK_IDENTIFIER, Parser_Location (p));
    id->string_val.text = Parser_Identifier (p);
    Node_List_Push (&node->object_decl.names, id);
  } while (Parser_Match (p, TK_COMMA));
  Parser_Expect (p, TK_COLON);

  // Check for exception declaration: identifier_list : EXCEPTION [RENAMES name]
  // Check for renaming
  if (Parser_Match (p, TK_EXCEPTION)) {
    if (Parser_Match (p, TK_RENAMES)) {
      node->kind = NK_EXCEPTION_RENAMING;
      node->exception_decl.names = node->object_decl.names;
      node->exception_decl.renamed = Parse_Name (p);
    } else {
      node->kind = NK_EXCEPTION_DECL;
      node->exception_decl.names = node->object_decl.names;
    }
    return node;
  }
  node->object_decl.is_aliased = Parser_Match (p, TK_ACCESS);  // ALIASED uses ACCESS token?
  node->object_decl.is_constant = Parser_Match (p, TK_CONSTANT);

  // Named number (number declaration): identifier : CONSTANT := static_expression;                 
  // No type specified, goes directly to :=                                                         
  // Check for anonymous array type: ARRAY (...) OF ...                                             
  //                                                                                                
  if (not node->object_decl.is_constant or not Parser_At (p, TK_ASSIGN)) {
    if (Parser_At (p, TK_ARRAY)) {
      node->object_decl.object_type = Parse_Array_Type (p);
    } else {
      node->object_decl.object_type = Parse_Subtype_Indication (p);
    }
  }

  // Renames: X : T RENAMES Y
  if (Parser_Match (p, TK_RENAMES)) {
    node->object_decl.is_rename = true;
    node->object_decl.init = Parse_Name (p);
    return node;
  }

  // Initialization
  if (Parser_Match (p, TK_ASSIGN)) {
    node->object_decl.init = Parse_Expression (p);
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.12.2 Type Declaration                                                                         
//                                                                                                  
// Discriminants parameterize the type with values fixed when the object is created.                
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Discriminant_Part (Parser *p) {
  if (not Parser_Match (p, TK_LPAREN)) return NULL;
  Source_Location loc = Parser_Location (p);
  Syntax_Node *disc_list = Node_New (NK_BLOCK, loc);  // Container for discriminants
  do {
    Source_Location d_loc = Parser_Location (p);
    Syntax_Node *disc = Node_New (NK_DISCRIMINANT_SPEC, d_loc);

    // Name list
    do {
      Syntax_Node *id = Node_New (NK_IDENTIFIER, Parser_Location (p));
      id->string_val.text = Parser_Identifier (p);
      Node_List_Push (&disc->discriminant.names, id);
    } while (Parser_Match (p, TK_COMMA));
    Parser_Expect (p, TK_COLON);
    disc->discriminant.disc_type = Parse_Subtype_Indication (p);
    if (Parser_Match (p, TK_ASSIGN)) {
      disc->discriminant.default_expr = Parse_Expression (p);
    }
    Node_List_Push (&disc_list->block_stmt.declarations, disc);
  } while (Parser_Match (p, TK_SEMICOLON));
  Parser_Expect (p, TK_RPAREN);
  return disc_list;
}
Syntax_Node *Parse_Type_Declaration(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_TYPE);
  Syntax_Node *node = Node_New (NK_TYPE_DECL, loc);
  node->type_decl.name = Parser_Identifier (p);

  // Discriminant part
  if (Parser_At (p, TK_LPAREN)) {
    Syntax_Node *discs = Parse_Discriminant_Part (p);
    if (discs) {
      node->type_decl.discriminants = discs->block_stmt.declarations;
    }
  }

  // Incomplete type declaration
  if (Parser_Match (p, TK_SEMICOLON)) {
    return node;
  }
  Parser_Expect (p, TK_IS);
  node->type_decl.is_limited = Parser_Match (p, TK_LIMITED);
  node->type_decl.is_private = Parser_Match (p, TK_PRIVATE);
  if (not node->type_decl.is_private) {
    node->type_decl.definition = Parse_Type_Definition (p);
  }
  return node;
}
Syntax_Node *Parse_Subtype_Declaration(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_SUBTYPE);
  Syntax_Node *node = Node_New (NK_SUBTYPE_DECL, loc);
  node->type_decl.name = Parser_Identifier (p);
  Parser_Expect (p, TK_IS);
  node->type_decl.definition = Parse_Subtype_Indication (p);
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.12.3 Type Definitions                                                                         
//                                                                                                  
// Parsing establishes structure while elaboration establishes meaning.                             
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Enumeration_Type (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_LPAREN);
  Syntax_Node *node = Node_New (NK_ENUMERATION_TYPE, loc);
  do {
    Source_Location lit_loc = Parser_Location (p);
    Syntax_Node *lit = Node_New (NK_IDENTIFIER, lit_loc);
    if (Parser_At (p, TK_IDENTIFIER)) {
      lit->string_val.text = Parser_Identifier (p);
    } else if (Parser_At (p, TK_CHARACTER)) {
      lit->string_val.text = Slice_Duplicate (p->current_token.text);
      Parser_Advance (p);
    } else {
      Parser_Error_At_Current (p, "enumeration literal");
      break;
    }
    Node_List_Push (&node->enum_type.literals, lit);
  } while (Parser_Match (p, TK_COMMA));
  Parser_Expect (p, TK_RPAREN);
  return node;
}
Syntax_Node *Parse_Array_Type (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_ARRAY);
  Parser_Expect (p, TK_LPAREN);
  Syntax_Node *node = Node_New (NK_ARRAY_TYPE, loc);

  // Index types: can be discrete_subtype_indication or discrete_range
  do {
    Syntax_Node *idx = Parse_Discrete_Range (p);
    Node_List_Push (&node->array_type.indices, idx);
  } while (Parser_Match (p, TK_COMMA));

  // Determine if constrained based on what we parsed.                                              
  // An index is unconstrained if it's just a type mark (identifier/selected)                       
  // without a range constraint. A range or subtype_indication with constraint                      
  // means constrained.                                                                             
  //                                                                                                
  node->array_type.is_constrained = true;
  for (size_t i = 0; i < node->array_type.indices.count; i++) {
    Syntax_Node *idx = node->array_type.indices.items[i];

    // Just a type name without constraint = unconstrained
    if (idx->kind == NK_IDENTIFIER or idx->kind == NK_SELECTED) {
      node->array_type.is_constrained = false;
      break;
    }
  }
  Parser_Expect (p, TK_RPAREN);
  Parser_Expect (p, TK_OF);
  node->array_type.component_type = Parse_Subtype_Indication (p);
  return node;
}

// Parse discrete_range: can be subtype_indication or range
Syntax_Node *Parse_Discrete_Range(Parser *p) {
  Source_Location loc = Parser_Location (p);

  // Check if this starts with an integer literal (anonymous range)
  if (Parser_At (p, TK_INTEGER) or Parser_At (p, TK_CHARACTER)) {
    Syntax_Node *range = Node_New (NK_RANGE, loc);
    range->range.low = Parse_Expression (p);
    if (Parser_Match (p, TK_DOTDOT)) {
      range->range.high = Parse_Expression (p);
    }
    return range;
  }

  // Otherwise try to parse as name, then check for range or constraint
  Syntax_Node *name = Parse_Name (p);

  // Type RANGE low..high or Type RANGE <>
  if (Parser_Match (p, TK_RANGE)) {

    // Unconstrained - return the type mark; <> is consumed
    if (Parser_Match (p, TK_BOX)) {
      return name;
    }
    Syntax_Node *range = Node_New (NK_RANGE, loc);
    range->range.low = Parse_Expression (p);
    Parser_Expect (p, TK_DOTDOT);
    range->range.high = Parse_Expression (p);

    // Create subtype indication with range constraint
    Syntax_Node *ind = Node_New (NK_SUBTYPE_INDICATION, loc);
    ind->subtype_ind.subtype_mark = name;
    Syntax_Node *constraint = Node_New (NK_RANGE_CONSTRAINT, loc);
    constraint->range_constraint.range = range;
    ind->subtype_ind.constraint = constraint;
    return ind;
  }

  // Name is actually the low bound of a range
  if (Parser_Match (p, TK_DOTDOT)) {
    Syntax_Node *range = Node_New (NK_RANGE, loc);
    range->range.low = name;
    range->range.high = Parse_Expression (p);
    return range;
  }

  // Just a type name
  return name;
}

Syntax_Node *Parse_Record_Type (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_RECORD);
  Syntax_Node *node = Node_New (NK_RECORD_TYPE, loc);

  // NULL; as empty component statement (vs NULL RECORD which is parsed elsewhere)
  // Skip this check - NULL inside record body is handled in the loop below

  // Component list
  while (not Parser_At (p, TK_END) and not Parser_At (p, TK_CASE) and not Parser_At (p, TK_EOF)) {
    if (not Parser_Check_Progress (p)) break;

    // NULL; as empty component list
    if (Parser_At (p, TK_NULL)) {
      Parser_Advance (p);
      Parser_Expect (p, TK_SEMICOLON);
      continue;
    }
    Source_Location c_loc = Parser_Location (p);
    Syntax_Node *comp = Node_New (NK_COMPONENT_DECL, c_loc);

    // Component names
    do {
      Syntax_Node *id = Node_New (NK_IDENTIFIER, Parser_Location (p));
      id->string_val.text = Parser_Identifier (p);
      Node_List_Push (&comp->component.names, id);
    } while (Parser_Match (p, TK_COMMA));
    Parser_Expect (p, TK_COLON);
    comp->component.component_type = Parse_Subtype_Indication (p);
    if (Parser_Match (p, TK_ASSIGN)) {
      comp->component.init = Parse_Expression (p);
    }
    Node_List_Push (&node->record_type.components, comp);
    Parser_Expect (p, TK_SEMICOLON);
  }

  // Variant part
  if (Parser_At (p, TK_CASE)) {
    node->record_type.variant_part = Parse_Variant_Part (p);
  }
  Parser_Expect (p, TK_END);
  Parser_Expect (p, TK_RECORD);
  return node;
}
Syntax_Node *Parse_Variant_Part (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_CASE);
  Syntax_Node *node = Node_New (NK_VARIANT_PART, loc);
  node->variant_part.discriminant = Parser_Identifier (p);
  Parser_Expect (p, TK_IS);

  // Variants
  while (Parser_At (p, TK_WHEN)) {
    Source_Location v_loc = Parser_Location (p);
    Parser_Advance (p);
    Syntax_Node *variant = Node_New (NK_VARIANT, v_loc);

    // Choices - can be expressions, ranges, or OTHERS
    do {
      Node_List_Push (&variant->variant.choices, Parse_Choice (p));
    } while (Parser_Match (p, TK_BAR));
    Parser_Expect (p, TK_ARROW);

    // Components in this variant
    while (not Parser_At (p, TK_WHEN) and not Parser_At (p, TK_END) and
         not Parser_At (p, TK_CASE) and not Parser_At (p, TK_EOF)) {
      if (not Parser_Check_Progress (p)) break;

      // NULL; as empty component list in variant
      if (Parser_At (p, TK_NULL)) {
        Parser_Advance (p);
        Parser_Expect (p, TK_SEMICOLON);
        continue;
      }
      Source_Location c_loc = Parser_Location (p);
      Syntax_Node *comp = Node_New (NK_COMPONENT_DECL, c_loc);
      do {
        Syntax_Node *id = Node_New (NK_IDENTIFIER, Parser_Location (p));
        id->string_val.text = Parser_Identifier (p);
        Node_List_Push (&comp->component.names, id);
      } while (Parser_Match (p, TK_COMMA));
      Parser_Expect (p, TK_COLON);
      comp->component.component_type = Parse_Subtype_Indication (p);
      if (Parser_Match (p, TK_ASSIGN)) {
        comp->component.init = Parse_Expression (p);
      }
      Node_List_Push (&variant->variant.components, comp);
      Parser_Expect (p, TK_SEMICOLON);
    }

    // Nested variant part
    if (Parser_At (p, TK_CASE)) {
      variant->variant.variant_part = Parse_Variant_Part (p);
    }
    Node_List_Push (&node->variant_part.variants, variant);
  }
  Parser_Expect (p, TK_END);
  Parser_Expect (p, TK_CASE);
  Parser_Expect (p, TK_SEMICOLON);
  return node;
}
Syntax_Node *Parse_Access_Type(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_ACCESS);
  Syntax_Node *node = Node_New (NK_ACCESS_TYPE, loc);
  node->access_type.is_constant = Parser_Match (p, TK_CONSTANT);
  node->access_type.designated = Parse_Subtype_Indication (p);
  return node;
}
Syntax_Node *Parse_Derived_Type(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_NEW);
  Syntax_Node *node = Node_New (NK_DERIVED_TYPE, loc);
  node->derived_type.parent_type = Parse_Subtype_Indication (p);

  // Parse_Subtype_Indication may have incorrectly consumed DIGITS/DELTA                            
  // constraints as part of the parent type. For derived types, these                               
  // constraints belong to the derived type definition, not the parent.                             
  // Extract them if present.                                                                       
  //                                                                                                
  Syntax_Node *parent = node->derived_type.parent_type;
  if (parent and parent->kind == NK_SUBTYPE_INDICATION and
    parent->subtype_ind.constraint) {
    Syntax_Node *c = parent->subtype_ind.constraint;

    // Move constraint from parent to derived type
    if (c->kind == NK_DIGITS_CONSTRAINT or c->kind == NK_DELTA_CONSTRAINT) {
      Syntax_Node *constraint = Node_New (NK_REAL_TYPE, c->location);
      if (c->kind == NK_DIGITS_CONSTRAINT) {
        constraint->real_type.precision = c->digits_constraint.digits_expr;
        constraint->real_type.range = c->digits_constraint.range;
      } else {
        constraint->real_type.delta = c->delta_constraint.delta_expr;
        constraint->real_type.range = c->delta_constraint.range;
      }
      node->derived_type.constraint = constraint;

      // Remove constraint from parent - use just the subtype mark
      node->derived_type.parent_type = parent->subtype_ind.subtype_mark;
    }
  }

  // Handle any remaining accuracy constraint (DIGITS or DELTA).                                    
  // Ada RM 3.5.7: derived_type_definition ::=                                                      
  //   new subtype_indication [accuracy_constraint] [range_constraint]                              
  // accuracy_constraint ::= DIGITS expression | DELTA expression                                   
  // Parse as NK_REAL_TYPE to reuse real type constraint handling                                   
  //                                                                                                
  if (Parser_At (p, TK_DIGITS) or Parser_At (p, TK_DELTA)) {
    Syntax_Node *constraint = Node_New (NK_REAL_TYPE, Parser_Location (p));
    if (Parser_Match (p, TK_DIGITS)) {
      constraint->real_type.precision = Parse_Expression (p);
    } else {
      Parser_Advance (p);  // Skip DELTA
      constraint->real_type.delta = Parse_Expression (p);
    }
    if (Parser_Match (p, TK_RANGE)) {
      constraint->real_type.range = Parse_Range (p);
    }
    node->derived_type.constraint = constraint;
  } else if (Parser_At (p, TK_RANGE) or Parser_At (p, TK_LPAREN)) {
    node->derived_type.constraint = Parse_Subtype_Indication (p);
  }
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.13.2 Procedure/Function Specification                                                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Procedure_Specification(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_PROCEDURE);
  Syntax_Node *node = Node_New (NK_PROCEDURE_SPEC, loc);

  // Name (can be identifier or operator string)
  if (Parser_At (p, TK_STRING)) {
    node->subprogram_spec.name = Slice_Duplicate (p->current_token.text);
    Parser_Advance (p);
  } else {
    node->subprogram_spec.name = Parser_Identifier (p);
  }
  Parse_Parameter_List (p, &node->subprogram_spec.parameters);
  return node;
}
Syntax_Node *Parse_Function_Specification(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_FUNCTION);
  Syntax_Node *node = Node_New (NK_FUNCTION_SPEC, loc);

  // Name
  if (Parser_At (p, TK_STRING)) {
    node->subprogram_spec.name = Slice_Duplicate (p->current_token.text);
    Parser_Advance (p);
  } else {
    node->subprogram_spec.name = Parser_Identifier (p);
  }
  Parse_Parameter_List (p, &node->subprogram_spec.parameters);
  Parser_Expect (p, TK_RETURN);
  node->subprogram_spec.return_type = Parse_Subtype_Indication (p);
  return node;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §9.13.3 Subprogram Body                                                                          
//                                                                                                  
// Declarations, then BEGIN, then statements. The structure is invariant.                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Parse_Subprogram_Body (Parser *p, Syntax_Node *spec) {
  Source_Location loc = spec ? spec->location : Parser_Location (p);
  bool is_function = spec and spec->kind == NK_FUNCTION_SPEC;
  Syntax_Node *node = Node_New (is_function ? NK_FUNCTION_BODY : NK_PROCEDURE_BODY, loc);
  node->subprogram_body.specification = spec;
  Parser_Expect (p, TK_IS);

  // Check for SEPARATE
  if (Parser_Match (p, TK_SEPARATE)) {
    node->subprogram_body.is_separate = true;
    return node;
  }
  Parse_Declarative_Part (p, &node->subprogram_body.declarations);
  Parser_Expect (p, TK_BEGIN);
  Parse_Statement_Sequence (p, &node->subprogram_body.statements);
  if (Parser_Match (p, TK_EXCEPTION)) {
    while (Parser_At (p, TK_WHEN)) {
      Source_Location h_loc = Parser_Location (p);
      Parser_Advance (p);
      Syntax_Node *handler = Node_New (NK_EXCEPTION_HANDLER, h_loc);
      do {
        if (Parser_Match (p, TK_OTHERS)) {
          Node_List_Push (&handler->handler.exceptions, Node_New (NK_OTHERS, h_loc));
        } else {
          Node_List_Push (&handler->handler.exceptions, Parse_Name (p));
        }
      } while (Parser_Match (p, TK_BAR));
      Parser_Expect (p, TK_ARROW);
      Parse_Statement_Sequence (p, &handler->handler.statements);
      Node_List_Push (&node->subprogram_body.handlers, handler);
    }
  }
  Parser_Expect (p, TK_END);

  // Check end name - handle both identifier and operator string
  if (spec and (Parser_At (p, TK_IDENTIFIER) or Parser_At (p, TK_STRING))) {
    Parser_Check_End_Name (p, spec->subprogram_spec.name);
  }
  return node;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.14 Package Declarations and Bodies                                                            
// ═════════════════════════════════════════════════════════════════════════════════════════════════


Syntax_Node *Parse_Package_Specification(Parser *p) {

  // Note: caller must consume TK_PACKAGE before calling
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node = Node_New (NK_PACKAGE_SPEC, loc);
  node->package_spec.name = Parser_Identifier (p);
  Parser_Expect (p, TK_IS);

  // Visible declarations
  Parse_Declarative_Part (p, &node->package_spec.visible_decls);

  // Private part
  if (Parser_Match (p, TK_PRIVATE)) {
    Parse_Declarative_Part (p, &node->package_spec.private_decls);
  }
  Parser_Expect (p, TK_END);
  if (Parser_At (p, TK_IDENTIFIER)) {
    Parser_Check_End_Name (p, node->package_spec.name);
  }
  return node;
}
Syntax_Node *Parse_Package_Body (Parser *p) {

  // Note: caller must consume TK_PACKAGE and TK_BODY before calling
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node = Node_New (NK_PACKAGE_BODY, loc);
  node->package_body.name = Parser_Identifier (p);
  Parser_Expect (p, TK_IS);

  // Check for SEPARATE
  if (Parser_Match (p, TK_SEPARATE)) {
    node->package_body.is_separate = true;
    return node;
  }
  Parse_Declarative_Part (p, &node->package_body.declarations);
  if (Parser_Match (p, TK_BEGIN)) {
    Parse_Statement_Sequence (p, &node->package_body.statements);
    if (Parser_Match (p, TK_EXCEPTION)) {
      while (Parser_At (p, TK_WHEN)) {
        Source_Location h_loc = Parser_Location (p);
        Parser_Advance (p);
        Syntax_Node *handler = Node_New (NK_EXCEPTION_HANDLER, h_loc);
        do {
          if (Parser_Match (p, TK_OTHERS)) {
            Node_List_Push (&handler->handler.exceptions, Node_New (NK_OTHERS, h_loc));
          } else {
            Node_List_Push (&handler->handler.exceptions, Parse_Name (p));
          }
        } while (Parser_Match (p, TK_BAR));
        Parser_Expect (p, TK_ARROW);
        Parse_Statement_Sequence (p, &handler->handler.statements);
        Node_List_Push (&node->package_body.handlers, handler);
      }
    }
  }
  Parser_Expect (p, TK_END);
  if (Parser_At (p, TK_IDENTIFIER)) {
    Parser_Check_End_Name (p, node->package_body.name);
  }
  return node;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.15 Generic Units                                                                              
// ═════════════════════════════════════════════════════════════════════════════════════════════════

void Parse_Generic_Formal_Part (Parser *p, Node_List *formals) {
  while (not Parser_At (p, TK_PROCEDURE) and not Parser_At (p, TK_FUNCTION) and
       not Parser_At (p, TK_PACKAGE) and not Parser_At (p, TK_EOF)) {
    if (not Parser_Check_Progress (p)) break;
    Source_Location loc = Parser_Location (p);

    // Generic type formal: type T[(discriminants)] is private | type T is (<>) | etc
    if (Parser_Match (p, TK_TYPE)) {
      Syntax_Node *formal = Node_New (NK_GENERIC_TYPE_PARAM, loc);

      // Parse type name
      formal->generic_type_param.name = Parser_Identifier (p);

      // Parse optional discriminant part: (discriminant_spec {; discriminant_spec})
      // RM 12.1.2: known discriminants for formal private types.
      if (Parser_At (p, TK_LPAREN)) {
        Syntax_Node *discs = Parse_Discriminant_Part (p);
        if (discs) {
          formal->generic_type_param.discriminants = discs->block_stmt.declarations;
        }
      }
      Parser_Expect (p, TK_IS);

      // Parse type definition form
      if (Parser_Match (p, TK_LIMITED)) {
        Parser_Expect (p, TK_PRIVATE);
        formal->generic_type_param.def_kind = GEN_DEF_LIMITED_PRIVATE;
      } else if (Parser_Match (p, TK_PRIVATE)) {
        formal->generic_type_param.def_kind = GEN_DEF_PRIVATE;

      // (<>) for discrete types
      } else if (Parser_Match (p, TK_LPAREN)) {
        Parser_Expect (p, TK_BOX);
        Parser_Expect (p, TK_RPAREN);
        formal->generic_type_param.def_kind = GEN_DEF_DISCRETE;

      // range <> for integer types
      } else if (Parser_Match (p, TK_RANGE)) {
        Parser_Expect (p, TK_BOX);
        formal->generic_type_param.def_kind = GEN_DEF_INTEGER;

      // digits <> for float types
      } else if (Parser_Match (p, TK_DIGITS)) {
        Parser_Expect (p, TK_BOX);
        formal->generic_type_param.def_kind = GEN_DEF_FLOAT;

      // delta <> for fixed types
      } else if (Parser_Match (p, TK_DELTA)) {
        Parser_Expect (p, TK_BOX);
        formal->generic_type_param.def_kind = GEN_DEF_FIXED;

      // array (index {, index}) of element_type for array types
      } else if (Parser_Match (p, TK_ARRAY)) {
        formal->generic_type_param.def_kind = GEN_DEF_ARRAY;
        Parser_Expect (p, TK_LPAREN);

        // Parse index subtypes: subtype_mark [RANGE <>] or discrete_range
        do {
          Parse_Name (p);  // index subtype mark
          if (Parser_Match (p, TK_RANGE)) {
            if (Parser_At (p, TK_BOX)) {
              Parser_Advance (p);  // <> for unconstrained

            // subtype_mark RANGE low..high (constrained)
            } else {
              Parse_Expression (p);  // low
              Parser_Expect (p, TK_DOTDOT);
              Parse_Expression (p);  // high
            }
          }

          // else: just a subtype mark as index (constrained)
        } while (Parser_Match (p, TK_COMMA));
        Parser_Expect (p, TK_RPAREN);
        Parser_Expect (p, TK_OF);
        formal->generic_type_param.def_detail = Parse_Subtype_Indication (p);

      // access type_name for access types
      } else if (Parser_Match (p, TK_ACCESS)) {
        formal->generic_type_param.def_kind = GEN_DEF_ACCESS;
        formal->generic_type_param.def_detail = Parse_Subtype_Indication (p);

      // new parent_type for derived types - skip NEW, parse parent
      } else if (Parser_At (p, TK_NEW)) {
        Parser_Advance (p);
        formal->generic_type_param.def_kind = GEN_DEF_DERIVED;
        formal->generic_type_param.def_detail = Parse_Subtype_Indication (p);

      // Unknown form - error recovery: skip to semicolon
      } else {
        Report_Error (formal->location, "unrecognized generic type definition form");
        formal->generic_type_param.def_kind = GEN_DEF_PRIVATE;
        while (not Parser_At (p, TK_SEMICOLON) and not Parser_At (p, TK_EOF)) {
          Parser_Advance (p);
        }
      }
      Node_List_Push (formals, formal);
      Parser_Expect (p, TK_SEMICOLON);
      continue;
    }

    // Generic object formal: identifier_list : [mode] type [:= default]
    if (Parser_At (p, TK_IDENTIFIER)) {
      Syntax_Node *formal = Node_New (NK_GENERIC_OBJECT_PARAM, loc);

      // Parse identifier list
      do {
        Syntax_Node *id = Node_New (NK_IDENTIFIER, Parser_Location (p));
        id->string_val.text = Parser_Identifier (p);
        Node_List_Push (&formal->generic_object_param.names, id);
      } while (Parser_Match (p, TK_COMMA));
      Parser_Expect (p, TK_COLON);

      // Parse mode: IN (default), OUT, or IN OUT
      formal->generic_object_param.mode = GEN_MODE_IN;
      if (Parser_Match (p, TK_IN)) {
        if (Parser_Match (p, TK_OUT)) {
          formal->generic_object_param.mode = GEN_MODE_IN_OUT;
        } else {
          formal->generic_object_param.mode = GEN_MODE_IN;
        }
      } else if (Parser_Match (p, TK_OUT)) {
        formal->generic_object_param.mode = GEN_MODE_OUT;
      }

      // Parse subtype mark
      formal->generic_object_param.object_type = Parse_Subtype_Indication (p);

      // Parse optional default expression
      if (Parser_Match (p, TK_ASSIGN)) {
        formal->generic_object_param.default_expr = Parse_Expression (p);
      }
      Node_List_Push (formals, formal);
      Parser_Expect (p, TK_SEMICOLON);
      continue;
    }

    // Generic subprogram formal: WITH PROCEDURE/FUNCTION spec [IS name | IS <>]
    if (Parser_At (p, TK_WITH)) {
      Parser_Advance (p);  // consume WITH
      Syntax_Node *formal = Node_New (NK_GENERIC_SUBPROGRAM_PARAM, loc);
      if (Parser_Match (p, TK_PROCEDURE)) {
        formal->generic_subprog_param.is_function = false;
        formal->generic_subprog_param.name = Parser_Identifier (p);

        // Optional parameters
        if (Parser_At (p, TK_LPAREN)) {
          Parse_Parameter_List (p, &formal->generic_subprog_param.parameters);
        }
      } else if (Parser_Match (p, TK_FUNCTION)) {
        formal->generic_subprog_param.is_function = true;

        // Function name - can be identifier or operator string
        if (Parser_At (p, TK_STRING)) {
          formal->generic_subprog_param.name = Slice_Duplicate (p->current_token.text);
          Parser_Advance (p);
        } else {
          formal->generic_subprog_param.name = Parser_Identifier (p);
        }

        // Optional parameters
        if (Parser_At (p, TK_LPAREN)) {
          Parse_Parameter_List (p, &formal->generic_subprog_param.parameters);
        }

        // Return type
        Parser_Expect (p, TK_RETURN);
        formal->generic_subprog_param.return_type = Parse_Name (p);
      }

      // Optional default: IS name | IS <>
      if (Parser_Match (p, TK_IS)) {

        // IS <> means any matching subprogram
        if (Parser_Match (p, TK_BOX)) {
          formal->generic_subprog_param.default_box = true;

        // IS name means default to that subprogram
        } else {
          formal->generic_subprog_param.default_name = Parse_Name (p);
        }
      }
      Node_List_Push (formals, formal);
      Parser_Expect (p, TK_SEMICOLON);
      continue;
    }
    break;
  }
}
Syntax_Node *Parse_Generic_Declaration(Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_GENERIC);
  Syntax_Node *node = Node_New (NK_GENERIC_DECL, loc);
  Parse_Generic_Formal_Part (p, &node->generic_decl.formals);

  // The actual unit
  if (Parser_At (p, TK_PROCEDURE)) {
    node->generic_decl.unit = Parse_Procedure_Specification (p);
  } else if (Parser_At (p, TK_FUNCTION)) {
    node->generic_decl.unit = Parse_Function_Specification (p);
  } else if (Parser_At (p, TK_PACKAGE)) {
    Parser_Advance (p);  // consume PACKAGE
    node->generic_decl.unit = Parse_Package_Specification (p);
  } else {
    Report_Error (node->location, "expected PROCEDURE, FUNCTION, or PACKAGE after generic formals");
  }
  return node;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.16 Use and With Clauses                                                                       
// ═════════════════════════════════════════════════════════════════════════════════════════════════


Syntax_Node *Parse_Use_Clause (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_USE);
  Syntax_Node *node = Node_New (NK_USE_CLAUSE, loc);
  do {
    Node_List_Push (&node->use_clause.names, Parse_Name (p));
  } while (Parser_Match (p, TK_COMMA));
  return node;
}
Syntax_Node *Parse_With_Clause (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_WITH);
  Syntax_Node *node = Node_New (NK_WITH_CLAUSE, loc);
  do {
    Node_List_Push (&node->use_clause.names, Parse_Name (p));
  } while (Parser_Match (p, TK_COMMA));
  return node;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.19 Representation Clauses                                                                     
// ═════════════════════════════════════════════════════════════════════════════════════════════════


Syntax_Node *Parse_Representation_Clause (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Parser_Expect (p, TK_FOR);
  Syntax_Node *node = Node_New (NK_REPRESENTATION_CLAUSE, loc);

  // Parse: FOR entity_name'attribute USE expression;                                               
  //    or: FOR type_name USE RECORD ... END RECORD;                                                
  //    or: FOR type_name USE (enum_rep_list);                                                      
  //    or: FOR object_name USE AT address;                                                         
  //                                                                                                

  // Parse entity name (possibly qualified: T or T'ATTRIBUTE).
  // Parse_Name may consume the tick+attribute, producing NK_ATTRIBUTE.
  node->rep_clause.entity_name = Parse_Name (p);

  // If Parse_Name already consumed T'SIZE, decompose the NK_ATTRIBUTE
  // so rep_clause.entity_name = T (the prefix) and .attribute = SIZE
  if (node->rep_clause.entity_name and
    node->rep_clause.entity_name->kind == NK_ATTRIBUTE) {
    Syntax_Node *attr_node = node->rep_clause.entity_name;
    node->rep_clause.attribute = attr_node->attribute.name;
    node->rep_clause.entity_name = attr_node->attribute.prefix;

  // Fallback: tick not consumed by Parse_Name
  } else if (Parser_At (p, TK_TICK)) {
    Parser_Advance (p);
    if (Parser_At (p, TK_IDENTIFIER)) {
      node->rep_clause.attribute = p->current_token.text;
      Parser_Advance (p);
    }
  }
  Parser_Expect (p, TK_USE);

  // Check for different representation clause forms
  // Record representation clause: FOR T USE RECORD ... END RECORD;
  if (Parser_Match (p, TK_RECORD)) {
    node->rep_clause.is_record_rep = true;

    // Parse optional alignment: AT MOD alignment;
    if (Parser_Match (p, TK_AT)) {
      Parser_Expect (p, TK_MOD);
      node->rep_clause.expression = Parse_Expression (p);
      Parser_Expect (p, TK_SEMICOLON);
    }

    // Parse component clauses: component_name AT position RANGE first_bit..last_bit;
    while (not Parser_At (p, TK_END) and not Parser_At (p, TK_EOF)) {
      Syntax_Node *comp_clause = Node_New (NK_ASSOCIATION, Parser_Location (p));
      Node_List_Push (&comp_clause->association.choices, Parse_Name (p));
      Parser_Expect (p, TK_AT);
      comp_clause->association.expression = Parse_Expression (p);

      // Optional RANGE clause
      // bit_range is now part of the expression
      if (Parser_Match (p, TK_RANGE)) {
        Parse_Range (p);  // first_bit .. last_bit
      }
      Parser_Expect (p, TK_SEMICOLON);
      Node_List_Push (&node->rep_clause.component_clauses, comp_clause);
    }
    Parser_Expect (p, TK_END);
    Parser_Expect (p, TK_RECORD);

  // Enumeration representation: FOR T USE (A => 0, B => 1, ...);
  } else if (Parser_At (p, TK_LPAREN)) {
    node->rep_clause.is_enum_rep = true;
    Parser_Advance (p);  // consume (
    Parse_Association_List (p, &node->rep_clause.component_clauses);
    Parser_Expect (p, TK_RPAREN);

  // Address clause: FOR X USE AT address;
  } else if (Parser_Match (p, TK_AT)) {
    node->rep_clause.expression = Parse_Expression (p);

  // Attribute value: FOR T'SIZE USE 32;
  } else {
    node->rep_clause.expression = Parse_Expression (p);
  }
  return node;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.20 Declaration Dispatch                                                                       
// ═════════════════════════════════════════════════════════════════════════════════════════════════


Syntax_Node *Parse_Declaration (Parser *p) {
  Source_Location loc = Parser_Location (p);

  // Generic
  if (Parser_At (p, TK_GENERIC)) {
    Syntax_Node *generic = Parse_Generic_Declaration (p);
    Parser_Expect (p, TK_SEMICOLON);
    return generic;
  }

  // Procedure/Function - could be spec, body, or generic instantiation
  if (Parser_At (p, TK_PROCEDURE) or Parser_At (p, TK_FUNCTION)) {
    Token_Kind kind = p->current_token.kind;
    Parser_Advance (p);  // consume PROCEDURE/FUNCTION

    // Get the name - can be identifier or operator string for functions
    String_Slice name;
    if (Parser_At (p, TK_STRING)) {
      name = Slice_Duplicate (p->current_token.text);
      Parser_Advance (p);
    } else {
      name = Parser_Identifier (p);
    }

    // Check for generic instantiation: NAME IS NEW
    // Peek ahead to see if it's IS NEW
    if (Parser_At (p, TK_IS)) {
      Token saved = p->current_token;
      Lexer saved_lexer = p->lexer;
      Parser_Advance (p);  // consume IS
      if (Parser_At (p, TK_NEW)) {
        Parser_Advance (p);  // consume NEW

        // Create generic instantiation node
        Syntax_Node *node = Node_New (NK_GENERIC_INST, loc);
        node->generic_inst.unit_kind = kind;
        node->generic_inst.instance_name = name;

        // Parse the generic unit name
        node->generic_inst.generic_name = Parse_Simple_Name (p);

        // Generic actuals
        if (Parser_Match (p, TK_LPAREN)) {
          Parse_Association_List (p, &node->generic_inst.actuals);
          Parser_Expect (p, TK_RPAREN);
        }
        Parser_Expect (p, TK_SEMICOLON);
        return node;
      }

      // Not IS NEW - restore and parse as spec/body
      p->current_token = saved;
      p->lexer = saved_lexer;
    }

    // Parse parameters (if any) - Parse_Parameter_List handles the parens
    Node_List params = {0};
    if (Parser_At (p, TK_LPAREN)) {
      Parse_Parameter_List (p, &params);
    }

    // Create the spec node
    Syntax_Node *spec = Node_New (kind == TK_PROCEDURE ? NK_PROCEDURE_SPEC : NK_FUNCTION_SPEC, loc);
    spec->subprogram_spec.name = name;
    spec->subprogram_spec.parameters = params;

    // For functions, parse return type
    if (kind == TK_FUNCTION) {
      Parser_Expect (p, TK_RETURN);
      spec->subprogram_spec.return_type = Parse_Name (p);
    }

    // Check for subprogram renaming: PROCEDURE P RENAMES Q; or FUNCTION F RENAMES G;
    if (Parser_Match (p, TK_RENAMES)) {
      spec->kind = NK_SUBPROGRAM_RENAMING;
      spec->subprogram_spec.renamed = Parse_Name (p);
      Parser_Expect (p, TK_SEMICOLON);
      return spec;
    }

    // Check for body or just spec
    if (Parser_At (p, TK_IS)) {
      Syntax_Node *body = Parse_Subprogram_Body (p, spec);
      Parser_Expect (p, TK_SEMICOLON);
      return body;
    }

    // Just a specification
    Parser_Expect (p, TK_SEMICOLON);
    return spec;
  }

  // Package
  if (Parser_At (p, TK_PACKAGE)) {
    Parser_Advance (p);  // consume PACKAGE
    if (Parser_At (p, TK_BODY)) {
      Parser_Advance (p);  // consume BODY
      Syntax_Node *body = Parse_Package_Body (p);
      Parser_Expect (p, TK_SEMICOLON);
      return body;
    }

    // Check for package renaming: PACKAGE name RENAMES old_name;
    String_Slice pkg_name = Parser_Identifier (p);
    if (Parser_Match (p, TK_RENAMES)) {
      Syntax_Node *node = Node_New (NK_PACKAGE_RENAMING, loc);
      node->package_renaming.new_name = pkg_name;
      node->package_renaming.old_name = Parse_Name (p);
      Parser_Expect (p, TK_SEMICOLON);
      return node;
    }
    Parser_Expect (p, TK_IS);

    // Check for generic instantiation: PACKAGE name IS NEW generic_name
    if (Parser_Match (p, TK_NEW)) {
      Syntax_Node *node = Node_New (NK_GENERIC_INST, loc);
      node->generic_inst.unit_kind = TK_PACKAGE;
      node->generic_inst.instance_name = pkg_name;

      // Parse the generic unit name
      node->generic_inst.generic_name = Parse_Simple_Name (p);

      // Generic actuals
      if (Parser_Match (p, TK_LPAREN)) {
        Parse_Association_List (p, &node->generic_inst.actuals);
        Parser_Expect (p, TK_RPAREN);
      }
      Parser_Expect (p, TK_SEMICOLON);
      return node;
    }

    // Not a generic instantiation - parse as specification
    Syntax_Node *spec = Node_New (NK_PACKAGE_SPEC, loc);
    spec->package_spec.name = pkg_name;

    // Parse visible declarations (each declaration consumes its own semicolon)
    while (not Parser_At (p, TK_PRIVATE) and not Parser_At (p, TK_END) and not Parser_At (p, TK_EOF)) {
      if (not Parser_Check_Progress (p)) break;
      Syntax_Node *decl = Parse_Declaration (p);
      Node_List_Push (&spec->package_spec.visible_decls, decl);
    }

    // Parse private declarations
    if (Parser_Match (p, TK_PRIVATE)) {
      while (not Parser_At (p, TK_END) and not Parser_At (p, TK_EOF)) {
        if (not Parser_Check_Progress (p)) break;
        Syntax_Node *decl = Parse_Declaration (p);
        Node_List_Push (&spec->package_spec.private_decls, decl);
      }
    }
    Parser_Expect (p, TK_END);
    if (Parser_At (p, TK_IDENTIFIER)) {
      Parser_Check_End_Name (p, spec->package_spec.name);
    }
    Parser_Expect (p, TK_SEMICOLON);
    return spec;
  }

  // Task declaration
  if (Parser_At (p, TK_TASK)) {
    Parser_Advance (p);  // consume TASK

    // TASK BODY name IS ...
    if (Parser_At (p, TK_BODY)) {
      Parser_Advance (p);  // consume BODY
      Source_Location t_loc = Parser_Location (p);
      Syntax_Node *node = Node_New (NK_TASK_BODY, t_loc);
      node->task_body.name = Parser_Identifier (p);
      Parser_Expect (p, TK_IS);
      if (Parser_Match (p, TK_SEPARATE)) {
        node->task_body.is_separate = true;
        Parser_Expect (p, TK_SEMICOLON);
        return node;
      }
      Parse_Declarative_Part (p, &node->task_body.declarations);
      Parser_Expect (p, TK_BEGIN);
      Parse_Statement_Sequence (p, &node->task_body.statements);
      if (Parser_Match (p, TK_EXCEPTION)) {
        while (Parser_At (p, TK_WHEN)) {
          Source_Location h_loc = Parser_Location (p);
          Parser_Advance (p);
          Syntax_Node *handler = Node_New (NK_EXCEPTION_HANDLER, h_loc);
          do {
            if (Parser_Match (p, TK_OTHERS)) {
              Node_List_Push (&handler->handler.exceptions, Node_New (NK_OTHERS, h_loc));
            } else {
              Node_List_Push (&handler->handler.exceptions, Parse_Name (p));
            }
          } while (Parser_Match (p, TK_BAR));
          Parser_Expect (p, TK_ARROW);
          Parse_Statement_Sequence (p, &handler->handler.statements);
          Node_List_Push (&node->task_body.handlers, handler);
        }
      }
      Parser_Expect (p, TK_END);
      if (Parser_At (p, TK_IDENTIFIER)) {
        Parser_Check_End_Name (p, node->task_body.name);
      }
      Parser_Expect (p, TK_SEMICOLON);
      return node;
    }

    // TASK [TYPE] name [IS ... END name];
    bool is_type = Parser_Match (p, TK_TYPE);
    Source_Location t_loc = Parser_Location (p);
    Syntax_Node *node = Node_New (NK_TASK_SPEC, t_loc);
    node->task_spec.name = Parser_Identifier (p);
    node->task_spec.is_type = is_type;

    // Task spec with entries
    if (Parser_Match (p, TK_IS)) {
      while (not Parser_At (p, TK_END) and not Parser_At (p, TK_EOF)) {
        if (not Parser_Check_Progress (p)) break;
        if (Parser_Match (p, TK_ENTRY)) {
          Source_Location e_loc = Parser_Location (p);
          Syntax_Node *entry = Node_New (NK_ENTRY_DECL, e_loc);
          entry->entry_decl.name = Parser_Identifier (p);

          // Entry may have family index: ENTRY name(index)                                         
          // and/or parameters: ENTRY name(...) or ENTRY name(index)(...)                           
          // Family index is a discrete_subtype_definition (like 1..10)                             
          // Parameters start with identifier : mode type                                           
          // Check if this is an entry family index or parameter list                               
          // Entry family: (discrete_range) like (1..10) or (T'RANGE)                               
          // Parameters: (id : mode type) - starts with identifier followed by :                    
          //                                                                                        
          if (Parser_At (p, TK_LPAREN)) {
            Token saved = p->current_token;
            Lexer saved_lexer = p->lexer;
            Parser_Advance (p);  // consume ( for lookahead
            bool is_family_index = false;

            // Not starting with identifier - must be family index
            if (not Parser_At (p, TK_IDENTIFIER)) {
              is_family_index = true;

            // Look ahead to see if it's id : (parameter) or just id (family)
            } else {
              Token saved2 = p->current_token;
              Lexer saved_lexer2 = p->lexer;
              Parser_Advance (p);  // past identifier
              is_family_index = not Parser_At (p, TK_COLON) and not Parser_At (p, TK_COMMA);
              p->current_token = saved2;
              p->lexer = saved_lexer2;
            }

            // Parse discrete subtype definition - already past (
            if (is_family_index) {
              Syntax_Node *range = Parse_Range (p);
              Node_List_Push (&entry->entry_decl.index_constraints, range);
              Parser_Expect (p, TK_RPAREN);

              // Optionally parse parameters after family index
              if (Parser_At (p, TK_LPAREN)) {
                Parse_Parameter_List (p, &entry->entry_decl.parameters);
              }

            // Restore and use Parse_Parameter_List which handles ( )
            } else {
              p->current_token = saved;
              p->lexer = saved_lexer;
              Parse_Parameter_List (p, &entry->entry_decl.parameters);
            }
          }
          Parser_Expect (p, TK_SEMICOLON);
          Node_List_Push (&node->task_spec.entries, entry);
        } else if (Parser_At (p, TK_PRAGMA)) {
          Node_List_Push (&node->task_spec.entries, Parse_Pragma (p));
          Parser_Expect (p, TK_SEMICOLON);

        // Representation clause in task spec
        } else if (Parser_At (p, TK_FOR)) {
          Node_List_Push (&node->task_spec.entries, Parse_Representation_Clause (p));
          Parser_Expect (p, TK_SEMICOLON);
        } else {
          Parser_Error (p, "expected ENTRY, PRAGMA, FOR, or END in task spec");
          Parser_Advance (p);
        }
      }
      Parser_Expect (p, TK_END);
      if (Parser_At (p, TK_IDENTIFIER)) {
        Parser_Check_End_Name (p, node->task_spec.name);
      }
    }
    Parser_Expect (p, TK_SEMICOLON);
    return node;
  }

  // Type declaration
  if (Parser_At (p, TK_TYPE)) {
    Syntax_Node *type_decl = Parse_Type_Declaration (p);

    // Incomplete type declaration (no definition, not private/limited) already consumed semicolon
    if (type_decl->type_decl.definition or type_decl->type_decl.is_private or
      type_decl->type_decl.is_limited) {
      Parser_Expect (p, TK_SEMICOLON);
    }
    return type_decl;
  }

  // Subtype declaration
  if (Parser_At (p, TK_SUBTYPE)) {
    Syntax_Node *subtype = Parse_Subtype_Declaration (p);
    Parser_Expect (p, TK_SEMICOLON);
    return subtype;
  }

  // Use clause
  if (Parser_At (p, TK_USE)) {
    Syntax_Node *use = Parse_Use_Clause (p);
    Parser_Expect (p, TK_SEMICOLON);
    return use;
  }

  // Pragma
  if (Parser_At (p, TK_PRAGMA)) {
    Syntax_Node *pragma = Parse_Pragma (p);
    Parser_Expect (p, TK_SEMICOLON);
    return pragma;
  }

  // FOR representation clause
  if (Parser_At (p, TK_FOR)) {
    Syntax_Node *rep = Parse_Representation_Clause (p);
    Parser_Expect (p, TK_SEMICOLON);
    return rep;
  }

  // Object or exception declaration
  if (Parser_At (p, TK_IDENTIFIER)) {
    Syntax_Node *obj = Parse_Object_Declaration (p);
    Parser_Expect (p, TK_SEMICOLON);
    return obj;
  }
  Parser_Error (p, "expected declaration");
  Parser_Synchronize (p);
  return Node_New (NK_NULL_STMT, loc);
}
void Parse_Declarative_Part (Parser *p, Node_List *list) {
  while (not Parser_At (p, TK_BEGIN) and not Parser_At (p, TK_END) and
       not Parser_At (p, TK_PRIVATE) and not Parser_At (p, TK_EOF)) {
    if (not Parser_Check_Progress (p)) break;
    Syntax_Node *decl = Parse_Declaration (p);
    Node_List_Push (list, decl);

    // Each declaration now consumes its own trailing semicolon
  }
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §9.21 Compilation Unit                                                                           
// ═════════════════════════════════════════════════════════════════════════════════════════════════

Syntax_Node *Parse_Context_Clause (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node = Node_New (NK_CONTEXT_CLAUSE, loc);
  while (Parser_At (p, TK_WITH) or Parser_At (p, TK_USE) or Parser_At (p, TK_PRAGMA)) {
    if (Parser_At (p, TK_WITH)) {
      Node_List_Push (&node->context.with_clauses, Parse_With_Clause (p));
      Parser_Expect (p, TK_SEMICOLON);
    } else if (Parser_At (p, TK_USE)) {
      Node_List_Push (&node->context.use_clauses, Parse_Use_Clause (p));
      Parser_Expect (p, TK_SEMICOLON);
    } else if (Parser_At (p, TK_PRAGMA)) {
      Parse_Pragma (p);  // Configuration pragmas
      Parser_Expect (p, TK_SEMICOLON);
    }
  }
  return node;
}
Syntax_Node *Parse_Compilation_Unit (Parser *p) {
  Source_Location loc = Parser_Location (p);
  Syntax_Node *node = Node_New (NK_COMPILATION_UNIT, loc);
  node->compilation_unit.context = Parse_Context_Clause (p);

  // Handle trailing pragmas at end of file (no more library units)
  if (Parser_At (p, TK_EOF)) {
    node->compilation_unit.unit = NULL;
    return node;
  }

  // Separate unit
  if (Parser_Match (p, TK_SEPARATE)) {
    Parser_Expect (p, TK_LPAREN);
    node->compilation_unit.separate_parent = Parse_Name (p);
    Parser_Expect (p, TK_RPAREN);

    // Parse the actual subunit below
  }

  // Main unit - Parse_Declaration now consumes its trailing semicolon
  node->compilation_unit.unit = Parse_Declaration (p);
  return node;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §10. TYPE SYSTEM - Ada Type Semantics                                                            
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §10.2.1 Frozen Composite Types List                                                              
//                                                                                                  
// Track composite types that need implicit equality operators.                                     
// These are added during Freeze_Type and processed during code generation.                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Type_Info *Frozen_Composite_Types[256];
uint32_t   Frozen_Composite_Count = 0;

// Global list of exception symbols for code generation
Symbol    *Exception_Symbols[256];
uint32_t   Exception_Symbol_Count = 0;

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §10.3 Type Construction                                                                          
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Type_Info *Type_New (Type_Kind kind, String_Slice name) {
  Type_Info *type_info  = Arena_Allocate (sizeof (Type_Info));
  type_info->kind       = kind;
  type_info->name       = name;
  type_info->size       = Default_Size_Bytes;
  type_info->alignment  = Default_Align_Bytes;
  return type_info;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §10.4 Type Predicates                                                                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

bool Type_Is_Scalar (const Type_Info *type_info) {
  return type_info and type_info->kind >= TYPE_BOOLEAN and type_info->kind <= TYPE_FIXED;
}
bool Type_Is_Discrete (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_BOOLEAN or type_info->kind == TYPE_CHARACTER or
         type_info->kind == TYPE_INTEGER or type_info->kind == TYPE_MODULAR or
         type_info->kind == TYPE_ENUMERATION);
}
bool Type_Is_Numeric (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_INTEGER or type_info->kind == TYPE_MODULAR or
         type_info->kind == TYPE_FLOAT or type_info->kind == TYPE_FIXED or
         type_info->kind == TYPE_UNIVERSAL_INTEGER or type_info->kind == TYPE_UNIVERSAL_REAL);
}
bool Type_Is_Real (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_FLOAT or type_info->kind == TYPE_FIXED or
         type_info->kind == TYPE_UNIVERSAL_REAL);
}

// Type uses floating-point LLVM representation (for codegen, not semantic analysis).
// Fixed-point types are NOT included as they use integer representation.
bool Type_Is_Float_Representation (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_FLOAT or type_info->kind == TYPE_UNIVERSAL_REAL);
}
bool Type_Is_Array_Like (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_ARRAY or type_info->kind == TYPE_STRING);
}
bool Type_Is_Composite (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_ARRAY or type_info->kind == TYPE_RECORD or
         type_info->kind == TYPE_STRING);
}
bool Type_Is_Access (const Type_Info *type_info) {
  return type_info and type_info->kind == TYPE_ACCESS;
}

// Per GNAT sem_util.ads - systematic predicates for every type class
bool Type_Is_Record (const Type_Info *type_info)    { return type_info and type_info->kind == TYPE_RECORD; }
bool Type_Is_Task (const Type_Info *type_info)      { return type_info and type_info->kind == TYPE_TASK; }
bool Type_Is_Float (const Type_Info *type_info)     { return type_info and type_info->kind == TYPE_FLOAT; }
bool Type_Is_Fixed_Point (const Type_Info *type_info) { return type_info and type_info->kind == TYPE_FIXED; }

// Derive LLVM float type string from a Type_Info.
// Falls back to "double" for UNIVERSAL_REAL or when type info is unavailable.
const char *Float_Llvm_Type_Of (const Type_Info *type_info) {
  if (type_info and type_info->size > 0)
    return Llvm_Float_Type ((uint32_t)To_Bits (type_info->size));
  return "double";  // UNIVERSAL_REAL / unknown > 64-bit
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// IEEE 754 Named Constants - replaces magic numbers throughout codegen.                            
// Single source of truth for float/double structural parameters.                                   
// ─────────────────────────────────────────────────────────────────────────────────────────────────

#define IEEE_FLOAT_DIGITS       6
#define IEEE_DOUBLE_DIGITS      15
#define IEEE_FLOAT_MANTISSA     24
#define IEEE_DOUBLE_MANTISSA    53
#define IEEE_FLOAT_EMAX         128
#define IEEE_DOUBLE_EMAX        1024
#define IEEE_FLOAT_EMIN  (-125)
#define IEEE_DOUBLE_EMIN (-1021)
#define IEEE_MACHINE_RADIX      2
#define IEEE_DOUBLE_MIN_NORMAL  2.2250738585072014e-308  // 2^(-1022)
#define IEEE_FLOAT_MIN_NORMAL   1.1754943508222875e-38  // 2^(-126)
#define LOG2_OF_10              3.321928094887362

// Check whether a float type maps to IEEE single precision (32-bit).
bool Float_Is_Single (const Type_Info *type_info) {
  return strcmp (Float_Llvm_Type_Of (type_info), "float") == 0;
}

// Resolve effective DIGITS for a float type: uses declared digits if > 0,
// else defaults from IEEE precision.
int Float_Effective_Digits (const Type_Info *type_info) {
  if (type_info and type_info->flt.digits > 0) return type_info->flt.digits;
  return Float_Is_Single (type_info) ? IEEE_FLOAT_DIGITS : IEEE_DOUBLE_DIGITS;
}

// Compute model parameters for a floating-point type (RM 3.5.8).                                   
// mantissa = ceil (DIGITS * log2 (10)) + 1                                                         
// emax     = 4 * mantissa                                                                          
// Used by MANTISSA, EMAX, EPSILON, SMALL, LARGE attributes.                                        
//                                                                                                  
void Float_Model_Parameters (const Type_Info *type_info,
                                           int64_t        *out_mantissa,
                                           int64_t        *out_emax) {
  int     digits   = Float_Effective_Digits (type_info);
  int64_t mantissa = (int64_t) ceil (digits * LOG2_OF_10) + 1;
  int64_t emax     = 4 * mantissa;
  if (out_mantissa) *out_mantissa = mantissa;
  if (out_emax)     *out_emax     = emax;
}
bool Type_Is_Private (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_PRIVATE or type_info->kind == TYPE_LIMITED_PRIVATE);
}
bool Type_Is_Limited (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_LIMITED_PRIVATE or type_info->kind == TYPE_TASK);
}
bool Type_Is_Integer_Like (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_INTEGER or type_info->kind == TYPE_MODULAR);
}

// Modular types are unsigned in Ada (RM 3.5.4).  This predicate drives
// codegen choices: zext vs sext, udiv vs sdiv, unsigned comparisons, etc.
bool Type_Is_Unsigned (const Type_Info *type_info) {
  return type_info and type_info->kind == TYPE_MODULAR;
}
bool Type_Is_Enumeration (const Type_Info *type_info) {
  return type_info and type_info->kind == TYPE_ENUMERATION;
}
bool Type_Is_Boolean (const Type_Info *type_info) { return type_info and type_info->kind == TYPE_BOOLEAN; }
bool Type_Is_Character (const Type_Info *type_info) { return type_info and type_info->kind == TYPE_CHARACTER; }
bool Type_Is_String (const Type_Info *type_info)  { return type_info and type_info->kind == TYPE_STRING; }

// Needs fat pointer { ptr, { bound, bound } } for unconstrained array or access thereto
bool Type_Needs_Fat_Pointer (const Type_Info *type_info) {
  if (not type_info) return false;

  // RM 3.10: Access to unconstrained array always needs fat pointer,                               
  // even when a subtype constraint narrows the bounds (e.g.,                                       
  // ACCESS ARR (1..N)).  Chase base_type to the root declaration.                                  
  //                                                                                                
  if (Type_Is_Access (type_info) and type_info->access.designated_type) {
    Type_Info *des = type_info->access.designated_type;
    while (des and Type_Is_Array_Like (des) and des->array.is_constrained
         and des->base_type and Type_Is_Array_Like (des->base_type))
      des = des->base_type;
    return Type_Is_Array_Like (des) and not des->array.is_constrained;
  }
  return Type_Is_Array_Like (type_info) and not type_info->array.is_constrained;
}

// Check if array type is unconstrained (needs fat pointer representation)
bool Type_Is_Unconstrained_Array (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_ARRAY or type_info->kind == TYPE_STRING) and
       not type_info->array.is_constrained;
}

// Constrained array: TYPE_ARRAY or TYPE_STRING with static bounds
bool Type_Is_Constrained_Array (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_ARRAY or type_info->kind == TYPE_STRING) and
       type_info->array.is_constrained;
}

// Universal numeric types: compile-time only, no storage
bool Type_Is_Universal_Integer (const Type_Info *type_info) {
  return type_info and type_info->kind == TYPE_UNIVERSAL_INTEGER;
}
bool Type_Is_Universal_Real (const Type_Info *type_info) {
  return type_info and type_info->kind == TYPE_UNIVERSAL_REAL;
}
bool Type_Is_Universal (const Type_Info *type_info) {
  return type_info and (type_info->kind == TYPE_UNIVERSAL_INTEGER or
         type_info->kind == TYPE_UNIVERSAL_REAL);
}

// Check if array type has dynamic bounds (BOUND_EXPR) that need runtime access.
// This includes constrained arrays like ARRAY (1..G) where G is a variable.
bool Type_Has_Dynamic_Bounds (const Type_Info *type_info) {
  if (not type_info or (type_info->kind != TYPE_ARRAY and type_info->kind != TYPE_STRING))
    return false;
  if (type_info->array.index_count == 0)
    return false;

  // Check if any bound is a runtime expression, either on the array                                
  // index entry itself or on the index type (e.g., ARRAY (SNI,..)                                  
  // where SNI has dynamic range -N..N).                                                            
  //                                                                                                
  for (uint32_t i = 0; i < type_info->array.index_count; i++) {
    if (type_info->array.indices[i].low_bound.kind == BOUND_EXPR or
      type_info->array.indices[i].high_bound.kind == BOUND_EXPR) {
      return true;
    }

    // Also check if the index type itself has dynamic bounds
    Type_Info *index_type = type_info->array.indices[i].index_type;
    if (index_type and (index_type->low_bound.kind == BOUND_EXPR or
            index_type->high_bound.kind == BOUND_EXPR)) {
      return true;
    }
  }
  return false;
}

// Check if an expression is a slice (NK_APPLY with NK_RANGE argument).                             
// Slices produce fat pointers at runtime even when their declared type                             
// is constrained, so they need special handling in comparisons.                                    
//                                                                                                  
bool Expression_Is_Slice (const Syntax_Node *node) {
  if (not node or node->kind != NK_APPLY) return false;
  for (uint32_t i = 0; i < node->apply.arguments.count; i++) {
    Syntax_Node *arg = node->apply.arguments.items[i];
    if (arg and arg->kind == NK_RANGE) return true;
  }
  return false;
}

// Check if an expression will produce a fat pointer value at runtime.                              
// This centralizes the "src_is_fat_ptr" detection pattern used in assignments                      
// and comparisons: STRING, unconstrained arrays, slices, and concatenations                        
// all produce fat pointer values { ptr, { bound, bound } }.                                        
//                                                                                                  
bool Expression_Produces_Fat_Pointer (const Syntax_Node *node,
                          const Type_Info *type) {

  // Operation-specific checks FIRST: these operations always produce fat                           
  // pointers at runtime regardless of the expression's declared type.                              
  // E.g., concatenation of two constrained STRINGs still builds { ptr, ptr }.                      
  // Aggregates return a fat pointer ALLOCA (ptr to { ptr, ptr }),                                  
  // not a loaded { ptr, ptr } value.  Callers that need the value                                  
  // must load from it.  Treat as "not fat" for the extractvalue                                    
  // callers - specific call sites (assignment, etc.) handle the                                    
  // alloca-based fat pointer specially.                                                            
  //                                                                                                
  if (node) {
    if (node->kind == NK_AGGREGATE)
      return false;

    // String literals always produce fat pointers
    if (node->kind == NK_STRING)
      return true;

    // Concatenation always returns fat pointer
    if (node->kind == NK_BINARY_OP and node->binary.op == TK_AMPERSAND)
      return true;

    // Slices always produce fat pointers even with constrained declared type
    if (Expression_Is_Slice (node))
      return true;
  }

  // Identifier-based check: if the node's own type has dynamic bounds or is
  // unconstrained, Generate_Identifier will load it as a fat pointer value.
  if (node and node->kind == NK_IDENTIFIER and node->type) {
    const Type_Info *node_type = node->type;
    if (Type_Has_Dynamic_Bounds (node_type) or Type_Is_Unconstrained_Array (node_type))
      return true;
  }

  // Type-based checks: constrained arrays with STATIC bounds are flat allocas.                     
  // Constrained arrays with DYNAMIC bounds (e.g., STRING (1..F(X))) are stored                     
  // as fat pointers because their bounds are runtime-determined (RM 3.6.1).                        
  //                                                                                                
  if (type and Type_Is_Constrained_Array (type) and not Type_Has_Dynamic_Bounds (type))
    return false;
  if (type and Type_Is_Constrained_Array (type) and Type_Has_Dynamic_Bounds (type))
    return true;
  if (type and (Type_Is_String (type) or Type_Is_Unconstrained_Array (type)))
    return true;
  return false;
}

// Check if a record field type requires loading as a fat pointer.                                  
// Unconstrained arrays, dynamic-bound arrays, and unconstrained STRING fields                      
// are stored as fat pointers { ptr, { bound, bound } } in records.                                 
// Constrained STRING subtypes (e.g., STRING (1..6)) are flat arrays.                               
//                                                                                                  
bool Type_Needs_Fat_Pointer_Load (const Type_Info *type_info) {
  if (not type_info) return false;

  // Constrained arrays with static bounds are flat - no fat pointer.
  if (Type_Is_Constrained_Array (type_info) and not Type_Has_Dynamic_Bounds (type_info))
    return false;

  // A constrained array with non-zero size has compile-time-known layout                           
  // even when bounds are stored as BOUND_EXPR (constant expressions like                           
  // OA = OTHER_ARRAY (2..4)).  Treat as static.                                                    
  //                                                                                                
  if (Type_Is_Constrained_Array (type_info) and type_info->size > 0)
    return false;
  if (Type_Is_String (type_info)) return true;
  if (type_info->kind == TYPE_ARRAY and
    (not type_info->array.is_constrained or Type_Has_Dynamic_Bounds (type_info)))
    return true;
  return false;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §10.5 Base Type Traversal                                                                        
//                                                                                                  
// Per RM 3.3.1: The base type of a type is the ultimate ancestor.                                  
// For subtypes, follow base_type links; for derived types, follow parent_type.                     
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Type_Info *Type_Base (Type_Info *type_info) {
  while (type_info and type_info->base_type) type_info = type_info->base_type;
  return type_info;
}

// Type_Root: Follow both base_type and parent_type chains to find the root                         
// ancestor type. This is used for derived type compatibility checking where                        
// we need to find the ultimate parent enumeration/integer type.                                    
//                                                                                                  
Type_Info *Type_Root (Type_Info *type_info) {
  while (type_info) {
    if (type_info->base_type) {
      type_info = type_info->base_type;
    } else if (type_info->parent_type) {
      type_info = type_info->parent_type;
    } else {
      break;
    }
  }
  return type_info;
}

// NOTE: Type compatibility checking is consolidated in Type_Covers ()                              
// defined in §11.6.2 (Overload Resolution section). That function provides                         
// coverage checking for:                                                                           
// - Same type identity                                                                             
// - Universal type compatibility                                                                   
// - Base type matching                                                                             
// - Array/string structural compatibility                                                          
// - Access type designated type compatibility                                                      
//                                                                                                  

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §10.6 Type Freezing                                                                              
//                                                                                                  
// Freezing determines the point at which a type's representation is fixed.                         
// The compiler must track what the RM permits but the programmer cannot see.                       
// Per RM 13.14:                                                                                    
// - Types are frozen by object declarations, bodies, end of declarative part                       
// - Subtypes freeze their base type                                                                
// - Composite types freeze their component types                                                   
// - Once frozen, size/alignment/layout cannot change                                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Forward declaration for Symbol
typedef struct Symbol Symbol;

// Freeze a type and all its dependencies
// Per RM 13.14: When a type is frozen, its representation is fixed
void Freeze_Type (Type_Info *type_info) {
  if (not type_info or type_info->is_frozen) return;

  // Mark as frozen first to prevent infinite recursion
  type_info->is_frozen = true;

  // Freeze base type if present
  if (type_info->base_type) {
    Freeze_Type (type_info->base_type);
  }

  // Freeze parent type for derived types
  if (type_info->parent_type) {
    Freeze_Type (type_info->parent_type);
  }

  // Freeze component types for composites
  switch (type_info->kind) {
    case TYPE_ARRAY:
    case TYPE_STRING:

      // Freeze element type
      if (type_info->array.element_type) {
        Freeze_Type (type_info->array.element_type);
      }

      // Freeze index types
      for (uint32_t i = 0; i < type_info->array.index_count; i++) {
        if (type_info->array.indices[i].index_type) {
          Freeze_Type (type_info->array.indices[i].index_type);
        }
      }
      break;
    case TYPE_RECORD:

      // Freeze all component types
      for (uint32_t i = 0; i < type_info->record.component_count; i++) {
        if (type_info->record.components[i].component_type) {
          Freeze_Type (type_info->record.components[i].component_type);
        }
      }
      break;
    case TYPE_ACCESS:

      // Access type freezing does NOT freeze designated type                                       
      // Per RM 13.14: "Freezing an access type does not freeze                                     
      // its designated subtype"                                                                    
      //                                                                                            
      break;
    default:
      break;
  }

  // Register composite types for implicit equality function generation
  // Per RM 4.5.2: Equality is predefined for all non-limited types
  if (Type_Is_Composite (type_info) and Frozen_Composite_Count < 256) {
    Frozen_Composite_Types[Frozen_Composite_Count++] = type_info;

    // Generate a unique function name for this type's equality
    char *name_buf = Arena_Allocate (64);
    snprintf (name_buf, 64, "_ada_eq_%.*s_%u",
              (int) (type_info->name.length > 20 ? 20 : type_info->name.length),
              type_info->name.data,
              Frozen_Composite_Count);
    type_info->equality_func_name = name_buf;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §10.7 LLVM Type Mapping                                                                          
//                                                                                                  
// The source type is semantic while the target type is representational.                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// File-scope map of generic formal>actual types for the current instance                           
// being code-generated.  Used by Type_To_Llvm to resolve generic formal                            
// types (TYPE_PRIVATE) to their actual types.                                                      
//                                                                                                  
Generic_Type_Map g_generic_type_map = {0};
const char *Type_To_Llvm (Type_Info *type_info) {
  if (not type_info) {
    fprintf (stderr, "error: Type_To_Llvm called with NULL type\n");
    return Llvm_Int_Type (64);  // ERROR path - caller bug
  }

  // For private/limited private/incomplete types, resolve through parent_type
  // chain to find the actual representation type (Ada RM 7.4.1, 3.4).
  if ((Type_Is_Private (type_info) or type_info->kind == TYPE_INCOMPLETE) and type_info->parent_type) {
    return Type_To_Llvm (type_info->parent_type);
  }

  // Unresolved private/limited private types without a full view (parent_type).                    
  // This occurs for generic formal type parameters whose actual type was not                       
  // propagated through expansion.  Resolve through the current generic                             
  // instance's actual type mapping (formal_name > actual_type).                                    
  //                                                                                                
  if (Type_Is_Private (type_info) and not type_info->parent_type) {
    if (g_generic_type_map.count > 0 and type_info->name.data) {
      for (uint32_t i = 0; i < g_generic_type_map.count; i++) {
        if (g_generic_type_map.mappings[i].actual_type and
          Slice_Equal_Ignore_Case (type_info->name,
                      g_generic_type_map.mappings[i].formal_name))
          return Type_To_Llvm (g_generic_type_map.mappings[i].actual_type);
      }
    }
    return "ptr";
  }
  switch (type_info->kind) {
    case TYPE_BOOLEAN:    return "i8";  // Boolean stored as i8, NOT i1
    case TYPE_CHARACTER:  return "i8";
    case TYPE_INTEGER:
    case TYPE_MODULAR:
    case TYPE_ENUMERATION:
    case TYPE_UNIVERSAL_INTEGER:
    case TYPE_FIXED:  // Fixed-point uses scaled integer representation
      return Llvm_Int_Type ((uint32_t)To_Bits (type_info->size));
    case TYPE_FLOAT:
    case TYPE_UNIVERSAL_REAL:
      return Llvm_Float_Type ((uint32_t)To_Bits (type_info->size));
    case TYPE_ACCESS:

      // Access to unconstrained array/STRING needs fat pointer { ptr, ptr }.                       
      // Chase through constrained subtypes to the root declaration so                              
      // ACCESS ARR (1..N) still uses fat pointer when ARR is unconstrained.                        
      // (RM 3.10, Standard convention)                                                             
      //                                                                                            
      if (type_info->access.designated_type) {
        Type_Info *designated = type_info->access.designated_type;
        while (designated and Type_Is_Array_Like (designated) and designated->array.is_constrained
             and designated->base_type and Type_Is_Array_Like (designated->base_type))
          designated = designated->base_type;
        if (Type_Is_String (designated) or Type_Is_Unconstrained_Array (designated)) {
          return FAT_PTR_TYPE;
        }
      }
      return "ptr";
    case TYPE_RECORD:
    case TYPE_TASK:
      return "ptr";
    case TYPE_ARRAY:

      // Unconstrained arrays use fat pointers { ptr, ptr }
      return (type_info->array.is_constrained) ? "ptr" : FAT_PTR_TYPE;
    case TYPE_STRING:

      // Unconstrained STRING > fat pointer { ptr, ptr }
      // Constrained STRING (e.g., STRING (1..6)) > ptr to flat array
      return (type_info->array.is_constrained) ? "ptr" : FAT_PTR_TYPE;
    default:
      fprintf (stderr, "error: Type_To_Llvm unhandled type kind %d for '%.*s'\n",
          type_info->kind, (int)type_info->name.length, type_info->name.data);
      return Llvm_Int_Type ((uint32_t)To_Bits (type_info->size));  // ERROR path - derive from size
  }
}

// Like Type_To_Llvm but for function signatures: constrained arrays with
// dynamic bounds are passed/returned as fat pointers {ptr, ptr}.
const char *Type_To_Llvm_Sig (Type_Info *type_info) {
  if (type_info and (type_info->kind == TYPE_ARRAY or type_info->kind == TYPE_STRING) and
    type_info->array.is_constrained and Type_Has_Dynamic_Bounds (type_info))
    return FAT_PTR_TYPE;
  return Type_To_Llvm (type_info);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §10.8 Standard-Style Fat Pointer Type Helpers                                                    
//                                                                                                  
// Standard uses native index types for array bounds in fat pointers.                               
// Instead of always i64, STRING (indexed by POSITIVE/INTEGER) uses i32,                            
// CHARACTER-indexed arrays use i8, etc.                                                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Get the native LLVM type for array bounds based on the index type.
// Every call site MUST supply an actual array/string/access-to-array type.
const char *Array_Bound_Llvm_Type (const Type_Info *type_info) {
  if (not type_info) {
    fprintf (stderr, "BUG: Array_Bound_Llvm_Type called with NULL\n");
    return STRING_BOUND_TYPE;  // safety net - default STRING bound type
  }

  // Access > designated type
  if (type_info->kind == TYPE_ACCESS and type_info->access.designated_type)
    type_info = type_info->access.designated_type;

  // Private/incomplete > parent
  if ((Type_Is_Private (type_info) or type_info->kind == TYPE_INCOMPLETE) and type_info->parent_type)
    return Array_Bound_Llvm_Type (type_info->parent_type);

  // STRING > derive bound type from index type (POSITIVE > INTEGER).
  // Standard style: Bound_Sub_GT from index subtype's base type.
  if (type_info->kind == TYPE_STRING) {
    if (type_info->array.index_count > 0 and type_info->array.indices and
      type_info->array.indices[0].index_type) {
      return Type_To_Llvm (type_info->array.indices[0].index_type);
    }
    return STRING_BOUND_TYPE;  // pre-init fallback only
  }
  if (type_info->kind != TYPE_ARRAY) {
    fprintf (stderr, "BUG: Array_Bound_Llvm_Type: non-array kind %d '%.*s'\n",
        type_info->kind, (int)type_info->name.length, type_info->name.data);
    return STRING_BOUND_TYPE;  // safety net - default STRING bound type
  }

  // Resolve from index_type - Standard style: use Bound_Sub_GT.                                    
  // For multi-dimensional arrays, return the WIDEST type across all                                
  // dimensions to avoid truncating bounds of wider index types.                                    
  // E.g., ARRAY (BOOLEAN, INTEGER RANGE ..) > use i32 not i8.                                      
  //                                                                                                
  if (type_info->array.index_count > 0 and type_info->array.indices and
    type_info->array.indices[0].index_type) {
    const char *widest = Type_To_Llvm (type_info->array.indices[0].index_type);
    uint32_t widest_sz = type_info->array.indices[0].index_type->size;
    for (uint32_t i = 1; i < type_info->array.index_count; i++) {
      if (type_info->array.indices[i].index_type and
        type_info->array.indices[i].index_type->size > widest_sz) {
        widest = Type_To_Llvm (type_info->array.indices[i].index_type);
        widest_sz = type_info->array.indices[i].index_type->size;
      }
    }
    return widest;
  }

  // No index type info - infer from array context.                                                 
  // This can happen for dynamically constrained arrays.                                            
  // Try to infer from bound values                                                                 
  //                                                                                                
  if (type_info->array.index_count > 0 and type_info->array.indices) {
    Type_Bound lb = type_info->array.indices[0].low_bound;
    Type_Bound hb = type_info->array.indices[0].high_bound;

    // Static bounds - use minimum width that fits
    if (lb.kind == BOUND_INTEGER and hb.kind == BOUND_INTEGER) {
      return Llvm_Int_Type (Bits_For_Range (lb.int_value, hb.int_value));
    }
  }

  // Last resort: INTEGER is the standard index type in Ada 83
  fprintf (stderr, "note: Array_Bound_Llvm_Type: no index type for '%.*s'\n",
      (int)type_info->name.length, type_info->name.data);
  return STRING_BOUND_TYPE;
}

// Get the LLVM bounds struct type string for a given bound type.                                   
// e.g., Bounds_Type_For ("i32") > "{ i32, i32 }".                                                  
// Used when allocating/loading/storing the bounds struct behind                                    
// the second pointer in a { ptr, ptr } fat pointer.                                                
//                                                                                                  
const char *Bounds_Type_For (const char *bound_type) {

  // Standard style: bounds struct uses the NATIVE index type.                                      
  // e.g. Bounds_Type_For ("i32") > "{ i32, i32 }"                                                  
  // See gnatllvm-arrays-create.adb:586-636.                                                        
  // Dispatch by bit width to avoid strcmp chain.                                                   
  //                                                                                                
  if (not bound_type or bound_type[0] != 'i') return STRING_BOUNDS_STRUCT;
  int bits = atoi (bound_type + 1);
  switch (bits) {
    case 8:   return "{ i8, i8 }";
    case 16:  return "{ i16, i16 }";
    case 32:  return "{ i32, i32 }";
    case 64:  return "{ i64, i64 }";
    case 128: return "{ i128, i128 }";
    default:  return STRING_BOUNDS_STRUCT;
  }
}

// Return the allocation size in bytes for a bounds struct.
// Used when allocating bounds on the secondary stack (for returned fat ptrs).
int Bounds_Alloc_Size (const char *bound_type) {

  // Size = 2 * (bits / 8).  E.g. { i32, i32 } = 8 bytes.
  if (not bound_type or bound_type[0] != 'i') return STRING_BOUNDS_ALLOC;
  int bits = atoi (bound_type + 1);
  return 2 * (bits / 8);
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §11. SYMBOL TABLE - Scoped Name Resolution                                                       
// ═════════════════════════════════════════════════════════════════════════════════════════════════

bool Param_Is_By_Reference (Parameter_Mode mode) {
  return mode == PARAM_OUT or mode == PARAM_IN_OUT;
}
// Populate the global type map from a generic instance's actuals.                                  
// Called when entering a generic instance codegen context so that                                  
// Type_To_Llvm can resolve formal private types to their actuals.                                  
//                                                                                                  
void Set_Generic_Type_Map (Symbol *inst) {
  g_generic_type_map.count = 0;
  if (not inst) return;

  // Walk up: subprogram inside generic package uses package's actuals
  Symbol *holder = inst;
  if (holder and not holder->generic_actuals and holder->parent and
    holder->parent->kind == SYMBOL_PACKAGE and holder->parent->generic_actuals)
    holder = holder->parent;
  if (not holder or not holder->generic_actuals) return;
  for (uint32_t i = 0; i < holder->generic_actual_count and i < 32; i++) {
    g_generic_type_map.mappings[i].formal_name =
      holder->generic_actuals[i].formal_name;
    g_generic_type_map.mappings[i].actual_type =
      holder->generic_actuals[i].actual_type;
    g_generic_type_map.count++;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Check_Is_Suppressed - GNAT-style check suppression query (RM 11.5)                               
//                                                                                                  
// Consults the suppressed_checks bitmask on:                                                       
//   1. The target type (if provided)                                                               
//   2. The target type's base_type (if different)                                                  
//   3. The symbol (if provided)                                                                    
// Returns true if the specified check_bit is suppressed at any level.                              
// ─────────────────────────────────────────────────────────────────────────────────────────────────

bool Check_Is_Suppressed (Type_Info *type, Symbol *sym, uint32_t check_bit) {
  if (type and (type->suppressed_checks & check_bit)) return true;
  if (type and type->base_type and (type->base_type->suppressed_checks & check_bit)) return true;
  if (sym and (sym->suppressed_checks & check_bit)) return true;
  return false;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.3 Scope Structure                                                                            
//                                                                                                  
// Each scope has its own hash table with 1024 buckets, which covers most programs.                 
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Symbol_Manager *sm;

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.4 Scope Operations                                                                           
//                                                                                                  
// Lexical scoping is a tree; visibility rules turn it into a forest.                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Scope *Scope_New (Scope *parent) {
  Scope *scope          = Arena_Allocate (sizeof (Scope));
  scope->parent         = parent;
  scope->nesting_level  = parent ? parent->nesting_level + 1 : 0;

  // Inherit frame_size from parent so nested scope variables get unique offsets.
  // This ensures variables in DECLARE blocks don't overlap with outer variables.
  scope->frame_size     = parent ? parent->frame_size : 0;
  return scope;
}
void Symbol_Manager_Push_Scope (Symbol *owner) {
  Scope *scope = Scope_New (sm->current_scope);
  scope->owner = owner;
  sm->current_scope = scope;
}
void Symbol_Manager_Pop_Scope (void) {

  // Propagate frame_size up to parent - parent needs to allocate enough
  // space for all variables, including those in nested blocks.
  if (sm->current_scope->parent) {
    if (sm->current_scope->frame_size > sm->current_scope->parent->frame_size) {
      sm->current_scope->parent->frame_size = sm->current_scope->frame_size;
    }

    // Propagate frame variables from child scope to parent scope.                                  
    // Variables in DECLARE blocks share the enclosing function's frame,                            
    // so nested functions need frame aliases for ALL variables, not just                           
    // those in the immediate parent scope. Only skip if this scope IS the                          
    // function's own body scope (i.e., child->owner->scope == child).                              
    // We use the separate frame_vars list to avoid polluting symbol lookup.                        
    //                                                                                              
    Scope *child = sm->current_scope;
    Scope *parent = child->parent;
    bool is_function_body_scope = (child->owner and
      (child->owner->kind == SYMBOL_FUNCTION or
       child->owner->kind == SYMBOL_PROCEDURE) and
      child->owner->scope == child);

    // This is a block scope (DECLARE/loop/etc), not a subprogram's own body scope.
    // Propagate its storage-bearing symbols to parent's frame_vars for alias generation.
    if (not is_function_body_scope) {
      for (uint32_t i = 0; i < child->symbol_count; i++) {
        Symbol *var = child->symbols[i];
        if (var and (var->kind == SYMBOL_VARIABLE or
              var->kind == SYMBOL_PARAMETER or
              var->kind == SYMBOL_DISCRIMINANT or
              (var->kind == SYMBOL_CONSTANT and not var->is_named_number))) {
          if (parent->frame_var_count >= parent->frame_var_capacity) {
            parent->frame_var_capacity = parent->frame_var_capacity ? parent->frame_var_capacity * 2 : 32;
            parent->frame_vars = realloc (parent->frame_vars,
              parent->frame_var_capacity * sizeof (Symbol *));
          }
          parent->frame_vars[parent->frame_var_count++] = var;
        }
      }

      // Also propagate any frame_vars that were already collected in the child
      // (from even deeper nested scopes).
      for (uint32_t i = 0; i < child->frame_var_count; i++) {
        if (parent->frame_var_count >= parent->frame_var_capacity) {
          parent->frame_var_capacity = parent->frame_var_capacity ? parent->frame_var_capacity * 2 : 32;
          parent->frame_vars = realloc (parent->frame_vars,
            parent->frame_var_capacity * sizeof (Symbol *));
        }
        parent->frame_vars[parent->frame_var_count++] = child->frame_vars[i];
      }
    }
    sm->current_scope = parent;
  }
}

// Push an existing scope (used for separate subunits to reuse parent's scope)
void Symbol_Manager_Push_Existing_Scope (Scope *scope) {
  if (scope) {
    scope->parent = sm->current_scope;
    sm->current_scope = scope;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.5 Symbol Table Operations                                                                    
// ─────────────────────────────────────────────────────────────────────────────────────────────────

uint32_t Symbol_Hash_Name (String_Slice name) {
  return (uint32_t)(Slice_Hash (name) % SYMBOL_TABLE_SIZE);
}
Symbol *Symbol_New (Symbol_Kind kind, String_Slice name, Source_Location location) {
  Symbol *sym      = Arena_Allocate (sizeof (Symbol));
  sym->kind        = kind;
  sym->name        = name;
  sym->location    = location;
  sym->visibility  = VIS_IMMEDIATELY_VISIBLE;
  return sym;
}
void Symbol_Add (Symbol *sym) {
  Scope    *scope    = sm->current_scope;
  uint32_t  hash     = Symbol_Hash_Name (sym->name);
  Symbol   *existing = scope->buckets[hash];

  // Check if symbol is already in this bucket (avoid self-cycle)
  while (existing) {
    if (existing == sym) return;  // Already added
    if (existing->defining_scope == scope and
      Slice_Equal_Ignore_Case (existing->name, sym->name)) {

      // Overloading: add to chain if subprograms or enumeration literals
      // Per RM 8.6, enumeration literals are overloadable like functions
      if ((existing->kind == SYMBOL_PROCEDURE or existing->kind == SYMBOL_FUNCTION or
         existing->kind == SYMBOL_LITERAL) and
        (sym->kind == SYMBOL_PROCEDURE or sym->kind == SYMBOL_FUNCTION or
         sym->kind == SYMBOL_LITERAL)) {

        // Check if sym is already in the overload chain (prevents cycles)
        Symbol *chain = existing;
        while (chain) {
          if (chain == sym) return;  // Already in chain
          chain = chain->next_overload;
        }

        // Only assign unique_id if not already set (symbol may have been
        // added to another scope already)
        if (sym->unique_id == 0)
          sym->unique_id = sm->next_unique_id++;
        sym->is_overloaded = true;
        existing->is_overloaded = true;  // Mark first decl as overloaded too
        sym->next_overload = existing->next_overload;
        existing->next_overload = sym;
        sym->parent = scope->owner;
        return;
      }

      // Allow variable to shadow type with same name (single task declaration).                    
      // Per RM 9.1, a single task declaration creates both a task type and                         
      // an anonymous object of that type with the same name. The object                            
      // shadows the type for normal name lookups.                                                  
      //                                                                                            
      if (sym->kind == SYMBOL_VARIABLE and existing->kind == SYMBOL_TYPE) {
        break;  // Proceed to add the variable - it will shadow the type
      }

      // Deferred constant completion (RM 7.4): update existing symbol's                            
      // declaration to the full declaration which has the initializer.                             
      // Also update the AST name nodes to point to the existing symbol                             
      // so code generation uses the same alloca for both.                                          
      //                                                                                            
      if (existing->kind == SYMBOL_CONSTANT and sym->kind == SYMBOL_CONSTANT
        and sym->declaration and sym->declaration->kind == NK_OBJECT_DECL
        and sym->declaration->object_decl.init) {
        existing->declaration = sym->declaration;
        if (sym->type) existing->type = sym->type;
        for (uint32_t ni = 0; ni < sym->declaration->object_decl.names.count; ni++) {
          Syntax_Node *nn = sym->declaration->object_decl.names.items[ni];
          if (nn) nn->symbol = existing;
        }
        return;
      }

      // Same symbol already exists at this scope - skip
      return;
    }
    existing = existing->next_in_bucket;
  }

  // Only assign unique_id if not already set
  if (sym->unique_id == 0)
    sym->unique_id = sm->next_unique_id++;
  sym->defining_scope  = scope;
  sym->nesting_level   = scope->nesting_level;
  sym->next_in_bucket  = scope->buckets[hash];
  scope->buckets[hash] = sym;

  // Set parent to enclosing package/subprogram for nested symbol support
  sym->parent = scope->owner;

  // Add to linear symbol list for enumeration (static link support)
  if (scope->symbol_count >= scope->symbol_capacity) {
    uint32_t new_cap = scope->symbol_capacity ? scope->symbol_capacity * 2 : 16;
    Symbol **new_syms = Arena_Allocate (new_cap * sizeof (Symbol*));
    if (scope->symbols) memcpy (new_syms, scope->symbols, scope->symbol_count * sizeof (Symbol*));
    scope->symbols = new_syms;
    scope->symbol_capacity = new_cap;
  }
  scope->symbols[scope->symbol_count++] = sym;

  // Track frame offset for variables/parameters/constants/discriminants.                           
  // Named numbers (is_named_number) have no storage - skip frame allocation                        
  // so their pre-set frame_offset (e.g. ASCII.DEL=127) is preserved.                               
  //                                                                                                
  if ((sym->kind == SYMBOL_VARIABLE or sym->kind == SYMBOL_PARAMETER or
     sym->kind == SYMBOL_CONSTANT or sym->kind == SYMBOL_DISCRIMINANT) and
    not (sym->kind == SYMBOL_CONSTANT and sym->is_named_number)) {
    sym->frame_offset = scope->frame_size;
    uint32_t var_size = sym->type ? sym->type->size : 8;

    // Fat pointers for dynamic/unconstrained arrays (or access thereto) need
    // { ptr, ptr } = 16 bytes, not just ptr = 8 bytes.
    if (sym->type and (Type_Has_Dynamic_Bounds (sym->type) or
               Type_Is_Unconstrained_Array (sym->type) or
               Type_Needs_Fat_Pointer (sym->type))) {
      var_size = FAT_PTR_ALLOC_SIZE;
      sym->needs_fat_ptr_storage = true;
    }
    if (var_size == 0) {
      fprintf (stderr, "warning: variable '%.*s' has zero size, defaulting to 8 bytes\n",
          (int)sym->name.length, sym->name.data);
      var_size = 8;
    }
    scope->frame_size += var_size;
  }
}

// Find symbol by name, searching enclosing scopes
Symbol *Symbol_Find (String_Slice name) {
  uint32_t hash = Symbol_Hash_Name (name);
  for (Scope *scope = sm->current_scope; scope; scope = scope->parent) {
    for (Symbol *sym = scope->buckets[hash]; sym; sym = sym->next_in_bucket) {
      if (Slice_Equal_Ignore_Case (sym->name, name) and
        sym->visibility >= VIS_IMMEDIATELY_VISIBLE) {
        return sym;
      }
    }
  }
  return NULL;
}

// Find symbol by name and type (for enumeration literal disambiguation)
Symbol *Symbol_Find_By_Type (String_Slice name, Type_Info *expected_type) {
  if (not expected_type) return Symbol_Find (name);

  // Get base type for matching (handles derived types and constrained subtypes).
  // Follow both parent_type (derived types) and base_type (constrained subtypes).
  Type_Info *base_expected = expected_type;
  while (base_expected) {
    if (base_expected->parent_type)
      base_expected = base_expected->parent_type;
    else if (base_expected->base_type)
      base_expected = base_expected->base_type;
    else
      break;
  }
  uint32_t hash = Symbol_Hash_Name (name);

  // Character literals are case-sensitive in Ada (RM 2.6), unlike identifiers.                     
  // Hash is case-insensitive so char lits with different case share a bucket.                      
  // Use case-insensitive at bucket level, case-sensitive in overload chain.                        
  //                                                                                                
  bool is_char_lit = (name.length >= 1 and name.data[0] == '\'');

  // Search all scopes for a matching symbol - don't stop at first name match,
  // keep searching if the type doesn't match (for enumeration literal overloading)
  for (Scope *scope = sm->current_scope; scope; scope = scope->parent) {
    for (Symbol *sym = scope->buckets[hash]; sym; sym = sym->next_in_bucket) {
      if (Slice_Equal_Ignore_Case (sym->name, name) and
        sym->visibility >= VIS_IMMEDIATELY_VISIBLE) {

        // Search through overload chain (for enumeration literals)
        // For character literals, require exact case match (RM 2.6)
        for (Symbol *ovl = sym; ovl; ovl = ovl->next_overload) {
          if (is_char_lit and not Slice_Equal (ovl->name, name)) continue;

          // Check if type matches (either directly or via base type)
          Type_Info *sym_base = ovl->type;
          while (sym_base) {
            if (sym_base->parent_type)
              sym_base = sym_base->parent_type;
            else if (sym_base->base_type)
              sym_base = sym_base->base_type;
            else
              break;
          }
          if (sym_base == base_expected) {
            return ovl;
          }
        }

        // Type didn't match in this scope - continue to parent scopes
      }
    }
  }
  return NULL;  // No matching symbol found
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §11.6 OVERLOAD RESOLUTION                                                                        
// ═════════════════════════════════════════════════════════════════════════════════════════════════

//    interpretation using disambiguation rules.                                                    
//                                                                                                  
// Key concepts:                                                                                    
// - Interp: Record of (Nam, Typ, Opnd_Typ) representing one interpretation                         
// - Covers: Type compatibility test (T1 covers T2 if T2's values are legal for T1)                 
// - Disambiguate: Select best interpretation when multiple are valid                               
//                                                                                                  
// Per RM 8.6: Overload resolution identifies the unique declaration for each                       
// identifier. It fails if no interpretation is valid or if multiple are valid.                     
//                                                                                                  

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.6.1 Interpretation Structure                                                                 
//                                                                                                  
// "type Interp is record Nam, Typ, Opnd_Typ..."                                                    
// We store interpretations in a contiguous array during resolution.                                
// Sixty-four interpretations suffices since deeper ambiguity signals a pathological program.       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.6.2 Type Covering (Compatibility)                                                            
//                                                                                                  
// For example: T1 covers T2 if values of T2 are legal where T1 is expected.                        
//                                                                                                  
// Key rules from RM 8.6:                                                                           
// - Same type: always covers                                                                       
// - Subtypes of same base type: cover each other                                                   
// - Universal types: Universal_Integer covers any integer type, etc.                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

bool Type_Covers (Type_Info *expected, Type_Info *actual) {

  // Null types are permissive (incomplete analysis)
  if (not expected or not actual) return true;

  // Same type always covers
  if (expected == actual) return true;

  // Universal_Integer covers any discrete type
  if (Type_Is_Universal_Integer (expected)) return Type_Is_Discrete (actual);
  if (Type_Is_Universal_Integer (actual))   return Type_Is_Discrete (expected);

  // Universal_Real covers any real type
  if (Type_Is_Universal_Real (expected)) return Type_Is_Real (actual);
  if (Type_Is_Universal_Real (actual))   return Type_Is_Real (expected);

  // Same base type covers
  Type_Info *base_exp = Type_Base (expected);
  Type_Info *base_act = Type_Base (actual);
  if (base_exp == base_act) return true;
  if (base_exp == actual or expected == base_act) return true;

  // For derived types, check if they share the same root type (RM 3.4).                            
  // This handles enumeration/integer literals from parent types being                              
  // compatible with derived types. E.g., if T is new PARENT, then                                  
  // enumeration literal E4 from PARENT is compatible with T.                                       
  //                                                                                                
  Type_Info *root_exp = Type_Root (expected);
  Type_Info *root_act = Type_Root (actual);
  if (root_exp and root_act and root_exp == root_act) return true;

  // SYSTEM.ADDRESS compatibility (RM 13.7): all ADDRESS types are interoperable                    
  // This handles the case where 'ADDRESS attribute returns a built-in ADDRESS                      
  // type but the target is declared as SYSTEM.ADDRESS from the package                             
  //                                                                                                
  if (Slice_Equal_Ignore_Case (expected->name, S("ADDRESS")) and
    Slice_Equal_Ignore_Case (actual->name, S("ADDRESS"))) {
    return true;
  }

  // Array/string compatibility: same structure
  // STRING is compatible with CHARACTER arrays
  if (Type_Is_Array_Like (expected) and Type_Is_Array_Like (actual)) {
    if (Type_Is_String (expected) or Type_Is_String (actual)) {
      return true;
    }

    // Arrays with same element type
    if (expected->array.element_type and actual->array.element_type) {
      return Type_Covers (expected->array.element_type,
                actual->array.element_type);
    }
    return true;
  }

  // Access types: check designated type compatibility
  if (Type_Is_Access (expected) and Type_Is_Access (actual)) {
    if (expected->access.designated_type and actual->access.designated_type) {
      return Type_Covers (expected->access.designated_type,
                actual->access.designated_type);
    }
    return true;
  }

  // NULL literal covers any access type
  if (Type_Is_Access (expected) and not actual) {
    return true;
  }

  // Enumeration types from generic instantiation: same name means compatible.                      
  // This handles the case where instantiation creates new type objects                             
  // that should be compatible with the original generic spec's types.                              
  //                                                                                                
  if (Type_Is_Enumeration (expected) and Type_Is_Enumeration (actual) and
    expected->name.data and actual->name.data and
    Slice_Equal_Ignore_Case (expected->name, actual->name)) {
    return true;
  }

  // Boolean types: both boolean types are always compatible
  if (Type_Is_Boolean (expected) and Type_Is_Boolean (actual)) {
    return true;
  }

  // Integer/derived types from generic instantiation: same name means compatible
  if (expected->kind == TYPE_INTEGER and actual->kind == TYPE_INTEGER and
    expected->name.data and actual->name.data and
    Slice_Equal_Ignore_Case (expected->name, actual->name)) {
    return true;
  }

  // Float/fixed-point types: same name means compatible (RM 12.3)
  if (Type_Is_Float (expected) and Type_Is_Float (actual) and
    expected->name.data and actual->name.data and
    Slice_Equal_Ignore_Case (expected->name, actual->name)) {
    return true;
  }
  if (Type_Is_Fixed_Point (expected) and Type_Is_Fixed_Point (actual) and
    expected->name.data and actual->name.data and
    Slice_Equal_Ignore_Case (expected->name, actual->name)) {
    return true;
  }

  // Character literals as enumeration literals (RM 3.5.1):                                         
  // An enumeration type can define character literals (e.g., TYPE T IS ('A', 'B');).               
  // When comparing, CHARACTER type should be compatible with such enumerations.                    
  // Check by looking for literals that start with single quote.                                    
  //                                                                                                
  {
    Type_Info *char_type = NULL;
    Type_Info *enum_type = NULL;
    if (Type_Is_Character (expected) and Type_Is_Enumeration (actual)) {
      char_type = expected;
      enum_type = actual;
    } else if (Type_Is_Character (actual) and Type_Is_Enumeration (expected)) {
      char_type = actual;
      enum_type = expected;
    }

    // Also check derived enumeration types via root
    if (not enum_type and Type_Is_Character (expected)) {
      Type_Info *act_root = Type_Root (actual);
      if (Type_Is_Enumeration (act_root)) {
        enum_type = act_root;
        char_type = expected;
      }
    }
    if (not enum_type and Type_Is_Character (actual)) {
      Type_Info *exp_root = Type_Root (expected);
      if (Type_Is_Enumeration (exp_root)) {
        enum_type = exp_root;
        char_type = actual;
      }
    }

    // Check if enum has any character literals
    if (char_type and enum_type and enum_type->enumeration.literals) {
      for (uint32_t i = 0; i < enum_type->enumeration.literal_count; i++) {
        String_Slice lit = enum_type->enumeration.literals[i];
        if (lit.length > 0 and lit.data[0] == '\'') {
          return true;  // Enum has character literals
        }
      }
    }
  }
  return false;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.6.3 Parameter Conformance                                                                    
//                                                                                                  
// Check if an argument list matches a subprogram's parameter profile.                              
// Per RM 6.4.1: actual parameters must be type conformant with formals.                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Check if arguments match a symbol's parameter profile
bool Arguments_Match_Profile (Symbol *sym, Argument_Info *args) {
  if (not sym) return false;

  // Track which formal parameters are covered by arguments
  bool *param_covered = Arena_Allocate (sym->parameter_count * sizeof (bool));
  for (uint32_t i = 0; i < sym->parameter_count; i++) {
    param_covered[i] = false;
  }

  // Check type compatibility for each argument and mark params covered
  for (uint32_t i = 0; i < args->count; i++) {
    Type_Info *arg_type = args->types[i];
    Type_Info *param_type = NULL;
    uint32_t param_idx = i;

    // Handle named association
    if (args->names and args->names[i].data) {
      bool found = false;
      for (uint32_t j = 0; j < sym->parameter_count; j++) {
        if (Slice_Equal_Ignore_Case (sym->parameters[j].name, args->names[i])) {
          param_type = sym->parameters[j].param_type;
          param_idx = j;
          found = true;
          break;
        }
      }
      if (not found) return false;

    // Positional: use i-th parameter
    } else {
      if (i >= sym->parameter_count) return false;
      param_type = sym->parameters[i].param_type;
    }

    // Mark this parameter as covered
    if (param_idx < sym->parameter_count) {
      param_covered[param_idx] = true;
    }
    if (not Type_Covers (param_type, arg_type)) {
      return false;
    }
  }

  // Verify all required parameters (no default value) are covered
  for (uint32_t i = 0; i < sym->parameter_count; i++) {
    if (not param_covered[i] and not sym->parameters[i].default_value) {
      return false;  // Required parameter not provided
    }
  }
  return true;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.6.4 Interpretation Collection                                                                
//                                                                                                  
// Gather candidates first, filter later. Visibility determines the set.                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Collect all visible interpretations of a name
void Collect_Interpretations (String_Slice name,
                  Interp_List *interps) {
  interps->count = 0;
  uint32_t hash = Symbol_Hash_Name (name);

  // Search all enclosing scopes
  for (Scope *scope = sm->current_scope; scope; scope = scope->parent) {
    for (Symbol *sym = scope->buckets[hash]; sym; sym = sym->next_in_bucket) {
      if (not Slice_Equal_Ignore_Case (sym->name, name)) continue;
      if (sym->visibility < VIS_IMMEDIATELY_VISIBLE) continue;

      // Add this interpretation and all overloads
      Symbol *s = sym;

      // Check if we already have this interpretation
      while (s and interps->count < MAX_INTERPRETATIONS) {
        bool duplicate = false;
        for (uint32_t i = 0; i < interps->count; i++) {
          if (interps->items[i].nam == s) {
            duplicate = true;
            break;
          }
        }
        if (not duplicate) {
          interps->items[interps->count++] = (Interpretation){
            .nam = s,
            .typ = (s->kind == SYMBOL_FUNCTION) ? s->return_type : s->type,
            .opnd_typ = NULL,
            .is_universal = false,
            .scope_depth = scope->nesting_level
          };
        }
        s = s->next_overload;
      }
    }
  }
}

// Filter interpretations by argument compatibility
void Filter_By_Arguments (Interp_List *interps, Argument_Info *args) {
  uint32_t write_idx = 0;
  for (uint32_t i = 0; i < interps->count; i++) {
    Symbol *sym = interps->items[i].nam;

    // Non-callable symbols don't filter by arguments
    if (sym->kind != SYMBOL_FUNCTION and sym->kind != SYMBOL_PROCEDURE) {
      interps->items[write_idx++] = interps->items[i];
      continue;
    }

    // Keep if arguments match
    if (Arguments_Match_Profile (sym, args)) {
      interps->items[write_idx++] = interps->items[i];
    }
  }
  interps->count = write_idx;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.6.5 Disambiguation                                                                           
//                                                                                                  
// Nearer scope, exact type match, and user definitions all take priority.                          
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Check if sym1 hides sym2 (user-defined hiding predefined, or inner scope)
bool Symbol_Hides (Symbol *sym1, Symbol *sym2) {
  if (not sym1 or not sym2) return false;

  // User-defined function can hide predefined operator
  if ((sym1->kind == SYMBOL_FUNCTION or sym1->kind == SYMBOL_PROCEDURE) and
    sym2->nesting_level == 0) {  // Predefined are at level 0
    return true;
  }

  // Inner scope hides outer scope
  if (sym1->nesting_level > sym2->nesting_level) {
    return true;
  }
  return false;
}

// Score an interpretation for preference ranking (higher = better)
int32_t Score_Interpretation (Interpretation *interp,
                  Type_Info *context_type,
                  Argument_Info *args) {
  int32_t score = 0;
  Symbol *sym = interp->nam;

  // Prefer non-universal interpretations
  if (not Type_Is_Universal (interp->typ)) {
    score += 1000;
  }

  // Prefer exact context type match
  if (context_type and interp->typ == context_type) {
    score += 500;
  }

  // Prefer immediately visible over USE-visible (RM 8.4).                                          
  // Derived type operations are immediately visible while parent                                   
  // operations via USE clause are use-visible.                                                     
  //                                                                                                
  if (sym and sym->visibility == VIS_IMMEDIATELY_VISIBLE) {
    score += 200;
  }

  // Prefer inner scopes (user-defined over predefined)
  score += (int32_t)(interp->scope_depth * 10);

  // For functions: prefer exact argument type matches
  if (sym and (sym->kind == SYMBOL_FUNCTION or sym->kind == SYMBOL_PROCEDURE) and args) {
    for (uint32_t i = 0; i < args->count and i < sym->parameter_count; i++) {
      Type_Info *arg_type = args->types[i];
      Type_Info *param_type = sym->parameters[i].param_type;

      // Exact match is better than just coverage
      if (arg_type == param_type) {
        score += 100;
      } else if (Type_Base (arg_type) == Type_Base (param_type)) {
        score += 50;
      }
    }
  }
  return score;
}

// Select the best interpretation from a list
Symbol *Disambiguate(Interp_List *interps, Type_Info *context_type,
               Argument_Info *args) {
  if (interps->count == 0) return NULL;
  if (interps->count == 1) return interps->items[0].nam;

  // Score all interpretations
  int32_t best_score = INT32_MIN;
  Symbol *best = NULL;
  int tied_count = 0;
  for (uint32_t i = 0; i < interps->count; i++) {
    int32_t score = Score_Interpretation (&interps->items[i], context_type, args);
    if (score > best_score) {
      best_score = score;
      best = interps->items[i].nam;
      tied_count = 1;

    // Check hiding rules
    } else if (score == best_score) {
      if (Symbol_Hides (interps->items[i].nam, best)) {
        best = interps->items[i].nam;
      } else if (not Symbol_Hides (best, interps->items[i].nam)) {
        tied_count++;
      }
    }
  }

  // If still tied, check for universal vs specific preference
  // Prefer interpretation matching context exactly
  if (tied_count > 1 and context_type) {
    for (uint32_t i = 0; i < interps->count; i++) {
      if (interps->items[i].typ == context_type) {
        return interps->items[i].nam;
      }
    }

    // Prefer non-universal
    for (uint32_t i = 0; i < interps->count; i++) {
      if (not Type_Is_Universal (interps->items[i].typ)) {
        return interps->items[i].nam;
      }
    }
  }
  return best;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.6.6 Unified Overload Resolution Entry Point                                                  
//                                                                                                  
// Collect, filter, disambiguate, fail if not unique.                                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Symbol *Resolve_Overloaded_Call (
                     String_Slice name,
                     Argument_Info *args,
                     Type_Info *context_type) {
  Interp_List interps;

  // Phase 1: Collect all visible interpretations
  Collect_Interpretations (name, &interps);
  if (interps.count == 0) {
    return NULL;  // No visible interpretation
  }

  // Phase 2: Filter by argument compatibility
  if (args and args->count > 0) {
    Filter_By_Arguments (&interps, args);
    if (interps.count == 0) {
      return NULL;  // No matching profile
    }
  }

  // Phase 3: Apply context type filtering if provided
  if (context_type and interps.count > 1) {
    uint32_t write_idx = 0;
    for (uint32_t i = 0; i < interps.count; i++) {
      if (Type_Covers (context_type, interps.items[i].typ)) {
        interps.items[write_idx++] = interps.items[i];
      }
    }
    if (write_idx > 0) {
      interps.count = write_idx;
    }

    // If no matches, keep all for better error reporting
  }

  // Phase 4: Disambiguate if multiple interpretations remain
  return Disambiguate (&interps, context_type, args);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §11.7 Symbol Manager Initialization                                                              
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Symbol_Manager_Init_Predefined (void) {

  // Create predefined types
  sm->type_boolean             = Type_New (TYPE_BOOLEAN, S ("BOOLEAN"));
  sm->type_boolean->size       = 1;
  sm->type_boolean->low_bound  = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 0 };
  sm->type_boolean->high_bound = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 1 };

  sm->type_integer             = Type_New (TYPE_INTEGER, S ("INTEGER"));
  sm->type_integer->size       = 4;  // 32-bit - matches GNAT and standard Ada convention
  sm->type_integer->low_bound  = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = INT32_MIN };
  sm->type_integer->high_bound = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = INT32_MAX };

  sm->type_float             = Type_New (TYPE_FLOAT, S ("FLOAT"));
  sm->type_float->size       = 8;  // double precision
  sm->type_float->flt.digits = 15;  // IEEE double: ~15 decimal digits

  sm->type_character             = Type_New (TYPE_CHARACTER, S ("CHARACTER"));
  sm->type_character->size       = 1;
  sm->type_character->low_bound  = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 0   };
  sm->type_character->high_bound = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 127 };
  sm->type_string                    = Type_New (TYPE_STRING, S ("STRING"));
  sm->type_string->size              = 16;  // Fat pointer: ptr + length
  sm->type_string->array.element_type = sm->type_character;

  // STRING's index type is POSITIVE (RM 3.6.3).  Wire it into the type                             
  // system so Array_Bound_Llvm_Type can derive the bound type from the                             
  // index, exactly as Standard's Bound_Sub_GT is derived from the index                            
  // subtype's base type (see gnatllvm-arrays-create.adb).                                          
  // NOTE: type_positive is allocated below; we back-patch after it exists.                         
  //                                                                                                

  sm->type_duration             = Type_New (TYPE_FIXED, S ("DURATION"));
  sm->type_duration->size       = 8;  // 64-bit for high precision
  sm->type_duration->fixed.delta = 0.00001;  // 10 microsecond resolution

  sm->type_universal_integer       = Type_New (TYPE_UNIVERSAL_INTEGER, S ("universal_integer"));
  sm->type_universal_integer->size = 4;  // 32 bits - same as INTEGER (RM 3.5.4)

  sm->type_universal_real       = Type_New (TYPE_UNIVERSAL_REAL, S ("universal_real"));
  sm->type_universal_real->size = 8;  // 64 bits / double precision

  // Add predefined type symbols to global scope
  Symbol *sym_boolean = Symbol_New (SYMBOL_TYPE, S("BOOLEAN"), No_Location);
  sym_boolean->type = sm->type_boolean;
  Symbol_Add (sym_boolean);
  Symbol *sym_integer = Symbol_New (SYMBOL_TYPE, S("INTEGER"), No_Location);
  sym_integer->type = sm->type_integer;
  Symbol_Add (sym_integer);

  // NATURAL is subtype INTEGER range 0..INTEGER'LAST
  Symbol    *sym_natural  = Symbol_New (SYMBOL_SUBTYPE, S ("NATURAL"), No_Location);
  Type_Info *type_natural = Type_New (TYPE_INTEGER, S ("NATURAL"));
  type_natural->base_type  = sm->type_integer;
  type_natural->size       = sm->type_integer->size;
  type_natural->alignment  = sm->type_integer->alignment;
  type_natural->low_bound  = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 0 };
  type_natural->high_bound = sm->type_integer->high_bound;
  sym_natural->type        = type_natural;
  Symbol_Add (sym_natural);

  // POSITIVE is subtype INTEGER range 1..INTEGER'LAST
  Symbol    *sym_positive  = Symbol_New (SYMBOL_SUBTYPE, S ("POSITIVE"), No_Location);
  Type_Info *type_positive = Type_New (TYPE_INTEGER, S ("POSITIVE"));
  type_positive->base_type  = sm->type_integer;
  type_positive->size       = sm->type_integer->size;
  type_positive->alignment  = sm->type_integer->alignment;
  type_positive->low_bound  = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 1 };
  type_positive->high_bound = sm->type_integer->high_bound;
  sym_positive->type        = type_positive;
  Symbol_Add (sym_positive);

  // Back-patch STRING's index type to POSITIVE (deferred from above).                              
  // This makes Array_Bound_Llvm_Type derive STRING's bound type from                               
  // POSITIVE's base type (INTEGER), matching Standard's Bound_Sub_GT.                              
  //                                                                                                
  sm->type_string->array.indices                = Arena_Allocate (sizeof (Index_Info));
  sm->type_string->array.index_count            = 1;
  sm->type_string->array.indices[0].index_type  = type_positive;
  sm->type_string->array.indices[0].low_bound   =
    (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 1 };
  sm->type_string->array.indices[0].high_bound  = sm->type_integer->high_bound;

  Symbol *sym_float      = Symbol_New (SYMBOL_TYPE, S ("FLOAT"), No_Location);
  sym_float->type        = sm->type_float;
  Symbol_Add (sym_float);

  Symbol *sym_duration   = Symbol_New (SYMBOL_TYPE, S ("DURATION"), No_Location);
  sym_duration->type     = sm->type_duration;
  Symbol_Add (sym_duration);

  Symbol *sym_character  = Symbol_New (SYMBOL_TYPE, S ("CHARACTER"), No_Location);
  sym_character->type    = sm->type_character;
  Symbol_Add (sym_character);

  Symbol *sym_string     = Symbol_New (SYMBOL_TYPE, S ("STRING"), No_Location);
  sym_string->type       = sm->type_string;
  Symbol_Add (sym_string);

  // Boolean literals
  Symbol *sym_false        = Symbol_New (SYMBOL_LITERAL, S ("FALSE"), No_Location);
  sym_false->type          = sm->type_boolean;
  Symbol_Add (sym_false);

  Symbol *sym_true         = Symbol_New (SYMBOL_LITERAL, S ("TRUE"), No_Location);
  sym_true->type           = sm->type_boolean;
  sym_true->frame_offset   = 1;  // TRUE has position 1 in BOOLEAN (RM 3.5.3)
  Symbol_Add (sym_true);

  // Predefined exceptions (RM 11.1)
  Symbol *sym_constraint_error = Symbol_New (SYMBOL_EXCEPTION, S ("CONSTRAINT_ERROR"), No_Location);
  Symbol *sym_numeric_error    = Symbol_New (SYMBOL_EXCEPTION, S ("NUMERIC_ERROR"),    No_Location);
  Symbol *sym_program_error    = Symbol_New (SYMBOL_EXCEPTION, S ("PROGRAM_ERROR"),    No_Location);
  Symbol *sym_storage_error    = Symbol_New (SYMBOL_EXCEPTION, S ("STORAGE_ERROR"),    No_Location);
  Symbol_Add (sym_constraint_error);
  Symbol_Add (sym_numeric_error);
  Symbol_Add (sym_program_error);
  Symbol_Add (sym_storage_error);
  Symbol *sym_tasking_error     = Symbol_New (SYMBOL_EXCEPTION, S ("TASKING_ERROR"),   No_Location);
  Symbol_Add (sym_tasking_error);

  // SYSTEM.ADDRESS (RM 13.7) - implementation-defined private type.                                
  // In our implementation, ADDRESS is a 64-bit integer type.                                       
  // Bounds cover full i64 range so ptrtoint values always pass checks.                             
  //                                                                                                
  sm->type_address             = Type_New (TYPE_INTEGER, S ("ADDRESS"));
  sm->type_address->size       = 8;  // 64-bit addresses
  sm->type_address->alignment  = 8;
  sm->type_address->low_bound  = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = INT64_MIN };
  sm->type_address->high_bound = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = INT64_MAX };

  // STANDARD package (RM 8.6) - the implicit library containing predefined types
  // All visible entities are implicitly declared here
  Symbol *pkg_standard = Symbol_New (SYMBOL_PACKAGE, S("STANDARD"), No_Location);
  Type_Info *pkg_standard_type = Type_New (TYPE_PACKAGE, S("STANDARD"));
  pkg_standard->type = pkg_standard_type;
  Symbol_Add (pkg_standard);

  // ASCII package (RM C.3) - predefined character constants
  // In Ada 83, ASCII is a package in STANDARD, always visible
  Symbol *pkg_ascii = Symbol_New (SYMBOL_PACKAGE, S("ASCII"), No_Location);
  Type_Info *pkg_ascii_type = Type_New (TYPE_PACKAGE, S("ASCII"));
  pkg_ascii->type = pkg_ascii_type;
  pkg_ascii->parent = pkg_standard;  // Child of STANDARD
  Symbol_Add (pkg_ascii);

  // STANDARD exports ASCII
  pkg_standard->exported = Arena_Allocate (1 * sizeof (Symbol*));
  pkg_standard->exported_count = 1;
  pkg_standard->exported[0] = pkg_ascii;

  // ASCII control characters and named constants
  static const struct { const char *name; uint8_t val; } ascii_chars[] = {
    {"NUL",0},{"SOH",1},{"STX",2},{"ETX",3},{"EOT",4},{"ENQ",5},{"ACK",6},{"BEL",7},
    {"BS",8},{"HT",9},{"LF",10},{"VT",11},{"FF",12},{"CR",13},{"SO",14},{"SI",15},
    {"DLE",16},{"DC1",17},{"DC2",18},{"DC3",19},{"DC4",20},{"NAK",21},{"SYN",22},
    {"ETB",23},{"CAN",24},{"EM",25},{"SUB",26},{"ESC",27},{"FS",28},{"GS",29},
    {"RS",30},{"US",31},{"DEL",127},

    // Named punctuation
    {"EXCLAM",'!'},{"QUOTATION",'"'},{"SHARP",'#'},{"DOLLAR",'$'},{"PERCENT",'%'},
    {"AMPERSAND",'&'},{"COLON",':'},{"SEMICOLON",';'},{"QUERY",'?'},{"AT_SIGN",'@'},
    {"L_BRACKET",'['},{"BACK_SLASH",'\\'},{"R_BRACKET",']'},{"CIRCUMFLEX",'^'},
    {"UNDERLINE",'_'},{"GRAVE",'`'},{"L_BRACE",'{'},{"BAR",'|'},{"R_BRACE",'}'},
    {"TILDE",'~'},

    // Lowercase letters
    {"LC_A",'a'},{"LC_B",'b'},{"LC_C",'c'},{"LC_D",'d'},{"LC_E",'e'},{"LC_F",'f'},
    {"LC_G",'g'},{"LC_H",'h'},{"LC_I",'i'},{"LC_J",'j'},{"LC_K",'k'},{"LC_L",'l'},
    {"LC_M",'m'},{"LC_N",'n'},{"LC_O",'o'},{"LC_P",'p'},{"LC_Q",'q'},{"LC_R",'r'},
    {"LC_S",'s'},{"LC_T",'t'},{"LC_U",'u'},{"LC_V",'v'},{"LC_W",'w'},{"LC_X",'x'},
    {"LC_Y",'y'},{"LC_Z",'z'},
  };
  uint32_t ascii_count = sizeof (ascii_chars) / sizeof (ascii_chars[0]);
  pkg_ascii->exported = Arena_Allocate (ascii_count * sizeof (Symbol*));
  pkg_ascii->exported_count = ascii_count;
  for (uint32_t i = 0; i < ascii_count; i++) {
    String_Slice name = { ascii_chars[i].name, strlen (ascii_chars[i].name) };
    Symbol *ch = Symbol_New (SYMBOL_CONSTANT, name, No_Location);
    ch->type = sm->type_character;
    ch->parent = pkg_ascii;
    ch->frame_offset = ascii_chars[i].val;  // Store char value
    ch->is_named_number = true;  // Treat as compile-time constant
    Symbol_Add (ch);
    pkg_ascii->exported[i] = ch;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Predefined Operators (RM 4.5) - Needed for operator renaming                                   
  //                                                                                                
  // Per LRM 4.5.3-4.5.6, predefined operators exist for all numeric types.                         
  // We add symbols for these so RENAMES "+" etc. can resolve them.                                 
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  static const struct { const char *name; bool is_binary; bool returns_bool; } predef_ops[] = {
    {"+", true, false}, {"-", true, false}, {"*", true, false},
    {"/", true, false}, {"mod", true, false}, {"rem", true, false},
    {"**", true, false}, {"abs", false, false},
    {"=", true, true}, {"/=", true, true},
    {"<", true, true}, {"<=", true, true},
    {">", true, true}, {">=", true, true},
  };
  Type_Info *num_types[] = { sm->type_integer, sm->type_float };
  for (uint32_t ti = 0; ti < 2; ti++) {
    Type_Info *numeric_type = num_types[ti];
    for (uint32_t oi = 0; oi < sizeof (predef_ops) / sizeof (predef_ops[0]); oi++) {
      String_Slice op_name = { .data   = predef_ops[oi].name,
                               .length = strlen (predef_ops[oi].name) };
      Symbol *op_sym         = Symbol_New (SYMBOL_FUNCTION, op_name, No_Location);
      op_sym->is_predefined  = true;
      op_sym->return_type    = predef_ops[oi].returns_bool ? sm->type_boolean : numeric_type;
      if (predef_ops[oi].is_binary) {
        op_sym->parameter_count = 2;
        op_sym->parameters      = Arena_Allocate (2 * sizeof (Parameter_Info));
        op_sym->parameters[0]   = (Parameter_Info){ .name       = S ("LEFT"),
                                                    .param_type = numeric_type,
                                                    .mode       = PARAM_IN };
        op_sym->parameters[1]   = (Parameter_Info){ .name       = S ("RIGHT"),
                                                    .param_type = numeric_type,
                                                    .mode       = PARAM_IN };
      } else {
        op_sym->parameter_count = 1;
        op_sym->parameters      = Arena_Allocate (1 * sizeof (Parameter_Info));
        op_sym->parameters[0]   = (Parameter_Info){ .name       = S ("RIGHT"),
                                                    .param_type = numeric_type,
                                                    .mode       = PARAM_IN };
      }
      Symbol_Add (op_sym);
    }
  }
}
void Symbol_Manager_Init (void) {
  sm                 = Arena_Allocate (sizeof (Symbol_Manager));
  sm->global_scope   = Scope_New (NULL);
  sm->current_scope  = sm->global_scope;
  sm->next_unique_id = 1;
  Symbol_Manager_Init_Predefined ();
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §12. SEMANTIC ANALYSIS - Type Checking and Resolution                                            
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §12.1 Expression Resolution                                                                      
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Type_Info *Resolve_Identifier (Syntax_Node *node) {
  Symbol *sym = Symbol_Find (node->string_val.text);
  if (not sym) {
    Report_Error (node->location, "undefined identifier '%.*s'",
                  node->string_val.text.length, node->string_val.text.data);
    return sm->type_integer;  // Return a valid type to allow continued analysis
  }
  node->symbol = sym;

  // For parameterless functions, the result type is the return type.
  // In Ada, a function name without parentheses is a valid call.
  if (sym->kind == SYMBOL_FUNCTION and sym->return_type) {
    node->type = sym->return_type;
    return sym->return_type;
  }
  node->type = sym->type;
  return sym->type;
}
Type_Info *Resolve_Selected (Syntax_Node *node) {

  // Resolve prefix first
  Type_Info *prefix_type = Resolve_Expression (node->selected.prefix);

  // Strip quotes from operator selectors: P."/=" > P./= (RM 6.1)                                   
  // The parser stores string-form operators with quotes, but                                       
  // the symbol table stores them without quotes.                                                   
  //                                                                                                
  if (node->selected.selector.length >= 3 and
    node->selected.selector.data[0] == '"' and
    node->selected.selector.data[node->selected.selector.length - 1] == '"') {
    node->selected.selector.data += 1;
    node->selected.selector.length -= 2;
  }

  // Handle .ALL for explicit dereference (RM 4.1)
  if (Type_Is_Access (prefix_type) and
    Slice_Equal_Ignore_Case (node->selected.selector, S("ALL"))) {
    node->type = prefix_type->access.designated_type;
    return node->type;
  }

  // Get effective type for record component lookup (handle implicit dereference)
  Type_Info *record_type = prefix_type;
  if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type) {
    record_type = prefix_type->access.designated_type;
  }

  // Unwrap private/derived types to reach the underlying record (RM 7.4.1).                        
  // Generic formal private types with known discriminants like                                     
  // TYPE PRIV (D : T) IS PRIVATE have parent_type pointing to the actual                           
  // record type after instantiation.                                                               
  //                                                                                                
  for (int depth = 0; depth < 10 and record_type; depth++) {
    if (Type_Is_Record (record_type)) break;
    if ((Type_Is_Private (record_type) or
       record_type->kind == TYPE_INCOMPLETE) and record_type->parent_type)
      record_type = record_type->parent_type;
    else break;
  }

  // Also check TYPE_PRIVATE types with known discriminants (RM 12.1.2).
  // These have discriminant components stored in the record sub-struct.
  bool has_components = Type_Is_Record (record_type) or
    (Type_Is_Private (record_type) and
     record_type->record.component_count > 0);

  // Look up component
  if (has_components) {
    for (uint32_t i = 0; i < record_type->record.component_count; i++) {
      if (Slice_Equal_Ignore_Case (record_type->record.components[i].name,
                    node->selected.selector)) {
        node->type = record_type->record.components[i].component_type;
        return node->type;
      }
    }
    Report_Error (node->location, "no component '%.*s' in record type",
          node->selected.selector.length, node->selected.selector.data);
  } else if (Type_Is_Task (prefix_type) or
         (Type_Is_Access (prefix_type) and prefix_type->access.designated_type and
        Type_Is_Task (prefix_type->access.designated_type))) {

    // Task entry selection: T.E1 where T is a task object (RM 9.5).
    // Also handles P.E1 where P is access-to-task (implicit dereference).
    Type_Info *task_type = Type_Is_Task (prefix_type) ? prefix_type
               : prefix_type->access.designated_type;
    Symbol *type_sym = task_type->defining_symbol;
    if (type_sym) {
      for (uint32_t i = 0; i < type_sym->exported_count; i++) {
        if (Slice_Equal_Ignore_Case (type_sym->exported[i]->name,
                       node->selected.selector)) {
          node->symbol = type_sym->exported[i];
          node->type = type_sym->exported[i]->type;
          return node->type;
        }
      }
    }
    Report_Error (node->location, "no entry '%.*s' in task type",
          (int)node->selected.selector.length, node->selected.selector.data);

  // Could be qualified name - look up in prefix's exported/visible symbols.                        
  // Per RM 4.1.3, qualified names can use package, procedure, or function                          
  // as prefix to access items declared within that scope.                                          
  //                                                                                                
  } else {
    Symbol *prefix_sym = node->selected.prefix->symbol;

    // Search package's exported symbols
    if (prefix_sym and prefix_sym->kind == SYMBOL_PACKAGE) {
      for (uint32_t i = 0; i < prefix_sym->exported_count; i++) {
        if (Slice_Equal_Ignore_Case (prefix_sym->exported[i]->name,
                       node->selected.selector)) {
          node->symbol = prefix_sym->exported[i];

          // For function symbols, the expression type is the return                                
          // type (RM 4.1.3).  sym->type may be NULL for locally                                    
          // declared functions where only return_type is set.                                      
          //                                                                                        
          Symbol *sel = prefix_sym->exported[i];
          node->type = (sel->kind == SYMBOL_FUNCTION and sel->return_type)
                 ? sel->return_type : sel->type;
          return node->type;
        }
      }

      // Also search the package's scope for predefined operators and                               
      // other symbols not explicitly in the exported list (RM 4.1.3).                              
      // This handles P."/=", P."=", P."<" etc. for predefined operators                            
      // of types declared in the package.                                                          
      //                                                                                            
      if (prefix_sym->scope) {
        uint32_t hash = Symbol_Hash_Name (node->selected.selector);
        for (Symbol *s = prefix_sym->scope->buckets[hash]; s; s = s->next_in_bucket) {
          if (Slice_Equal_Ignore_Case (s->name, node->selected.selector) and
            s->visibility >= VIS_IMMEDIATELY_VISIBLE) {
            node->symbol = s;
            node->type = (s->kind == SYMBOL_FUNCTION and s->return_type)
                   ? s->return_type : s->type;
            return node->type;
          }
        }
      }

      // Inherited enum literals for derived types (RM 3.4(12)).                                    
      // When a package has TYPE T IS NEW BOOLEAN, P.FALSE and P.TRUE                               
      // must resolve to literals of T. Find a global literal matching                              
      // the selector name whose type is a parent of some exported type.                            
      //                                                                                            
      {
        String_Slice sel = node->selected.selector;
        uint32_t h = Symbol_Hash_Name (sel);
        for (Scope *sc = sm->current_scope; sc; sc = sc->parent) {
          for (Symbol *lit = sc->buckets[h]; lit; lit = lit->next_in_bucket) {
            if (lit->kind != SYMBOL_LITERAL) continue;
            if (not Slice_Equal_Ignore_Case (lit->name, sel)) continue;
            Type_Info *lit_type = lit->type;
            if (not lit_type) continue;
            for (uint32_t i = 0; i < prefix_sym->exported_count; i++) {
              Symbol *es = prefix_sym->exported[i];
              if (es->kind != SYMBOL_TYPE and es->kind != SYMBOL_SUBTYPE) continue;
              Type_Info *et = es->type;
              if (not et) continue;

              // Check if exported type == or derives from literal's type.                          
              // Walk both parent_type and base_type chains at each level                           
              // to handle anonymous intermediate types.                                            
              //                                                                                    
              {
                Type_Info *anc = et;
                int depth = 0;
                while (anc and depth < 20) {
                  if (anc == lit_type) {
                    if (anc == et) {
                      node->symbol = lit;
                    } else {
                      Symbol *new_lit = Symbol_New (SYMBOL_LITERAL, sel, No_Location);
                      new_lit->type = et;
                      new_lit->frame_offset = lit->frame_offset;
                      node->symbol = new_lit;
                    }
                    node->type = et;
                    return node->type;
                  }

                  // Try parent_type first, then base_type
                  if (anc->parent_type) anc = anc->parent_type;
                  else if (anc->base_type) anc = anc->base_type;
                  else break;
                  depth++;
                }
              }
            }
          }
        }
      }

      // Synthesize predefined operators for types declared in the package                          
      // (RM 4.5). Every type implicitly declares =, /=, and for ordered                            
      // types also <, <=, >, >=. Numeric types add +, -, *, /, etc.                                
      // We lazily create these symbols when first referenced via P."op".                           
      //                                                                                            
      {
        String_Slice sel = node->selected.selector;
        bool is_comparison = (Slice_Equal_Ignore_Case (sel, S("=")) or
                    Slice_Equal_Ignore_Case (sel, S("/=")) or
                    Slice_Equal_Ignore_Case (sel, S("<")) or
                    Slice_Equal_Ignore_Case (sel, S("<=")) or
                    Slice_Equal_Ignore_Case (sel, S(">")) or
                    Slice_Equal_Ignore_Case (sel, S(">=")));
        bool is_logical = (Slice_Equal_Ignore_Case (sel, S("and")) or
                   Slice_Equal_Ignore_Case (sel, S("or")) or
                   Slice_Equal_Ignore_Case (sel, S("xor")) or
                   Slice_Equal_Ignore_Case (sel, S("not")));
        bool is_arith = (Slice_Equal_Ignore_Case (sel, S("+")) or
                 Slice_Equal_Ignore_Case (sel, S("-")) or
                 Slice_Equal_Ignore_Case (sel, S("*")) or
                 Slice_Equal_Ignore_Case (sel, S("/")) or
                 Slice_Equal_Ignore_Case (sel, S("mod")) or
                 Slice_Equal_Ignore_Case (sel, S("rem")) or
                 Slice_Equal_Ignore_Case (sel, S("**")) or
                 Slice_Equal_Ignore_Case (sel, S("abs")));
        bool is_concat = Slice_Equal_Ignore_Case (sel, S("&"));

        // Find first type in the package's exports
        if (is_comparison or is_arith or is_logical or is_concat) {
          Type_Info *op_type = NULL;
          for (uint32_t i = 0; i < prefix_sym->exported_count; i++) {
            Symbol *es = prefix_sym->exported[i];
            if (es->kind == SYMBOL_TYPE and es->type) {
              bool type_ok = false;
              if (is_comparison) type_ok = true;  // All types have = /= < etc
              else if (is_arith) type_ok = Type_Is_Numeric (es->type);
              else if (is_logical) type_ok = Type_Is_Boolean (es->type) or
                  (Type_Is_Array_Like (es->type) and es->type->array.element_type and
                   Type_Is_Boolean (es->type->array.element_type));
              else if (is_concat) type_ok = Type_Is_Array_Like (es->type);
              if (type_ok) { op_type = es->type; break; }
            }
          }
          if (op_type) {
            bool is_unary = (Slice_Equal_Ignore_Case (sel, S("abs")) or
                     Slice_Equal_Ignore_Case (sel, S("not")));
            bool returns_bool = is_comparison;

            // Create a synthetic predefined operator symbol
            Symbol *op_sym = Symbol_New (SYMBOL_FUNCTION, sel, No_Location);
            op_sym->is_predefined = true;
            op_sym->return_type = returns_bool ? sm->type_boolean : op_type;
            if (is_unary) {
              op_sym->parameter_count = 1;
              op_sym->parameters = Arena_Allocate (1 * sizeof (Parameter_Info));
              op_sym->parameters[0] = (Parameter_Info){S("RIGHT"), op_type, PARAM_IN, NULL, NULL};
            } else {
              op_sym->parameter_count = 2;
              op_sym->parameters = Arena_Allocate (2 * sizeof (Parameter_Info));
              op_sym->parameters[0] = (Parameter_Info){S("LEFT"), op_type, PARAM_IN, NULL, NULL};
              op_sym->parameters[1] = (Parameter_Info){S("RIGHT"), op_type, PARAM_IN, NULL, NULL};
            }

            // Install in package scope for future lookups
            if (prefix_sym->scope) {
              uint32_t h = Symbol_Hash_Name (sel);
              op_sym->defining_scope = prefix_sym->scope;
              op_sym->next_in_bucket = prefix_sym->scope->buckets[h];
              prefix_sym->scope->buckets[h] = op_sym;
            }
            node->symbol = op_sym;
            node->type = op_sym->return_type;
            return node->type;
          }
        }
      }
    }

    // For procedure/function prefix, search the subprogram's scope.                                
    // This handles cases like MAIN.A_B_C where MAIN is a procedure                                 
    // and A_B_C is an enum literal or type declared within it.                                     
    //                                                                                              
    if (prefix_sym and (prefix_sym->kind == SYMBOL_PROCEDURE or
              prefix_sym->kind == SYMBOL_FUNCTION) and
      prefix_sym->scope) {
      Scope *subp_scope = prefix_sym->scope;
      uint32_t hash = Symbol_Hash_Name (node->selected.selector);
      for (Symbol *s = subp_scope->buckets[hash]; s; s = s->next_in_bucket) {
        if (Slice_Equal_Ignore_Case (s->name, node->selected.selector) and
          s->visibility >= VIS_IMMEDIATELY_VISIBLE) {
          node->symbol = s;
          node->type = s->type;
          return node->type;
        }
      }
    }
  }
  Report_Error (node->location, "cannot resolve selected component '%.*s'",
         (int)node->selected.selector.length, node->selected.selector.data);
  return sm->type_integer;  // Error recovery
}

// Get the operator name string for a token kind
String_Slice Operator_Name (Token_Kind op) {

  // Return bare (unquoted) operator designators.  The lexer strips quotes                          
  // from operator symbol declarations (RM 6.1), so symbol table entries                            
  // store bare names like +, mod, **.  Match that convention here.                                 
  //                                                                                                
  switch (op) {
    case TK_PLUS:      return S("+");
    case TK_MINUS:     return S("-");
    case TK_STAR:      return S("*");
    case TK_SLASH:     return S("/");
    case TK_MOD:       return S("mod");
    case TK_REM:       return S("rem");
    case TK_EXPON:     return S("**");
    case TK_AMPERSAND: return S("&");
    case TK_AND:       return S("and");
    case TK_OR:        return S("or");
    case TK_XOR:       return S("xor");
    case TK_EQ:        return S("=");
    case TK_NE:        return S("/=");
    case TK_LT:        return S("<");
    case TK_LE:        return S("<=");
    case TK_GT:        return S(">");
    case TK_GE:        return S(">=");
    case TK_NOT:       return S("not");
    case TK_ABS:       return S("abs");
    default:           return S("");
  }
}

// Resolve a character literal as an enumeration literal given a context type.                      
// Used for comparisons and assignments where a character literal should be                         
// interpreted as an enumeration value. Returns true if resolved.                                   
//                                                                                                  
bool Resolve_Char_As_Enum (Syntax_Node *char_node, Type_Info *enum_type) {
  if (not char_node or char_node->kind != NK_CHARACTER or not enum_type)
    return false;

  // Find base enumeration type by following both parent_type and base_type chains.                 
  // parent_type is used for derived types (TYPE T IS NEW X)                                        
  // base_type is used for constrained subtypes (SUBTYPE S IS X RANGE ...)                          
  //                                                                                                
  Type_Info *base_enum = enum_type;
  while (base_enum) {
    if (base_enum->parent_type)
      base_enum = base_enum->parent_type;
    else if (base_enum->base_type)
      base_enum = base_enum->base_type;
    else
      break;
  }
  if (not base_enum or base_enum->kind != TYPE_ENUMERATION or not base_enum->enumeration.literals)
    return false;

  // Get the character from the literal (format: 'X')
  String_Slice lit_text = char_node->string_val.text;
  char ch = lit_text.length >= 2 ? lit_text.data[1] : 0;

  // Look for matching character literal in enum
  for (uint32_t j = 0; j < base_enum->enumeration.literal_count; j++) {
    String_Slice lit_name = base_enum->enumeration.literals[j];
    if (lit_name.length == 3 and
      lit_name.data[0] == '\'' and
      lit_name.data[1] == ch and
      lit_name.data[2] == '\'') {

      // Found matching enum literal - find symbol with matching type
      Symbol *lit_sym = Symbol_Find_By_Type (lit_name, base_enum);
      if (lit_sym and lit_sym->kind == SYMBOL_LITERAL) {
        char_node->symbol = lit_sym;
        char_node->type = enum_type;
        return true;
      }
    }
  }
  return false;
}
Type_Info *Resolve_Binary_Op (Syntax_Node *node) {
  Token_Kind op = node->binary.op;

  // Membership tests (RM 4.4): X IN T, where X can be an aggregate.                                
  // Resolve type name first, propagate type to left aggregate (RM 4.3.3).                          
  // Must resolve right BEFORE left so type can propagate to aggregate.                             
  //                                                                                                
  if ((op == TK_IN or op == TK_NOT) and node->binary.left->kind == NK_AGGREGATE and
    not node->binary.left->type and node->binary.right) {
    Syntax_Node *type_name = node->binary.right;

    // Resolve right (type name) first to get its symbol
    Resolve_Expression (type_name);
    if (type_name->symbol and type_name->symbol->kind == SYMBOL_TYPE)
      node->binary.left->type = type_name->symbol->type;
  }

  // Resolve left first; then propagate its type to an untyped aggregate                            
  // on the right BEFORE resolving, so record component choices can be                              
  // looked up in the correct type (RM 4.3, 4.5.2).                                                 
  // For concatenation with context type (e.g., from RETURN), propagate                             
  // to untyped aggregate operands before resolving (RM 4.5.3).                                     
  //                                                                                                
  if (op == TK_AMPERSAND and node->type) {
    if (node->binary.left->kind == NK_AGGREGATE and not node->binary.left->type)
      node->binary.left->type = node->type;
    if (node->binary.right->kind == NK_AGGREGATE and not node->binary.right->type)
      node->binary.right->type = node->type;
  }
  Type_Info *left_type = Resolve_Expression (node->binary.left);
  if (node->binary.right->kind == NK_AGGREGATE and not node->binary.right->type and left_type)
    node->binary.right->type = left_type;
  Type_Info *right_type = Resolve_Expression (node->binary.right);

  // Per RM 4.5: Binary operators can be user-defined. We first check for                           
  // user-defined operators, then fall back to predefined semantics.                                
  //                                                                                                
  // User-defined operators are functions with designator names like "+" that                       
  // take two parameters of the appropriate types.                                                  
  //                                                                                                

  // Try to find a user-defined operator
  String_Slice op_name = Operator_Name (op);
  if (op_name.length > 0) {
    Type_Info *arg_types[2] = { left_type, right_type };
    Argument_Info args = {
      .types = arg_types,
      .count = 2,
      .names = NULL
    };
    Symbol *user_op = Resolve_Overloaded_Call (op_name, &args, NULL);

    // Skip predefined operators when either operand is universal:                                  
    // universal types must propagate through arithmetic (RM 4.10).                                 
    // Predefined *(FLOAT,FLOAT)>FLOAT would swallow UNIVERSAL_REAL.                                
    //                                                                                              
    if (user_op and user_op->kind == SYMBOL_FUNCTION) {
      if (user_op->is_predefined and
        (Type_Is_Universal (left_type) or Type_Is_Universal (right_type)))
        goto predefined_semantics;
      node->symbol = user_op;
      node->type = user_op->return_type;
      return node->type;
    }
  }

  // Fall back to predefined operator semantics
  predefined_semantics:
  switch (op) {
    case TK_PLUS: case TK_MINUS: case TK_STAR: case TK_SLASH:
    case TK_MOD: case TK_REM: case TK_EXPON:

      // Numeric operators
      if (not Type_Is_Numeric (left_type) or not Type_Is_Numeric (right_type)) {
        Report_Error (node->location, "numeric operands required for %s",
              Token_Name[op]);
      }

      // Result type determination (RM 4.5.5):                                                      
      // - Mixed real/integer: result is the real type (real "wins")                                
      // - Same class: prefer non-universal type                                                    
      // - Both universal: keep universal (propagates to context)                                   
      // Left is real, right is integer -> result is left (real)                                    
      //                                                                                            
      if (Type_Is_Real (left_type) and not Type_Is_Real (right_type)) {
        node->type = left_type;

      // Right is real, left is integer -> result is right (real)
      } else if (Type_Is_Real (right_type) and not Type_Is_Real (left_type)) {
        node->type = right_type;
      } else if (Type_Is_Universal (left_type) and not Type_Is_Universal (right_type)) {
        node->type = right_type;
      } else if (not Type_Is_Universal (left_type)) {
        node->type = left_type;

      // Both universal - result is universal
      } else {
        node->type = left_type;
      }
      break;

    // Propagate type to aggregate operands before validation (RM 4.3):                             
    // In A & (1, 2), the aggregate gets its type from A.                                           
    // Must happen before the left_ok/right_ok check.                                               
    //                                                                                              
    case TK_AMPERSAND:

      // String/array/character concatenation (RM 4.5.3).                                           
      // Valid operand combinations:                                                                
      //   STRING & STRING -> STRING                                                                
      //   STRING & CHARACTER -> STRING                                                             
      //   CHARACTER & STRING -> STRING                                                             
      //   CHARACTER & CHARACTER -> STRING                                                          
      //   ARRAY & ARRAY -> ARRAY (same element type)                                               
      //   ARRAY & ELEMENT -> ARRAY                                                                 
      //   ELEMENT & ARRAY -> ARRAY                                                                 
      //                                                                                            
      {
        if (node->binary.right->kind == NK_AGGREGATE and not node->binary.right->type and left_type) {
          node->binary.right->type = left_type;
          Resolve_Expression (node->binary.right);
          right_type = node->binary.right->type;
        }
        if (node->binary.left->kind == NK_AGGREGATE and not node->binary.left->type and right_type) {
          node->binary.left->type = right_type;
          Resolve_Expression (node->binary.left);
          left_type = node->binary.left->type;
        }
        bool left_ok = Type_Is_Array_Like (left_type) or
                 Type_Is_Character (left_type);
        bool right_ok = Type_Is_Array_Like (right_type) or
                Type_Is_Character (right_type);
        if (not left_ok and not right_ok) {
          Report_Error (node->location, "concatenation requires string, array, or character");
        }

        // Result type: prefer user-defined array type over predefined STRING.                      
        // Per RM 4.5.3, concatenation returns the array type.                                      
        // For element & element, the result type comes from context.                               
        //                                                                                          
        if (Type_Is_Array_Like (left_type) and left_type->kind == TYPE_ARRAY) {
          node->type = left_type;
        } else if (Type_Is_Array_Like (right_type) and right_type->kind == TYPE_ARRAY) {
          node->type = right_type;
        } else if (Type_Is_String (left_type)) {
          node->type = left_type;
        } else if (Type_Is_String (right_type)) {
          node->type = right_type;
        } else if (Type_Is_Array_Like (left_type)) {
          node->type = left_type;
        } else if (Type_Is_Array_Like (right_type)) {
          node->type = right_type;

        // CHARACTER & CHARACTER -> STRING
        } else {
          node->type = sm->type_string;
        }
      }
      break;
    case TK_AND: case TK_OR: case TK_XOR:
    case TK_AND_THEN: case TK_OR_ELSE:

      // Boolean operators - can also operate on arrays of Boolean
      if (left_type and left_type->kind != TYPE_BOOLEAN) {
        if (left_type->kind != TYPE_ARRAY or
          not left_type->array.element_type or
          left_type->array.element_type->kind != TYPE_BOOLEAN) {
          Report_Error (node->location, "Boolean operands required");
        }
      }
      node->type = left_type ? left_type : sm->type_boolean;
      break;

    // Propagate type from left operand to aggregate
    case TK_EQ: case TK_NE: case TK_LT: case TK_LE: case TK_GT: case TK_GE:

      // Comparison operators                                                                       
      // Handle aggregates without type context (RM 4.3):                                           
      // In A = (1, 2, 3), the aggregate gets its type from A.                                      
      // Per GNAT sem_res.adb Find_Unique_Type, propagate type context.                             
      //                                                                                            
      if (node->binary.right->kind == NK_AGGREGATE and not node->binary.right->type and left_type) {
        node->binary.right->type = left_type;

        // Re-resolve aggregate with proper type context
        Resolve_Expression (node->binary.right);
        right_type = node->binary.right->type;
      }

      // Propagate type from right operand to aggregate
      if (node->binary.left->kind == NK_AGGREGATE and not node->binary.left->type and right_type) {
        node->binary.left->type = right_type;

        // Re-resolve aggregate with proper type context
        Resolve_Expression (node->binary.left);
        left_type = node->binary.left->type;
      }

      // Handle character literals that should be enum literals
      if (Type_Is_Enumeration (left_type) or
        Type_Is_Enumeration (left_type ? left_type->parent_type : NULL)) {
        if (node->binary.right->kind == NK_CHARACTER) {
          if (Resolve_Char_As_Enum (node->binary.right, left_type)) {
            right_type = node->binary.right->type;
          }
        }
      }
      if (Type_Is_Enumeration (right_type) or
        Type_Is_Enumeration (right_type ? right_type->parent_type : NULL)) {
        if (node->binary.left->kind == NK_CHARACTER) {
          if (Resolve_Char_As_Enum (node->binary.left, right_type)) {
            left_type = node->binary.left->type;
          }
        }
      }

      // Disambiguate overloaded enum literals using comparison context                             
      // (RM 8.6): if types mismatch and one operand is a literal, re-resolve                       
      // it against the other operand's type via Symbol_Find_By_Type.                               
      //                                                                                            
      if (not Type_Covers (left_type, right_type) and not Type_Covers (right_type, left_type)) {
        if (node->binary.right->kind == NK_IDENTIFIER and
          node->binary.right->symbol and
          node->binary.right->symbol->kind == SYMBOL_LITERAL and left_type) {
          Symbol *s = Symbol_Find_By_Type (
            node->binary.right->string_val.text, left_type);
          if (s) {
            node->binary.right->symbol = s;
            node->binary.right->type = s->type ? s->type : left_type;
            right_type = node->binary.right->type;
          }
        }
        if (node->binary.left->kind == NK_IDENTIFIER and
          node->binary.left->symbol and
          node->binary.left->symbol->kind == SYMBOL_LITERAL and right_type) {
          Symbol *s = Symbol_Find_By_Type (
            node->binary.left->string_val.text, right_type);
          if (s) {
            node->binary.left->symbol = s;
            node->binary.left->type = s->type ? s->type : right_type;
            left_type = node->binary.left->type;
          }
        }
      }
      if (not Type_Covers (left_type, right_type) and not Type_Covers (right_type, left_type)) {
        Report_Error (node->location, "incompatible types for comparison");
      }
      node->type = sm->type_boolean;
      break;
    case TK_IN:
    case TK_NOT:  // NOT IN is encoded as TK_NOT in binary op

      // Membership test: X IN range.  Per Ada RM 4.5.2, the range                                  
      // is resolved in the context of the tested expression's type.                                
      // GNAT: Resolve_Membership_Op propagates left type to range.                                 
      //                                                                                            
      if (left_type and (Type_Is_Enumeration (left_type) or
        (left_type->parent_type and Type_Is_Enumeration (left_type->parent_type)))) {
        Syntax_Node *rhs = node->binary.right;
        if (rhs and rhs->kind == NK_RANGE) {
          if (rhs->range.low and rhs->range.low->kind == NK_CHARACTER)
            Resolve_Char_As_Enum (rhs->range.low, left_type);
          if (rhs->range.high and rhs->range.high->kind == NK_CHARACTER)
            Resolve_Char_As_Enum (rhs->range.high, left_type);
        }
      }
      node->type = sm->type_boolean;
      break;

    // Apply node resolution - handles multiple Ada constructs:                                     
    // 1. Function/procedure calls: Put (X), Process (A, B)                                         
    // 2. Array indexing: Arr (I), Matrix (I, J)                                                    
    // 3. Type conversions: Integer (X), Float (Y)                                                  
    // 4. Constrained subtype indications: String (1..10)                                           
    //                                                                                              
    // For calls, we use the overload resolution engine to handle:                                  
    // - Overloaded subprogram names                                                                
    // - Named parameter associations                                                               
    // - Default parameter values                                                                   
    //                                                                                              
    default:
      Report_Error (node->location, "unhandled binary operator in type resolution");
      node->type = sm->type_integer;
  }
  return node->type;
}
Type_Info *Resolve_Apply (Syntax_Node *node) {

  // First, resolve all arguments to get their types
  uint32_t arg_count = (uint32_t)node->apply.arguments.count;
  Type_Info **arg_types = NULL;
  String_Slice *arg_names = NULL;
  if (arg_count > 0) {
    arg_types = Arena_Allocate (arg_count * sizeof (Type_Info*));
    arg_names = Arena_Allocate (arg_count * sizeof (String_Slice));
    for (uint32_t i = 0; i < arg_count; i++) {
      Syntax_Node *arg = node->apply.arguments.items[i];

      // Handle named associations: Name => Value
      if (arg->kind == NK_ASSOCIATION and arg->association.choices.count == 1) {
        Syntax_Node *name_node = arg->association.choices.items[0];
        if (name_node->kind == NK_IDENTIFIER) {
          arg_names[i] = name_node->string_val.text;
        }

        // Resolve the value expression
        if (arg->association.expression) {
          arg_types[i] = Resolve_Expression (arg->association.expression);
        }
      } else {
        arg_names[i] = (String_Slice){0};  // Positional

        // Defer aggregate resolution: aggregates need parameter type                               
        // context for record component names (RM 4.3, 6.4). They'll                                
        // be resolved after the callable is identified.                                            
        //                                                                                          
        if (arg->kind == NK_AGGREGATE)
          arg_types[i] = NULL;
        else
          arg_types[i] = Resolve_Expression (arg);
      }
    }
  }
  Argument_Info args = {
    .types = arg_types,
    .count = arg_count,
    .names = arg_names
  };

  // Resolve the prefix
  Syntax_Node *prefix = node->apply.prefix;
  Symbol *prefix_sym = NULL;

  // For identifier prefix, use overload resolution
  if (prefix->kind == NK_IDENTIFIER) {
    prefix_sym = Resolve_Overloaded_Call (prefix->string_val.text, &args, NULL);
    if (prefix_sym) {
      prefix->symbol = prefix_sym;
      prefix->type = (prefix_sym->kind == SYMBOL_FUNCTION) ?
               prefix_sym->return_type : prefix_sym->type;

    // Fall back to simple lookup for non-callable names
    } else {
      prefix_sym = Symbol_Find (prefix->string_val.text);
      if (prefix_sym) {
        prefix->symbol = prefix_sym;
        prefix->type = prefix_sym->type;
      }
    }

  // For complex prefix (selected, etc.), resolve normally
  } else {
    Resolve_Expression (prefix);
    prefix_sym = prefix->symbol;
  }
  Type_Info *prefix_type = prefix->type;

  // Handle based on what the prefix resolves to
  // ─── Case 1: Function/Procedure Call ───────────────────────────────────────────────────────────

  if (prefix_sym) {

    // Only treat as a call if prefix is an identifier referring to a callable.                     
    // If prefix is a complex expression (e.g., func(...)), it's already resolved                   
    // and we should check its result type for indexing instead.                                    
    //                                                                                              
    bool prefix_is_call_target = (prefix->kind == NK_IDENTIFIER or
                    prefix->kind == NK_SELECTED);
    if (prefix_is_call_target and
      (prefix_sym->kind == SYMBOL_FUNCTION or prefix_sym->kind == SYMBOL_PROCEDURE)) {
      node->symbol = prefix_sym;
      node->type = prefix_sym->return_type;  // NULL for procedures

      // Re-resolve arguments based on parameter types.                                             
      // This handles:                                                                              
      // - Character literals like FN ('A') where 'A' must be resolved as                           
      //   the enumeration literal for the parameter's type, not ASCII.                             
      // - Aggregates like FN ((1,2,3)) where the aggregate needs the                               
      //   parameter type to determine its type (RM 4.3).                                           
      //                                                                                            
      for (uint32_t i = 0; i < arg_count and i < prefix_sym->parameter_count; i++) {
        Syntax_Node *arg = node->apply.arguments.items[i];

        // Handle named associations
        if (arg->kind == NK_ASSOCIATION and arg->association.expression) {
          arg = arg->association.expression;
        }
        Type_Info *param_type = prefix_sym->parameters[i].param_type;
        if (arg->kind == NK_CHARACTER) {
          Resolve_Char_As_Enum (arg, param_type);
        }

        // Propagate type to aggregate arguments
        if (arg->kind == NK_AGGREGATE and not arg->type and param_type) {
          arg->type = param_type;
          Resolve_Expression (arg);
        }
      }
      return node->type;
    }

    // ─── Case 2: Type Conversion or Constrained Subtype ──────────────────────────────────────────

    if (prefix_sym->kind == SYMBOL_TYPE or prefix_sym->kind == SYMBOL_SUBTYPE) {
      Type_Info *base_type = prefix_sym->type;

      // Check for constrained subtype indication: STRING (1..5)
      // Check if arguments are ranges (subtype indication) vs values (indexing)
      if (Type_Is_Array_Like (base_type)) {
        bool has_range = false;
        for (uint32_t i = 0; i < arg_count; i++) {
          Syntax_Node *arg = node->apply.arguments.items[i];
          if (arg->kind == NK_RANGE) {
            has_range = true;
            break;
          }
        }

        // Constrained array/string subtype indication
        if (has_range) {
          Type_Info *constrained = Type_New (TYPE_ARRAY, base_type->name);
          constrained->array.is_constrained = true;
          constrained->array.index_count = arg_count;

          // Element type
          if (Type_Is_String (base_type)) {
            constrained->array.element_type = sm->type_character;
          } else if (base_type->array.element_type) {
            constrained->array.element_type = base_type->array.element_type;
          }

          // Process index constraints
          if (arg_count > 0) {
            constrained->array.indices = Arena_Allocate (
              arg_count * sizeof (Index_Info));
            for (uint32_t i = 0; i < arg_count; i++) {
              Syntax_Node *arg = node->apply.arguments.items[i];
              Index_Info *info = &constrained->array.indices[i];

              // Inherit index_type from base type (e.g., POSITIVE for STRING).
              // This is critical for RM 4.5.3 concatenation length checks.
              if (base_type->array.index_count > i and base_type->array.indices[i].index_type) {
                info->index_type = base_type->array.indices[i].index_type;
              } else {
                info->index_type = sm->type_integer;  // Default fallback
              }
              if (arg->kind == NK_RANGE) {
                if (arg->range.low and arg->range.low->kind == NK_INTEGER) {
                  info->low_bound = (Type_Bound){
                    .kind = BOUND_INTEGER,
                    .int_value = arg->range.low->integer_lit.value
                  };
                }
                if (arg->range.high and arg->range.high->kind == NK_INTEGER) {
                  info->high_bound = (Type_Bound){
                    .kind = BOUND_INTEGER,
                    .int_value = arg->range.high->integer_lit.value
                  };
                }
              }
            }

            // Compute size - only for fully static bounds
            {
              bool all_static = true;
              int128_t count = 1;
              for (uint32_t i = 0; i < arg_count; i++) {
                Type_Bound *lo_b = &constrained->array.indices[i].low_bound;
                Type_Bound *hi_b = &constrained->array.indices[i].high_bound;
                if ((lo_b->kind == BOUND_EXPR and lo_b->expr and lo_b->expr->symbol) or
                  (hi_b->kind == BOUND_EXPR and hi_b->expr and hi_b->expr->symbol)) {
                  all_static = false; break;
                }
                int128_t lo = Type_Bound_Value (*lo_b);
                int128_t hi = Type_Bound_Value (*hi_b);
                int128_t dim = hi - lo + 1;
                if (dim < 0) dim = 0;
                count *= dim;
              }
              if (not all_static) {
                constrained->size = 0;
              } else if (all_static and count >= 0) {
                uint32_t elem_size = constrained->array.element_type ?
                           constrained->array.element_type->size : 1;
                if (elem_size == 0 and Type_Needs_Fat_Pointer_Load (constrained->array.element_type))
                  elem_size = FAT_PTR_ALLOC_SIZE;
                constrained->size = (uint32_t)(count * elem_size);
              }
            }
          }
          node->type = constrained;
          return constrained;
        }
      }

      // Regular type conversion: Integer (X)
      if (arg_count == 1) {
        node->type = prefix_sym->type;
        return node->type;
      }
    }
  }

  // ─── Case 2b: Predefined operator called via string syntax ─────────────────────────────────────
  // Ada allows "+"(X, Y) or "&"(A, B) as equivalent to X + Y or A & B.                             
  // Handle predefined operators that don't have explicit symbol entries.                           
  // Note: The lexer strips quotes from operator strings, so "&" becomes just &.                    
  //                                                                                                
  if (not prefix_sym and prefix->kind == NK_IDENTIFIER) {
    String_Slice name = prefix->string_val.text;

    // Check for single-character operators (lexer stripped quotes)
    if (name.length == 1 and arg_count == 2) {
      char op_char = name.data[0];

      // "&"(A, B) is array/string concatenation
      if (op_char == '&') {
        Syntax_Node *left = node->apply.arguments.items[0];
        Syntax_Node *right = node->apply.arguments.items[1];
        Type_Info *left_type = Resolve_Expression (left);
        Type_Info *right_type = Resolve_Expression (right);

        // Result is the array type (prefer left if array, else right)
        if (left_type and Type_Is_Array_Like (left_type)) {
          node->type = left_type;
        } else if (right_type and Type_Is_Array_Like (right_type)) {
          node->type = right_type;
        } else if (Type_Is_String (left_type)) {
          node->type = left_type;
        } else if (Type_Is_String (right_type)) {
          node->type = right_type;
        } else {
          node->type = sm->type_string;  // Default for character concat
        }
        return node->type;
      }

      // Handle arithmetic operators
      if (op_char == '+' or op_char == '-' or op_char == '*' or op_char == '/') {
        Syntax_Node *left = node->apply.arguments.items[0];
        Syntax_Node *right = node->apply.arguments.items[1];
        Type_Info *left_type = Resolve_Expression (left);
        Type_Info *right_type = Resolve_Expression (right);

        // Result is the numeric type (prefer left)
        node->type = left_type ? left_type : right_type;
        return node->type;
      }
    }

    // Check for two-character operators like <=, >=, /=, **
    if (name.length == 2 and arg_count == 2) {
      if ((name.data[0] == '<' and name.data[1] == '=') or
        (name.data[0] == '>' and name.data[1] == '=') or
        (name.data[0] == '/' and name.data[1] == '=')) {

        // Comparison operators return BOOLEAN
        Syntax_Node *left = node->apply.arguments.items[0];
        Syntax_Node *right = node->apply.arguments.items[1];
        Resolve_Expression (left);
        Resolve_Expression (right);
        node->type = sm->type_boolean;
        return node->type;
      }
      if (name.data[0] == '*' and name.data[1] == '*') {

        // Exponentiation
        Syntax_Node *left = node->apply.arguments.items[0];
        Syntax_Node *right = node->apply.arguments.items[1];
        Type_Info *left_type = Resolve_Expression (left);
        Resolve_Expression (right);
        node->type = left_type;
        return node->type;
      }
    }

    // Unary operators with one argument
    if (arg_count == 1) {
      if (name.length == 1) {
        char op_char = name.data[0];
        if (op_char == '+' or op_char == '-') {
          node->type = Resolve_Expression (node->apply.arguments.items[0]);
          return node->type;
        }
      }

      // Unary word operators: "NOT"(X), "ABS"(X) (RM 4.5.6, 4.5.7)
      if (Slice_Equal_Ignore_Case (name, S("not"))) {
        Syntax_Node *operand = node->apply.arguments.items[0];
        Type_Info *ot = Resolve_Expression (operand);

        // NOT preserves boolean array type
        if (ot and Type_Is_Array_Like (ot) and ot->array.element_type and
          Type_Is_Boolean (ot->array.element_type))
          node->type = ot;
        else
          node->type = sm->type_boolean;
        return node->type;
      }
      if (Slice_Equal_Ignore_Case (name, S("abs"))) {
        node->type = Resolve_Expression (node->apply.arguments.items[0]);
        return node->type;
      }
    }

    // Binary word operators: "AND"/"OR"/"XOR"/"MOD"/"REM"(L,R) (RM 4.5)
    if (arg_count == 2) {
      if (Slice_Equal_Ignore_Case (name, S("and")) or
        Slice_Equal_Ignore_Case (name, S("or"))  or
        Slice_Equal_Ignore_Case (name, S("xor")) or
        Slice_Equal_Ignore_Case (name, S("mod")) or
        Slice_Equal_Ignore_Case (name, S("rem"))) {
        Type_Info *lt = Resolve_Expression (node->apply.arguments.items[0]);
        Resolve_Expression (node->apply.arguments.items[1]);
        node->type = lt;
        return node->type;
      }
    }
  }

  // ─── Case 3: Array Indexing/Slicing (with implicit access dereference) ─────────────────────────
  // Per RM 4.1(3), A(I) where A is access-to-array is equivalent to A.ALL(I)
  Type_Info *indexed_type = prefix_type;
  if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type) {
    indexed_type = prefix_type->access.designated_type;  // Implicit dereference
  }

  // Check if this is a slice (range argument) vs indexing (scalar argument)
  if (Type_Is_Array_Like (indexed_type)) {
    bool is_slice = false;
    for (uint32_t i = 0; i < arg_count; i++) {
      Syntax_Node *arg = node->apply.arguments.items[i];
      if (arg and arg->kind == NK_RANGE) {
        is_slice = true;
        break;
      }
    }

    // Slice: result type is the same array/string type
    if (is_slice) {
      node->type = indexed_type;

    // Indexing: result type is the element type
    } else {
      node->type = indexed_type->array.element_type;
      if (not node->type and Type_Is_String (indexed_type)) {
        node->type = sm->type_character;
      }
    }
    return node->type;
  }

  // ─── Case 3b: Generic subprogram recursive call ────────────────────────────────────────────────
  // Per Ada RM 12.3(17), within a generic subprogram body the name of                              
  // the subprogram denotes the current instance (recursive call).                                  
  // GNAT: Analyze_Call handles this via the Is_Generic_Subprogram check.                           
  //                                                                                                
  if (prefix_sym and prefix_sym->kind == SYMBOL_GENERIC and prefix_sym->generic_unit) {
    Syntax_Node *gu = prefix_sym->generic_unit;

    // Return type from the generic function spec
    if (gu->kind == NK_FUNCTION_SPEC) {
      if (gu->subprogram_spec.return_type)
        node->type = gu->subprogram_spec.return_type->type;
      if (not node->type) node->type = sm->type_integer;
      return node->type;
    }
    if (gu->kind == NK_PROCEDURE_SPEC) {
      node->type = NULL;  // Procedure call - no return type
      return NULL;
    }
  }

  // ─── Case 4: Unresolved - report error and recover ─────────────────────────────────────────────

  if (prefix->kind == NK_IDENTIFIER) {
    Report_Error (node->location, "cannot resolve '%.*s' as callable or indexable",
          (int)prefix->string_val.text.length, prefix->string_val.text.data);
  }
  return sm->type_integer;  // Error recovery
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Evaluate Constant Numeric Expression (for delta, bounds in type defs)                            
// Returns NaN if not a static constant expression                                                  
// ─────────────────────────────────────────────────────────────────────────────────────────────────

bool Is_Integer_Expr (Syntax_Node *node) {

  // Returns true if the expression is integer-typed (for division semantics)
  if (not node) return false;
  switch (node->kind) {

    // Check if named number/constant is integer
    case NK_INTEGER: return true;
    case NK_REAL:    return false;
    case NK_IDENTIFIER:
    case NK_SELECTED: {
      Symbol *sym = node->symbol;
      if (sym and sym->kind == SYMBOL_CONSTANT and sym->is_named_number and
        sym->declaration and sym->declaration->kind == NK_OBJECT_DECL) {
        return Is_Integer_Expr (sym->declaration->object_decl.init);
      }

      // Check type if available
      if (Type_Is_Integer_Like (node->type)) return true;
      return false;
    }
    case NK_UNARY_OP:
      return Is_Integer_Expr (node->unary.operand);
    case NK_BINARY_OP:
      return Is_Integer_Expr (node->binary.left) and Is_Integer_Expr (node->binary.right);
    case NK_APPLY:

      // For type conversions TYPE (arg), check if arg is integer
      if (node->apply.arguments.count == 1 and
        node->apply.prefix and node->apply.prefix->symbol and
        node->apply.prefix->symbol->kind == SYMBOL_TYPE) {
        return Is_Integer_Expr (node->apply.arguments.items[0]);
      }
      return false;
    default:
      return false;
  }
}
double Eval_Const_Numeric (Syntax_Node *node) {
  if (not node) return 0.0/0.0;
  switch (node->kind) {
    case NK_REAL:    return node->real_lit.value;
    case NK_INTEGER: return (double)node->integer_lit.value;
    case NK_CHARACTER:

      // Character literals have position value in symbol->frame_offset
      if (node->symbol and node->symbol->kind == SYMBOL_LITERAL) {
        return (double)node->symbol->frame_offset;
      }

      // Fallback: use ASCII code - character literals store their value in string_val.text
      // Format is 'X' (length 3) - extract the middle character at index 1
      if (node->string_val.text.length >= 2) {
        return (double)(unsigned char)node->string_val.text.data[1];
      }
      if (node->string_val.text.length == 1) {
        return (double)(unsigned char)node->string_val.text.data[0];
      }
      return 0.0/0.0;

    // Check for character/enum literal first
    case NK_IDENTIFIER:
    case NK_SELECTED: {
      Symbol *sym = node->symbol;
      if (sym and sym->kind == SYMBOL_LITERAL) {
        return (double)sym->frame_offset;
      }

      // Named number or constant - evaluate via symbol's declaration
      if (sym and sym->kind == SYMBOL_CONSTANT) {
        if (sym->declaration and sym->declaration->kind == NK_OBJECT_DECL
          and sym->declaration->object_decl.init)
          return Eval_Const_Numeric (sym->declaration->object_decl.init);

        // Predefined constant (e.g. ASCII.DEL) - value in frame_offset
        return (double)sym->frame_offset;
      }

      // Variable with constant init (typed constants get variable-like alloc)
      if (sym and sym->kind == SYMBOL_VARIABLE and sym->declaration
        and sym->declaration->kind == NK_OBJECT_DECL
        and sym->declaration->object_decl.is_constant
        and sym->declaration->object_decl.init) {
        return Eval_Const_Numeric (sym->declaration->object_decl.init);
      }
      return 0.0/0.0;
    }
    case NK_QUALIFIED:

      // Qualified expression: TYPE'(expr) - evaluate the inner expression
      if (node->qualified.expression) {
        Syntax_Node *inner = node->qualified.expression;

        // Handle character literal inside qualified expression:
        // look up in the qualifying type's enumeration literals
        if (inner->kind == NK_CHARACTER and node->qualified.subtype_mark and
          node->qualified.subtype_mark->type) {
          Type_Info *qual_type = node->qualified.subtype_mark->type;

          // Walk up to find the base enumeration type with literals
          while (Type_Is_Enumeration (qual_type) and
               not qual_type->enumeration.literals) {
            qual_type = qual_type->base_type ? qual_type->base_type : qual_type->parent_type;
          }
          if (Type_Is_Enumeration (qual_type) and
            qual_type->enumeration.literals) {

            // Extract the character from 'X' format
            String_Slice lit_text = inner->string_val.text;
            char ch = lit_text.length >= 2 ? lit_text.data[1] : 0;

            // Look for matching character literal in enum
            for (uint32_t j = 0; j < qual_type->enumeration.literal_count; j++) {
              String_Slice lit_name = qual_type->enumeration.literals[j];
              if (lit_name.length == 3 and
                lit_name.data[0] == '\'' and
                lit_name.data[1] == ch and
                lit_name.data[2] == '\'') {
                return (double)j;  // Position in enumeration
              }
            }
          }
        }
        return Eval_Const_Numeric (inner);
      }
      return 0.0/0.0;

    // Type conversions: TYPE_NAME (expr) - evaluate the argument
    case NK_APPLY: {
      if (node->apply.prefix and node->apply.arguments.count == 1) {
        Syntax_Node *arg = node->apply.arguments.items[0];
        Syntax_Node *prefix = node->apply.prefix;
        if (prefix->kind == NK_IDENTIFIER and prefix->symbol and
          prefix->symbol->kind == SYMBOL_TYPE) {
          return Eval_Const_Numeric (arg);
        }
      }
      return 0.0/0.0;
    }

    // T'SIZE, T'FIRST, T'LAST, T'POS, T'LENGTH etc.
    case NK_UNARY_OP:
      if (node->unary.op == TK_MINUS) return -Eval_Const_Numeric (node->unary.operand);
      if (node->unary.op == TK_PLUS)  return Eval_Const_Numeric (node->unary.operand);
      if (node->unary.op == TK_ABS)   return fabs (Eval_Const_Numeric (node->unary.operand));
      return 0.0/0.0;
    case NK_ATTRIBUTE: {
      Type_Info *ty = node->attribute.prefix ? node->attribute.prefix->type : NULL;
      String_Slice a = node->attribute.name;
      if (not ty) return 0.0/0.0;
      if (Slice_Equal_Ignore_Case (a, S("SIZE")))
        return (double)(ty->size * 8);
      if (Slice_Equal_Ignore_Case (a, S("FIRST"))) {
        if (ty->low_bound.kind == BOUND_INTEGER) return (double)ty->low_bound.int_value;
        if (ty->low_bound.kind == BOUND_FLOAT)   return ty->low_bound.float_value;
      }
      if (Slice_Equal_Ignore_Case (a, S("LAST"))) {
        if (ty->high_bound.kind == BOUND_INTEGER) return (double)ty->high_bound.int_value;
        if (ty->high_bound.kind == BOUND_FLOAT)   return ty->high_bound.float_value;
      }
      if (Slice_Equal_Ignore_Case (a, S("LENGTH"))) {
        if (Type_Is_Array_Like (ty) and ty->array.index_count > 0) {
          int128_t lo = Type_Bound_Value (ty->array.indices[0].low_bound);
          int128_t hi = Type_Bound_Value (ty->array.indices[0].high_bound);
          return (double)(hi - lo + 1);
        }
      }
      if (Slice_Equal_Ignore_Case (a, S("POS")) and
        node->attribute.arguments.count == 1)
        return Eval_Const_Numeric (node->attribute.arguments.items[0]);
      return 0.0/0.0;
    }
    case NK_BINARY_OP: {
      double l = Eval_Const_Numeric (node->binary.left);
      double r = Eval_Const_Numeric (node->binary.right);
      switch (node->binary.op) {
        case TK_PLUS:  return l + r;
        case TK_MINUS: return l - r;
        case TK_STAR:  return l * r;
        case TK_SLASH:
          if (r == 0) return 0.0/0.0;

          // Ada integer division truncates toward zero (RM 4.5.5)
          // Only use integer division if BOTH operands are integer-typed
          if (Is_Integer_Expr (node->binary.left) and Is_Integer_Expr (node->binary.right) and
            l == floor (l) and r == floor (r) and
            fabs (l) < 1e15 and fabs (r) < 1e15) {
            int64_t li = (int64_t)l;
            int64_t ri = (int64_t)r;
            return (double)(li / ri);  // Integer division
          }
          return l / r;
        case TK_EXPON: return pow (l, r);
        default:       return 0.0/0.0;
      }
    }
    default: return 0.0/0.0;
  }
}

// RM 4.10: Exact rational evaluator for static universal_real expressions.                         
// Returns true and fills *out if the expression is a compile-time constant                         
// representable as an exact rational number.  Used so that comparisons like                        
// 0.1*0.1 = 0.01 or (2/3)**10 chain correctly without IEEE rounding errors.                        
//                                                                                                  
bool Eval_Const_Rational (Syntax_Node *node, Rational *out) {
  if (not node) return false;
  switch (node->kind) {
    case NK_REAL:
      if (node->real_lit.big_value) {
        *out = Rational_From_Big_Real (node->real_lit.big_value);
        return true;
      }

      // Fallback: wrap double as rational - loses exactness
      return false;
    case NK_INTEGER:
      *out = Rational_From_Int ((int64_t)node->integer_lit.value);
      return true;
    case NK_IDENTIFIER:
    case NK_SELECTED: {
      Symbol *sym = node->symbol;
      if (sym and sym->kind == SYMBOL_CONSTANT and sym->declaration
        and sym->declaration->kind == NK_OBJECT_DECL
        and sym->declaration->object_decl.init)
        return Eval_Const_Rational (sym->declaration->object_decl.init, out);
      if (sym and sym->kind == SYMBOL_VARIABLE and sym->declaration
        and sym->declaration->kind == NK_OBJECT_DECL
        and sym->declaration->object_decl.is_constant
        and sym->declaration->object_decl.init)
        return Eval_Const_Rational (sym->declaration->object_decl.init, out);
      return false;
    }
    case NK_QUALIFIED:
      if (node->qualified.expression)
        return Eval_Const_Rational (node->qualified.expression, out);
      return false;
    case NK_APPLY:
      if (node->apply.prefix and node->apply.arguments.count == 1 and
        node->apply.prefix->symbol and
        node->apply.prefix->symbol->kind == SYMBOL_TYPE)
        return Eval_Const_Rational (node->apply.arguments.items[0], out);
      return false;
    case NK_UNARY_OP: {
      Rational operand;
      if (not Eval_Const_Rational (node->unary.operand, &operand)) return false;
      if (node->unary.op == TK_MINUS) {
        operand.numerator = Big_Integer_Clone (operand.numerator);
        operand.numerator->is_negative = not operand.numerator->is_negative;
      } else if (node->unary.op == TK_ABS) {
        operand.numerator = Big_Integer_Clone (operand.numerator);
        operand.numerator->is_negative = false;
      }
      *out = operand;
      return true;
    }
    case NK_BINARY_OP: {
      Rational l, r;

      // For exponentiation, exponent must be integer
      if (node->binary.op == TK_EXPON) {
        if (not Eval_Const_Rational (node->binary.left, &l)) return false;
        double re = Eval_Const_Numeric (node->binary.right);
        if (re != re or re != floor (re) or fabs (re) > 1000) return false;
        *out = Rational_Pow (l, (int)re);
        return true;
      }
      if (not Eval_Const_Rational (node->binary.left, &l)) return false;
      if (not Eval_Const_Rational (node->binary.right, &r)) return false;
      switch (node->binary.op) {
        case TK_PLUS:  *out = Rational_Add (l, r); return true;
        case TK_MINUS: *out = Rational_Sub (l, r); return true;
        case TK_STAR:  *out = Rational_Mul (l, r); return true;
        case TK_SLASH:
          if (r.numerator->count == 0) return false;
          *out = Rational_Div (l, r); return true;
        default: return false;
      }
    }
    default: return false;
  }
}

// Integer-precise constant evaluator for modular type modulus expressions.                         
// Unlike Eval_Const_Numeric (which uses double, losing precision above 2^53),                      
// this evaluates in uint128_t for exact results up to 2^128.                                       
// Returns true on success, false if the expression is not a compile-time                           
// integer constant.  Handles 2**64, 2**128, and all intermediate values                            
// without sentinel hacks.                                                                          
//                                                                                                  
bool Eval_Const_Uint128 (Syntax_Node *node, uint128_t *out) {
  if (not node) return false;
  switch (node->kind) {
    case NK_INTEGER:

      // integer_lit.value is int64_t; for values > INT64_MAX, use big_value
      if (node->integer_lit.big_value) {
        return Big_Integer_To_Uint128 (node->integer_lit.big_value, out);
      }
      *out = (uint128_t)(uint64_t)node->integer_lit.value;
      return true;
    case NK_IDENTIFIER:
    case NK_SELECTED: {
      Symbol *sym = node ? node->symbol : NULL;
      if (sym and sym->kind == SYMBOL_CONSTANT and sym->is_named_number and
        sym->declaration and sym->declaration->kind == NK_OBJECT_DECL) {
        return Eval_Const_Uint128 (sym->declaration->object_decl.init, out);
      }
      return false;
    }
    case NK_UNARY_OP:
      if (node->unary.op == TK_PLUS) return Eval_Const_Uint128 (node->unary.operand, out);
      if (node->unary.op == TK_MINUS) {
        uint128_t v;
        if (Eval_Const_Uint128 (node->unary.operand, &v)) {
          *out = (uint128_t)(-(int128_t)v);
          return true;
        }
      }
      return false;
    case NK_BINARY_OP: {
      uint128_t l, r;
      if (not Eval_Const_Uint128 (node->binary.left, &l)) return false;
      if (not Eval_Const_Uint128 (node->binary.right, &r)) return false;
      switch (node->binary.op) {

        // Integer exponentiation: l ** r.  For modular type declarations,                          
        // the common case is 2**N.  2**64 and 2**128 compute exactly                               
        // in uint128_t (2**128 wraps to 0, but we cap r at 127 for                                 
        // the shift path below).                                                                   
        //                                                                                          
        case TK_PLUS:  *out = l + r; return true;
        case TK_MINUS: *out = l - r; return true;
        case TK_STAR:  *out = l * r; return true;
        case TK_SLASH: if (r == 0) return false; *out = l / r; return true;
        case TK_EXPON: {

          // Fast path: 2**N via shift.  2**128 = 0 in uint128_t,                                   
          // but we return it as 0 which the caller interprets as                                   
          // "the 128-bit boundary" for mod types.                                                  
          //                                                                                        
          if (l == 2 and r <= 128) {
            *out = (r == 128) ? (uint128_t)0 : ((uint128_t)1 << r);
            return true;
          }
          uint128_t result = 1;

          // Type conversions: TYPE_NAME (expr)
          for (uint128_t i = 0; i < r; i++) result *= l;
          *out = result;
          return true;
        }
        default: return false;
      }
    }
    case NK_APPLY: {
      if (node->apply.prefix and node->apply.arguments.count == 1) {
        Syntax_Node *prefix = node->apply.prefix;
        if (prefix->kind == NK_IDENTIFIER and prefix->symbol and
          prefix->symbol->kind == SYMBOL_TYPE) {
          return Eval_Const_Uint128 (node->apply.arguments.items[0], out);
        }
      }
      return false;
    }
    default: return false;
  }
}

// Return bound value as int128_t.  For most types this fits in 64 bits;
// for mod 2**128 the high bound is 2^128-1 which requires the full range.
int128_t Type_Bound_Value (Type_Bound bound) {
  if (bound.kind == BOUND_INTEGER) return bound.int_value;
  if (bound.kind == BOUND_FLOAT)   return (int128_t) bound.float_value;

  // Try to evaluate expression bound at compile time
  if (bound.kind == BOUND_EXPR and bound.expr) {
    double val = Eval_Const_Numeric (bound.expr);
    if (val == val) return (int128_t) val;  // Not NaN
  }
  return 0;  // BOUND_NONE or unevaluable expression
}

// Format an int128_t as a decimal string.  Returns pointer to a static                             
// thread-local buffer.  Handles the full range -2^127 .. 2^127-1.                                  
// Used for emitting i128 constants in LLVM IR (which accepts arbitrary                             
// width decimal literals).                                                                         
//                                                                                                  
const char *I128_Decimal (int128_t value) {
  static _Thread_local char buf[42];  // -170141183460469231731687303715884105728 + NUL
  if (value == 0) return "0";
  char *cursor      = buf + sizeof (buf) - 1;
  *cursor            = '\0';
  bool      negative = value < 0;
  uint128_t absolute = negative ? (uint128_t) (-(value + 1)) + 1 : (uint128_t) value;
  while (absolute) { *--cursor = '0' + (char) (absolute % 10); absolute /= 10; }
  if (negative) *--cursor = '-';
  return cursor;
}

// Format a uint128_t as a decimal string.  Returns pointer to a static
// thread-local buffer.  Handles 0 .. 2^128-1.
const char *U128_Decimal (uint128_t value) {
  static _Thread_local char buf[40];  // 340282366920938463463374607431768211455 + NUL
  if (value == 0) return "0";
  char *cursor = buf + sizeof (buf) - 1;
  *cursor       = '\0';
  while (value) { *--cursor = '0' + (char) (value % 10); value /= 10; }
  return cursor;
}

// Get array element count for constrained arrays, 0 for unconstrained
int128_t Array_Element_Count (Type_Info *type_info) {
  if (not type_info or type_info->kind != TYPE_ARRAY or not type_info->array.is_constrained)
    return 0;
  if (type_info->array.index_count == 0)
    return 0;

  // Product of all dimension lengths (RM 3.6.1)
  int128_t total = 1;
  for (uint32_t dim = 0; dim < type_info->array.index_count; dim++) {
    int128_t lo     = Type_Bound_Value (type_info->array.indices[dim].low_bound);
    int128_t hi     = Type_Bound_Value (type_info->array.indices[dim].high_bound);
    int128_t length = hi - lo + 1;
    if (length <= 0) return 0;
    total *= length;
  }
  return total;
}

// Get array low bound for index adjustment
int128_t Array_Low_Bound (Type_Info *type_info) {
  if (not type_info or type_info->kind != TYPE_ARRAY or type_info->array.index_count == 0)
    return 0;
  return Type_Bound_Value (type_info->array.indices[0].low_bound);
}
Type_Info *Resolve_Expression (Syntax_Node *node) {
  if (not node) return NULL;
  switch (node->kind) {
    case NK_INTEGER:    node->type = sm->type_universal_integer;  return node->type;
    case NK_REAL:       node->type = sm->type_universal_real;     return node->type;
    case NK_CHARACTER:  node->type = sm->type_character;          return node->type;
    case NK_STRING:     node->type = sm->type_string;             return node->type;
    case NK_NULL:       node->type = NULL;                        return NULL;
    case NK_IDENTIFIER: return Resolve_Identifier (node);
    case NK_SELECTED:   return Resolve_Selected (node);
    case NK_BINARY_OP:  return Resolve_Binary_Op (node);
    case NK_UNARY_OP:
      node->type = Resolve_Expression (node->unary.operand);

      // Check for user-defined unary operators (RM 4.5.6, 6.7)
      if (node->unary.op == TK_PLUS or node->unary.op == TK_MINUS or
        node->unary.op == TK_ABS or node->unary.op == TK_NOT) {
        String_Slice op_name = {0};
        if (node->unary.op == TK_PLUS) op_name = S("+");
        else if (node->unary.op == TK_MINUS) op_name = S("-");
        else if (node->unary.op == TK_ABS) op_name = S("abs");
        else if (node->unary.op == TK_NOT) op_name = S("not");
        Type_Info *operand_type = node->unary.operand ? node->unary.operand->type : NULL;
        if (op_name.length > 0 and operand_type) {
          Type_Info *arg_types[1] = { operand_type };
          Argument_Info args = { .types = arg_types, .count = 1, .names = NULL };
          Symbol *user_op = Resolve_Overloaded_Call (op_name, &args, NULL);
          if (user_op and user_op->kind == SYMBOL_FUNCTION and
            not user_op->is_predefined) {
            node->symbol = user_op;
            node->type = user_op->return_type;
            return node->type;
          }
        }
      }

      // NOT preserves array-of-BOOLEAN type (RM 4.5.6);
      // for scalar operands it returns BOOLEAN.
      if (node->unary.op == TK_NOT) {
        Type_Info *operand_type = node->unary.operand ? node->unary.operand->type : NULL;
        if (operand_type and Type_Is_Array_Like (operand_type) and
          operand_type->array.element_type and
          Type_Is_Boolean (operand_type->array.element_type)) {
          node->type = operand_type;  // boolean array > boolean array
        } else {
          node->type = sm->type_boolean;
        }

      // .ALL dereference: result is the designated type (RM 4.1)
      } else if (node->unary.op == TK_ALL) {
        Type_Info *operand_type = node->unary.operand->type;
        if (Type_Is_Access (operand_type)) {
          node->type = operand_type->access.designated_type;
        }
      }
      return node->type;
    case NK_APPLY:
      return Resolve_Apply (node);
    case NK_ATTRIBUTE:
      Resolve_Expression (node->attribute.prefix);

      // Resolve attribute arguments with type context for certain attributes
      {
        Type_Info *prefix_type = node->attribute.prefix->type;
        String_Slice attr = node->attribute.name;

        // For POS, SUCC, PRED, IMAGE - argument should be of prefix type
        bool needs_enum_context =
          (Type_Is_Enumeration (prefix_type) or
           Type_Is_Enumeration (prefix_type ? prefix_type->parent_type : NULL)) and
          (Slice_Equal_Ignore_Case (attr, S("POS")) or
           Slice_Equal_Ignore_Case (attr, S("SUCC")) or
           Slice_Equal_Ignore_Case (attr, S("PRED")) or
           Slice_Equal_Ignore_Case (attr, S("IMAGE")));
        for (uint32_t i = 0; i < node->attribute.arguments.count; i++) {
          Syntax_Node *arg = node->attribute.arguments.items[i];
          bool resolved_as_enum = false;

          // Check if character literal should be resolved as enum literal
          // Get the character from the literal (format: 'X')
          if (needs_enum_context and arg and arg->kind == NK_CHARACTER) {
            String_Slice lit_text = arg->string_val.text;
            char ch = lit_text.length >= 2 ? lit_text.data[1] : 0;

            // Find enum type (handle derived types)
            Type_Info *enum_type = prefix_type;
            while (enum_type and enum_type->parent_type)
              enum_type = enum_type->parent_type;

            // Look for matching character literal in enum
            if (Type_Is_Enumeration (enum_type) and
              enum_type->enumeration.literals) {
              for (uint32_t j = 0; j < enum_type->enumeration.literal_count; j++) {
                String_Slice lit_name = enum_type->enumeration.literals[j];
                if (lit_name.length == 3 and
                  lit_name.data[0] == '\'' and
                  lit_name.data[1] == ch and
                  lit_name.data[2] == '\'') {

                  // Found matching enum literal - set symbol with type match
                  Symbol *lit_sym = Symbol_Find_By_Type (lit_name, enum_type);
                  if (lit_sym and lit_sym->kind == SYMBOL_LITERAL) {
                    arg->symbol = lit_sym;
                    arg->type = prefix_type;
                    resolved_as_enum = true;
                  }
                  break;
                }
              }
            }
          }

          // Only resolve if not already resolved as enum literal
          if (not resolved_as_enum) {
            Resolve_Expression (arg);
          }
        }
      }

      // Attribute type depends on attribute name and prefix type
      {
        Type_Info *prefix_type = node->attribute.prefix->type;
        String_Slice attr = node->attribute.name;

        // Implicit dereference for access types (RM 4.1(3))                                        
        // A1'FIRST where A1 is access-to-array is equivalent to A1.ALL'FIRST                       
        // But NOT for type-level attributes like SIZE, STORAGE_SIZE, BASE (RM 13.7.2)              
        //                                                                                          
        if (Type_Is_Access (prefix_type) and
          prefix_type->access.designated_type and
          not Slice_Equal_Ignore_Case (attr, S("SIZE")) and
          not Slice_Equal_Ignore_Case (attr, S("STORAGE_SIZE")) and
          not Slice_Equal_Ignore_Case (attr, S("BASE")) and
          not Slice_Equal_Ignore_Case (attr, S("ADDRESS"))) {
          prefix_type = prefix_type->access.designated_type;
        }

        // FIRST, LAST return the index type for arrays, base type for scalars
        if (Slice_Equal_Ignore_Case (attr, S("FIRST")) or
          Slice_Equal_Ignore_Case (attr, S("LAST"))) {

          // For arrays, FIRST/LAST return the actual index type for that dimension
          if (Type_Is_Array_Like (prefix_type)) {
            uint32_t dim = 0;  // Default to first dimension (0-indexed)
            if (node->attribute.arguments.count > 0) {
              Syntax_Node *dim_arg = node->attribute.arguments.items[0];
              if (dim_arg and dim_arg->kind == NK_INTEGER) {
                dim = (uint32_t)(dim_arg->integer_lit.value - 1);
              }
            }

            // Get the index type for this dimension
            if (prefix_type->kind == TYPE_ARRAY and
              prefix_type->array.indices and
              dim < prefix_type->array.index_count and
              prefix_type->array.indices[dim].index_type) {
              node->type = prefix_type->array.indices[dim].index_type;

            // Default to INTEGER for strings or missing info
            } else {
              node->type = sm->type_integer;
            }

          // For scalar types, return the type itself
          } else {
            node->type = prefix_type ? prefix_type : sm->type_integer;
          }
        }

        // VAL, SUCC, PRED return the base type (for scalar types)
        else if (Slice_Equal_Ignore_Case (attr, S("VAL")) or
             Slice_Equal_Ignore_Case (attr, S("SUCC")) or
             Slice_Equal_Ignore_Case (attr, S("PRED"))) {
          node->type = prefix_type ? prefix_type : sm->type_integer;
        }

        // POS returns universal integer
        else if (Slice_Equal_Ignore_Case (attr, S("POS"))) {
          node->type = sm->type_universal_integer;
        }

        // IMAGE returns STRING
        else if (Slice_Equal_Ignore_Case (attr, S("IMAGE"))) {
          node->type = sm->type_string;
        }

        // SIZE, LENGTH, COUNT, WIDTH, MANTISSA, etc. return universal integer
        else if (Slice_Equal_Ignore_Case (attr, S("SIZE")) or
             Slice_Equal_Ignore_Case (attr, S("LENGTH")) or
             Slice_Equal_Ignore_Case (attr, S("COUNT")) or
             Slice_Equal_Ignore_Case (attr, S("WIDTH")) or
             Slice_Equal_Ignore_Case (attr, S("MANTISSA")) or
             Slice_Equal_Ignore_Case (attr, S("MACHINE_MANTISSA")) or
             Slice_Equal_Ignore_Case (attr, S("DIGITS")) or
             Slice_Equal_Ignore_Case (attr, S("EMAX")) or
             Slice_Equal_Ignore_Case (attr, S("MACHINE_EMAX")) or
             Slice_Equal_Ignore_Case (attr, S("MACHINE_EMIN")) or
             Slice_Equal_Ignore_Case (attr, S("MACHINE_RADIX")) or
             Slice_Equal_Ignore_Case (attr, S("SAFE_EMAX")) or
             Slice_Equal_Ignore_Case (attr, S("STORAGE_SIZE")) or
             Slice_Equal_Ignore_Case (attr, S("MODULUS")) or
             Slice_Equal_Ignore_Case (attr, S("AFT")) or
             Slice_Equal_Ignore_Case (attr, S("FORE"))) {
          node->type = sm->type_universal_integer;
        }

        // Floating-point type attributes returning universal_real (RM 3.5.8)
        else if (Slice_Equal_Ignore_Case (attr, S("EPSILON")) or
             Slice_Equal_Ignore_Case (attr, S("SMALL")) or
             Slice_Equal_Ignore_Case (attr, S("LARGE")) or
             Slice_Equal_Ignore_Case (attr, S("SAFE_SMALL")) or
             Slice_Equal_Ignore_Case (attr, S("SAFE_LARGE")) or
             Slice_Equal_Ignore_Case (attr, S("DELTA")) or
             Slice_Equal_Ignore_Case (attr, S("MODEL_EPSILON")) or
             Slice_Equal_Ignore_Case (attr, S("MODEL_SMALL"))) {
          node->type = sm->type_universal_real;
        }

        // Boolean attributes (RM 3.5.8, 3.7.1, 9.9)
        else if (Slice_Equal_Ignore_Case (attr, S("MACHINE_OVERFLOWS")) or
             Slice_Equal_Ignore_Case (attr, S("MACHINE_ROUNDS")) or
             Slice_Equal_Ignore_Case (attr, S("CONSTRAINED")) or
             Slice_Equal_Ignore_Case (attr, S("CALLABLE")) or
             Slice_Equal_Ignore_Case (attr, S("TERMINATED"))) {
          node->type = sm->type_boolean;
        }

        // ADDRESS attribute returns SYSTEM.ADDRESS (RM 13.7.2)
        else if (Slice_Equal_Ignore_Case (attr, S("ADDRESS"))) {
          node->type = sm->type_address;
        }

        // BASE attribute returns the base type (RM 3.3.2)
        else if (Slice_Equal_Ignore_Case (attr, S("BASE"))) {

          // T'BASE is a type, used as prefix for other attributes like T'BASE'FIRST                
          // The type should be the base type of the prefix type.                                   
          // For derived types (TYPE T IS NEW X), follow parent_type chain.                         
          // For constrained subtypes (SUBTYPE S IS X RANGE ...), follow base_type chain.           
          // Use Type_Root to handle both cases and find the root type.                             
          //                                                                                        
          if (prefix_type) {
            Type_Info *base = Type_Root (prefix_type);
            node->type = base ? base : prefix_type;
          } else {
            node->type = sm->type_integer;
          }
        }

        // VALUE attribute returns the type itself (converts string to type)
        else if (Slice_Equal_Ignore_Case (attr, S("VALUE"))) {

          // T'VALUE (S) returns a value of type T
          node->type = prefix_type ? prefix_type : sm->type_integer;
        }

        // Default to integer for unhandled attributes
        else {
          node->type = sm->type_integer;
        }
      }
      return node->type;
    case NK_QUALIFIED:

      // Resolve subtype mark first to get the type
      Resolve_Expression (node->qualified.subtype_mark);

      // Propagate type to expression (critical for aggregates)
      if (node->qualified.expression and
        node->qualified.expression->kind == NK_AGGREGATE and
        node->qualified.subtype_mark->type) {
        node->qualified.expression->type = node->qualified.subtype_mark->type;
      }
      Resolve_Expression (node->qualified.expression);

      // Re-resolve overloaded literals against qualifying type (RM 4.7):                           
      // WEEKEND'(SAT) must pick WEEKEND.SAT, not WEEK.SAT;                                         
      // CHAR'('B') must use CHAR position, not ASCII code.                                         
      //                                                                                            
      if (node->qualified.expression and node->qualified.subtype_mark->type) {
        Type_Info *qt = node->qualified.subtype_mark->type;
        Syntax_Node *inner = node->qualified.expression;
        if (inner->kind == NK_IDENTIFIER) {
          if (not Type_Covers (qt, inner->type) and not Type_Covers (inner->type, qt)) {
            Symbol *s = Symbol_Find_By_Type (inner->string_val.text, qt);
            if (s) { inner->symbol = s; inner->type = s->type ? s->type : qt; }
          }
        } else if (inner->kind == NK_CHARACTER) {
          Resolve_Char_As_Enum (inner, qt);
        }
      }
      node->type = node->qualified.subtype_mark->type;
      return node->type;
    case NK_AGGREGATE:
      {
        Type_Info *agg_type = node->type;
        bool is_record_agg = Type_Is_Record (agg_type);
        uint32_t positional_idx = 0;
        for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
          Syntax_Node *item = node->aggregate.items.items[i];

          // For record aggregates, choices are field names - don't resolve as variables
          if (is_record_agg and item->kind == NK_ASSOCIATION) {

            // Find component type from first choice for nested aggregates
            Type_Info *comp_type = NULL;
            if (item->association.choices.count > 0) {
              Syntax_Node *choice = item->association.choices.items[0];
              if (choice->kind == NK_IDENTIFIER) {
                String_Slice comp_name = choice->string_val.text;
                for (uint32_t j = 0; j < agg_type->record.component_count; j++) {
                  if (Slice_Equal_Ignore_Case (agg_type->record.components[j].name, comp_name)) {
                    comp_type = agg_type->record.components[j].component_type;
                    break;
                  }
                }
              }
            }

            // Propagate component type to nested aggregates
            if (item->association.expression) {
              if (item->association.expression->kind == NK_AGGREGATE and comp_type) {
                item->association.expression->type = comp_type;
              }
              Resolve_Expression (item->association.expression);
            }

          // Positional item in record aggregate - propagate component type
          } else if (is_record_agg) {
            if (positional_idx < agg_type->record.component_count) {
              Type_Info *comp_type = agg_type->record.components[positional_idx].component_type;
              if (item->kind == NK_AGGREGATE and comp_type) {
                item->type = comp_type;
              }
              positional_idx++;
            }
            Resolve_Expression (item);

          // For array aggregates, propagate element type to nested aggregates
          } else {
            Type_Info *elem_type = NULL;
            if (Type_Is_Array_Like (agg_type) and agg_type->array.element_type) {
              elem_type = agg_type->array.element_type;
            }

            // For multi-dimensional arrays (index_count > 1), inner aggregates                     
            // represent "rows" (slices along the first dimension).  Create an                      
            // implicit 1-D array type from the remaining dimensions so that                        
            // Generate_Aggregate can handle them as composite elements (RM 4.3.2).                 
            //                                                                                      
            Type_Info *inner_agg_type = elem_type;
            if (Type_Is_Array_Like (agg_type) and agg_type->array.index_count > 1) {
              Type_Info *row_type = Type_New (TYPE_ARRAY, S(""));
              row_type->array.element_type = agg_type->array.element_type;
              row_type->array.is_constrained = true;
              row_type->array.index_count = agg_type->array.index_count - 1;
              row_type->array.indices = Arena_Allocate (
                row_type->array.index_count * sizeof (Index_Info));

              // Copy remaining dimensions (skip first) and derive                                  
              // bounds from index_type when BOUND_NONE (unconstrained).                            
              // RM 4.3.3(6): lower bound comes from index subtype.                                 
              //                                                                                    
              for (uint32_t d = 0; d < row_type->array.index_count; d++) {
                row_type->array.indices[d] = agg_type->array.indices[d + 1];
                if (row_type->array.indices[d].low_bound.kind == BOUND_NONE and
                  row_type->array.indices[d].index_type)
                  row_type->array.indices[d].low_bound =
                    row_type->array.indices[d].index_type->low_bound;
                if (row_type->array.indices[d].high_bound.kind == BOUND_NONE and
                  row_type->array.indices[d].index_type)
                  row_type->array.indices[d].high_bound =
                    row_type->array.indices[d].index_type->high_bound;
              }

              // Calculate row size
              uint32_t row_elems = 1;
              for (uint32_t dim = 0; dim < row_type->array.index_count; dim++) {
                int128_t lo     = Type_Bound_Value (row_type->array.indices[dim].low_bound);
                int128_t hi     = Type_Bound_Value (row_type->array.indices[dim].high_bound);
                int128_t extent = hi - lo + 1;
                if (extent > 0) row_elems *= (uint32_t) extent;
              }
              uint32_t element_size = agg_type->array.element_type
                                    ? agg_type->array.element_type->size : 8;
              if (element_size == 0) element_size = 8;
              row_type->size = row_elems * element_size;
              row_type->alignment = agg_type->alignment;
              inner_agg_type = row_type;
            }
            if (inner_agg_type and item->kind == NK_ASSOCIATION and item->association.expression) {
              Syntax_Node *expr = item->association.expression;
              if (expr->kind == NK_AGGREGATE) {
                expr->type = inner_agg_type;
              }
            } else if (inner_agg_type and item->kind == NK_AGGREGATE) {
              item->type = inner_agg_type;
            }
            Resolve_Expression (item);
          }
        }
        return node->type;  // Type from context
      }

    // Propagate type to initializer (critical for aggregates).
    // Parser destructures T'(agg) so expression is directly the aggregate
    case NK_ALLOCATOR:

      // Resolve subtype mark first to get allocated type
      Resolve_Expression (node->allocator.subtype_mark);
      if (node->allocator.expression) {
        if (node->allocator.expression->kind == NK_AGGREGATE and
          node->allocator.subtype_mark and
          node->allocator.subtype_mark->type) {
          node->allocator.expression->type = node->allocator.subtype_mark->type;
        }
        Resolve_Expression (node->allocator.expression);
      }

      // Create access type pointing to allocated type
      {
        Type_Info *access_type = Type_New (TYPE_ACCESS, S(""));
        access_type->size = 8;
        access_type->alignment = 8;
        if (node->allocator.subtype_mark and node->allocator.subtype_mark->type) {
          access_type->access.designated_type = node->allocator.subtype_mark->type;
        }
        node->type = access_type;
      }
      return node->type;
    case NK_RANGE:
      if (node->range.low) Resolve_Expression (node->range.low);
      if (node->range.high) Resolve_Expression (node->range.high);

      // Ada RM 4.1.1: in a range L..H, character literals must be                                  
      // resolved against the other operand's enum type (like binary ops).                          
      // GNAT: overload resolution propagates expected type to both bounds.                         
      //                                                                                            
      {
        Type_Info *lt = node->range.low  ? node->range.low->type  : NULL;
        Type_Info *ht = node->range.high ? node->range.high->type : NULL;
        if (ht and Type_Is_Enumeration (ht) and
          node->range.low and node->range.low->kind == NK_CHARACTER)
          Resolve_Char_As_Enum (node->range.low, ht);
        else if (lt and Type_Is_Enumeration (lt) and
             node->range.high and node->range.high->kind == NK_CHARACTER)
          Resolve_Char_As_Enum (node->range.high, lt);

        // Also handle derived enum types via parent chain
        if (ht and ht->parent_type and Type_Is_Enumeration (ht->parent_type) and
          node->range.low and node->range.low->kind == NK_CHARACTER)
          Resolve_Char_As_Enum (node->range.low, ht);
        else if (lt and lt->parent_type and Type_Is_Enumeration (lt->parent_type) and
             node->range.high and node->range.high->kind == NK_CHARACTER)
          Resolve_Char_As_Enum (node->range.high, lt);
      }
      node->type = node->range.low ? node->range.low->type : NULL;
      if (not node->type) node->type = node->range.high ? node->range.high->type : NULL;
      return node->type;
    case NK_ASSOCIATION:
      for (uint32_t i = 0; i < node->association.choices.count; i++) {
        Resolve_Expression (node->association.choices.items[i]);
      }

      // For case alternatives, expression is a block with statements
      if (node->association.expression) {
        if (node->association.expression->kind == NK_BLOCK) {
          Resolve_Statement (node->association.expression);
        } else {
          Resolve_Expression (node->association.expression);
        }
      }
      return node->association.expression ? node->association.expression->type : NULL;

    // Create array type info from syntax node
    case NK_ARRAY_TYPE:
      {
        Type_Info *array_type = Type_New (TYPE_ARRAY, S(""));
        array_type->array.is_constrained = node->array_type.is_constrained;
        array_type->array.index_count = (uint32_t)node->array_type.indices.count;

        // Allocate index info
        if (array_type->array.index_count > 0) {
          array_type->array.indices = Arena_Allocate (
            array_type->array.index_count * sizeof (Index_Info));
          for (uint32_t i = 0; i < array_type->array.index_count; i++) {
            Syntax_Node *idx = node->array_type.indices.items[i];
            Resolve_Expression (idx);
            Index_Info *info = &array_type->array.indices[i];
            info->index_type = sm->type_integer;

            // Extract bounds from range or subtype indication
            Syntax_Node *range_node = NULL;
            if (idx->kind == NK_RANGE) {
              range_node = idx;

              // Infer index type from range bounds' type
              if (idx->range.low and idx->range.low->type) {
                info->index_type = idx->range.low->type;
              } else if (idx->range.high and idx->range.high->type) {
                info->index_type = idx->range.high->type;
              }
            } else if (idx->kind == NK_SUBTYPE_INDICATION and
                   idx->subtype_ind.constraint and
                   idx->subtype_ind.constraint->kind == NK_RANGE_CONSTRAINT) {
              range_node = idx->subtype_ind.constraint->range_constraint.range;

              // Also use the subtype mark's type for index type
              if (idx->subtype_ind.subtype_mark and idx->subtype_ind.subtype_mark->type) {
                info->index_type = idx->subtype_ind.subtype_mark->type;
              }
            } else if (idx->kind == NK_SUBTYPE_INDICATION and
                   idx->subtype_ind.subtype_mark and
                   idx->subtype_ind.subtype_mark->type) {

              // Unconstrained index type (just a type mark, no constraint)
              info->index_type = idx->subtype_ind.subtype_mark->type;

            // Use resolved type from identifier/expression (e.g., BOOLEAN)
            } else if (idx->type) {
              info->index_type = idx->type;
            }
            if (range_node and range_node->kind == NK_RANGE and
              range_node->range.low and range_node->range.high) {

              // Helper to extract static integer from various expression forms
              bool extract_static_bound(Syntax_Node *expr, int64_t *out) {
                if (not expr) return false;

                // Integer literal
                if (expr->kind == NK_INTEGER) {
                  *out = expr->integer_lit.value;
                  return true;
                }

                // Character/enum literal (symbol with frame_offset as position)
                if (expr->symbol and expr->symbol->kind == SYMBOL_LITERAL) {
                  *out = expr->symbol->frame_offset;
                  return true;
                }

                // Qualified expression: TYPE'(expr) - evaluate inner expression
                if (expr->kind == NK_QUALIFIED and expr->qualified.expression) {
                  Syntax_Node *inner = expr->qualified.expression;
                  if (inner->kind == NK_INTEGER) {
                    *out = inner->integer_lit.value;
                    return true;
                  }
                  if (inner->symbol and inner->symbol->kind == SYMBOL_LITERAL) {
                    *out = inner->symbol->frame_offset;
                    return true;
                  }

                  // Handle character literal inside qualified expression:                          
                  // look up in the qualifying type's enumeration literals                          
                  // First resolve the subtype_mark to get its type                                 
                  //                                                                                
                  if (inner->kind == NK_CHARACTER and expr->qualified.subtype_mark) {
                    if (not expr->qualified.subtype_mark->type) {
                      Resolve_Expression (expr->qualified.subtype_mark);
                    }
                    Type_Info *qual_type = expr->qualified.subtype_mark->type;

                    // Walk up to find the base enumeration type with literals
                    while (Type_Is_Enumeration (qual_type) and
                         not qual_type->enumeration.literals) {
                      qual_type = qual_type->base_type ? qual_type->base_type : qual_type->parent_type;
                    }
                    if (Type_Is_Enumeration (qual_type) and
                      qual_type->enumeration.literals) {

                      // Extract the character from 'X' format
                      String_Slice lit_text = inner->string_val.text;
                      char ch = lit_text.length >= 2 ? lit_text.data[1] : 0;

                      // Look for matching character literal in enum
                      for (uint32_t j = 0; j < qual_type->enumeration.literal_count; j++) {
                        String_Slice lit_name = qual_type->enumeration.literals[j];
                        if (lit_name.length == 3 and
                          lit_name.data[0] == '\'' and
                          lit_name.data[1] == ch and
                          lit_name.data[2] == '\'') {
                          *out = (int64_t)j;  // Position in enumeration
                          return true;
                        }
                      }
                    }
                  }

                  // Try constant evaluation for other inner expressions
                  double val = Eval_Const_Numeric (inner);
                  if (val == val) {  // Not NaN
                    *out = (int64_t)val;
                    return true;
                  }
                }

                // Try general constant evaluation
                double val = Eval_Const_Numeric (expr);
                if (val == val) {  // Not NaN
                  *out = (int64_t)val;
                  return true;
                }
                return false;
              }
              int64_t low_val, high_val;
              if (extract_static_bound(range_node->range.low, &low_val)) {
                info->low_bound = (Type_Bound){
                  .kind = BOUND_INTEGER,
                  .int_value = low_val
                };

              // Non-static bound - store expression reference
              } else {
                info->low_bound = (Type_Bound){
                  .kind = BOUND_EXPR,
                  .expr = range_node->range.low
                };
              }
              if (extract_static_bound(range_node->range.high, &high_val)) {
                info->high_bound = (Type_Bound){
                  .kind = BOUND_INTEGER,
                  .int_value = high_val
                };

              // Non-static bound - store expression reference
              } else {
                info->high_bound = (Type_Bound){
                  .kind = BOUND_EXPR,
                  .expr = range_node->range.high
                };
              }
            }
          }
        }

        // Resolve component type
        if (node->array_type.component_type) {
          Resolve_Expression (node->array_type.component_type);
          array_type->array.element_type = node->array_type.component_type->type;
        } else {
          array_type->array.element_type = sm->type_integer;
        }

        // Compute size - only when ALL bounds are statically known.                                
        // Skip if any bound is BOUND_EXPR that evaluates to a value                                
        // suggesting the bound can't be determined at compile time                                 
        // (e.g. discriminant references). RM 3.6.1                                                 
        //                                                                                          
        if (array_type->array.is_constrained and array_type->array.index_count > 0) {
          bool all_static = true;
          int128_t count = 1;
          for (uint32_t i = 0; i < array_type->array.index_count; i++) {
            Type_Bound *lo_b = &array_type->array.indices[i].low_bound;
            Type_Bound *hi_b = &array_type->array.indices[i].high_bound;
            if ((lo_b->kind == BOUND_EXPR or lo_b->kind == BOUND_NONE) and
              lo_b->kind != BOUND_INTEGER) {

              // Try static eval; if it returns 0 for a bound that
              // references a symbol, treat as non-static
              if (lo_b->kind == BOUND_EXPR and lo_b->expr and lo_b->expr->symbol) {
                all_static = false; break;
              }
            }
            if ((hi_b->kind == BOUND_EXPR or hi_b->kind == BOUND_NONE) and
              hi_b->kind != BOUND_INTEGER) {
              if (hi_b->kind == BOUND_EXPR and hi_b->expr and hi_b->expr->symbol) {
                all_static = false; break;
              }
            }
            int128_t lo = Type_Bound_Value (*lo_b);
            int128_t hi = Type_Bound_Value (*hi_b);
            int128_t dim = hi - lo + 1;
            if (dim < 0) dim = 0;
            count *= dim;
          }
          if (not all_static) {
            array_type->size = 0;
          } else if (all_static and count >= 0) {
            uint32_t elem_size = array_type->array.element_type ?
                       array_type->array.element_type->size : 8;
            if (elem_size == 0 and Type_Needs_Fat_Pointer_Load (array_type->array.element_type))
              elem_size = FAT_PTR_ALLOC_SIZE;
            array_type->size = (uint32_t)(count * elem_size);
          }
        }
        node->type = array_type;
        return array_type;
      }

    // Create enumeration type info from syntax node                                                
    // Note: Literal symbols are created later in NK_TYPE_DECL processing                           
    // so they reference the named type, not this anonymous type                                    
    //                                                                                              
    case NK_ENUMERATION_TYPE:
      {
        Type_Info *enum_type = Type_New (TYPE_ENUMERATION, S(""));
        uint32_t lit_count = (uint32_t)node->enum_type.literals.count;
        enum_type->enumeration.literal_count = lit_count;
        if (lit_count > 0) {
          enum_type->enumeration.literals = Arena_Allocate (
            lit_count * sizeof (String_Slice));
          for (uint32_t i = 0; i < lit_count; i++) {
            Syntax_Node *lit = node->enum_type.literals.items[i];
            enum_type->enumeration.literals[i] = lit->string_val.text;
          }
        }

        // Size based on number of literals
        if (lit_count <= 256) {
          enum_type->size = 1;  // Fits in 1 byte
        } else if (lit_count <= 65536) {
          enum_type->size = 2;  // Fits in 2 bytes
        } else {
          enum_type->size = 4;  // 4 bytes for large enums
        }
        enum_type->alignment = enum_type->size;
        enum_type->low_bound = (Type_Bound){.kind = BOUND_INTEGER, .int_value = 0};
        enum_type->high_bound = (Type_Bound){.kind = BOUND_INTEGER, .int_value = lit_count - 1};
        node->type = enum_type;
        return enum_type;
      }

    // Derived type: type T is new Parent [constraint]
    case NK_DERIVED_TYPE:
      {
        Resolve_Expression (node->derived_type.parent_type);
        Type_Info *parent = node->derived_type.parent_type ?
                  node->derived_type.parent_type->type : NULL;
        if (not parent) {
          node->type = NULL;
          return NULL;
        }

        // Create new type that inherits from parent
        Type_Info *derived = Type_New (parent->kind, S(""));
        derived->parent_type = parent;
        derived->size = parent->size;
        derived->alignment = parent->alignment;
        derived->specified_bit_size = parent->specified_bit_size;
        derived->storage_size = parent->storage_size;
        derived->low_bound = parent->low_bound;
        derived->high_bound = parent->high_bound;

        // Copy kind-specific info
        if (Type_Is_Enumeration (parent)) {
          derived->enumeration = parent->enumeration;
        } else if (Type_Is_Array_Like (parent)) {
          derived->array = parent->array;
        } else if (Type_Is_Record (parent)) {
          derived->record = parent->record;
        } else if (Type_Is_Access (parent)) {
          derived->access = parent->access;
        } else if (Type_Is_Fixed_Point (parent)) {
          derived->fixed = parent->fixed;
        } else if (Type_Is_Float (parent)) {
          derived->flt = parent->flt;
        }

        // Apply constraint if present
        if (node->derived_type.constraint) {
          Resolve_Expression (node->derived_type.constraint);

          // Handle real type constraints (DIGITS/DELTA with optional RANGE)
          Syntax_Node *c = node->derived_type.constraint;

          // Apply DIGITS constraint
          if (c->kind == NK_REAL_TYPE and Type_Is_Float (derived)) {
            if (c->real_type.precision and
              c->real_type.precision->kind == NK_INTEGER) {
              int digits = (int)c->real_type.precision->integer_lit.value;
              derived->flt.digits = digits;

              // Adjust size if needed
              derived->size = (digits <= 6) ? 4 : 8;
              derived->alignment = derived->size;
            }

            // Apply RANGE constraint
            if (c->real_type.range and c->real_type.range->kind == NK_RANGE) {
              Syntax_Node *range = c->real_type.range;
              if (range->range.low) {
                double lo = Eval_Const_Numeric (range->range.low);
                if (lo == lo) {
                  derived->low_bound = (Type_Bound){
                    .kind = BOUND_FLOAT, .float_value = lo
                  };
                } else {
                  derived->low_bound = (Type_Bound){
                    .kind = BOUND_EXPR, .expr = range->range.low
                  };
                }
              }
              if (range->range.high) {
                double hi = Eval_Const_Numeric (range->range.high);
                if (hi == hi) {
                  derived->high_bound = (Type_Bound){
                    .kind = BOUND_FLOAT, .float_value = hi
                  };
                } else {
                  derived->high_bound = (Type_Bound){
                    .kind = BOUND_EXPR, .expr = range->range.high
                  };
                }
              }
            }
          }

          // Other constraint types handled in subtype_indication
        }
        node->type = derived;
        return derived;
      }

    // Create access type pointing to designated type
    case NK_ACCESS_TYPE:
      {
        Type_Info *access_type = Type_New (TYPE_ACCESS, S(""));
        access_type->size = 8;  // Pointer size
        access_type->alignment = 8;

        // Resolve designated subtype
        if (node->access_type.designated) {
          Resolve_Expression (node->access_type.designated);
          access_type->access.designated_type = node->access_type.designated->type;
        }
        node->type = access_type;
        return access_type;
      }

    // Create record type info from syntax node (RM 3.7, 3.7.3)
    case NK_RECORD_TYPE:
      {
        Type_Info *record_type = Type_New (TYPE_RECORD, S(""));

        // Helper: count components in a variant part recursively
        uint32_t Count_Variant_Components (Syntax_Node *vp) {
          if (not vp) return 0;
          uint32_t count = 0;
          for (uint32_t i = 0; i < vp->variant_part.variants.count; i++) {
            Syntax_Node *v = vp->variant_part.variants.items[i];
            for (uint32_t j = 0; j < v->variant.components.count; j++) {
              Syntax_Node *c = v->variant.components.items[j];
              if (c->kind == NK_COMPONENT_DECL)
                count += (uint32_t)c->component.names.count;
            }
            count += Count_Variant_Components (v->variant.variant_part);
          }
          return count;
        }

        // Helper: count variants (top-level only)
        uint32_t Count_Variants (Syntax_Node *vp) {
          if (not vp) return 0;
          return (uint32_t)vp->variant_part.variants.count;
        }

        // Count total components (each decl may have multiple names)
        uint32_t total_comps = 0;
        for (uint32_t i = 0; i < node->record_type.components.count; i++) {
          Syntax_Node *comp = node->record_type.components.items[i];
          if (comp->kind == NK_COMPONENT_DECL) {
            total_comps += (uint32_t)comp->component.names.count;
          }
        }

        // Also count components in variant parts
        total_comps += Count_Variant_Components (node->record_type.variant_part);
        bool has_variant_part = node->record_type.variant_part != NULL;
        uint32_t num_variants = Count_Variants (node->record_type.variant_part);
        record_type->record.component_count = total_comps;
        if (total_comps > 0) {
          record_type->record.components = Arena_Allocate (
            total_comps * sizeof (Component_Info));
          uint32_t offset = 0;
          uint32_t comp_idx = 0;

          // Helper: add a fixed-part component to the list
          void Add_Fixed_Component (Syntax_Node *comp) {
            if (comp->kind != NK_COMPONENT_DECL) return;
            Resolve_Expression (comp->component.component_type);
            Type_Info *comp_type = comp->component.component_type ?
                         comp->component.component_type->type : sm->type_integer;
            uint32_t comp_size = comp_type ? comp_type->size : 8;

            // Discriminant-dependent array components: compute maximum                             
            // size from the discriminant subtype's range (RM 3.7.1).                               
            // The static size is 0 because bounds aren't known at compile                          
            // time; use max extent for the record layout so subsequent                             
            // components are placed at the correct fixed offset.                                   
            //                                                                                      
            if (comp_type and comp_size == 0 and Type_Is_Array_Like (comp_type)
              and comp_type->array.is_constrained
              and comp_type->array.index_count > 0) {
              int128_t max_count = 1;
              bool got_max = true;
              for (uint32_t xi = 0; xi < comp_type->array.index_count; xi++) {
                Type_Bound *alb = &comp_type->array.indices[xi].low_bound;
                Type_Bound *ahb = &comp_type->array.indices[xi].high_bound;
                int128_t lo = 0, hi = 0;

                // Low bound: static or from index type
                if (alb->kind == BOUND_INTEGER) {
                  lo = alb->int_value;
                } else if (alb->kind == BOUND_EXPR and alb->expr
                       and alb->expr->symbol and alb->expr->symbol->type) {
                  Type_Info *st = alb->expr->symbol->type;
                  if (st->low_bound.kind == BOUND_INTEGER)
                    lo = st->low_bound.int_value;
                  else { got_max = false; break; }

                // High bound: static or max from discriminant type
                } else { got_max = false; break; }
                if (ahb->kind == BOUND_INTEGER) {
                  hi = ahb->int_value;
                } else if (ahb->kind == BOUND_EXPR and ahb->expr
                       and ahb->expr->symbol and ahb->expr->symbol->type) {
                  Type_Info *st = ahb->expr->symbol->type;
                  if (st->high_bound.kind == BOUND_INTEGER)
                    hi = st->high_bound.int_value;
                  else { got_max = false; break; }
                } else { got_max = false; break; }
                int128_t extent = hi - lo + 1;
                if (extent < 0) extent = 0;
                max_count *= extent;
              }
              if (got_max and max_count > 0) {
                uint32_t esz = (comp_type->array.element_type and
                  comp_type->array.element_type->size > 0)
                  ? comp_type->array.element_type->size : 1;
                comp_size = (uint32_t)(max_count * esz);
              }
            }

            // Propagate component type to aggregate inits
            if (comp->component.init) {
              if (comp->component.init->kind == NK_AGGREGATE and
                not comp->component.init->type and comp_type)
                comp->component.init->type = comp_type;
              Resolve_Expression (comp->component.init);
            }
            for (uint32_t j = 0; j < comp->component.names.count; j++) {
              Component_Info *info = &record_type->record.components[comp_idx++];
              info->name = comp->component.names.items[j]->string_val.text;
              info->component_type = comp_type;
              info->byte_offset = offset;
              info->bit_offset = 0;
              info->bit_size = comp_type ? comp_type->size * 8 : 64;
              info->default_expr = comp->component.init;
              info->is_discriminant = false;
              info->variant_index = -1;  // Fixed part
              offset += comp_size;
            }
          }

          // Process fixed (non-variant) components
          for (uint32_t i = 0; i < node->record_type.components.count; i++) {
            Add_Fixed_Component (node->record_type.components.items[i]);
          }

          // Record the variant_offset = end of fixed part
          uint32_t variant_offset = offset;
          record_type->record.variant_offset = variant_offset;

          // Process variant part: all variants OVERLAP at variant_offset (RM 3.7.3)
          // Record size = fixed_part + MAX (variant_sizes), not SUM
          if (has_variant_part) {
            Syntax_Node *vp = node->record_type.variant_part;
            record_type->record.variant_part_node = vp;

            // Allocate variant info array
            record_type->record.variant_count = num_variants;
            record_type->record.variants = Arena_Allocate (
              num_variants * sizeof (Variant_Info));
            uint32_t max_variant_size = 0;
            for (uint32_t vi = 0; vi < vp->variant_part.variants.count; vi++) {
              Syntax_Node *v = vp->variant_part.variants.items[vi];
              Variant_Info *vinfo = &record_type->record.variants[vi];

              // Extract discriminant value/range from first choice.                                
              // Supports single values, negated literals, and ranges                               
              // (RM 3.7.3: WHEN -5..10 => ...).                                                    
              //                                                                                    
              vinfo->disc_value_low = 0;
              vinfo->disc_value_high = 0;
              vinfo->is_others = false;
              if (v->variant.choices.count > 0) {
                Syntax_Node *choice = v->variant.choices.items[0];
                if (choice->kind == NK_INTEGER) {
                  vinfo->disc_value_low = choice->integer_lit.value;
                  vinfo->disc_value_high = choice->integer_lit.value;
                } else if (choice->kind == NK_UNARY_OP and
                       (choice->unary.op == TK_MINUS or
                      choice->unary.op == TK_PLUS) and
                       choice->unary.operand and
                       choice->unary.operand->kind == NK_INTEGER) {
                  int64_t v2 = choice->unary.operand->integer_lit.value;
                  if (choice->unary.op == TK_MINUS) v2 = -v2;
                  vinfo->disc_value_low = v2;
                  vinfo->disc_value_high = v2;

                // Range choice: WHEN -5..10 => ...
                } else if (choice->kind == NK_RANGE) {
                  double lo = Eval_Const_Numeric (choice->range.low);
                  double hi = Eval_Const_Numeric (choice->range.high);
                  if (lo == lo) vinfo->disc_value_low = (int64_t)lo;
                  if (hi == hi) vinfo->disc_value_high = (int64_t)hi;

                // Enumeration literal - covers BOOLEAN, CHARACTER,                                 
                // and user-defined enum types (RM 3.7.3).                                          
                // frame_offset stores the ordinal position for all                                 
                // SYMBOL_LITERAL symbols.                                                          
                //                                                                                  
                } else if (choice->kind == NK_IDENTIFIER) {
                  Resolve_Expression (choice);
                  if (choice->symbol and
                    choice->symbol->kind == SYMBOL_LITERAL) {
                    vinfo->disc_value_low =
                      (int64_t)choice->symbol->frame_offset;
                    vinfo->disc_value_high = vinfo->disc_value_low;
                  } else if (choice->symbol and choice->symbol->type and
                    Type_Is_Enumeration (choice->symbol->type)) {

                    // Fallback: search enumeration literals by name
                    Type_Info *et = choice->symbol->type;
                    for (uint32_t li = 0; li < et->enumeration.literal_count; li++) {
                      if (Slice_Equal_Ignore_Case (et->enumeration.literals[li],
                                    choice->string_val.text)) {
                        vinfo->disc_value_low = (int64_t)li;
                        vinfo->disc_value_high = (int64_t)li;
                        break;
                      }
                    }
                  }
                } else if (choice->kind == NK_OTHERS) {
                  vinfo->is_others = true;
                }
              }
              vinfo->first_component = comp_idx;

              // Add variant components - all start at variant_offset
              uint32_t var_local_offset = 0;
              uint32_t var_comp_count = 0;
              for (uint32_t j = 0; j < v->variant.components.count; j++) {
                Syntax_Node *vc = v->variant.components.items[j];
                if (vc->kind != NK_COMPONENT_DECL) continue;
                Resolve_Expression (vc->component.component_type);
                Type_Info *comp_type = vc->component.component_type ?
                             vc->component.component_type->type : sm->type_integer;
                uint32_t comp_size = comp_type ? comp_type->size : 8;
                if (vc->component.init) {
                  if (vc->component.init->kind == NK_AGGREGATE and
                    not vc->component.init->type and comp_type)
                    vc->component.init->type = comp_type;
                  Resolve_Expression (vc->component.init);
                }
                for (uint32_t k = 0; k < vc->component.names.count; k++) {
                  Component_Info *info = &record_type->record.components[comp_idx++];
                  info->name = vc->component.names.items[k]->string_val.text;
                  info->component_type = comp_type;
                  info->byte_offset = variant_offset + var_local_offset;
                  info->bit_offset = 0;
                  info->bit_size = comp_type ? comp_type->size * 8 : 64;
                  info->default_expr = vc->component.init;
                  info->is_discriminant = false;
                  info->variant_index = (int32_t)vi;
                  var_local_offset += comp_size;
                  var_comp_count++;
                }
              }

              // Handle nested variant parts recursively (simple flattening)
              if (v->variant.variant_part) {
                Syntax_Node *nvp = v->variant.variant_part;
                for (uint32_t ni = 0; ni < nvp->variant_part.variants.count; ni++) {
                  Syntax_Node *nv = nvp->variant_part.variants.items[ni];
                  for (uint32_t nj = 0; nj < nv->variant.components.count; nj++) {
                    Syntax_Node *nc = nv->variant.components.items[nj];
                    if (nc->kind != NK_COMPONENT_DECL) continue;
                    Resolve_Expression (nc->component.component_type);
                    Type_Info *comp_type = nc->component.component_type ?
                                 nc->component.component_type->type : sm->type_integer;
                    uint32_t comp_size = comp_type ? comp_type->size : 8;
                    if (nc->component.init) {
                      if (nc->component.init->kind == NK_AGGREGATE and
                        not nc->component.init->type and comp_type)
                        nc->component.init->type = comp_type;
                      Resolve_Expression (nc->component.init);
                    }
                    for (uint32_t k = 0; k < nc->component.names.count; k++) {
                      Component_Info *info = &record_type->record.components[comp_idx++];
                      info->name = nc->component.names.items[k]->string_val.text;
                      info->component_type = comp_type;
                      info->byte_offset = variant_offset + var_local_offset;
                      info->bit_offset = 0;
                      info->bit_size = comp_type ? comp_type->size * 8 : 64;
                      info->default_expr = nc->component.init;
                      info->is_discriminant = false;
                      info->variant_index = (int32_t)vi;
                      var_local_offset += comp_size;
                      var_comp_count++;
                    }
                  }
                }
              }
              vinfo->component_count = var_comp_count;
              vinfo->variant_size = var_local_offset;
              if (var_local_offset > max_variant_size) {
                max_variant_size = var_local_offset;
              }
            }
            record_type->record.max_variant_size = max_variant_size;

            // Record size = fixed part + maximum variant size (RM 3.7.3)
            record_type->size = variant_offset + max_variant_size;
          } else {
            record_type->size = offset;
          }
          record_type->alignment = 8;

          // Adjust size for discriminant-dependent array/string components                         
          // whose sizes are not included in the static offset sum. The max                         
          // size is derived from the discriminant subtype's range (RM 3.7.1).                      
          //                                                                                        
          for (uint32_t aci = 0; aci < record_type->record.component_count; aci++) {
            Component_Info *acomp = &record_type->record.components[aci];
            Type_Info *acti = acomp->component_type;
            if (not acti or not Type_Is_Array_Like (acti)) continue;
            for (uint32_t axi = 0; axi < acti->array.index_count; axi++) {
              Type_Bound *alo = &acti->array.indices[axi].low_bound;
              Type_Bound *ahi = &acti->array.indices[axi].high_bound;
              if (ahi->kind == BOUND_EXPR and ahi->expr and ahi->expr->symbol) {
                Symbol *disc_s = ahi->expr->symbol;
                Type_Info *disc_ty = disc_s->type;
                if (disc_ty and disc_ty->high_bound.kind == BOUND_INTEGER) {
                  int64_t max_hi = disc_ty->high_bound.int_value;
                  int64_t lo_val = (alo->kind == BOUND_INTEGER) ?
                            alo->int_value : 0;
                  int64_t max_ext = max_hi - lo_val + 1;
                  if (max_ext < 0) max_ext = 0;
                  uint32_t elem_sz = (acti->array.element_type and
                    acti->array.element_type->size > 0) ?
                    acti->array.element_type->size : 1;
                  uint32_t needed = acomp->byte_offset +
                    (uint32_t)(max_ext * elem_sz);
                  if (needed > record_type->size)
                    record_type->size = needed;
                }
              }
            }
          }
        }
        node->type = record_type;
        return record_type;
      }

    // Helper: Try to extract a static integer value from a bound expression.
    // Returns true if value was extracted, false otherwise.
    case NK_SUBTYPE_INDICATION:
      {
        bool Try_Static_Bound (Syntax_Node *expr, int64_t *out_val) {
          if (not expr) return false;
          if (expr->kind == NK_INTEGER) {
            *out_val = expr->integer_lit.value;
            return true;
          }
          if (expr->kind == NK_UNARY_OP and expr->unary.operand and
            expr->unary.operand->kind == NK_INTEGER) {
            int64_t val = expr->unary.operand->integer_lit.value;
            if (expr->unary.op == TK_MINUS) val = -val;
            *out_val = val;
            return true;
          }
          if (expr->symbol and expr->symbol->kind == SYMBOL_LITERAL) {
            *out_val = expr->symbol->frame_offset;
            return true;
          }

          // Character literal 'X' - extract ASCII value from text.                                 
          // Per Ada RM 3.5.2, character position = Character'Pos (C).                              
          // Eval_Scalar_Literal handles char literals via                                          
          // Enumeration_Rep of the entity.                                                         
          //                                                                                        
          if (expr->kind == NK_CHARACTER) {
            String_Slice t = expr->string_val.text;
            if (t.length >= 2) { *out_val = (unsigned char)t.data[1]; return true; }
            if (t.length == 1) { *out_val = (unsigned char)t.data[0]; return true; }
          }

          // Handle TYPE'POS (X) or TYPE'VAL (N) as NK_APPLY
          if (expr->kind == NK_APPLY and expr->apply.prefix and
            expr->apply.prefix->kind == NK_ATTRIBUTE) {
            Syntax_Node *attr = expr->apply.prefix;
            String_Slice attr_name = attr->attribute.name;
            if (Slice_Equal_Ignore_Case (attr_name, S("POS")) and
              expr->apply.arguments.count == 1) {
              Syntax_Node *arg = expr->apply.arguments.items[0];
              if (arg and arg->symbol and arg->symbol->kind == SYMBOL_LITERAL) {
                *out_val = arg->symbol->frame_offset;
                return true;
              }
            }

            // Handle TYPE'VAL (N) where N is static
            if (Slice_Equal_Ignore_Case (attr_name, S("VAL")) and
              expr->apply.arguments.count == 1) {
              Syntax_Node *arg = expr->apply.arguments.items[0];
              int64_t inner_val;
              if (Try_Static_Bound (arg, &inner_val)) {
                *out_val = inner_val;
                return true;
              }
            }
          }

          // Handle type conversions: TYPE_NAME (arg) where arg is static
          if (expr->kind == NK_APPLY and expr->apply.prefix and
            expr->apply.prefix->kind == NK_IDENTIFIER and
            expr->apply.prefix->symbol and
            expr->apply.prefix->symbol->kind == SYMBOL_TYPE and
            expr->apply.arguments.count == 1) {
            int64_t arg_val;
            if (Try_Static_Bound (expr->apply.arguments.items[0], &arg_val)) {
              *out_val = arg_val;
              return true;
            }
          }

          // Handle TYPE'VAL (...) or TYPE'POS (...) as NK_ATTRIBUTE with arguments
          if (expr->kind == NK_ATTRIBUTE and expr->attribute.arguments.count == 1) {
            String_Slice attr_name = expr->attribute.name;
            Syntax_Node *arg = expr->attribute.arguments.items[0];
            if (Slice_Equal_Ignore_Case (attr_name, S("VAL"))) {
              int64_t inner_val;
              if (Try_Static_Bound (arg, &inner_val)) {
                *out_val = inner_val;
                return true;
              }
            }
            if (Slice_Equal_Ignore_Case (attr_name, S("POS"))) {
              if (arg and arg->symbol and arg->symbol->kind == SYMBOL_LITERAL) {
                *out_val = arg->symbol->frame_offset;
                return true;
              }
            }
          }
          return false;
        }

        // Resolve the base type
        Resolve_Expression (node->subtype_ind.subtype_mark);
        Type_Info *base_type = node->subtype_ind.subtype_mark->type;
        if (not base_type) {
          return NULL;
        }

        // Check for index constraint (STRING (1..5) style)
        Syntax_Node *constraint = node->subtype_ind.constraint;

        // For access-to-array, the index constraint applies to the                                 
        // designated type.  Resolve by creating a constrained access                               
        // type whose designated_type is a constrained array. (RM 3.7.1)                            
        //                                                                                          
        if (constraint and constraint->kind == NK_INDEX_CONSTRAINT and
          Type_Is_Access (base_type) and base_type->access.designated_type and
          Type_Is_Array_Like (base_type->access.designated_type)) {
          Type_Info *des_base = base_type->access.designated_type;
          Type_Info *des_con = Type_New (TYPE_ARRAY, des_base->name);
          *des_con = *des_base;
          des_con->base_type = des_base;
          des_con->array.is_constrained = true;
          des_con->array.index_count = (uint32_t)constraint->index_constraint.ranges.count;
          if (des_con->array.index_count > 0) {
            des_con->array.indices = Arena_Allocate (
              des_con->array.index_count * sizeof (Index_Info));
            for (uint32_t i = 0; i < des_con->array.index_count; i++) {
              Syntax_Node *range = constraint->index_constraint.ranges.items[i];
              Resolve_Expression (range);
              Index_Info *info = &des_con->array.indices[i];

              // Inherit index_type from base (e.g., POSITIVE for STRING)
              if (des_base->array.index_count > i and des_base->array.indices[i].index_type) {
                info->index_type = des_base->array.indices[i].index_type;
              } else {
                info->index_type = sm->type_integer;
              }
              if (range->kind == NK_RANGE) {
                if (range->range.low) {
                  double v = Eval_Const_Numeric (range->range.low);
                  if (v == v) info->low_bound = (Type_Bound){
                    .kind = BOUND_INTEGER, .int_value = (int128_t)v};
                  else info->low_bound = (Type_Bound){
                    .kind = BOUND_EXPR, .expr = range->range.low};
                }
                if (range->range.high) {
                  double v = Eval_Const_Numeric (range->range.high);
                  if (v == v) info->high_bound = (Type_Bound){
                    .kind = BOUND_INTEGER, .int_value = (int128_t)v};
                  else info->high_bound = (Type_Bound){
                    .kind = BOUND_EXPR, .expr = range->range.high};
                }
              }
            }
          }
          Type_Info *acc_con = Type_New (TYPE_ACCESS, base_type->name);
          *acc_con = *base_type;
          acc_con->base_type = base_type;
          acc_con->access.designated_type = des_con;
          node->type = acc_con;
          return acc_con;
        }
        if (constraint and constraint->kind == NK_INDEX_CONSTRAINT and
          Type_Is_Array_Like (base_type)) {

          // Create constrained array type
          Type_Info *constrained = Type_New (TYPE_ARRAY, base_type->name);
          constrained->array.is_constrained = true;
          constrained->array.index_count = (uint32_t)constraint->index_constraint.ranges.count;

          // Set base_type to original type for tracing unconstrained arrays
          constrained->base_type = base_type;

          // For STRING, element type is CHARACTER
          if (Type_Is_String (base_type)) {
            constrained->array.element_type = sm->type_character;
          } else if (base_type->array.element_type) {
            constrained->array.element_type = base_type->array.element_type;
          }

          // Process index constraints
          if (constrained->array.index_count > 0) {
            constrained->array.indices = Arena_Allocate (
              constrained->array.index_count * sizeof (Index_Info));
            for (uint32_t i = 0; i < constrained->array.index_count; i++) {
              Syntax_Node *range = constraint->index_constraint.ranges.items[i];
              Resolve_Expression (range);
              Index_Info *info = &constrained->array.indices[i];

              // Inherit index_type from base (e.g., POSITIVE for STRING)
              if (base_type->array.index_count > i and base_type->array.indices[i].index_type) {
                info->index_type = base_type->array.indices[i].index_type;
              } else {
                info->index_type = sm->type_integer;
              }

              // Try to evaluate bounds as static constants
              if (range->kind == NK_RANGE) {
                if (range->range.low) {
                  double val = Eval_Const_Numeric (range->range.low);
                  if (val == val) {  // Not NaN - static value
                    info->low_bound = (Type_Bound){
                      .kind = BOUND_INTEGER,
                      .int_value = (int64_t)val
                    };

                  // Non-static bound - store expression reference
                  } else {
                    info->low_bound = (Type_Bound){
                      .kind = BOUND_EXPR,
                      .expr = range->range.low
                    };
                  }
                }
                if (range->range.high) {
                  double val = Eval_Const_Numeric (range->range.high);
                  if (val == val) {  // Not NaN - static value
                    info->high_bound = (Type_Bound){
                      .kind = BOUND_INTEGER,
                      .int_value = (int64_t)val
                    };

                  // Non-static bound - store expression reference
                  } else {
                    info->high_bound = (Type_Bound){
                      .kind = BOUND_EXPR,
                      .expr = range->range.high
                    };
                  }
                }
              }
            }

            // Compute size - only for fully static bounds
            {
              bool all_static = true;
              int128_t count = 1;
              for (uint32_t i = 0; i < constrained->array.index_count; i++) {
                Type_Bound *lo_b = &constrained->array.indices[i].low_bound;
                Type_Bound *hi_b = &constrained->array.indices[i].high_bound;
                if ((lo_b->kind == BOUND_EXPR and lo_b->expr and lo_b->expr->symbol) or
                  (hi_b->kind == BOUND_EXPR and hi_b->expr and hi_b->expr->symbol)) {
                  all_static = false; break;
                }
                int128_t lo = Type_Bound_Value (*lo_b);
                int128_t hi = Type_Bound_Value (*hi_b);
                int128_t dim = hi - lo + 1;
                if (dim < 0) dim = 0;
                count *= dim;
              }

              // Discriminant-dependent bounds: size unknown at compile
              // time.  Mark with 0 so record layout uses max. (RM 3.7.1)
              if (not all_static) {
                constrained->size = 0;
              } else if (all_static and count >= 0) {
                uint32_t elem_size = constrained->array.element_type ?
                           constrained->array.element_type->size : 1;
                if (elem_size == 0 and Type_Needs_Fat_Pointer_Load (constrained->array.element_type))
                  elem_size = FAT_PTR_ALLOC_SIZE;
                constrained->size = (uint32_t)(count * elem_size);
              }
            }
          }
          node->type = constrained;
          return constrained;
        }

        // Check for scalar range constraint (ENUM RANGE A..B or INTEGER RANGE X..Y)
        if (constraint and constraint->kind == NK_RANGE_CONSTRAINT) {
          Syntax_Node *range = constraint->range_constraint.range;
          if (range) {
            Resolve_Expression (range);

            // Create a constrained subtype
            Type_Info *constrained = Type_New (base_type->kind, base_type->name);
            constrained->base_type = base_type;
            constrained->size = base_type->size;
            constrained->alignment = base_type->alignment;

            // Copy type-specific info from base type
            if (Type_Is_Enumeration (base_type)) {
              constrained->enumeration = base_type->enumeration;
            } else if (Type_Is_Fixed_Point (base_type)) {
              constrained->fixed = base_type->fixed;
            }

            // Set bounds from range.                                                               
            // Following GNAT's approach: try compile-time evaluation first,                        
            // if not possible, store expression for later evaluation.                              
            //                                                                                      
            if (range->kind == NK_RANGE) {
              Syntax_Node *lo = range->range.low;
              Syntax_Node *hi = range->range.high;
              int64_t val;
              if (lo) {
                if (Try_Static_Bound (lo, &val)) {
                  constrained->low_bound = (Type_Bound){
                    .kind = BOUND_INTEGER, .int_value = val
                  };

                // Store expression for later evaluation
                } else {
                  constrained->low_bound = (Type_Bound){
                    .kind = BOUND_EXPR, .expr = lo
                  };
                }
              }
              if (hi) {
                if (Try_Static_Bound (hi, &val)) {
                  constrained->high_bound = (Type_Bound){
                    .kind = BOUND_INTEGER, .int_value = val
                  };

                // Store expression for later evaluation
                } else {
                  constrained->high_bound = (Type_Bound){
                    .kind = BOUND_EXPR, .expr = hi
                  };
                }
              }
            }
            node->type = constrained;
            return constrained;
          }
        }

        // Check for DELTA constraint (fixed-point subtypes with different delta)
        if (constraint and constraint->kind == NK_DELTA_CONSTRAINT) {
          Resolve_Expression (constraint->delta_constraint.delta_expr);
          if (constraint->delta_constraint.range)
            Resolve_Expression (constraint->delta_constraint.range);

          // Create constrained fixed-point subtype with new delta
          Type_Info *constrained = Type_New (TYPE_FIXED, base_type->name);
          constrained->base_type = base_type;
          constrained->size = base_type->size;
          constrained->alignment = base_type->alignment;

          // Evaluate the delta expression
          double delta = Eval_Const_Numeric (constraint->delta_constraint.delta_expr);
          if (delta != delta or delta <= 0.0) delta = base_type->fixed.delta;

          // Compute small as largest power of 2 <= delta (per RM 3.5.9)
          double small = 1.0;
          if (delta > 0.0) {
            while (small > delta) small /= 2.0;
            while (small * 2.0 <= delta) small *= 2.0;
          }
          constrained->fixed.delta = delta;
          constrained->fixed.small = small;

          // Compute scale factor: small = 2^scale
          int scale = 0;
          double temp = small;
          while (temp < 1.0 and scale > -64) { temp *= 2.0; scale--; }
          while (temp > 1.0 and scale < 64) { temp /= 2.0; scale++; }
          constrained->fixed.scale = scale;

          // Set bounds from range, or inherit from base type
          // Start by inheriting base type bounds
          constrained->low_bound = base_type->low_bound;
          constrained->high_bound = base_type->high_bound;

          // If explicit range given, override with evaluated bounds.                               
          // Following GNAT's approach: try compile-time evaluation first,                          
          // if not possible, store expression for later evaluation.                                
          //                                                                                        
          if (constraint->delta_constraint.range and
            constraint->delta_constraint.range->kind == NK_RANGE) {
            Syntax_Node *range = constraint->delta_constraint.range;
            if (range->range.low) {
              double lo = Eval_Const_Numeric (range->range.low);

              // Compile-time known
              if (lo == lo) {
                constrained->low_bound = (Type_Bound){
                  .kind = BOUND_FLOAT, .float_value = lo
                };

              // Store expression for later evaluation (per GNAT sem_attr)
              } else {
                constrained->low_bound = (Type_Bound){
                  .kind = BOUND_EXPR, .expr = range->range.low
                };
              }
            }
            if (range->range.high) {
              double hi = Eval_Const_Numeric (range->range.high);

              // Compile-time known
              if (hi == hi) {
                constrained->high_bound = (Type_Bound){
                  .kind = BOUND_FLOAT, .float_value = hi
                };

              // Store expression for later evaluation (per GNAT sem_attr)
              } else {
                constrained->high_bound = (Type_Bound){
                  .kind = BOUND_EXPR, .expr = range->range.high
                };
              }
            }
          }
          node->type = constrained;
          return constrained;
        }

        // Check for DIGITS constraint (floating-point subtypes)
        if (constraint and constraint->kind == NK_DIGITS_CONSTRAINT) {
          Resolve_Expression (constraint->digits_constraint.digits_expr);
          if (constraint->digits_constraint.range)
            Resolve_Expression (constraint->digits_constraint.range);

          // Create constrained floating-point subtype with specified DIGITS
          Type_Info *constrained = Type_New (TYPE_FLOAT, base_type->name);
          constrained->base_type = base_type;
          constrained->size = base_type->size;
          constrained->alignment = base_type->alignment;

          // Evaluate the digits expression
          double digits_val = Eval_Const_Numeric (constraint->digits_constraint.digits_expr);
          if (digits_val != digits_val or digits_val < 1) digits_val = base_type->flt.digits;
          constrained->flt.digits = (int)digits_val;

          // Set bounds from range, or inherit from base type
          constrained->low_bound = base_type->low_bound;
          constrained->high_bound = base_type->high_bound;

          // If explicit range given, override with evaluated bounds
          if (constraint->digits_constraint.range and
            constraint->digits_constraint.range->kind == NK_RANGE) {
            Syntax_Node *range = constraint->digits_constraint.range;
            if (range->range.low) {
              double lo = Eval_Const_Numeric (range->range.low);
              if (lo == lo) {
                constrained->low_bound = (Type_Bound){
                  .kind = BOUND_FLOAT, .float_value = lo
                };
              } else {
                constrained->low_bound = (Type_Bound){
                  .kind = BOUND_EXPR, .expr = range->range.low
                };
              }
            }
            if (range->range.high) {
              double hi = Eval_Const_Numeric (range->range.high);
              if (hi == hi) {
                constrained->high_bound = (Type_Bound){
                  .kind = BOUND_FLOAT, .float_value = hi
                };
              } else {
                constrained->high_bound = (Type_Bound){
                  .kind = BOUND_EXPR, .expr = range->range.high
                };
              }
            }
          }
          node->type = constrained;
          return constrained;
        }

        // Reclassify NK_INDEX_CONSTRAINT as discriminant constraint                                
        // when the base type is a discriminated record (RM 3.7.2).                                 
        // Also handles access-to-record types (RM 3.7.1).                                          
        // Positional discriminant constraints like R1 (IDENT_BOOL (TRUE))                          
        // get parsed as NK_INDEX_CONSTRAINT because they lack named assocs.                        
        // Convert each range item to a positional association.                                     
        //                                                                                          
        Type_Info *disc_target = base_type;
        bool disc_via_access = false;
        if (Type_Is_Access (base_type) and base_type->access.designated_type and
          Type_Is_Record (base_type->access.designated_type) and
          base_type->access.designated_type->record.has_discriminants) {
          disc_target = base_type->access.designated_type;
          disc_via_access = true;
        }
        if (constraint and constraint->kind == NK_INDEX_CONSTRAINT and
          (Type_Is_Record (disc_target) or Type_Is_Private (disc_target)) and
          disc_target->record.has_discriminants) {
          Syntax_Node *disc_c = Node_New (NK_DISCRIMINANT_CONSTRAINT, constraint->location);
          disc_c->discriminant_constraint.associations.count = constraint->index_constraint.ranges.count;
          disc_c->discriminant_constraint.associations.capacity = constraint->index_constraint.ranges.capacity;
          disc_c->discriminant_constraint.associations.items = Arena_Allocate (
            disc_c->discriminant_constraint.associations.count * sizeof (Syntax_Node *));
          for (uint32_t i = 0; i < constraint->index_constraint.ranges.count; i++) {
            Syntax_Node *expr = constraint->index_constraint.ranges.items[i];

            // Wrap each expression in an association node (positional)
            Syntax_Node *assoc = Node_New (NK_ASSOCIATION, expr->location);
            assoc->association.expression = expr;
            assoc->association.choices.count = 0;
            disc_c->discriminant_constraint.associations.items[i] = assoc;
          }
          node->subtype_ind.constraint = disc_c;
          constraint = disc_c;
        }

        // Check for discriminant constraint (REC (A => E1, B => E2) style)                         
        // Discriminant constraints use named associations where the choices                        
        // are discriminant names - they should NOT be resolved as identifiers.                     
        // Only the value expressions should be resolved. (RM 3.7.2)                                
        //                                                                                          
        if (constraint and constraint->kind == NK_DISCRIMINANT_CONSTRAINT) {
          uint32_t assoc_count = constraint->discriminant_constraint.associations.count;
          for (uint32_t i = 0; i < assoc_count; i++) {
            Syntax_Node *assoc = constraint->discriminant_constraint.associations.items[i];

            // Only resolve the value expression, not the choices
            if (assoc->kind == NK_ASSOCIATION and assoc->association.expression) {
              Resolve_Expression (assoc->association.expression);
            }
          }

          // Create a constrained subtype with discriminant values stored.                          
          // For access-to-record, create a constrained designated type                             
          // and wrap it in a new access type. (RM 3.7.1)                                           
          //                                                                                        
          Type_Info *rec_base = disc_target;
          if ((Type_Is_Record (rec_base) or Type_Is_Private (rec_base)) and
            rec_base->record.has_discriminants) {
            Type_Info *constrained = Type_New (rec_base->kind, rec_base->name);
            *constrained = *rec_base;  // Copy all fields
            constrained->base_type = rec_base;
            constrained->record.is_constrained = true;
            constrained->record.has_disc_constraints = true;

            // Allocate and fill constraint value + expression arrays
            uint32_t dc = rec_base->record.discriminant_count;
            constrained->record.disc_constraint_values = Arena_Allocate (
              dc * sizeof (int64_t));
            constrained->record.disc_constraint_exprs = Arena_Allocate (
              dc * sizeof (Syntax_Node *));
            for (uint32_t ci = 0; ci < dc; ci++) {
              constrained->record.disc_constraint_values[ci] = 0;
              constrained->record.disc_constraint_exprs[ci] = NULL;
            }

            // Extract discriminant values from associations.                                       
            // For static values, store the integer. For runtime expressions                        
            // (function calls etc.), store the AST node for codegen evaluation.                    
            //                                                                                      
            for (uint32_t i = 0; i < assoc_count; i++) {
              Syntax_Node *assoc = constraint->discriminant_constraint.associations.items[i];
              if (assoc->kind != NK_ASSOCIATION) continue;
              Syntax_Node *expr = assoc->association.expression;
              int64_t val = 0;
              bool is_static = false;
              if (expr and expr->kind == NK_INTEGER) {
                val = expr->integer_lit.value;
                is_static = true;
              } else if (expr and expr->kind == NK_IDENTIFIER and expr->symbol and
                     expr->symbol->type and Type_Is_Boolean (expr->symbol->type)) {

                // BOOLEAN literal: FALSE=0, TRUE=1
                val = Slice_Equal_Ignore_Case (expr->string_val.text, S("TRUE"))
                  ? 1 : 0;
                is_static = true;
              } else if (expr and expr->kind == NK_IDENTIFIER and expr->symbol and
                     expr->symbol->type and Type_Is_Enumeration (expr->symbol->type)) {

                // Enum literal: find position
                Type_Info *et = expr->symbol->type;
                for (uint32_t li = 0; li < et->enumeration.literal_count; li++) {
                  if (Slice_Equal_Ignore_Case (et->enumeration.literals[li],
                                expr->string_val.text)) {
                    val = (int64_t)li;
                    break;
                  }
                }
                is_static = true;
              } else if (expr and expr->kind == NK_IDENTIFIER and expr->symbol and
                     expr->symbol->kind == SYMBOL_CONSTANT) {

                // Named number or constant
                double cv = Eval_Const_Numeric (expr);

                // Fallback: try Eval_Const_Numeric for negative                                    
                // literals like -6 (NK_UNARY_OP), type conversions,                                
                // and other statically computable expressions.                                     
                //                                                                                  
                if (cv == cv) { val = (int64_t)cv; is_static = true; }
              } else if (expr and not is_static) {
                double cv = Eval_Const_Numeric (expr);
                if (cv == cv) { val = (int64_t)cv; is_static = true; }
              }

              // Match association to discriminant by position or name
              if (assoc->association.choices.count > 0) {
                for (uint32_t ci = 0; ci < assoc->association.choices.count; ci++) {
                  Syntax_Node *choice = assoc->association.choices.items[ci];
                  if (choice->kind == NK_IDENTIFIER) {
                    for (uint32_t di = 0; di < dc; di++) {
                      if (Slice_Equal_Ignore_Case (
                          constrained->record.components[di].name,
                          choice->string_val.text)) {
                        constrained->record.disc_constraint_values[di] = val;
                        if (not is_static)
                          constrained->record.disc_constraint_exprs[di] = expr;
                        break;
                      }
                    }
                  }
                }
              } else if (i < dc) {

                // Positional
                constrained->record.disc_constraint_values[i] = val;
                if (not is_static)
                  constrained->record.disc_constraint_exprs[i] = expr;
              }
            }

            // Wrap constrained record in a new access type
            if (disc_via_access) {
              Type_Info *acc_con = Type_New (TYPE_ACCESS, base_type->name);
              *acc_con = *base_type;
              acc_con->base_type = base_type;
              acc_con->access.designated_type = constrained;
              node->type = acc_con;
              return acc_con;
            }
            node->type = constrained;
            return constrained;
          }

          // Fallback: return base type if not a discriminated record
          node->type = base_type;
          return base_type;
        }

        // Otherwise just use base type
        node->type = base_type;
        return base_type;
      }
    case NK_INTEGER_TYPE:
      {

        // Modular type definition: type T is mod M (RM 3.5.4)                                      
        // Range is 0 .. M-1, all arithmetic wraps modulo M.                                        
        // The modulus expression is stored in integer_type.range.                                  
        //                                                                                          
        if (node->integer_type.is_modular) {
          Type_Info *mod_type = Type_New (TYPE_MODULAR, S(""));
          Resolve_Expression (node->integer_type.range);

          // Evaluate modulus using 128-bit-precise evaluator.                                      
          // uint128_t handles 2**64 and 2**128 directly - no sentinel                              
          // hacks needed.  2**128 wraps to 0 in uint128_t arithmetic                               
          // (same as 2**64 wraps to 0 in uint64_t), so we detect it                                
          // by checking if the exponent is 128.  For 2**64, the                                    
          // evaluator returns (uint128_t)1 << 64 = exact value.                                    
          //                                                                                        
          uint128_t modulus = 0;
          bool is_pow2_128 = false;  // true only for mod 2**128
          if (Eval_Const_Uint128 (node->integer_type.range, &modulus)) {

            // 2**128 overflows uint128_t to 0.  Detect it
            // explicitly from the AST exponent value.
            if (modulus == 0) {
              Syntax_Node *expr = node->integer_type.range;
              if (expr->kind == NK_BINARY_OP and expr->binary.op == TK_EXPON) {
                uint128_t base, exp;
                if (Eval_Const_Uint128 (expr->binary.left, &base) and
                  Eval_Const_Uint128 (expr->binary.right, &exp) and
                  base == 2 and exp == 128) {
                  is_pow2_128 = true;
                }
              }
            }

          // Non-constant modulus: use double evaluator as fallback
          } else {
            double dval = Eval_Const_Numeric (node->integer_type.range);
            if (dval == dval and dval > 0) modulus = (uint128_t)dval;
          }
          mod_type->modulus = modulus;
          node->integer_type.modulus = modulus;

          // Bounds: 0 .. modulus-1 (RM 3.5.4(9)).                                                  
          // int128_t can represent 0 .. 2^127-1 directly.  For                                     
          // mod 2**128, the high bound is 2^128-1 which overflows                                  
          // int128_t, so we store it as -1 (all bits set).                                         
          //                                                                                        
          mod_type->low_bound = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = 0 };

          // 2^128 - 1 = (int128_t)(-1) - all 128 bits set
          if (is_pow2_128) {
            mod_type->high_bound = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = (int128_t)-1 };
          } else {
            mod_type->high_bound = (Type_Bound){ .kind = BOUND_INTEGER, .int_value = (int128_t)(modulus - 1) };
          }

          // Compute size from modulus
          uint32_t bits;
          if (is_pow2_128) {
            bits = Width_128;
          } else {
            bits = Bits_For_Modulus (modulus);
          }
          mod_type->size = (uint32_t)(bits / 8);
          mod_type->alignment = mod_type->size;
          if (mod_type->alignment > 16) mod_type->alignment = 16;  // Max alignment 16 bytes for i128
          node->type = mod_type;
          return mod_type;

        // Signed integer type definition: range L .. H
        } else {
          Type_Info *int_type = Type_New (TYPE_INTEGER, S(""));

          // Resolve range bounds if present
          if (node->integer_type.range) {
            Resolve_Expression (node->integer_type.range);
            Syntax_Node *range = node->integer_type.range;

            // Extract low bound - literals, unary minus, or constants
            if (range->kind == NK_RANGE and range->range.low and range->range.high) {
              Syntax_Node *lo = range->range.low;
              if (lo->kind == NK_INTEGER) {
                int_type->low_bound = (Type_Bound){
                  .kind = BOUND_INTEGER,
                  .int_value = lo->integer_lit.value
                };
              } else if (lo->kind == NK_UNARY_OP and lo->unary.operand and
                     lo->unary.operand->kind == NK_INTEGER) {
                int64_t val = lo->unary.operand->integer_lit.value;
                if (lo->unary.op == TK_MINUS) val = -val;
                int_type->low_bound = (Type_Bound){
                  .kind = BOUND_INTEGER,
                  .int_value = val
                };
              } else {
                double val = Eval_Const_Numeric (lo);
                if (val == val) {
                  int_type->low_bound = (Type_Bound){
                    .kind = BOUND_INTEGER,
                    .int_value = (int64_t)val
                  };
                }
              }

              // Extract high bound - literals, unary minus, or constants
              Syntax_Node *hi = range->range.high;
              if (hi->kind == NK_INTEGER) {
                int_type->high_bound = (Type_Bound){
                  .kind = BOUND_INTEGER,
                  .int_value = hi->integer_lit.value
                };
              } else if (hi->kind == NK_UNARY_OP and hi->unary.operand and
                     hi->unary.operand->kind == NK_INTEGER) {
                int64_t val = hi->unary.operand->integer_lit.value;
                if (hi->unary.op == TK_MINUS) val = -val;
                int_type->high_bound = (Type_Bound){
                  .kind = BOUND_INTEGER,
                  .int_value = val
                };
              } else {
                double val = Eval_Const_Numeric (hi);
                if (val == val) {
                  int_type->high_bound = (Type_Bound){
                    .kind = BOUND_INTEGER,
                    .int_value = (int64_t)val
                  };
                }
              }
            }
          }

          // Compute appropriate size from bounds
          if (int_type->low_bound.kind == BOUND_INTEGER and
            int_type->high_bound.kind == BOUND_INTEGER) {
            uint32_t bits = Bits_For_Range (int_type->low_bound.int_value,
                             int_type->high_bound.int_value);
            int_type->size = (uint32_t)(bits / 8);
            if (int_type->size == 0) int_type->size = 1;
            int_type->alignment = int_type->size;
            if (int_type->alignment > 8) int_type->alignment = 8;
          } else {
            int_type->size = 8;  // Default to 64-bit
            int_type->alignment = 8;
          }
          node->type = int_type;
          return int_type;
        }
      }

    // Real type definition: digits D or delta D
    case NK_REAL_TYPE:
      {

        // Fixed-point type: TYPE_FIXED with delta
        if (node->real_type.delta) {
          Type_Info *fixed_type = Type_New (TYPE_FIXED, S(""));
          Resolve_Expression (node->real_type.delta);

          // Extract delta value using constant expression evaluation
          double delta = Eval_Const_Numeric (node->real_type.delta);
          if (delta != delta or delta <= 0.0) delta = 0.001;  // NaN or invalid -> fallback

          // Compute small as largest power of 2 <= delta (per RM 3.5.9)
          double small = 1.0;

          // Find largest 2^n <= delta
          if (delta > 0.0) {
            while (small > delta) small /= 2.0;
            while (small * 2.0 <= delta) small *= 2.0;
          }

          // Compute scale factor: small = 2^scale
          int scale = 0;
          double temp = small;
          if (temp >= 1.0) {
            while (temp >= 2.0) { temp /= 2.0; scale++; }
          } else {
            while (temp < 1.0) { temp *= 2.0; scale--; }
          }
          fixed_type->fixed.delta = delta;
          fixed_type->fixed.small = small;
          fixed_type->fixed.scale = scale;

          // Resolve range if present
          if (node->real_type.range) {
            Resolve_Expression (node->real_type.range);
            Syntax_Node *range = node->real_type.range;

            // Extract bound values using constant expression evaluation
            if (range->kind == NK_RANGE) {
              Syntax_Node *lo = range->range.low;
              Syntax_Node *hi = range->range.high;
              if (lo) {
                double val = Eval_Const_Numeric (lo);
                if (val == val) {  // not NaN
                  fixed_type->low_bound = (Type_Bound){.kind = BOUND_FLOAT, .float_value = val};
                }
              }
              if (hi) {
                double val = Eval_Const_Numeric (hi);
                if (val == val) {  // not NaN
                  fixed_type->high_bound = (Type_Bound){.kind = BOUND_FLOAT, .float_value = val};
                }
              }
            }
          }

          // Size: typically 32 or 64 bits depending on range and precision
          fixed_type->size = 8;  // 64-bit for safe default
          fixed_type->alignment = 8;
          node->type = fixed_type;
          return fixed_type;

        // Floating-point type: digits D
        } else {
          Type_Info *float_type = Type_New (TYPE_FLOAT, S(""));

          // Resolve digits expression
          if (node->real_type.precision) {
            Resolve_Expression (node->real_type.precision);
          }

          // Resolve range if present
          if (node->real_type.range) {
            Resolve_Expression (node->real_type.range);
          }

          // Size based on digits: <=6 = float, >6 = double
          int digits = 15;  // Default double precision
          if (node->real_type.precision) {
            if (node->real_type.precision->kind == NK_INTEGER) {
              digits = (int)node->real_type.precision->integer_lit.value;
            } else {
              double val = Eval_Const_Numeric (node->real_type.precision);
              if (val == val) digits = (int)val;
            }
          }
          float_type->flt.digits = digits;  // Store declared DIGITS
          float_type->size = (digits <= 6) ? 4 : 8;
          float_type->alignment = float_type->size;

          // Set bounds from range constraint if present
          if (node->real_type.range and node->real_type.range->kind == NK_RANGE) {
            Syntax_Node *range = node->real_type.range;
            if (range->range.low) {
              double lo = Eval_Const_Numeric (range->range.low);
              if (lo == lo) {
                float_type->low_bound = (Type_Bound){
                  .kind = BOUND_FLOAT, .float_value = lo
                };
              } else {
                float_type->low_bound = (Type_Bound){
                  .kind = BOUND_EXPR, .expr = range->range.low
                };
              }
            }
            if (range->range.high) {
              double hi = Eval_Const_Numeric (range->range.high);
              if (hi == hi) {
                float_type->high_bound = (Type_Bound){
                  .kind = BOUND_FLOAT, .float_value = hi
                };
              } else {
                float_type->high_bound = (Type_Bound){
                  .kind = BOUND_EXPR, .expr = range->range.high
                };
              }
            }
          }
          node->type = float_type;
          return float_type;
        }
      }
    default:
      return NULL;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §12.2 Statement Resolution                                                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Freeze all types declared in a list
// Per RM 13.14: At the end of a declarative part, all entities are frozen
void Freeze_Declaration_List (Node_List *list) {
  for (uint32_t i = 0; i < list->count; i++) {
    Syntax_Node *node = list->items[i];
    if (not node) continue;
    switch (node->kind) {
      case NK_TYPE_DECL:
      case NK_SUBTYPE_DECL:
        if (node->symbol and node->symbol->type) {
          Freeze_Type (node->symbol->type);
        }
        break;
      case NK_OBJECT_DECL:

        // Objects already freeze their type at declaration
        break;
      case NK_PROCEDURE_BODY:
      case NK_FUNCTION_BODY:

        // Subprogram bodies freeze all visible entities
        break;
      default:
        break;
    }
  }
}

// Populate a package symbol's exported[] array from its visible declarations.                      
// This must be called after all visible declarations are resolved so that                          
// decl->symbol pointers are valid. Used by both inline packages and loaded specs.                  
//                                                                                                  
void Populate_Package_Exports (Symbol *pkg_sym, Syntax_Node *pkg_spec) {
  if (not pkg_sym or not pkg_spec or pkg_spec->kind != NK_PACKAGE_SPEC) return;
  Node_List *visible = &pkg_spec->package_spec.visible_decls;

  // Count exports including nested items (enum literals, multiple object names)
  uint32_t count = 0;
  for (uint32_t i = 0; i < visible->count; i++) {
    Syntax_Node *decl = visible->items[i];
    if (not decl) continue;
    if (decl->kind == NK_OBJECT_DECL) {
      count += (uint32_t)decl->object_decl.names.count;
    } else if (decl->kind == NK_TYPE_DECL or decl->kind == NK_SUBTYPE_DECL) {
      count++;

      // Enumeration literals (direct or inherited from derived type)
      if (decl->type_decl.definition and
        decl->type_decl.definition->kind == NK_ENUMERATION_TYPE) {
        count += (uint32_t)decl->type_decl.definition->enum_type.literals.count;
      } else if (decl->symbol and decl->symbol->type and
             Type_Is_Enumeration (decl->symbol->type) and
             decl->symbol->type->enumeration.literal_count > 0) {

        // Derived enum type: count inherited literals
        count += decl->symbol->type->enumeration.literal_count;
      }
    } else if (decl->kind == NK_PROCEDURE_SPEC or decl->kind == NK_FUNCTION_SPEC or
           decl->kind == NK_PROCEDURE_BODY or decl->kind == NK_FUNCTION_BODY) {
      count++;
    } else if (decl->kind == NK_EXCEPTION_DECL) {
      count += (uint32_t)decl->exception_decl.names.count;
    } else if (decl->kind == NK_PACKAGE_SPEC) {
      count++;  // Nested packages
    } else if (decl->kind == NK_GENERIC_DECL) {
      count++;  // Nested generics (e.g., TEXT_IO.INTEGER_IO)
    } else if (decl->kind == NK_TASK_SPEC) {
      count++;  // Task type/object symbol
    }
  }
  if (count == 0) return;

  // Allocate and fill
  pkg_sym->exported = Arena_Allocate (count * sizeof (Symbol*));
  pkg_sym->exported_count = 0;
  for (uint32_t i = 0; i < visible->count; i++) {
    Syntax_Node *decl = visible->items[i];
    if (not decl) continue;
    if (decl->kind == NK_OBJECT_DECL) {
      for (uint32_t j = 0; j < decl->object_decl.names.count; j++) {
        Syntax_Node *name_node = decl->object_decl.names.items[j];
        if (name_node->symbol) {
          pkg_sym->exported[pkg_sym->exported_count++] = name_node->symbol;
        }
      }
    } else if ((decl->kind == NK_TYPE_DECL or decl->kind == NK_SUBTYPE_DECL) and decl->symbol) {
      pkg_sym->exported[pkg_sym->exported_count++] = decl->symbol;

      // Enumeration literals
      if (decl->type_decl.definition and
        decl->type_decl.definition->kind == NK_ENUMERATION_TYPE) {
        Node_List *lits = &decl->type_decl.definition->enum_type.literals;
        for (uint32_t j = 0; j < lits->count; j++) {
          if (lits->items[j]->symbol) {
            pkg_sym->exported[pkg_sym->exported_count++] = lits->items[j]->symbol;
          }
        }
      } else if (decl->symbol->type and Type_Is_Enumeration (decl->symbol->type) and
             decl->symbol->type->enumeration.literal_count > 0) {

        // Derived enum type: export inherited literal symbols.
        // These were created during NK_TYPE_DECL resolution for NK_DERIVED_TYPE.
        Type_Info *etype = decl->symbol->type;
        for (uint32_t j = 0; j < etype->enumeration.literal_count; j++) {
          String_Slice lit_name = etype->enumeration.literals[j];

          // Find the literal symbol with matching name and type in scope
          bool found = false;
          if (pkg_sym->scope) {
            uint32_t h = Symbol_Hash_Name (lit_name);
            for (Symbol *s = pkg_sym->scope->buckets[h]; s; s = s->next_in_bucket) {
              if (s->kind == SYMBOL_LITERAL and s->type == etype and
                Slice_Equal_Ignore_Case (s->name, lit_name)) {
                pkg_sym->exported[pkg_sym->exported_count++] = s;
                found = true;
                break;
              }
            }
          }

          // If not found in scope (e.g., overloaded away), create a new symbol
          if (not found) {
            Symbol *lit_sym = Symbol_New (SYMBOL_LITERAL, lit_name, decl->location);
            lit_sym->type = etype;
            lit_sym->frame_offset = (int64_t)j;
            pkg_sym->exported[pkg_sym->exported_count++] = lit_sym;
          }
        }
      }
    } else if ((decl->kind == NK_PROCEDURE_SPEC or decl->kind == NK_FUNCTION_SPEC or
          decl->kind == NK_PROCEDURE_BODY or decl->kind == NK_FUNCTION_BODY) and decl->symbol) {
      pkg_sym->exported[pkg_sym->exported_count++] = decl->symbol;
    } else if (decl->kind == NK_EXCEPTION_DECL) {
      for (uint32_t j = 0; j < decl->exception_decl.names.count; j++) {
        Syntax_Node *name_node = decl->exception_decl.names.items[j];
        if (name_node->symbol) {
          pkg_sym->exported[pkg_sym->exported_count++] = name_node->symbol;
        }
      }
    } else if (decl->kind == NK_PACKAGE_SPEC and decl->symbol) {
      pkg_sym->exported[pkg_sym->exported_count++] = decl->symbol;

    // Export nested generic packages/subprograms (e.g., TEXT_IO.INTEGER_IO)
    } else if (decl->kind == NK_GENERIC_DECL and decl->symbol) {
      pkg_sym->exported[pkg_sym->exported_count++] = decl->symbol;

    // Export task type/object symbol.                                                              
    // For single tasks (not task types), decl->symbol is the type symbol                           
    // which carries the TYPE_TASK info and exported entries.                                       
    // RM 9.1: task objects declared in a package spec are visible via                              
    // selected component notation (e.g., PKG.TASK_NAME.ENTRY).                                     
    //                                                                                              
    } else if (decl->kind == NK_TASK_SPEC and decl->symbol) {
      pkg_sym->exported[pkg_sym->exported_count++] = decl->symbol;
    }
  }
}

// Pre-register labels in a statement list to allow forward gotos.                                  
// Labels can appear as:                                                                            
//   1. NK_LABEL nodes wrapping other statements                                                    
//   2. The .label field of NK_BLOCK or NK_LOOP nodes (Ada allows naming blocks/loops)              
//                                                                                                  
void Preregister_Labels (Node_List *list) {
  for (uint32_t i = 0; i < list->count; i++) {
    Syntax_Node *node = list->items[i];
    if (not node) continue;
    String_Slice label_name = Empty_Slice;
    Source_Location label_loc = node->location;
    Symbol **label_sym_ptr = NULL;
    switch (node->kind) {
      case NK_LABEL:
        label_name = node->label_node.name;
        label_sym_ptr = &node->label_node.symbol;
        break;
      case NK_BLOCK:
        label_name = node->block_stmt.label;
        label_sym_ptr = &node->block_stmt.label_symbol;
        break;
      case NK_LOOP:
        label_name = node->loop_stmt.label;
        label_sym_ptr = &node->loop_stmt.label_symbol;
        break;
      default:
        break;
    }
    if (label_name.data and label_name.length > 0) {
      Symbol *label_sym = Symbol_New (SYMBOL_LABEL, label_name, label_loc);
      label_sym->type = sm->type_address;
      Symbol_Add (label_sym);
      if (label_sym_ptr) *label_sym_ptr = label_sym;
    }
  }
}
void Resolve_Statement_List (Node_List *list) {

  // First pass: register all labels to allow forward gotos
  Preregister_Labels (list);

  // Second pass: resolve all statements
  for (uint32_t i = 0; i < list->count; i++) {
    Resolve_Statement (list->items[i]);
  }
}
void Resolve_Statement (Syntax_Node *node) {
  if (not node) return;
  switch (node->kind) {
    case NK_ASSIGNMENT:
      Resolve_Expression (node->assignment.target);

      // Propagate target type to aggregate values for context-dependent typing
      if (node->assignment.value->kind == NK_AGGREGATE and
        node->assignment.target->type) {
        node->assignment.value->type = node->assignment.target->type;
      }
      Resolve_Expression (node->assignment.value);

      // Resolve character literal as enum literal in assignment (RM 3.5.1)
      if (node->assignment.value->kind == NK_CHARACTER and
        node->assignment.target->type) {
        Type_Info *et = node->assignment.target->type;

        // Follow parent_type (private>full), base_type chains to enum
        while (et and et->kind != TYPE_ENUMERATION) {
          if (et->parent_type) et = et->parent_type;
          else if (et->base_type) et = et->base_type;
          else break;
        }
        if (et and et->kind == TYPE_ENUMERATION)
          Resolve_Char_As_Enum (node->assignment.value, et);
      }

      // Type check: value must be compatible with target
      if (node->assignment.target->type and node->assignment.value->type) {
        if (not Type_Covers (node->assignment.target->type,
                node->assignment.value->type)) {
          Report_Error (node->location, "type mismatch in assignment");
        }
      }
      break;

    // Propagate enclosing function's return type to untyped
    // aggregates and concatenations (RM 5.8, 4.5.3)
    case NK_CALL_STMT:
      Resolve_Expression (node->assignment.target);
      break;
    case NK_RETURN:
      if (node->return_stmt.expression) {
        Syntax_Node *rexpr = node->return_stmt.expression;
        Symbol *owner = sm->current_scope->owner;
        Type_Info *ret_type = (owner and owner->kind == SYMBOL_FUNCTION)
          ? owner->return_type : NULL;
        if (rexpr->kind == NK_AGGREGATE and not rexpr->type and ret_type)
          rexpr->type = ret_type;

        // For RETURN A & B where both sides may be aggregates,
        // set the expression type so operand aggregates inherit it
        if (ret_type and rexpr->kind == NK_BINARY_OP and
          rexpr->binary.op == TK_AMPERSAND and not rexpr->type)
          rexpr->type = ret_type;
        Resolve_Expression (rexpr);
      }
      break;
    case NK_IF:
      Resolve_Expression (node->if_stmt.condition);
      Resolve_Statement_List (&node->if_stmt.then_stmts);
      for (uint32_t i = 0; i < node->if_stmt.elsif_parts.count; i++) {
        Resolve_Statement (node->if_stmt.elsif_parts.items[i]);
      }
      Resolve_Statement_List (&node->if_stmt.else_stmts);
      break;
    case NK_CASE:
      Resolve_Expression (node->case_stmt.expression);
      for (uint32_t i = 0; i < node->case_stmt.alternatives.count; i++) {
        Resolve_Statement (node->case_stmt.alternatives.items[i]);
      }
      break;
    case NK_LOOP:
      {
        Syntax_Node *iter = node->loop_stmt.iteration_scheme;
        bool is_for_loop = iter and iter->kind == NK_BINARY_OP and
                   iter->binary.op == TK_IN;

        // FOR loop - create loop variable in new scope.
        // Inherit owner from enclosing scope for proper name mangling
        if (is_for_loop) {
          Symbol *enclosing_owner = sm->current_scope->owner;
          Symbol_Manager_Push_Scope (enclosing_owner);
          Syntax_Node *loop_id = iter->binary.left;

          // Resolve range expression FIRST to get its type
          Resolve_Expression (iter->binary.right);
          if (loop_id and loop_id->kind == NK_IDENTIFIER) {
            Symbol *loop_var = Symbol_New (SYMBOL_VARIABLE,
                            loop_id->string_val.text,
                            loop_id->location);

            // Loop variable type comes from the range expression.                                  
            // Per Ada RM 5.5(6): if the range is universal_integer,                                
            // the loop parameter is of type INTEGER. Standard:                                     
            // see Emit_Loop_Statement - uses Standard_Integer for                                  
            // universal ranges.                                                                    
            //                                                                                      
            Type_Info *range_type = iter->binary.right->type;
            if (not range_type or range_type->kind == TYPE_UNIVERSAL_INTEGER)
              range_type = sm->type_integer;
            loop_var->type = range_type;
            Symbol_Add (loop_var);
            loop_id->symbol = loop_var;
          }
          Resolve_Statement_List (&node->loop_stmt.statements);
          Symbol_Manager_Pop_Scope ();

        // WHILE or bare LOOP
        } else {
          if (iter) Resolve_Expression (iter);
          Resolve_Statement_List (&node->loop_stmt.statements);
        }
      }
      break;
    case NK_BLOCK:

      // Inherit owner from enclosing scope for proper symbol parenting
      Symbol_Manager_Push_Scope (sm->current_scope->owner);

      // Resolve declarations first (adds symbols to scope)
      Resolve_Declaration_List (&node->block_stmt.declarations);

      // Freeze all types at end of declarative part (RM 13.14)
      Freeze_Declaration_List (&node->block_stmt.declarations);

      // Then resolve statements that use those symbols
      Resolve_Statement_List (&node->block_stmt.statements);
      for (uint32_t i = 0; i < node->block_stmt.handlers.count; i++) {
        Resolve_Statement (node->block_stmt.handlers.items[i]);
      }
      Symbol_Manager_Pop_Scope ();
      break;
    case NK_EXIT:
      if (node->exit_stmt.loop_name.data) {
        Symbol *loop_sym = Symbol_Find (node->exit_stmt.loop_name);
        if (loop_sym and (loop_sym->kind == SYMBOL_LOOP or loop_sym->kind == SYMBOL_LABEL))
          node->exit_stmt.target = loop_sym;
      }
      if (node->exit_stmt.condition) {
        Resolve_Expression (node->exit_stmt.condition);
      }
      break;
    case NK_RAISE:
      if (node->raise_stmt.exception_name) {
        Resolve_Expression (node->raise_stmt.exception_name);
      }
      break;
    case NK_EXCEPTION_HANDLER:

      // Resolve exception names
      for (uint32_t i = 0; i < node->handler.exceptions.count; i++) {
        Syntax_Node *exc = node->handler.exceptions.items[i];
        if (exc and exc->kind != NK_OTHERS) {
          Resolve_Expression (exc);
        }
      }
      Resolve_Statement_List (&node->handler.statements);
      break;
    case NK_ASSOCIATION:

      // Case alternative - resolve choices and body
      for (uint32_t i = 0; i < node->association.choices.count; i++) {
        Resolve_Expression (node->association.choices.items[i]);
      }
      if (node->association.expression) {
        if (node->association.expression->kind == NK_BLOCK) {
          Resolve_Statement (node->association.expression);
        } else {
          Resolve_Expression (node->association.expression);
        }
      }
      break;

    // Label symbol was pre-registered by Preregister_Labels.
    // Just resolve the labeled statement.
    case NK_LABEL:
      {
        if (node->label_node.statement) {
          Resolve_Statement (node->label_node.statement);
        }
      }
      break;

    // Look up the target label
    case NK_GOTO:
      {
        Symbol *label = Symbol_Find (node->goto_stmt.name);
        if (not label) {
          Report_Error (node->location, "undefined label '%.*s'",
                (int)node->goto_stmt.name.length,
                node->goto_stmt.name.data);
        } else if (label->kind != SYMBOL_LABEL and label->kind != SYMBOL_LOOP) {
          Report_Error (node->location, "'%.*s' is not a label",
                (int)node->goto_stmt.name.length,
                node->goto_stmt.name.data);

        // Store resolved label for code generation
        } else {
          node->goto_stmt.target = label;
        }
      }
      break;
    case NK_ACCEPT:

      // ACCEPT statement for task entry - resolve index and parameters.                            
      // Each accept statement has its own scope for parameters to avoid                            
      // naming conflicts with parameters from other accept statements                              
      // in the same selective wait (e.g., multiple accepts with param X).                          
      //                                                                                            

      // Look up the entry symbol by name - it should be in the task's scope
      node->accept_stmt.entry_sym = Symbol_Find (node->accept_stmt.entry_name);
      if (node->accept_stmt.index) {
        Resolve_Expression (node->accept_stmt.index);
      }

      // Push new scope for accept parameters - use current scope owner                             
      // so parameters are considered local (not global) for naming.                                
      // Each accept statement needs its own scope because multiple accepts                         
      // in a selective wait may have parameters with the same name.                                
      //                                                                                            
      Symbol_Manager_Push_Scope (sm->current_scope->owner);

      // Resolve accept parameters
      for (uint32_t i = 0; i < node->accept_stmt.parameters.count; i++) {
        Syntax_Node *param = node->accept_stmt.parameters.items[i];
        if (param and param->kind == NK_PARAM_SPEC) {
          if (param->param_spec.param_type) {
            Resolve_Expression (param->param_spec.param_type);
          }

          // Add parameter names to scope for the accept body
          for (uint32_t j = 0; j < param->param_spec.names.count; j++) {
            Syntax_Node *name = param->param_spec.names.items[j];
            if (name and name->kind == NK_IDENTIFIER) {
              Symbol *param_sym = Symbol_New (SYMBOL_PARAMETER,
                name->string_val.text, name->location);
              if (param->param_spec.param_type)
                param_sym->type = param->param_spec.param_type->type;
              Symbol_Add (param_sym);
              name->symbol = param_sym;
            }
          }
        }
      }

      // Resolve accept body statements
      Resolve_Statement_List (&node->accept_stmt.statements);

      // Pop accept scope
      Symbol_Manager_Pop_Scope ();
      break;
    case NK_SELECT:

      // SELECT statement - resolve alternatives
      for (uint32_t i = 0; i < node->select_stmt.alternatives.count; i++) {
        Syntax_Node *alt = node->select_stmt.alternatives.items[i];
        if (alt) Resolve_Statement (alt);
      }
      break;
    case NK_DELAY:

      // DELAY statement - resolve delay expression
      if (node->delay_stmt.expression) {
        Resolve_Expression (node->delay_stmt.expression);
      }
      break;
    default:
      break;
  }
}
// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §12.3 Declaration Resolution                                                                     
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Install all exported symbols from a declaration list into the symbol table.                      
// Handles objects, exceptions, and enumeration literals uniformly.                                 
// Extracted from two identical visible/private installation blocks.                                
//                                                                                                  
void Install_Declaration_Symbols (Node_List *decls) {
  for (uint32_t i = 0; i < decls->count; i++) {
    Syntax_Node *decl = decls->items[i];

    // For single task declarations (RM 9.1), both a type symbol and an                             
    // anonymous object variable were created during semantic analysis.                             
    // decl->symbol is the type; we must also install the variable so that                          
    // codegen can find it for global allocation and task start.                                    
    // Grab the variable from the type's original defining_scope (the spec                          
    // scope) BEFORE Symbol_Add changes defining_scope to the body scope.                           
    //                                                                                              
    Symbol *task_obj_sym = NULL;
    if (decl->kind == NK_TASK_SPEC and not decl->task_spec.is_type and decl->symbol) {
      Scope *orig_scope = decl->symbol->defining_scope;
      if (orig_scope) {
        for (uint32_t j = 0; j < orig_scope->symbol_count; j++) {
          Symbol *s = orig_scope->symbols[j];
          if (s and s->kind == SYMBOL_VARIABLE and
            Type_Is_Task (s->type) and
            Slice_Equal_Ignore_Case (s->name, decl->task_spec.name)) {
            task_obj_sym = s;
            break;
          }
        }
      }
    }
    if (decl->symbol) Symbol_Add (decl->symbol);
    if (task_obj_sym) Symbol_Add (task_obj_sym);

    // Multi-name object declarations: install each named symbol
    if (decl->kind == NK_OBJECT_DECL)
      for (uint32_t j = 0; j < decl->object_decl.names.count; j++) {
        Syntax_Node *name = decl->object_decl.names.items[j];
        if (name->symbol) Symbol_Add (name->symbol);
      }

    // Exception declarations: same multi-name pattern
    if (decl->kind == NK_EXCEPTION_DECL)
      for (uint32_t j = 0; j < decl->exception_decl.names.count; j++) {
        Syntax_Node *name = decl->exception_decl.names.items[j];
        if (name->symbol) Symbol_Add (name->symbol);
      }

    // Enumeration type: install each literal as a separate symbol
    if (decl->kind == NK_TYPE_DECL and decl->type_decl.definition and
      decl->type_decl.definition->kind == NK_ENUMERATION_TYPE) {
      Node_List *lits = &decl->type_decl.definition->enum_type.literals;
      for (uint32_t j = 0; j < lits->count; j++)
        if (lits->items[j]->symbol) Symbol_Add (lits->items[j]->symbol);
    }
  }
}
void Resolve_Declaration_List (Node_List *list) {
  for (uint32_t i = 0; i < list->count; i++) {
    Resolve_Declaration (list->items[i]);
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §12.4.1 Derived Type Operation Inheritance (RM 3.4)                                              
//                                                                                                  
// When TYPE T IS NEW PARENT is declared, T inherits primitive operations of                        
// PARENT. A primitive operation is a subprogram declared in the same scope as                      
// the type, with a parameter or return type of that type.                                          
//                                                                                                  
// The inherited operation has T substituted for PARENT in its profile. We create                   
// a new symbol that wraps the parent's implementation with type conversions.                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Check if two types represent the same named type.                                                
// Handles private types where partial (visible) and full (private) views                           
// have different Type_Info pointers but represent the same type.                                   
//                                                                                                  
bool Types_Same_Named (Type_Info *t1, Type_Info *t2) {
  if (not t1 or not t2) return false;
  if (t1 == t2) return true;

  // For private types, partial and full views have same name but different Type_Info.
  if (t1->name.data and t2->name.data and
    Slice_Equal_Ignore_Case (t1->name, t2->name)) {

    // Same name: check if they share a defining symbol
    if (t1->defining_symbol and t2->defining_symbol and
      t1->defining_symbol == t2->defining_symbol) return true;

    // For types declared in the same package, same name = same type
    if (t1->defining_symbol and t2->defining_symbol and
      t1->defining_symbol->defining_scope == t2->defining_symbol->defining_scope)
      return true;
  }
  return false;
}

// Check if a subprogram has the given type in its profile (parameter or return)
bool Subprogram_Is_Primitive_Of (Symbol *sub, Type_Info *type) {
  if (not sub or not type) return false;
  if (sub->kind != SYMBOL_FUNCTION and sub->kind != SYMBOL_PROCEDURE) return false;

  // Check return type for functions
  if (sub->kind == SYMBOL_FUNCTION and Types_Same_Named (sub->return_type, type)) return true;

  // Check parameter types
  for (uint32_t i = 0; i < sub->parameter_count; i++) {
    if (Types_Same_Named (sub->parameters[i].param_type, type)) return true;
  }
  return false;
}

// Helper to create a derived operation from a parent operation
void Create_Derived_Operation (Symbol *sub,
                   Type_Info *derived_type, Type_Info *parent_type,
                   Symbol *type_sym) {
  (void)type_sym;  // reserved for future use
  Symbol *derived_sub = Symbol_New (sub->kind, sub->name, sub->location);
  derived_sub->parameter_count  = sub->parameter_count;
  derived_sub->parent_operation = sub;  // Link to parent's implementation
  derived_sub->derived_from_type = derived_type;
  derived_sub->is_overloaded    = true;  // Needs unique_id suffix

  // Copy and adjust parameters - substitute derived_type for parent_type
  if (sub->parameter_count > 0) {
    derived_sub->parameters = Arena_Allocate (sub->parameter_count * sizeof (Parameter_Info));
    for (uint32_t j = 0; j < sub->parameter_count; j++) {
      derived_sub->parameters[j] = sub->parameters[j];
      if (Types_Same_Named (sub->parameters[j].param_type, parent_type)) {
        derived_sub->parameters[j].param_type = derived_type;
      }
    }
  }

  // Adjust return type for functions
  if (sub->kind == SYMBOL_FUNCTION) {
    derived_sub->return_type = Types_Same_Named (sub->return_type, parent_type)
                   ? derived_type : sub->return_type;
  }

  // Make visible so overload resolution can find it
  derived_sub->visibility = VIS_IMMEDIATELY_VISIBLE;

  // Add to current scope
  Symbol_Add (derived_sub);

  // Set parent to match the parent operation's parent for proper name mangling                     
  // and to avoid false nested function detection. The derived operation                            
  // wraps the parent operation so should be at the same nesting level.                             
  // Done AFTER Symbol_Add since it overwrites parent.                                              
  //                                                                                                
  derived_sub->parent = sub->parent;
}

// Create inherited operations for a derived type (RM 3.4)
void Derive_Subprograms (Type_Info *derived_type,
                 Type_Info *parent_type, Symbol *type_sym) {
  if (not derived_type or not parent_type or not type_sym) return;

  // Find the parent type's symbol and its owning scope/package
  Symbol *parent_sym = parent_type->defining_symbol;
  if (not parent_sym) return;

  // For private types in packages, look at the package's exported symbols.
  // The parent of the type symbol is the enclosing package/scope.
  Symbol *pkg = parent_sym->parent;

  // Search package exports for primitive operations
  if (pkg and pkg->kind == SYMBOL_PACKAGE and pkg->exported_count > 0) {
    for (uint32_t i = 0; i < pkg->exported_count; i++) {
      Symbol *sub = pkg->exported[i];
      if (not sub) continue;
      if (sub->kind != SYMBOL_FUNCTION and sub->kind != SYMBOL_PROCEDURE) continue;
      if (not Subprogram_Is_Primitive_Of (sub, parent_type)) continue;
      Create_Derived_Operation (sub, derived_type, parent_type, type_sym);
    }
    return;
  }

  // Fallback: search the scope where the parent type is declared.
  // Iterate through hash buckets to find all symbols including overloads.
  Scope *parent_scope = parent_sym->defining_scope;
  if (not parent_scope) return;
  for (uint32_t h = 0; h < SYMBOL_TABLE_SIZE; h++) {

    // Check this symbol and all in its overload chain
    for (Symbol *sym = parent_scope->buckets[h]; sym; sym = sym->next_in_bucket) {
      for (Symbol *sub = sym; sub; sub = sub->next_overload) {
        if (sub->kind != SYMBOL_FUNCTION and sub->kind != SYMBOL_PROCEDURE) continue;
        if (not Subprogram_Is_Primitive_Of (sub, parent_type)) continue;
        Create_Derived_Operation (sub, derived_type, parent_type, type_sym);
      }
    }
  }
}
void Resolve_Declaration (Syntax_Node *node) {
  if (not node) return;
  switch (node->kind) {
    case NK_OBJECT_DECL:

      // Resolve type
      if (node->object_decl.object_type) {
        Resolve_Expression (node->object_decl.object_type);

        // Object declarations freeze the type (RM 13.14)
        if (node->object_decl.object_type->type) {
          Freeze_Type (node->object_decl.object_type->type);
        }
      }

      // Resolve initializer/renamed object - propagate type to aggregates first
      // Propagate type to aggregate initializer from declared type
      if (node->object_decl.init) {
        if (node->object_decl.init->kind == NK_AGGREGATE and
          node->object_decl.object_type and node->object_decl.object_type->type) {
          node->object_decl.init->type = node->object_decl.object_type->type;
        }
        Resolve_Expression (node->object_decl.init);

        // Resolve character literal as enumeration literal (RM 3.5.1)
        if (node->object_decl.init->kind == NK_CHARACTER and
          node->object_decl.object_type and node->object_decl.object_type->type) {
          Type_Info *et = node->object_decl.object_type->type;
          while (et and et->kind != TYPE_ENUMERATION) {
            if (et->parent_type) et = et->parent_type;
            else if (et->base_type) et = et->base_type;
            else break;
          }
          if (et and et->kind == TYPE_ENUMERATION)
            Resolve_Char_As_Enum (node->object_decl.init, et);
        }
      }

      // Add symbols for each name
      for (uint32_t i = 0; i < node->object_decl.names.count; i++) {
        Syntax_Node *name_node = node->object_decl.names.items[i];
        Symbol *sym = Symbol_New (
          node->object_decl.is_constant ? SYMBOL_CONSTANT : SYMBOL_VARIABLE,
          name_node->string_val.text,
          name_node->location);

        // Object renames: type comes from renamed object
        if (node->object_decl.is_rename and node->object_decl.init) {
          sym->type            = node->object_decl.init->type;
          sym->renamed_object  = node->object_decl.init;  // Point to renamed
          sym->is_named_number = false;
        }

        // For named numbers (constant without type mark), use init expression's type
        else if (node->object_decl.object_type) {
          sym->type = node->object_decl.object_type->type;
          sym->is_named_number = false;

        // Named number: use universal type from init expression
        } else if (node->object_decl.is_constant and node->object_decl.init) {
          sym->type = node->object_decl.init->type;
          sym->is_named_number = true;  // Mark as named number for inline generation
        } else {
          sym->type = NULL;
          sym->is_named_number = node->object_decl.is_constant and not node->object_decl.object_type;
        }
        sym->declaration = node;
        Symbol_Add (sym);

        // RM 7.4: If Symbol_Add detected a deferred constant completion,                           
        // it already set name_node->symbol to the existing (deferred)                              
        // symbol.  Preserve that binding so codegen uses one symbol.                               
        //                                                                                          
        if (not name_node->symbol)
          name_node->symbol = sym;
      }
      break;

    // Check for existing incomplete type to complete (RM 3.8.1)
    // Must be in same declarative region (current scope)
    case NK_TYPE_DECL:
      {
        Symbol *existing = Symbol_Find (node->type_decl.name);
        Symbol *sym;
        Type_Info *type;
        if (existing and existing->kind == SYMBOL_TYPE and
          existing->type and (existing->type->kind == TYPE_UNKNOWN or
          existing->type->kind == TYPE_PRIVATE or
          existing->type->kind == TYPE_LIMITED_PRIVATE) and
          existing->defining_scope == sm->current_scope and
          node->type_decl.definition) {

          // Complete the existing incomplete type
          sym = existing;
          type = sym->type;

        // Create new type symbol
        } else {
          sym = Symbol_New (SYMBOL_TYPE, node->type_decl.name, node->location);
          type = Type_New (TYPE_UNKNOWN, node->type_decl.name);
          sym->type = type;
          type->defining_symbol = sym;
          Symbol_Add (sym);
        }
        node->symbol = sym;

        // For discriminated types, add discriminant symbols to scope first
        // so they can be referenced in the type definition (e.g., CASE A IS)
        bool has_discriminants = node->type_decl.discriminants.count > 0;
        if (has_discriminants) {
          Symbol_Manager_Push_Scope (sym);
          for (uint32_t i = 0; i < node->type_decl.discriminants.count; i++) {
            Syntax_Node *disc_spec = node->type_decl.discriminants.items[i];

            // Resolve discriminant type first
            if (disc_spec->kind == NK_DISCRIMINANT_SPEC) {
              Type_Info *disc_type = sm->type_integer;
              if (disc_spec->discriminant.disc_type) {
                disc_type = Resolve_Expression (disc_spec->discriminant.disc_type);
              }

              // Resolve default expression if present (RM 3.7.1)
              if (disc_spec->discriminant.default_expr) {
                Resolve_Expression (disc_spec->discriminant.default_expr);
              }

              // Add each discriminant name as a symbol (RM 3.7.1)
              for (uint32_t j = 0; j < disc_spec->discriminant.names.count; j++) {
                Syntax_Node *name_node = disc_spec->discriminant.names.items[j];
                Symbol *disc_sym = Symbol_New (SYMBOL_DISCRIMINANT, name_node->string_val.text,
                                name_node->location);
                disc_sym->type   = disc_type;
                disc_sym->parent = sym;
                Symbol_Add (disc_sym);
                name_node->symbol = disc_sym;
              }
            }
          }
        }

        // Resolve definition and copy type info
        if (node->type_decl.definition) {
          Type_Info *def_type = Resolve_Expression (node->type_decl.definition);

          // Copy type info from definition to named type
          if (def_type) {
            type->kind        = def_type->kind;
            type->size        = def_type->size;
            type->alignment   = def_type->alignment;
            type->low_bound   = def_type->low_bound;
            type->high_bound  = def_type->high_bound;

            // For derived and subtype types, preserve base/parent chain
            type->base_type   = def_type->base_type;
            type->parent_type = def_type->parent_type;

            // For user-defined integer types (TYPE T IS RANGE L..R), the declared                  
            // type is the "first subtype" and needs a base type (RM 3.5.4).                        
            // Use INTEGER as the base type for constraint checking in 'PRED/'SUCC.                 
            //                                                                                      
            if (def_type->kind == TYPE_INTEGER and type->base_type == NULL) {
              type->base_type = sm->type_integer;
            }
            if (Type_Is_Array_Like (def_type)) {
              type->array = def_type->array;
            } else if (Type_Is_Fixed_Point (def_type)) {
              type->fixed = def_type->fixed;
            } else if (Type_Is_Float (def_type)) {
              type->flt = def_type->flt;
            } else if (Type_Is_Record (def_type)) {
              type->record = def_type->record;

              // Add discriminants as accessible record components (RM 3.7.1)                       
              // Discriminants can be accessed like normal components: R.D1                         
              // Count total discriminant names and check for defaults                              
              //                                                                                    
              if (has_discriminants) {
                uint32_t disc_count = 0;
                bool all_have_defaults = true;
                for (uint32_t i = 0; i < node->type_decl.discriminants.count; i++) {
                  Syntax_Node *disc_spec = node->type_decl.discriminants.items[i];
                  if (disc_spec->kind == NK_DISCRIMINANT_SPEC) {
                    disc_count += disc_spec->discriminant.names.count;
                    if (not disc_spec->discriminant.default_expr) {
                      all_have_defaults = false;
                    }
                  }
                }

                // Create new components array with discriminants at the start
                uint32_t old_count = type->record.component_count;
                uint32_t new_count = old_count + disc_count;
                Component_Info *new_comps = Arena_Allocate (
                  new_count * sizeof (Component_Info));

                // Add discriminants first (they come before other components)
                uint32_t disc_idx = 0;
                uint32_t disc_offset = 0;  // Running offset for discriminants
                for (uint32_t i = 0; i < node->type_decl.discriminants.count; i++) {
                  Syntax_Node *disc_spec = node->type_decl.discriminants.items[i];
                  if (disc_spec->kind == NK_DISCRIMINANT_SPEC) {
                    Type_Info *disc_type = sm->type_integer;
                    if (disc_spec->discriminant.disc_type and
                      disc_spec->discriminant.disc_type->type) {
                      disc_type = disc_spec->discriminant.disc_type->type;
                    }
                    uint32_t disc_size = disc_type ? disc_type->size : 8;
                    for (uint32_t j = 0; j < disc_spec->discriminant.names.count; j++) {
                      Syntax_Node *name_node = disc_spec->discriminant.names.items[j];
                      new_comps[disc_idx].name = name_node->string_val.text;
                      new_comps[disc_idx].component_type = disc_type;
                      new_comps[disc_idx].byte_offset = disc_offset;
                      new_comps[disc_idx].bit_offset = 0;
                      new_comps[disc_idx].bit_size = disc_size * 8;
                      new_comps[disc_idx].default_expr = disc_spec->discriminant.default_expr;
                      new_comps[disc_idx].is_discriminant = true;
                      new_comps[disc_idx].variant_index = -1;  // Fixed part
                      disc_offset += disc_size;
                      disc_idx++;
                    }
                  }
                }

                // Copy existing components after discriminants, adjusting offsets
                for (uint32_t i = 0; i < old_count; i++) {
                  new_comps[disc_count + i] = type->record.components[i];
                  new_comps[disc_count + i].byte_offset += disc_offset;
                }

                // Adjust variant_offset to account for discriminant size
                if (type->record.variant_count > 0) {
                  type->record.variant_offset += disc_offset;

                  // Also adjust variant component offsets
                  for (uint32_t i = 0; i < type->record.variant_count; i++) {
                    type->record.variants[i].first_component += disc_count;
                  }
                }

                // Update record size to include discriminants
                type->size                      += disc_offset;
                type->record.components          = new_comps;
                type->record.component_count     = new_count;

                // Set discriminant tracking fields
                type->record.has_discriminants   = true;
                type->record.discriminant_count  = disc_count;
                type->record.all_defaults        = all_have_defaults;
              }
            } else if (Type_Is_Access (def_type)) {
              type->access = def_type->access;
            } else if (Type_Is_Enumeration (def_type)) {
              type->enumeration = def_type->enumeration;

              // Create symbols for enumeration literals
              // They must reference the named type (type), not the anonymous def_type
              if (node->type_decl.definition and
                node->type_decl.definition->kind == NK_ENUMERATION_TYPE) {
                Node_List *lits = &node->type_decl.definition->enum_type.literals;
                for (uint32_t i = 0; i < lits->count; i++) {
                  Syntax_Node *lit = lits->items[i];
                  Symbol *lit_sym = Symbol_New (SYMBOL_LITERAL, lit->string_val.text, lit->location);
                  lit_sym->type = type;  // Reference the named type
                  lit_sym->frame_offset = (int64_t)i;  // Store ordinal position
                  Symbol_Add (lit_sym);
                  lit->symbol = lit_sym;
                }
              }

              // For derived enumeration types (TYPE T IS NEW BOOLEAN),
              // create inherited literal symbols (RM 3.4(12))
              if (node->type_decl.definition and
                node->type_decl.definition->kind == NK_DERIVED_TYPE and
                type->enumeration.literals and type->enumeration.literal_count > 0) {
                for (uint32_t i = 0; i < type->enumeration.literal_count; i++) {
                  String_Slice lit_name = type->enumeration.literals[i];
                  Symbol *lit_sym = Symbol_New (SYMBOL_LITERAL, lit_name, node->location);
                  lit_sym->type = type;
                  lit_sym->frame_offset = (int64_t)i;
                  Symbol_Add (lit_sym);
                }
              }
            }

            // Propagate limited flag from AST node (RM 7.5)
            if (node->type_decl.is_limited) {
              type->is_limited = true;
            }

            // RM 7.4.1: Propagate private→record completion to constrained                         
            // subtypes.  SUBTYPE S IS T(V) copies T's Type_Info when T is                          
            // still TYPE_PRIVATE, creating a snapshot with stale `kind` and                        
            // partial `components`.  After the full declaration completes T                        
            // to TYPE_RECORD, refresh every such snapshot so that record                           
            // aggregates, selected components, and codegen see the full view.                      
            // Constraint-specific fields (disc_constraint_values/exprs) are                        
            // untouched - only definition-derived fields are updated.                              
            //                                                                                      
            if (Type_Is_Record (type)) {
              for (Scope *scp = sm->current_scope; scp; scp = scp->parent) {
                for (uint32_t si = 0; si < scp->symbol_count; si++) {
                  Symbol *sub = scp->symbols[si];
                  if (sub and sub->type and sub->type != type
                    and sub->type->base_type == type
                    and Type_Is_Private (sub->type)) {
                    sub->type->kind = TYPE_RECORD;
                    sub->type->record.components      = type->record.components;
                    sub->type->record.component_count = type->record.component_count;
                    sub->type->record.variants        = type->record.variants;
                    sub->type->record.variant_count   = type->record.variant_count;
                    sub->type->record.variant_offset  = type->record.variant_offset;
                    sub->type->record.max_variant_size = type->record.max_variant_size;
                    sub->type->record.variant_part_node = type->record.variant_part_node;
                    sub->type->size = type->size;
                  }
                }
              }
            }
          }
        }

        // Handle private type declarations (TYPE T (...) IS [LIMITED] PRIVATE).                    
        // Set up type kind and discriminant components so subtypes like                            
        // SUBTYPE S IS T(V) can be created before the full declaration.                            
        //                                                                                          
        if (not node->type_decl.definition and
          (node->type_decl.is_private or node->type_decl.is_limited) and
          type->kind == TYPE_UNKNOWN) {
          type->kind = node->type_decl.is_limited ?
            TYPE_LIMITED_PRIVATE : TYPE_PRIVATE;
          if (has_discriminants) {
            uint32_t disc_count = 0;
            bool all_have_defaults = true;
            for (uint32_t i = 0; i < node->type_decl.discriminants.count; i++) {
              Syntax_Node *disc_spec = node->type_decl.discriminants.items[i];
              if (disc_spec->kind == NK_DISCRIMINANT_SPEC) {
                disc_count += disc_spec->discriminant.names.count;
                if (not disc_spec->discriminant.default_expr)
                  all_have_defaults = false;
              }
            }
            Component_Info *comps = Arena_Allocate (
              disc_count * sizeof (Component_Info));
            uint32_t di = 0, off = 0;
            for (uint32_t i = 0; i < node->type_decl.discriminants.count; i++) {
              Syntax_Node *disc_spec = node->type_decl.discriminants.items[i];
              if (disc_spec->kind != NK_DISCRIMINANT_SPEC) continue;
              Type_Info *dt = sm->type_integer;
              if (disc_spec->discriminant.disc_type and
                disc_spec->discriminant.disc_type->type)
                dt = disc_spec->discriminant.disc_type->type;
              uint32_t dsz = dt ? dt->size : 8;
              for (uint32_t j = 0; j < disc_spec->discriminant.names.count; j++) {
                Syntax_Node *name_node = disc_spec->discriminant.names.items[j];
                comps[di].name = name_node->string_val.text;
                comps[di].component_type = dt;
                comps[di].byte_offset = off;
                comps[di].bit_offset = 0;
                comps[di].bit_size = dsz * 8;
                comps[di].default_expr = disc_spec->discriminant.default_expr;
                comps[di].is_discriminant = true;
                comps[di].variant_index = -1;
                off += dsz;
                di++;
              }
            }
            type->record.components         = comps;
            type->record.component_count    = disc_count;
            type->record.has_discriminants  = true;
            type->record.discriminant_count = disc_count;
            type->record.all_defaults       = all_have_defaults;
            type->size                      = off;
          }
        }

        // For derived types (TYPE T IS NEW PARENT), inherit primitive
        // subprograms from the parent type (RM 3.4)
        if (type->parent_type) {
          Derive_Subprograms (type, type->parent_type, sym);
        }

        // Pop discriminant scope if we pushed one
        if (has_discriminants) {
          Symbol_Manager_Pop_Scope ();
        }
      }
      break;
    case NK_SUBTYPE_DECL:
      {
        Symbol *sym = Symbol_New (SYMBOL_SUBTYPE, node->type_decl.name, node->location);
        Symbol_Add (sym);
        node->symbol = sym;
        if (node->type_decl.definition) {
          Resolve_Expression (node->type_decl.definition);
          sym->type = node->type_decl.definition->type;
        }
      }
      break;

    // Count total parameters first (needed for overload matching)
    case NK_PROCEDURE_SPEC:
    case NK_FUNCTION_SPEC:
      {
        Node_List *param_list = &node->subprogram_spec.parameters;
        uint32_t total_params = 0;
        for (uint32_t i = 0; i < param_list->count; i++) {
          Syntax_Node *ps = param_list->items[i];
          if (ps->kind == NK_PARAM_SPEC) {
            total_params += ps->param_spec.names.count;
          }
        }

        // Check if there's already a matching symbol from package spec.                            
        // This happens when resolving a subprogram body that completes a spec.                     
        // For overloaded subprograms, must match parameter count AND types.                        
        // Per RM 6.3, only match specs declared in the same scope - not                            
        // USE-visible homographs from other packages (those are hidden).                           
        //                                                                                          
        Symbol_Kind expected_kind = node->kind == NK_PROCEDURE_SPEC ?
                       SYMBOL_PROCEDURE : SYMBOL_FUNCTION;
        Symbol *scope_owner = sm->current_scope ? sm->current_scope->owner : NULL;
        Symbol *sym = Symbol_Find (node->subprogram_spec.name);

        // Only match specs from the current declarative region
        while (sym) {
          if (sym->kind == expected_kind and sym->parameter_count == total_params
            and sym->parent == scope_owner) {

            // Check parameter types match
            bool types_match = true;
            uint32_t param_idx = 0;
            for (uint32_t i = 0; i < param_list->count and types_match; i++) {
              Syntax_Node *ps = param_list->items[i];

              // Resolve param type first
              if (ps->kind == NK_PARAM_SPEC) {
                if (ps->param_spec.param_type) {
                  Resolve_Expression (ps->param_spec.param_type);
                }
                Type_Info *body_type = ps->param_spec.param_type ?
                             ps->param_spec.param_type->type : NULL;
                for (uint32_t j = 0; j < ps->param_spec.names.count; j++) {
                  if (param_idx < sym->parameter_count) {
                    Type_Info *spec_type = sym->parameters[param_idx].param_type;
                    if (not Types_Same_Named (body_type, spec_type)) {
                      types_match = false;
                      break;
                    }
                  }
                  param_idx++;
                }
              }
            }
            if (types_match and expected_kind == SYMBOL_FUNCTION) {
              if (node->subprogram_spec.return_type) {
                Resolve_Expression (node->subprogram_spec.return_type);
                Type_Info *body_return = node->subprogram_spec.return_type->type;
                if (not Types_Same_Named (body_return, sym->return_type)) {
                  types_match = false;
                }
              }
            }

            // Found matching spec - check it's not already claimed
            if (types_match and not sym->body_claimed) {
              sym->body_claimed = true;
              node->symbol = sym;
              goto spec_matched;
            }
          }
          sym = sym->next_overload;
        }

        // No matching spec found - create new symbol
        sym = Symbol_New (expected_kind, node->subprogram_spec.name, node->location);
        sym->parameter_count = total_params;
        if (total_params > 0) {
          sym->parameters = Arena_Allocate (total_params * sizeof (Parameter_Info));
          uint32_t param_idx = 0;
          for (uint32_t i = 0; i < param_list->count; i++) {
            Syntax_Node *ps = param_list->items[i];

            // Resolve parameter type
            if (ps->kind == NK_PARAM_SPEC) {
              if (ps->param_spec.param_type) {
                Resolve_Expression (ps->param_spec.param_type);
              }
              Type_Info *pt = ps->param_spec.param_type ?
                      ps->param_spec.param_type->type : NULL;

              // Resolve default expression, propagating param type
              // to untyped aggregates (RM 6.1, 3.2.1)
              if (ps->param_spec.default_expr) {
                if (ps->param_spec.default_expr->kind == NK_AGGREGATE and
                  not ps->param_spec.default_expr->type and pt)
                  ps->param_spec.default_expr->type = pt;
                Resolve_Expression (ps->param_spec.default_expr);
              }
              for (uint32_t j = 0; j < ps->param_spec.names.count; j++) {
                Syntax_Node *name = ps->param_spec.names.items[j];
                sym->parameters[param_idx].name          = name->string_val.text;
                sym->parameters[param_idx].param_type    = pt;
                sym->parameters[param_idx].mode          = (Parameter_Mode)ps->param_spec.mode;
                sym->parameters[param_idx].default_value = ps->param_spec.default_expr;
                param_idx++;
              }
            }
          }
        }
        if (node->subprogram_spec.return_type) {
          Resolve_Expression (node->subprogram_spec.return_type);
          sym->return_type = node->subprogram_spec.return_type->type;
        }
        Symbol_Add (sym);
        node->symbol = sym;
      spec_matched:;
      }
      break;

    // Resolve the renamed subprogram
    case NK_SUBPROGRAM_RENAMING:

      // PROCEDURE P RENAMES Q; or FUNCTION F RENAMES G;
      {
        if (node->subprogram_spec.renamed) {
          Resolve_Expression (node->subprogram_spec.renamed);
        }
        Symbol *renamed_sym = node->subprogram_spec.renamed ?
                    node->subprogram_spec.renamed->symbol : NULL;

        // Create new symbol for the rename
        bool is_proc = not node->subprogram_spec.return_type;
        Symbol *sym = Symbol_New (
          is_proc ? SYMBOL_PROCEDURE : SYMBOL_FUNCTION,
          node->subprogram_spec.name, node->location);

        // Copy info from renamed subprogram if available
        if (renamed_sym) {
          sym->parameters      = renamed_sym->parameters;
          sym->parameter_count = renamed_sym->parameter_count;
          sym->return_type     = renamed_sym->return_type;
          sym->renamed_object  = (Syntax_Node *)renamed_sym;  // Store reference

        // Build parameter info from our own spec
        } else {
          Node_List *param_list = &node->subprogram_spec.parameters;
          uint32_t total_params = 0;
          for (uint32_t i = 0; i < param_list->count; i++) {
            Syntax_Node *ps = param_list->items[i];
            if (ps->kind == NK_PARAM_SPEC)
              total_params += ps->param_spec.names.count;
          }
          sym->parameter_count = total_params;
          if (total_params > 0) {
            sym->parameters = Arena_Allocate (total_params * sizeof (Parameter_Info));
            uint32_t idx = 0;
            for (uint32_t i = 0; i < param_list->count; i++) {
              Syntax_Node *ps = param_list->items[i];
              if (ps->kind == NK_PARAM_SPEC) {
                if (ps->param_spec.param_type)
                  Resolve_Expression (ps->param_spec.param_type);
                Type_Info *pt = ps->param_spec.param_type ?
                        ps->param_spec.param_type->type : NULL;
                for (uint32_t j = 0; j < ps->param_spec.names.count; j++) {
                  Syntax_Node *nm = ps->param_spec.names.items[j];
                  sym->parameters[idx].name          = nm->string_val.text;
                  sym->parameters[idx].param_type    = pt;
                  sym->parameters[idx].mode          = (Parameter_Mode)ps->param_spec.mode;
                  sym->parameters[idx].default_value = ps->param_spec.default_expr;
                  idx++;
                }
              }
            }
          }
          if (node->subprogram_spec.return_type) {
            Resolve_Expression (node->subprogram_spec.return_type);
            sym->return_type = node->subprogram_spec.return_type->type;
          }
        }
        Symbol_Add (sym);
        node->symbol = sym;
      }
      break;

    // Check if this body completes a generic declaration
    case NK_PROCEDURE_BODY:
    case NK_FUNCTION_BODY:
      {
        Syntax_Node *spec = node->subprogram_body.specification;
        String_Slice body_name = spec ? spec->subprogram_spec.name : (String_Slice){0};
        Symbol *matching_generic = Symbol_Find (body_name);

        // This body completes a generic - store it and resolve it.
        // Push scope with generic formals so T, F etc. are visible.
        if (matching_generic and matching_generic->kind == SYMBOL_GENERIC) {
          matching_generic->generic_body = node;
          node->symbol = matching_generic;

          // Push scope for generic body resolution
          Symbol_Manager_Push_Scope (matching_generic);

          // Add generic formal parameters (types, objects, subprograms) to scope
          if (matching_generic->declaration and
            matching_generic->declaration->kind == NK_GENERIC_DECL) {
            Node_List *formals = &matching_generic->declaration->generic_decl.formals;
            for (uint32_t i = 0; i < formals->count; i++) {
              Syntax_Node *formal = formals->items[i];
              if (not formal) continue;

              // Map def_kind > Type_Kind so numeric formals                                        
              // are recognised by Type_Is_Numeric (RM 12.1.2):                                     
              // 0=PRIVATE, 1=LIMITED, 2=DISCRETE, 3=INTEGER,                                       
              // 4=FLOAT, 5=FIXED, 6=ARRAY, 7=ACCESS                                                
              //                                                                                    
              if (formal->kind == NK_GENERIC_TYPE_PARAM) {
                Type_Kind formal_kind = TYPE_PRIVATE;
                switch (formal->generic_type_param.def_kind) {
                  case 2:  formal_kind = TYPE_ENUMERATION; break;  // DISCRETE
                  case 3:  formal_kind = TYPE_INTEGER;     break;  // INTEGER
                  case 4:  formal_kind = TYPE_FLOAT;       break;  // FLOAT
                  case 5:  formal_kind = TYPE_FIXED;       break;  // FIXED
                  case 6:  formal_kind = TYPE_ARRAY;       break;  // ARRAY
                  case 7:  formal_kind = TYPE_ACCESS;      break;  // ACCESS
                  default: break;
                }
                Symbol *type_sym = Symbol_New (SYMBOL_TYPE,
                  formal->generic_type_param.name, formal->location);
                type_sym->type = Type_New (formal_kind,
                  formal->generic_type_param.name);
                Symbol_Add (type_sym);
                formal->symbol = type_sym;

                // RM 12.1.2: Process known discriminants for formal                                
                // private types.  Create discriminant symbols and add                              
                // them as components so P1.D resolves correctly.                                   
                //                                                                                  
                if (formal->generic_type_param.discriminants.count > 0) {
                  Type_Info *ftype = type_sym->type;
                  uint32_t disc_count = 0;
                  for (uint32_t di = 0; di < formal->generic_type_param.discriminants.count; di++) {
                    Syntax_Node *ds = formal->generic_type_param.discriminants.items[di];
                    if (ds and ds->kind == NK_DISCRIMINANT_SPEC)
                      disc_count += ds->discriminant.names.count;
                  }
                  if (disc_count > 0) {
                    Component_Info *comps = Arena_Allocate (
                      disc_count * sizeof (Component_Info));
                    uint32_t ci = 0;
                    uint32_t offset = 0;
                    for (uint32_t di = 0; di < formal->generic_type_param.discriminants.count; di++) {
                      Syntax_Node *ds = formal->generic_type_param.discriminants.items[di];
                      if (not ds or ds->kind != NK_DISCRIMINANT_SPEC) continue;
                      Type_Info *dty = sm->type_integer;
                      if (ds->discriminant.disc_type) {
                        Resolve_Expression (ds->discriminant.disc_type);
                        if (ds->discriminant.disc_type->type)
                          dty = ds->discriminant.disc_type->type;
                      }
                      uint32_t dsz = dty ? dty->size : 4;
                      for (uint32_t dj = 0; dj < ds->discriminant.names.count; dj++) {
                        Syntax_Node *dn = ds->discriminant.names.items[dj];
                        comps[ci].name = dn->string_val.text;
                        comps[ci].component_type = dty;
                        comps[ci].byte_offset = offset;
                        comps[ci].bit_offset = 0;
                        comps[ci].bit_size = dsz * 8;
                        comps[ci].is_discriminant = true;
                        comps[ci].variant_index = -1;
                        offset += dsz;
                        ci++;
                      }
                    }
                    ftype->record.components         = comps;
                    ftype->record.component_count    = disc_count;
                    ftype->record.has_discriminants  = true;
                    ftype->record.discriminant_count = disc_count;
                    ftype->size                      = offset;
                  }
                }

              // Create and add formal subprogram symbol if not exists
              } else if (formal->kind == NK_GENERIC_SUBPROGRAM_PARAM) {
                if (not formal->symbol) {
                  String_Slice name = formal->generic_subprog_param.name;
                  Symbol_Kind sk = formal->generic_subprog_param.is_function ?
                           SYMBOL_FUNCTION : SYMBOL_PROCEDURE;
                  Symbol *subprog_sym = Symbol_New (sk, name, formal->location);

                  // Set up parameter info
                  Node_List *fparams = &formal->generic_subprog_param.parameters;
                  uint32_t total_params = 0;
                  for (uint32_t j = 0; j < fparams->count; j++) {
                    Syntax_Node *ps = fparams->items[j];
                    if (ps and ps->kind == NK_PARAM_SPEC)
                      total_params += ps->param_spec.names.count;
                  }
                  subprog_sym->parameter_count = total_params;
                  if (total_params > 0) {
                    subprog_sym->parameters = Arena_Allocate (
                      total_params * sizeof (Parameter_Info));
                    uint32_t idx = 0;
                    for (uint32_t j = 0; j < fparams->count; j++) {
                      Syntax_Node *ps = fparams->items[j];
                      if (ps and ps->kind == NK_PARAM_SPEC) {
                        Type_Info *pt = NULL;
                        if (ps->param_spec.param_type) {
                          Resolve_Expression (ps->param_spec.param_type);
                          pt = ps->param_spec.param_type->type;
                        }
                        for (uint32_t k = 0; k < ps->param_spec.names.count; k++) {
                          Syntax_Node *pn = ps->param_spec.names.items[k];
                          subprog_sym->parameters[idx].name       = pn->string_val.text;
                          subprog_sym->parameters[idx].param_type = pt;
                          subprog_sym->parameters[idx].mode       =
                            (Parameter_Mode)ps->param_spec.mode;
                          idx++;
                        }
                      }
                    }
                  }

                  // Set return type for functions
                  if (formal->generic_subprog_param.is_function and
                    formal->generic_subprog_param.return_type) {
                    Resolve_Expression (formal->generic_subprog_param.return_type);
                    subprog_sym->type = formal->generic_subprog_param.return_type->type;
                    subprog_sym->return_type = subprog_sym->type;
                  }
                  formal->symbol = subprog_sym;
                }
                Symbol_Add (formal->symbol);

              // Add formal object(s) as variable(s) with their declared type.
              // e.g., "I1, I2 : INTEGER" creates two symbols of type INTEGER
              } else if (formal->kind == NK_GENERIC_OBJECT_PARAM) {
                Type_Info *obj_type = NULL;
                if (formal->generic_object_param.object_type) {
                  Resolve_Expression (formal->generic_object_param.object_type);
                  obj_type = formal->generic_object_param.object_type->type;
                }

                // Resolve default expression with formal's type (RM 12.4)
                if (obj_type and formal->generic_object_param.default_expr) {
                  Syntax_Node *def = formal->generic_object_param.default_expr;
                  def->type = obj_type;
                  Resolve_Expression (def);
                  if (not def->type or def->kind == NK_AGGREGATE)
                    def->type = obj_type;
                }
                for (uint32_t j = 0; j < formal->generic_object_param.names.count; j++) {
                  Syntax_Node *name_node = formal->generic_object_param.names.items[j];
                  Symbol *obj_sym = Symbol_New (SYMBOL_VARIABLE,
                    name_node->string_val.text, formal->location);
                  obj_sym->type = obj_type;
                  Symbol_Add (obj_sym);
                  name_node->symbol = obj_sym;
                }
              }
            }
          }

          // Add body parameters to scope
          if (spec) {
            Node_List *params = &spec->subprogram_spec.parameters;
            for (uint32_t i = 0; i < params->count; i++) {
              Syntax_Node *param = params->items[i];

              // Resolve parameter type (may reference generic formals)
              if (param and param->kind == NK_PARAM_SPEC) {
                if (param->param_spec.param_type)
                  Resolve_Expression (param->param_spec.param_type);

                // Add each parameter name as a symbol
                for (uint32_t j = 0; j < param->param_spec.names.count; j++) {
                  Syntax_Node *name = param->param_spec.names.items[j];
                  Symbol *param_sym = Symbol_New (SYMBOL_PARAMETER,
                    name->string_val.text, name->location);
                  if (param->param_spec.param_type)
                    param_sym->type = param->param_spec.param_type->type;
                  Symbol_Add (param_sym);
                  name->symbol = param_sym;
                }
              }
            }
          }

          // Resolve declarations and statements in the generic body
          Resolve_Declaration_List (&node->subprogram_body.declarations);
          Resolve_Statement_List (&node->subprogram_body.statements);

          // Resolve exception handlers
          for (uint32_t i = 0; i < node->subprogram_body.handlers.count; i++) {
            Resolve_Statement (node->subprogram_body.handlers.items[i]);
          }
          Symbol_Manager_Pop_Scope ();
          break;
        }

        // Resolve spec if present
        if (spec) {
          Resolve_Declaration (spec);
          node->symbol = spec->symbol;
        }

        // For stubs (IS SEPARATE), don't claim the symbol - the separate
        // subunit will provide the actual body and should claim it.
        if (node->subprogram_body.is_separate and node->symbol) {
          node->symbol->body_claimed = false;
        }

        // Push scope for body
        Symbol_Manager_Push_Scope (node->symbol);

        // Link the scope to the symbol for static link access
        if (node->symbol) {
          node->symbol->scope = sm->current_scope;
        }

        // Add parameters to scope and link to Parameter_Info
        if (spec) {
          Symbol *func_sym = node->symbol;
          uint32_t param_idx = 0;
          Node_List *params = &spec->subprogram_spec.parameters;
          for (uint32_t i = 0; i < params->count; i++) {
            Syntax_Node *param = params->items[i];

            // Resolve parameter type
            if (param->kind == NK_PARAM_SPEC) {
              if (param->param_spec.param_type) {
                Resolve_Expression (param->param_spec.param_type);
              }

              // Add each parameter name as a symbol
              for (uint32_t j = 0; j < param->param_spec.names.count; j++) {
                Syntax_Node *name = param->param_spec.names.items[j];
                Symbol *param_sym = Symbol_New (SYMBOL_PARAMETER,
                  name->string_val.text, name->location);
                if (param->param_spec.param_type) {
                  param_sym->type = param->param_spec.param_type->type;
                }
                Symbol_Add (param_sym);
                name->symbol = param_sym;

                // Link to Parameter_Info for code generation
                if (func_sym and param_idx < func_sym->parameter_count) {
                  func_sym->parameters[param_idx].param_sym = param_sym;
                }
                param_idx++;
              }
            }
          }
        }

        // Resolve declarations and statements
        Resolve_Declaration_List (&node->subprogram_body.declarations);

        // Freeze all types at end of declarative part (RM 13.14)
        Freeze_Declaration_List (&node->subprogram_body.declarations);
        Resolve_Statement_List (&node->subprogram_body.statements);

        // Resolve exception handlers
        for (uint32_t i = 0; i < node->subprogram_body.handlers.count; i++) {
          Resolve_Statement (node->subprogram_body.handlers.items[i]);
        }
        Symbol_Manager_Pop_Scope ();
      }
      break;

    // Task declaration creates a task type and optionally an object
    case NK_TASK_SPEC:
      {
        Symbol *type_sym = Symbol_New (SYMBOL_TYPE, node->task_spec.name, node->location);
        Type_Info *type = Type_New (TYPE_TASK, node->task_spec.name);
        type_sym->type = type;
        type->defining_symbol = type_sym;
        type_sym->declaration = node;
        Symbol_Add (type_sym);
        node->symbol = type_sym;

        // If not a task TYPE, also create an object of that type
        if (not node->task_spec.is_type) {
          Symbol *obj_sym = Symbol_New (SYMBOL_VARIABLE, node->task_spec.name, node->location);
          obj_sym->type = type;
          obj_sym->declaration = node;

          // Add the object symbol - it will shadow the type for normal lookups
          Symbol_Add (obj_sym);
        }

        // Push scope for task entries
        Symbol_Manager_Push_Scope (type_sym);

        // Resolve and add entry declarations
        uint32_t entry_idx_counter = 0;  // Counter for unique entry indices
        for (uint32_t i = 0; i < node->task_spec.entries.count; i++) {
          Syntax_Node *entry = node->task_spec.entries.items[i];
          if (entry->kind == NK_ENTRY_DECL) {
            Symbol *entry_sym = Symbol_New (SYMBOL_ENTRY,
              entry->entry_decl.name, entry->location);
            entry_sym->declaration = entry;
            entry_sym->parent = type_sym;
            entry_sym->entry_index = entry_idx_counter++;  // Assign unique entry index

            // Count entry parameters
            uint32_t param_count = 0;
            for (uint32_t j = 0; j < entry->entry_decl.parameters.count; j++) {
              Syntax_Node *ps = entry->entry_decl.parameters.items[j];
              if (ps->kind == NK_PARAM_SPEC) {
                param_count += ps->param_spec.names.count;
              }
            }
            entry_sym->parameter_count = param_count;
            if (param_count > 0) {
              entry_sym->parameters = Arena_Allocate (param_count * sizeof (Parameter_Info));
              uint32_t pi = 0;
              for (uint32_t j = 0; j < entry->entry_decl.parameters.count; j++) {
                Syntax_Node *ps = entry->entry_decl.parameters.items[j];
                if (ps->kind == NK_PARAM_SPEC) {
                  if (ps->param_spec.param_type) {
                    Resolve_Expression (ps->param_spec.param_type);
                  }

                  // Resolve default expression if present
                  if (ps->param_spec.default_expr) {
                    if (ps->param_spec.default_expr->kind == NK_AGGREGATE and
                      not ps->param_spec.default_expr->type and
                      ps->param_spec.param_type)
                      ps->param_spec.default_expr->type = ps->param_spec.param_type->type;
                    Resolve_Expression (ps->param_spec.default_expr);
                  }
                  for (uint32_t k = 0; k < ps->param_spec.names.count; k++) {
                    entry_sym->parameters[pi].name = ps->param_spec.names.items[k]->string_val.text;
                    entry_sym->parameters[pi].param_type = ps->param_spec.param_type ?
                      ps->param_spec.param_type->type : NULL;
                    entry_sym->parameters[pi].mode = (Parameter_Mode)ps->param_spec.mode;
                    entry_sym->parameters[pi].default_value = ps->param_spec.default_expr;
                    pi++;
                  }
                }
              }
            }
            Symbol_Add (entry_sym);
            entry->symbol = entry_sym;

            // Add entry to type's exported symbols
            if (type_sym->exported_count < 100) {
              if (not type_sym->exported) {
                type_sym->exported = Arena_Allocate (100 * sizeof (Symbol*));
              }
              type_sym->exported[type_sym->exported_count++] = entry_sym;
            }
          }
        }
        Symbol_Manager_Pop_Scope ();
      }
      break;

    // Find the task spec symbol
    case NK_TASK_BODY:
      {
        Symbol *task_sym = Symbol_Find (node->task_body.name);

        // Create a symbol for the task body if spec wasn't found
        if (not task_sym) {
          task_sym = Symbol_New (SYMBOL_PROCEDURE, node->task_body.name, node->location);
          task_sym->declaration = node;
          Symbol_Add (task_sym);
        }
        node->symbol = task_sym;

        // Push scope for task body
        Symbol_Manager_Push_Scope (task_sym);

        // Import entries from task spec into body scope (RM 9.1)                                   
        // Entries declared in the task spec are visible inside the task body.                      
        // For single tasks, task_sym is SYMBOL_VARIABLE with TYPE_TASK;                            
        // the entries are on the type's defining_symbol.                                           
        //                                                                                          
        Symbol *type_sym = NULL;

        // task_sym is the type symbol directly (task type declaration)
        if (task_sym->kind == SYMBOL_TYPE and Type_Is_Task (task_sym->type)) {
          type_sym = task_sym;
        } else if (Type_Is_Task (task_sym->type) and
               task_sym->type->defining_symbol) {

          // task_sym is the variable; get the type's defining symbol
          type_sym = task_sym->type->defining_symbol;
        }
        if (type_sym and type_sym->exported) {
          for (uint32_t i = 0; i < type_sym->exported_count; i++) {
            Symbol *entry_sym = type_sym->exported[i];
            if (entry_sym) {
              Symbol_Add (entry_sym);
            }
          }
        }

        // Resolve declarations and statements
        Resolve_Declaration_List (&node->task_body.declarations);
        Resolve_Statement_List (&node->task_body.statements);

        // Resolve exception handlers
        for (uint32_t i = 0; i < node->task_body.handlers.count; i++) {
          Resolve_Statement (node->task_body.handlers.items[i]);
        }
        Symbol_Manager_Pop_Scope ();
      }
      break;
    case NK_PACKAGE_SPEC:
      {
        Symbol *sym = Symbol_New (SYMBOL_PACKAGE, node->package_spec.name, node->location);
        sym->declaration = node;  // Store declaration for body to find
        Symbol_Add (sym);
        node->symbol = sym;
        Symbol_Manager_Push_Scope (sym);
        sym->scope = sm->current_scope;  // Link scope to symbol for P.X lookups
        Resolve_Declaration_List (&node->package_spec.visible_decls);
        Resolve_Declaration_List (&node->package_spec.private_decls);

        // End of package spec freezes all declared entities (RM 13.14)
        Freeze_Declaration_List (&node->package_spec.visible_decls);
        Freeze_Declaration_List (&node->package_spec.private_decls);

        // Populate exports for nested/inline package access (e.g., INNER.ACC)
        Populate_Package_Exports (sym, node);
        Symbol_Manager_Pop_Scope ();
      }
      break;

    // Find or create package symbol for proper name mangling
    case NK_PACKAGE_BODY:
      {
        Symbol *pkg_sym = NULL;
        String_Slice pkg_name = node->package_body.name;

        // Mark this body as loaded BEFORE trying to load the spec.                                 
        // This prevents Load_Package_Spec from recursively loading                                 
        // the same body when we're compiling a .adb file directly.                                 
        //                                                                                          
        if (pkg_name.length > 0) {
          Mark_Body_Loaded (pkg_name);
          pkg_sym = Symbol_Find (pkg_name);

          // Try to load corresponding package spec first.
          // This ensures body uses same symbol IDs as spec
          if (not pkg_sym) {
            char *spec_src = Lookup_Path (pkg_name);
            if (spec_src) {
              Load_Package_Spec (pkg_name, spec_src);
              pkg_sym = Symbol_Find (pkg_name);
            }

            // Create package symbol if spec not found
            if (not pkg_sym) {
              pkg_sym = Symbol_New (SYMBOL_PACKAGE, pkg_name, node->location);
              Symbol_Add (pkg_sym);
            }
          }
          node->symbol = pkg_sym;
        }
        Symbol_Manager_Push_Scope (pkg_sym);

        // Set package symbol's scope for separate subunit resolution.
        // This allows SEPARATE (Parent) subunits to find stub symbols.
        if (pkg_sym) {
          pkg_sym->scope = sm->current_scope;
        }

        // Install visible and private declarations from package spec
        // into the body's scope (RM 7.1, 7.2)
        Syntax_Node *spec = NULL;

        // Handle generic packages: formals and unit are in the generic declaration
        // Store this body as the generic's body for later instantiation
        if (pkg_sym and pkg_sym->kind == SYMBOL_GENERIC) {
          pkg_sym->generic_body = node;

          // Install generic formal parameters first
          if (pkg_sym->declaration and
            pkg_sym->declaration->kind == NK_GENERIC_DECL) {
            Node_List *formals = &pkg_sym->declaration->generic_decl.formals;
            for (uint32_t i = 0; i < formals->count; i++) {
              Syntax_Node *formal = formals->items[i];
              if (formal->symbol) {
                Symbol_Add (formal->symbol);
              }

              // For generic type parameters, create/install a type symbol
              if (formal->kind == NK_GENERIC_TYPE_PARAM and not formal->symbol) {
                Symbol *type_sym = Symbol_New (SYMBOL_TYPE,
                  formal->generic_type_param.name, formal->location);

                // Map def_kind to appropriate Type_Kind:                                           
                // 0=PRIVATE, 1=LIMITED_PRIVATE, 2=DISCRETE, 3=INTEGER,                             
                // 4=FLOAT, 5=FIXED, 6=ARRAY, 7=ACCESS                                              
                //                                                                                  
                Type_Kind formal_kind = TYPE_PRIVATE;
                switch (formal->generic_type_param.def_kind) {
                  case 2:  formal_kind = TYPE_ENUMERATION; break;  // DISCRETE
                  case 3:  formal_kind = TYPE_INTEGER;     break;  // INTEGER range<>
                  case 4:  formal_kind = TYPE_FLOAT;       break;  // FLOAT digits<>
                  case 5:  formal_kind = TYPE_FIXED;       break;  // FIXED delta<>
                  case 6:  formal_kind = TYPE_ARRAY;       break;  // ARRAY
                  case 7:  formal_kind = TYPE_ACCESS;      break;  // ACCESS
                  default: formal_kind = TYPE_PRIVATE;     break;
                }
                Type_Info *type = Type_New (formal_kind, formal->generic_type_param.name);
                type_sym->type = type;
                formal->symbol = type_sym;
                Symbol_Add (type_sym);
              }

              // For generic object parameters, create/install object symbols
              if (formal->kind == NK_GENERIC_OBJECT_PARAM) {
                Type_Info *obj_type = NULL;
                if (formal->generic_object_param.object_type) {
                  Resolve_Expression (formal->generic_object_param.object_type);
                  obj_type = formal->generic_object_param.object_type->type;
                }

                // Resolve default expression with formal's type (RM 12.4)
                if (obj_type and formal->generic_object_param.default_expr) {
                  Syntax_Node *def = formal->generic_object_param.default_expr;
                  def->type = obj_type;
                  Resolve_Expression (def);
                  if (not def->type or def->kind == NK_AGGREGATE)
                    def->type = obj_type;
                }

                // Create symbols for each name
                for (uint32_t j = 0; j < formal->generic_object_param.names.count; j++) {
                  Syntax_Node *name_node = formal->generic_object_param.names.items[j];
                  if (name_node and name_node->kind == NK_IDENTIFIER) {
                    Symbol *obj_sym = Symbol_New (SYMBOL_CONSTANT,
                      name_node->string_val.text, name_node->location);
                    obj_sym->type = obj_type;
                    name_node->symbol = obj_sym;
                    Symbol_Add (obj_sym);
                  }
                }
              }

              // For generic subprogram parameters, create procedure/function symbols
              // so calls to formal subprograms resolve during generic body analysis
              if (formal->kind == NK_GENERIC_SUBPROGRAM_PARAM) {
                String_Slice name = formal->generic_subprog_param.name;
                Symbol_Kind sk = formal->generic_subprog_param.is_function ?
                         SYMBOL_FUNCTION : SYMBOL_PROCEDURE;
                Symbol *subprog_sym = Symbol_New (sk, name, formal->location);

                // Count total parameters
                Node_List *params = &formal->generic_subprog_param.parameters;
                uint32_t total_params = 0;
                for (uint32_t j = 0; j < params->count; j++) {
                  Syntax_Node *ps = params->items[j];
                  if (ps->kind == NK_PARAM_SPEC) {
                    total_params += ps->param_spec.names.count;
                  }
                }

                // Allocate and fill parameter info
                subprog_sym->parameter_count = total_params;
                if (total_params > 0) {
                  subprog_sym->parameters = Arena_Allocate (
                    total_params * sizeof (Parameter_Info));
                  uint32_t idx = 0;
                  for (uint32_t j = 0; j < params->count; j++) {
                    Syntax_Node *ps = params->items[j];

                    // Resolve param type - may reference earlier formal types
                    if (ps->kind == NK_PARAM_SPEC) {
                      Type_Info *pt = NULL;
                      if (ps->param_spec.param_type) {
                        Resolve_Expression (ps->param_spec.param_type);
                        pt = ps->param_spec.param_type->type;
                      }
                      for (uint32_t k = 0; k < ps->param_spec.names.count; k++) {
                        Syntax_Node *pn = ps->param_spec.names.items[k];
                        subprog_sym->parameters[idx].name = pn->string_val.text;
                        subprog_sym->parameters[idx].param_type = pt;
                        subprog_sym->parameters[idx].mode =
                          (Parameter_Mode)ps->param_spec.mode;
                        idx++;
                      }
                    }
                  }
                }

                // For functions, set return type
                if (formal->generic_subprog_param.is_function and
                  formal->generic_subprog_param.return_type) {
                  Resolve_Expression (formal->generic_subprog_param.return_type);
                  subprog_sym->type = formal->generic_subprog_param.return_type->type;
                }
                formal->symbol = subprog_sym;
                Symbol_Add (subprog_sym);
              }
            }
          }

          // Get the package spec from the generic unit
          spec = pkg_sym->generic_unit;

          // For generic package body, resolve the spec first if not done
          if (spec and spec->kind == NK_PACKAGE_SPEC) {
            Resolve_Declaration_List (&spec->package_spec.visible_decls);
            Resolve_Declaration_List (&spec->package_spec.private_decls);
          }
        } else if (pkg_sym and pkg_sym->declaration and
               pkg_sym->declaration->kind == NK_PACKAGE_SPEC) {
          spec = pkg_sym->declaration;
        }
        if (spec and spec->kind == NK_PACKAGE_SPEC) {
          Install_Declaration_Symbols (&spec->package_spec.visible_decls);
          Install_Declaration_Symbols (&spec->package_spec.private_decls);
        }
        Resolve_Declaration_List (&node->package_body.declarations);

        // Freeze all types at end of declarative part (RM 13.14)
        Freeze_Declaration_List (&node->package_body.declarations);
        Resolve_Statement_List (&node->package_body.statements);

        // Resolve exception handlers (RM 11.4) - local variables and
        // parameters remain visible in the handler body.
        for (uint32_t hi = 0; hi < node->package_body.handlers.count; hi++) {
          Syntax_Node *handler = node->package_body.handlers.items[hi];
          if (handler) {
            for (uint32_t ei = 0; ei < handler->handler.exceptions.count; ei++)
              Resolve_Expression (handler->handler.exceptions.items[ei]);
            Resolve_Statement_List (&handler->handler.statements);
          }
        }
        Symbol_Manager_Pop_Scope ();
      }
      break;
    case NK_USE_CLAUSE:

      // Make package contents directly visible (Ada 83 8.4)                                        
      // "A use clause achieves direct visibility of declarations                                   
      //  that appear in the visible parts of the named packages"                                   
      //                                                                                            
      for (uint32_t i = 0; i < node->use_clause.names.count; i++) {
        Syntax_Node *pkg_name_node = node->use_clause.names.items[i];
        Resolve_Expression (pkg_name_node);

        // Find the package symbol
        Symbol *pkg_sym = NULL;
        if (pkg_name_node->kind == NK_IDENTIFIER) {
          pkg_sym = Symbol_Find (pkg_name_node->string_val.text);
        } else if (pkg_name_node->symbol) {
          pkg_sym = pkg_name_node->symbol;
        }

        // Helper macro: add use-visible alias for a symbol
        if (pkg_sym and pkg_sym->kind == SYMBOL_PACKAGE) {
          #define ADD_USE_ALIAS(orig) do { \
            /* Check if alias already exists for this symbol */ \
            if (orig) { \
              uint32_t _hash = Symbol_Hash_Name ((orig)->name); \
              bool _already_aliased = false; \
              for (Symbol *_ex = sm->current_scope->buckets[_hash]; _ex; _ex = _ex->next_in_bucket) { \
                if (Slice_Equal_Ignore_Case (_ex->name, (orig)->name) and \
                  _ex->visibility == VIS_USE_VISIBLE and \
                  _ex->unique_id == (orig)->unique_id) { \
                  _already_aliased = true; \
                  break; \
                } \
              } \
              if (not _already_aliased) { \
                Symbol *alias = Symbol_New ((orig)->kind, (orig)->name, (orig)->location); \
                alias->type = (orig)->type; \
                alias->declaration = (orig)->declaration; \
                alias->visibility = VIS_USE_VISIBLE; \
                alias->parameter_count = (orig)->parameter_count; \
                alias->parameters = (orig)->parameters; \
                alias->return_type = (orig)->return_type; \
                alias->is_named_number = (orig)->is_named_number; \
                alias->generic_unit = (orig)->generic_unit; \
                alias->generic_body = (orig)->generic_body; \
                alias->generic_formals = (orig)->generic_formals; \
                Symbol_Add (alias); \
                alias->parent = (orig)->parent; \
                alias->unique_id = (orig)->unique_id; \
              } \
            } \
          } while (0)

          // For loaded packages with exported array, use it
          if (pkg_sym->exported_count > 0) {
            for (uint32_t j = 0; j < pkg_sym->exported_count; j++)
              ADD_USE_ALIAS (pkg_sym->exported[j]);
          }

          // For inline packages, iterate visible declarations
          else if (pkg_sym->declaration and pkg_sym->declaration->kind == NK_PACKAGE_SPEC) {
            Syntax_Node *pkg_decl = pkg_sym->declaration;
            for (uint32_t j = 0; j < pkg_decl->package_spec.visible_decls.count; j++) {
              Syntax_Node *decl = pkg_decl->package_spec.visible_decls.items[j];
              if (not decl) continue;
              if (decl->symbol) ADD_USE_ALIAS (decl->symbol);

              // Handle object_decl names (constants, variables)
              if (decl->kind == NK_OBJECT_DECL) {
                for (uint32_t k = 0; k < decl->object_decl.names.count; k++) {
                  Syntax_Node *nm = decl->object_decl.names.items[k];
                  if (nm and nm->symbol) ADD_USE_ALIAS (nm->symbol);
                }
              }

              // Handle enumeration literals
              if ((decl->kind == NK_TYPE_DECL or decl->kind == NK_SUBTYPE_DECL) and
                decl->type_decl.definition and
                decl->type_decl.definition->kind == NK_ENUMERATION_TYPE) {
                Node_List *lits = &decl->type_decl.definition->enum_type.literals;
                for (uint32_t k = 0; k < lits->count; k++) {
                  if (lits->items[k] and lits->items[k]->symbol)
                    ADD_USE_ALIAS (lits->items[k]->symbol);
                }
              }

              // Handle exception declarations
              if (decl->kind == NK_EXCEPTION_DECL) {
                for (uint32_t k = 0; k < decl->exception_decl.names.count; k++) {
                  Syntax_Node *nm = decl->exception_decl.names.items[k];
                  if (nm and nm->symbol) ADD_USE_ALIAS (nm->symbol);
                }
              }
            }
          }
          #undef ADD_USE_ALIAS
        }
      }
      break;
    case NK_PRAGMA:

      // Process pragma effects
      {
        String_Slice pragma_name = node->pragma_node.name;

        // pragma Inline (subprogram_name, ...)
        if (Slice_Equal_Ignore_Case (pragma_name, S("INLINE"))) {
          for (uint32_t i = 0; i < node->pragma_node.arguments.count; i++) {
            Syntax_Node *arg = node->pragma_node.arguments.items[i];
            Syntax_Node *name_node = (arg->kind == NK_ASSOCIATION) ?
                          arg->association.expression : arg;
            if (name_node and name_node->kind == NK_IDENTIFIER) {
              Symbol *sym = Symbol_Find (name_node->string_val.text);
              if (sym and (sym->kind == SYMBOL_PROCEDURE or
                    sym->kind == SYMBOL_FUNCTION)) {
                sym->is_inline = true;
              }
            }
          }
        }

        // pragma Pack (type_name)
        else if (Slice_Equal_Ignore_Case (pragma_name, S("PACK"))) {
          if (node->pragma_node.arguments.count > 0) {
            Syntax_Node *arg = node->pragma_node.arguments.items[0];
            Syntax_Node *name_node = (arg->kind == NK_ASSOCIATION) ?
                          arg->association.expression : arg;
            if (name_node and name_node->kind == NK_IDENTIFIER) {
              Symbol *sym = Symbol_Find (name_node->string_val.text);
              if (sym and sym->type) {
                sym->type->is_packed = true;
              }
            }
          }
        }

        // pragma Suppress (check_name) or pragma Suppress (check_name, entity_name)
        else if (Slice_Equal_Ignore_Case (pragma_name, S("SUPPRESS"))) {
          uint32_t check_bit = 0;
          if (node->pragma_node.arguments.count > 0) {
            Syntax_Node *arg = node->pragma_node.arguments.items[0];
            Syntax_Node *check_node = (arg->kind == NK_ASSOCIATION) ?
                           arg->association.expression : arg;
            if (check_node and check_node->kind == NK_IDENTIFIER) {
              String_Slice check = check_node->string_val.text;
              if (Slice_Equal_Ignore_Case (check, S("RANGE_CHECK")))
                check_bit = CHK_RANGE;
              else if (Slice_Equal_Ignore_Case (check, S("OVERFLOW_CHECK")))
                check_bit = CHK_OVERFLOW;
              else if (Slice_Equal_Ignore_Case (check, S("INDEX_CHECK")))
                check_bit = CHK_INDEX;
              else if (Slice_Equal_Ignore_Case (check, S("LENGTH_CHECK")))
                check_bit = CHK_LENGTH;
              else if (Slice_Equal_Ignore_Case (check, S("DIVISION_CHECK")))
                check_bit = CHK_DIVISION;
              else if (Slice_Equal_Ignore_Case (check, S("ACCESS_CHECK")))
                check_bit = CHK_ACCESS;
              else if (Slice_Equal_Ignore_Case (check, S("DISCRIMINANT_CHECK")))
                check_bit = CHK_DISCRIMINANT;
              else if (Slice_Equal_Ignore_Case (check, S("ELABORATION_CHECK")))
                check_bit = CHK_ELABORATION;
              else if (Slice_Equal_Ignore_Case (check, S("STORAGE_CHECK")))
                check_bit = CHK_STORAGE;
              else if (Slice_Equal_Ignore_Case (check, S("ALL_CHECKS")))
                check_bit = CHK_ALL;
            }
          }

          // Apply to specific entity or current scope
          if (node->pragma_node.arguments.count > 1) {
            Syntax_Node *arg = node->pragma_node.arguments.items[1];
            Syntax_Node *entity = (arg->kind == NK_ASSOCIATION) ?
                         arg->association.expression : arg;
            if (entity and entity->kind == NK_IDENTIFIER) {
              Symbol *sym = Symbol_Find (entity->string_val.text);
              if (sym) sym->suppressed_checks |= check_bit;
            }
          }

          // else: would apply to enclosing scope
        }

        // pragma Import (Convention, Entity, External_Name, Link_Name)
        else if (Slice_Equal_Ignore_Case (pragma_name, S("IMPORT"))) {
          if (node->pragma_node.arguments.count >= 2) {

            // Get convention
            Syntax_Node *conv_arg = node->pragma_node.arguments.items[0];
            Syntax_Node *conv_node = (conv_arg->kind == NK_ASSOCIATION) ?
                          conv_arg->association.expression : conv_arg;

            // Get entity
            Syntax_Node *ent_arg = node->pragma_node.arguments.items[1];
            Syntax_Node *ent_node = (ent_arg->kind == NK_ASSOCIATION) ?
                         ent_arg->association.expression : ent_arg;
            if (ent_node and ent_node->kind == NK_IDENTIFIER) {
              Symbol *sym = Symbol_Find (ent_node->string_val.text);
              if (sym) {
                sym->is_imported = true;

                // Set convention
                if (conv_node and conv_node->kind == NK_IDENTIFIER) {
                  String_Slice conv = conv_node->string_val.text;
                  if (Slice_Equal_Ignore_Case (conv, S("C")))
                    sym->convention = CONVENTION_C;
                  else if (Slice_Equal_Ignore_Case (conv, S("STDCALL")))
                    sym->convention = CONVENTION_STDCALL;
                  else if (Slice_Equal_Ignore_Case (conv, S("INTRINSIC")))
                    sym->convention = CONVENTION_INTRINSIC;
                }

                // Get external name if provided
                if (node->pragma_node.arguments.count >= 3) {
                  Syntax_Node *name_arg = node->pragma_node.arguments.items[2];
                  Syntax_Node *name_node = (name_arg->kind == NK_ASSOCIATION) ?
                                name_arg->association.expression : name_arg;
                  if (name_node and name_node->kind == NK_STRING) {
                    sym->external_name = name_node->string_val.text;
                  }
                }
              }
            }
          }
        }

        // pragma Export (Convention, Entity, External_Name)
        else if (Slice_Equal_Ignore_Case (pragma_name, S("EXPORT"))) {
          if (node->pragma_node.arguments.count >= 2) {
            Syntax_Node *conv_arg = node->pragma_node.arguments.items[0];
            Syntax_Node *conv_node = (conv_arg->kind == NK_ASSOCIATION) ?
                          conv_arg->association.expression : conv_arg;
            Syntax_Node *ent_arg = node->pragma_node.arguments.items[1];
            Syntax_Node *ent_node = (ent_arg->kind == NK_ASSOCIATION) ?
                         ent_arg->association.expression : ent_arg;
            if (ent_node and ent_node->kind == NK_IDENTIFIER) {
              Symbol *sym = Symbol_Find (ent_node->string_val.text);
              if (sym) {
                sym->is_exported = true;
                if (conv_node and conv_node->kind == NK_IDENTIFIER) {
                  String_Slice conv = conv_node->string_val.text;
                  if (Slice_Equal_Ignore_Case (conv, S("C")))
                    sym->convention = CONVENTION_C;
                }
                if (node->pragma_node.arguments.count >= 3) {
                  Syntax_Node *name_arg = node->pragma_node.arguments.items[2];
                  Syntax_Node *name_node = (name_arg->kind == NK_ASSOCIATION) ?
                                name_arg->association.expression : name_arg;
                  if (name_node and name_node->kind == NK_STRING) {
                    sym->external_name = name_node->string_val.text;
                  }
                }
              }
            }
          }
        }

        // pragma Unreferenced (name, ...)
        else if (Slice_Equal_Ignore_Case (pragma_name, S("UNREFERENCED"))) {
          for (uint32_t i = 0; i < node->pragma_node.arguments.count; i++) {
            Syntax_Node *arg = node->pragma_node.arguments.items[i];
            Syntax_Node *name_node = (arg->kind == NK_ASSOCIATION) ?
                          arg->association.expression : arg;
            if (name_node and name_node->kind == NK_IDENTIFIER) {
              Symbol *sym = Symbol_Find (name_node->string_val.text);
              if (sym) sym->is_unreferenced = true;
            }
          }
        }

        // pragma Convention (convention, entity)
        else if (Slice_Equal_Ignore_Case (pragma_name, S("CONVENTION"))) {
          if (node->pragma_node.arguments.count >= 2) {
            Syntax_Node *conv_arg = node->pragma_node.arguments.items[0];
            Syntax_Node *conv_node = (conv_arg->kind == NK_ASSOCIATION) ?
                          conv_arg->association.expression : conv_arg;
            Syntax_Node *ent_arg = node->pragma_node.arguments.items[1];
            Syntax_Node *ent_node = (ent_arg->kind == NK_ASSOCIATION) ?
                         ent_arg->association.expression : ent_arg;
            if (ent_node and ent_node->kind == NK_IDENTIFIER) {
              Symbol *sym = Symbol_Find (ent_node->string_val.text);
              if (sym and conv_node and conv_node->kind == NK_IDENTIFIER) {
                String_Slice conv = conv_node->string_val.text;
                if (Slice_Equal_Ignore_Case (conv, S("C")))
                  sym->convention = CONVENTION_C;
                else if (Slice_Equal_Ignore_Case (conv, S("STDCALL")))
                  sym->convention = CONVENTION_STDCALL;
              }
            }
          }
        }

        // pragma Pure, pragma Preelaborate - informational only, accepted and ignored              
        // pragma Elaborate, pragma Elaborate_All - handled at link time                            
        // pragma Restrictions - accepted and ignored                                               
        //                                                                                          
      }
      break;
    case NK_EXCEPTION_DECL:

      // Exception declaration: E : exception;
      for (uint32_t i = 0; i < node->exception_decl.names.count; i++) {
        Syntax_Node *name_node = node->exception_decl.names.items[i];
        if (name_node and name_node->kind == NK_IDENTIFIER) {
          Symbol *sym = Symbol_New (SYMBOL_EXCEPTION,
                       name_node->string_val.text,
                       name_node->location);
          Symbol_Add (sym);
          name_node->symbol = sym;

          // Add to global exception list for codegen
          if (Exception_Symbol_Count < 256) {
            Exception_Symbols[Exception_Symbol_Count++] = sym;
          }
        }
      }
      break;
    case NK_GENERIC_DECL:

      // Generic declaration: generic ... procedure/function/package spec
      {
        Syntax_Node *unit = node->generic_decl.unit;
        if (not unit) break;

        // Get name from the unit
        String_Slice name = {0};
        if (unit->kind == NK_PROCEDURE_SPEC or unit->kind == NK_FUNCTION_SPEC) {
          name = unit->subprogram_spec.name;
        } else if (unit->kind == NK_PACKAGE_SPEC) {
          name = unit->package_spec.name;
        }

        // Create generic symbol
        Symbol *sym = Symbol_New (SYMBOL_GENERIC, name, node->location);
        sym->declaration = node;
        sym->generic_unit = unit;

        // Store formals list for later
        if (node->generic_decl.formals.count > 0) {
          sym->generic_formals = node->generic_decl.formals.items[0];
        }
        Symbol_Add (sym);
        node->symbol = sym;

        // DON'T resolve the unit here - it contains generic formals
        // that need to be substituted during instantiation
      }
      break;

    // Find the generic template
    case NK_GENERIC_INST:

      // Generic instantiation: procedure/function X is new GENERIC_NAME (actuals)
      {
        Syntax_Node *gen_name = node->generic_inst.generic_name;
        if (not gen_name) {
          Report_Error (node->location, "expected a generic unit name");
          break;
        }

        // Resolve the generic name to find template
        Resolve_Expression (gen_name);
        Symbol *template = NULL;
        if (gen_name->kind == NK_IDENTIFIER) {
          template = Symbol_Find (gen_name->string_val.text);

        // Handle qualified names like P.PP (NK_SELECTED resolved to a symbol)
        } else if (gen_name->symbol) {
          template = gen_name->symbol;
        }
        if (not template or template->kind != SYMBOL_GENERIC) {
          Report_Error (node->location, "expected a generic unit name");
          break;
        }

        // Create instance symbol
        Symbol_Kind inst_kind;
        if (node->generic_inst.unit_kind == TK_FUNCTION)
          inst_kind = SYMBOL_FUNCTION;
        else if (node->generic_inst.unit_kind == TK_PACKAGE)
          inst_kind = SYMBOL_PACKAGE;
        else
          inst_kind = SYMBOL_PROCEDURE;
        Symbol *inst_sym = Symbol_New (inst_kind,
                        node->generic_inst.instance_name,
                        node->location);
        inst_sym->declaration = node;
        inst_sym->generic_template = template;

        // Process generic actuals and build mapping
        Node_List *formals = &template->declaration->generic_decl.formals;
        Node_List *actuals = &node->generic_inst.actuals;

        // Count total actual slots: multi-name object formals                                      
        // like "F, L : E" consume one actual per name.                                             
        // Type and subprogram formals consume one actual each.                                     
        //                                                                                          
        uint32_t total_actual_slots = 0;
        for (uint32_t i = 0; i < formals->count; i++) {
          Syntax_Node *formal = formals->items[i];
          if (formal->kind == NK_GENERIC_OBJECT_PARAM)
            total_actual_slots += formal->generic_object_param.names.count;
          else
            total_actual_slots++;
        }

        // Use the larger of computed slots and actual params provided
        if (total_actual_slots < actuals->count)
          total_actual_slots = actuals->count;
        inst_sym->generic_actual_count = total_actual_slots;
        if (total_actual_slots > 0) {
          inst_sym->generic_actuals = Arena_Allocate (
            total_actual_slots * sizeof (*inst_sym->generic_actuals));

          // First pass: resolve type formals (using actual_idx to track                            
          // position in the actuals list, since object formals with multiple                       
          // names consume multiple actual slots).                                                  
          //                                                                                        
          uint32_t actual_idx = 0;
          for (uint32_t i = 0; i < formals->count; i++) {
            Syntax_Node *formal = formals->items[i];
            if (formal->kind == NK_GENERIC_TYPE_PARAM) {
              Syntax_Node *actual = (actual_idx < actuals->count) ?
                actuals->items[actual_idx] : NULL;
              if (actual_idx < total_actual_slots) {
                inst_sym->generic_actuals[actual_idx].formal_name =
                  formal->generic_type_param.name;
                if (actual) {
                  Syntax_Node *type_node = actual;
                  if (actual->kind == NK_ASSOCIATION)
                    type_node = actual->association.expression;
                  Resolve_Expression (type_node);
                  inst_sym->generic_actuals[actual_idx].actual_type = type_node->type;
                }
              }
              actual_idx++;
            } else if (formal->kind == NK_GENERIC_OBJECT_PARAM) {
              actual_idx += formal->generic_object_param.names.count;
            } else if (formal->kind == NK_GENERIC_SUBPROGRAM_PARAM) {
              actual_idx++;
            }
          }

          // Second pass: resolve object formals with substituted types.
          // Each name in a multi-name object formal gets its own actual slot.
          actual_idx = 0;
          for (uint32_t i = 0; i < formals->count; i++) {
            Syntax_Node *formal = formals->items[i];

            // Look up the formal's type, substituting any type formals
            if (formal->kind == NK_GENERIC_OBJECT_PARAM) {
              Syntax_Node *obj_type_node = formal->generic_object_param.object_type;
              Type_Info *obj_type = NULL;

              // Check generic type actuals (substituted type formals)
              if (obj_type_node and obj_type_node->kind == NK_IDENTIFIER) {
                for (uint32_t k = 0; k < total_actual_slots; k++) {
                  if (inst_sym->generic_actuals[k].actual_type and
                    Slice_Equal_Ignore_Case (obj_type_node->string_val.text,
                          inst_sym->generic_actuals[k].formal_name)) {
                    obj_type = inst_sym->generic_actuals[k].actual_type;
                    break;
                  }
                }

                // Concrete types: resolve from scope (RM 12.3)
                if (not obj_type) {
                  Symbol *ts = Symbol_Find (obj_type_node->string_val.text);
                  if (not ts or (ts->kind != SYMBOL_TYPE and ts->kind != SYMBOL_SUBTYPE)
                    or not ts->type) {
                    Report_Error (obj_type_node->location,
                      "cannot resolve generic formal object type '%.*s'",
                      obj_type_node->string_val.text.length,
                      obj_type_node->string_val.text.data);
                  } else {
                    obj_type = ts->type;
                  }
                }
              }

              // Map each name to its own actual
              for (uint32_t j = 0; j < formal->generic_object_param.names.count; j++) {
                Syntax_Node *actual = (actual_idx < actuals->count) ?
                  actuals->items[actual_idx] : NULL;
                if (actual and actual_idx < total_actual_slots) {
                  Syntax_Node *expr = actual;
                  if (actual->kind == NK_ASSOCIATION)
                    expr = actual->association.expression;
                  if (obj_type) {
                    expr->type = obj_type;
                    Resolve_Expression (expr);
                    if (not expr->type or expr->kind == NK_AGGREGATE)
                      expr->type = obj_type;

                    // Character literal actuals for enum formal types need                         
                    // explicit resolution to find their position (RM 3.5.1).                       
                    // Resolve_Expression sets type to CHARACTER, losing the                        
                    // enum context; restore and resolve as enum literal.                           
                    //                                                                              
                    if (expr->kind == NK_CHARACTER)
                      Resolve_Char_As_Enum (expr, obj_type);
                  } else {
                    Resolve_Expression (expr);
                  }
                  inst_sym->generic_actuals[actual_idx].actual_expr = expr;
                  Syntax_Node *fname = formal->generic_object_param.names.items[j];
                  if (fname)
                    inst_sym->generic_actuals[actual_idx].formal_name =
                      fname->string_val.text;
                }
                actual_idx++;
              }
            } else if (formal->kind == NK_GENERIC_TYPE_PARAM) {
              actual_idx++;
            } else if (formal->kind == NK_GENERIC_SUBPROGRAM_PARAM) {
              actual_idx++;
            }
          }

          // Third pass: resolve subprogram formals to actual subprograms
          actual_idx = 0;
          for (uint32_t i = 0; i < formals->count; i++) {
            Syntax_Node *formal = formals->items[i];
            if (formal->kind == NK_GENERIC_OBJECT_PARAM) {
              actual_idx += formal->generic_object_param.names.count;
              continue;
            }
            Syntax_Node *actual = (actual_idx < actuals->count) ?
              actuals->items[actual_idx] : NULL;
            if (formal->kind == NK_GENERIC_SUBPROGRAM_PARAM) {
              if (actual_idx < total_actual_slots)
                inst_sym->generic_actuals[actual_idx].formal_name =
                  formal->generic_subprog_param.name;

              // Resolve actual subprogram name
              if (actual) {
                Syntax_Node *name_node = actual;
                if (actual->kind == NK_ASSOCIATION) {
                  name_node = actual->association.expression;
                }
                if (not name_node) continue;

                // Handle operator designators: "&" is the "&" operator
                // Look up operator by name
                if (name_node->kind == NK_STRING) {
                  if (name_node->string_val.text.data) {
                    Symbol *op = Symbol_Find (name_node->string_val.text);

                    // Check type profile matches: the formal's type parameter                      
                    // must match the found operator's return type. Otherwise                       
                    // fall through to built-in operator path.                                      
                    //                                                                              
                    bool type_matches = false;

                    // Find the actual type for the first type formal
                    if (op and (op->kind == SYMBOL_FUNCTION or op->kind == SYMBOL_PROCEDURE)) {
                      Type_Info *expected_type = NULL;
                      for (uint32_t k = 0; k < total_actual_slots; k++) {
                        if (inst_sym->generic_actuals[k].actual_type) {
                          expected_type = inst_sym->generic_actuals[k].actual_type;
                          break;
                        }
                      }
                      if (expected_type and op->return_type) {
                        type_matches = (expected_type == op->return_type or
                          Type_Base (expected_type) == Type_Base (op->return_type));
                      } else {
                        type_matches = true;  // no type info to check
                      }
                    }
                    if (op and type_matches and
                      (op->kind == SYMBOL_FUNCTION or op->kind == SYMBOL_PROCEDURE)) {
                      name_node->symbol = op;
                      inst_sym->generic_actuals[actual_idx].actual_subprogram = op;

                    // Check for built-in operators
                    } else {

                      // String lexer strips quotes: "&" becomes just &
                      String_Slice s = name_node->string_val.text;
                      if (s.length == 1 and s.data[0] == '&')
                        inst_sym->generic_actuals[actual_idx].builtin_operator = TK_AMPERSAND;
                      else if (s.length == 1 and s.data[0] == '+')
                        inst_sym->generic_actuals[actual_idx].builtin_operator = TK_PLUS;
                      else if (s.length == 1 and s.data[0] == '-')
                        inst_sym->generic_actuals[actual_idx].builtin_operator = TK_MINUS;
                      else if (s.length == 1 and s.data[0] == '*')
                        inst_sym->generic_actuals[actual_idx].builtin_operator = TK_STAR;
                      else if (s.length == 1 and s.data[0] == '/')
                        inst_sym->generic_actuals[actual_idx].builtin_operator = TK_SLASH;
                    }
                  }
                }

                // Handle character literals as enum literals: '&' from ADD_OPS
                else if (name_node->kind == NK_CHARACTER) {

                  // Character literal as enum literal (parameterless function)
                  if (name_node->string_val.text.data) {
                    Symbol *lit = Symbol_Find (name_node->string_val.text);
                    if (lit and lit->kind == SYMBOL_LITERAL) {
                      name_node->symbol = lit;
                      inst_sym->generic_actuals[actual_idx].actual_subprogram = lit;
                    }
                  }
                }
                else {

                  // Look up the actual subprogram symbol
                  Resolve_Expression (name_node);
                  if (name_node->symbol) {
                    inst_sym->generic_actuals[actual_idx].actual_subprogram = name_node->symbol;
                  }
                }

              // IS <> default: resolve to a visible subprogram
              // with the same name as the formal (RM 12.3(6))
              } else if (formal->generic_subprog_param.default_box) {
                Symbol *default_sym = Symbol_Find (
                  formal->generic_subprog_param.name);
                if (default_sym and (default_sym->kind == SYMBOL_FUNCTION or
                           default_sym->kind == SYMBOL_PROCEDURE)) {
                  if (actual_idx < total_actual_slots)
                    inst_sym->generic_actuals[actual_idx].actual_subprogram = default_sym;
                }
              }
              actual_idx++;
            } else if (formal->kind == NK_GENERIC_TYPE_PARAM) {
              actual_idx++;
            }
          }
        }

        // Copy parameter info from template unit
        Syntax_Node *unit = template->generic_unit;
        if (unit and (unit->kind == NK_FUNCTION_SPEC or unit->kind == NK_PROCEDURE_SPEC)) {
          Node_List *params = &unit->subprogram_spec.parameters;
          uint32_t total_params = 0;
          for (uint32_t i = 0; i < params->count; i++) {
            Syntax_Node *ps = params->items[i];
            if (ps->kind == NK_PARAM_SPEC) {
              total_params += ps->param_spec.names.count;
            }
          }
          inst_sym->parameter_count = total_params;
          if (total_params > 0) {
            inst_sym->parameters = Arena_Allocate (total_params * sizeof (Parameter_Info));
            uint32_t idx = 0;
            for (uint32_t i = 0; i < params->count; i++) {
              Syntax_Node *ps = params->items[i];

              // Resolve param type and substitute formals with actuals
              if (ps->kind == NK_PARAM_SPEC) {
                Type_Info *param_type = NULL;
                if (ps->param_spec.param_type) {
                  Syntax_Node *pt = ps->param_spec.param_type;

                  // First check if formal type parameter > substitute
                  if (pt->kind == NK_IDENTIFIER) {
                    for (uint32_t k = 0; k < inst_sym->generic_actual_count; k++) {
                      if (Slice_Equal_Ignore_Case (pt->string_val.text,
                              inst_sym->generic_actuals[k].formal_name)) {
                        param_type = inst_sym->generic_actuals[k].actual_type;
                        break;
                      }
                    }

                    // If not a formal, resolve actual type (e.g. STRING)
                    if (not param_type) {
                      Symbol *type_sym = Symbol_Find (pt->string_val.text);
                      if (type_sym and type_sym->type)
                        param_type = type_sym->type;
                    }
                  }
                }
                for (uint32_t j = 0; j < ps->param_spec.names.count; j++) {
                  Syntax_Node *name = ps->param_spec.names.items[j];
                  inst_sym->parameters[idx].name = name->string_val.text;
                  inst_sym->parameters[idx].param_type = param_type;
                  inst_sym->parameters[idx].mode = (Parameter_Mode)ps->param_spec.mode;
                  idx++;
                }
              }
            }
          }

          // Handle return type for functions
          if (unit->kind == NK_FUNCTION_SPEC and unit->subprogram_spec.return_type) {
            Syntax_Node *rt = unit->subprogram_spec.return_type;

            // Check if return type is a formal type parameter
            if (rt->kind == NK_IDENTIFIER) {
              for (uint32_t k = 0; k < inst_sym->generic_actual_count; k++) {
                if (Slice_Equal_Ignore_Case (rt->string_val.text,
                        inst_sym->generic_actuals[k].formal_name)) {
                  inst_sym->return_type = inst_sym->generic_actuals[k].actual_type;
                  break;
                }
              }

              // If not a formal, look up the actual type (e.g. INTEGER)
              if (not inst_sym->return_type) {
                Symbol *type_sym = Symbol_Find (rt->string_val.text);
                if (type_sym and type_sym->type)
                  inst_sym->return_type = type_sym->type;
              }
            }
          }
        }

        // For package instantiations, copy and instantiate exported symbols
        // Look for exports from the template's generic_unit (package spec)
        if (node->generic_inst.unit_kind == TK_PACKAGE) {
          Syntax_Node *pkg_spec = template->generic_unit;

          // Count visible declarations
          if (pkg_spec and pkg_spec->kind == NK_PACKAGE_SPEC) {
            uint32_t export_count = 0;
            for (uint32_t i = 0; i < pkg_spec->package_spec.visible_decls.count; i++) {
              Syntax_Node *decl = pkg_spec->package_spec.visible_decls.items[i];
              if (not decl) continue;
              if (decl->kind == NK_TYPE_DECL or decl->kind == NK_SUBTYPE_DECL) {
                export_count++;

                // Count enum literals too
                if (decl->type_decl.definition and
                  decl->type_decl.definition->kind == NK_ENUMERATION_TYPE) {
                  export_count += decl->type_decl.definition->enum_type.literals.count;
                }
              }
              else if (decl->kind == NK_PROCEDURE_SPEC or decl->kind == NK_FUNCTION_SPEC)
                export_count++;
              else if (decl->kind == NK_EXCEPTION_DECL)
                export_count += decl->exception_decl.names.count;
              else if (decl->kind == NK_OBJECT_DECL)
                export_count += decl->object_decl.names.count;
              else if (decl->kind == NK_TASK_SPEC)
                export_count++;
            }
            if (export_count > 0) {
              inst_sym->exported = Arena_Allocate (export_count * sizeof (Symbol*));
              inst_sym->exported_count = 0;

              // Helper: substitute generic formals with actuals in a type
              #define SUBSTITUTE_TYPE(ty) do { \
                if (ty and ty->name.data) { \
                  for (uint32_t k = 0; k < inst_sym->generic_actual_count; k++) { \
                    if (Slice_Equal_Ignore_Case (ty->name, \
                            inst_sym->generic_actuals[k].formal_name)) { \
                      ty = inst_sym->generic_actuals[k].actual_type; \
                      break; \
                    } \
                  } \
                } \
              } while (0)
              for (uint32_t i = 0; i < pkg_spec->package_spec.visible_decls.count; i++) {
                Syntax_Node *decl = pkg_spec->package_spec.visible_decls.items[i];
                if (not decl) continue;

                // Create type symbol for the instance
                if (decl->kind == NK_TYPE_DECL or decl->kind == NK_SUBTYPE_DECL) {
                  String_Slice name = decl->type_decl.name;
                  Symbol *exp = Symbol_New (SYMBOL_TYPE, name, decl->location);

                  // Use the symbol's type (has the named type) if available
                  if (decl->symbol and decl->symbol->type)
                    exp->type = decl->symbol->type;
                  else if (decl->type)
                    exp->type = decl->type;
                  else if (decl->type_decl.definition)
                    exp->type = decl->type_decl.definition->type;

                  // For subtypes, substitute formal type with actual
                  if (not exp->type and decl->kind == NK_SUBTYPE_DECL and
                    decl->type_decl.definition) {
                    Syntax_Node *def = decl->type_decl.definition;
                    String_Slice def_name = {0};

                    // Handle plain identifier: SUBTYPE X IS GEN
                    if (def->kind == NK_IDENTIFIER) {
                      def_name = def->string_val.text;
                    }

                    // Handle constrained: SUBTYPE X IS GEN (4)
                    else if (def->kind == NK_APPLY and def->apply.prefix and
                         def->apply.prefix->kind == NK_IDENTIFIER) {
                      def_name = def->apply.prefix->string_val.text;
                    }

                    // Handle subtype indication: SUBTYPE X IS GEN RANGE ...
                    else if (def->kind == NK_SUBTYPE_INDICATION and
                         def->subtype_ind.subtype_mark and
                         def->subtype_ind.subtype_mark->kind == NK_IDENTIFIER) {
                      def_name = def->subtype_ind.subtype_mark->string_val.text;
                    }
                    if (def_name.data) {
                      for (uint32_t k = 0; k < inst_sym->generic_actual_count; k++) {
                        Syntax_Node *formal_k = formals->items[k];
                        if (formal_k->kind == NK_GENERIC_TYPE_PARAM and
                          Slice_Equal_Ignore_Case (def_name, formal_k->generic_type_param.name)) {
                          exp->type = inst_sym->generic_actuals[k].actual_type;
                          break;
                        }
                      }
                    }
                  }

                  // Create Type_Info for unresolved enum/range types
                  if (not exp->type and decl->type_decl.definition) {
                    Syntax_Node *def = decl->type_decl.definition;
                    if (def->kind == NK_ENUMERATION_TYPE) {
                      Type_Info *ti = Type_New (TYPE_ENUMERATION, name);
                      ti->size = 4;
                      uint32_t lit_count = def->enum_type.literals.count;
                      ti->enumeration.literal_count = lit_count;
                      if (lit_count > 0) {
                        ti->enumeration.literals = Arena_Allocate (
                          lit_count * sizeof (String_Slice));
                        for (uint32_t j = 0; j < lit_count; j++) {
                          Syntax_Node *lit = def->enum_type.literals.items[j];
                          if (lit and lit->kind == NK_IDENTIFIER)
                            ti->enumeration.literals[j] = lit->string_val.text;
                        }
                      }
                      exp->type = ti;
                      def->type = ti;
                      decl->type = ti;

                    // Integer type with range constraint
                    } else if (def->kind == NK_RANGE) {
                      Type_Info *ti = Type_New (TYPE_INTEGER, name);
                      ti->size = 4;
                      exp->type = ti;
                      def->type = ti;
                      decl->type = ti;
                    }
                  }
                  exp->declaration = decl;
                  exp->parent = inst_sym;
                  inst_sym->exported[inst_sym->exported_count++] = exp;

                  // Also export enum literals
                  if (decl->type_decl.definition and
                    decl->type_decl.definition->kind == NK_ENUMERATION_TYPE) {
                    Node_List *lits = &decl->type_decl.definition->enum_type.literals;
                    for (uint32_t j = 0; j < lits->count; j++) {
                      Syntax_Node *lit = lits->items[j];
                      if (lit and lit->kind == NK_IDENTIFIER) {
                        Symbol *lit_sym = Symbol_New (SYMBOL_LITERAL,
                          lit->string_val.text, lit->location);
                        lit_sym->type = exp->type;
                        lit_sym->parent = inst_sym;
                        inst_sym->exported[inst_sym->exported_count++] = lit_sym;
                      }
                    }
                  }
                }
                else if (decl->kind == NK_PROCEDURE_SPEC or
                     decl->kind == NK_FUNCTION_SPEC) {

                  // Create subprogram symbol with instantiated types.
                  // Assign unique_id immediately so homographs get distinct IDs.
                  String_Slice name = decl->subprogram_spec.name;
                  Symbol_Kind sk = (decl->kind == NK_PROCEDURE_SPEC) ?
                           SYMBOL_PROCEDURE : SYMBOL_FUNCTION;
                  Symbol *exp = Symbol_New (sk, name, decl->location);
                  exp->unique_id = sm->next_unique_id++;
                  exp->declaration = decl;
                  exp->parent = inst_sym;
                  exp->generic_template = template;

                  // Copy and substitute parameter types
                  Node_List *params = &decl->subprogram_spec.parameters;
                  uint32_t total_params = 0;
                  for (uint32_t j = 0; j < params->count; j++) {
                    Syntax_Node *ps = params->items[j];
                    if (ps->kind == NK_PARAM_SPEC)
                      total_params += ps->param_spec.names.count;
                  }
                  exp->parameter_count = total_params;
                  if (total_params > 0) {
                    exp->parameters = Arena_Allocate (
                      total_params * sizeof (Parameter_Info));
                    uint32_t idx = 0;
                    for (uint32_t j = 0; j < params->count; j++) {
                      Syntax_Node *ps = params->items[j];
                      if (ps->kind == NK_PARAM_SPEC) {
                        Type_Info *ptype = ps->param_spec.param_type ?
                          ps->param_spec.param_type->type : NULL;

                        // If type not resolved, look up by name
                        if (not ptype and ps->param_spec.param_type and
                          ps->param_spec.param_type->kind == NK_IDENTIFIER) {
                          String_Slice pt_name = ps->param_spec.param_type->string_val.text;

                          // First try instance's own exports
                          for (uint32_t k = 0; k < inst_sym->exported_count; k++) {
                            Symbol *es = inst_sym->exported[k];
                            if (es and es->kind == SYMBOL_TYPE and
                              Slice_Equal_Ignore_Case (es->name, pt_name)) {
                              ptype = es->type;
                              break;
                            }
                          }

                          // Then try global symbol table
                          if (not ptype) {
                            Symbol *tsym = Symbol_Find (pt_name);
                            if (tsym) ptype = tsym->type;
                          }
                        }
                        SUBSTITUTE_TYPE (ptype);
                        for (uint32_t m = 0; m < ps->param_spec.names.count; m++) {
                          Syntax_Node *pname = ps->param_spec.names.items[m];
                          exp->parameters[idx].name = pname->string_val.text;
                          exp->parameters[idx].param_type = ptype;
                          exp->parameters[idx].mode =
                            (Parameter_Mode)ps->param_spec.mode;

                          // Create param symbol if not present (specs don't have symbols)
                          if (not pname->symbol) {
                            Symbol *ps_sym = Symbol_New (SYMBOL_PARAMETER,
                              pname->string_val.text, pname->location);
                            ps_sym->type = ptype;
                            ps_sym->unique_id = sm->next_unique_id++;
                            pname->symbol = ps_sym;
                          }
                          exp->parameters[idx].param_sym = pname->symbol;
                          idx++;
                        }
                      }
                    }
                  }

                  // Handle return type
                  if (decl->kind == NK_FUNCTION_SPEC and
                    decl->subprogram_spec.return_type) {
                    Type_Info *rtype = decl->subprogram_spec.return_type->type;

                    // If return type not resolved, look up by name
                    if (not rtype and decl->subprogram_spec.return_type->kind == NK_IDENTIFIER) {
                      String_Slice rt_name = decl->subprogram_spec.return_type->string_val.text;

                      // First try instance's own exports (for types like FILE_MODE)
                      for (uint32_t k = 0; k < inst_sym->exported_count; k++) {
                        Symbol *es = inst_sym->exported[k];
                        if (es and es->kind == SYMBOL_TYPE and
                          Slice_Equal_Ignore_Case (es->name, rt_name)) {
                          rtype = es->type;
                          break;
                        }
                      }

                      // Then try global symbol table
                      if (not rtype) {
                        Symbol *tsym = Symbol_Find (rt_name);
                        if (tsym) rtype = tsym->type;
                      }
                    }
                    SUBSTITUTE_TYPE (rtype);
                    exp->return_type = rtype;
                  }
                  inst_sym->exported[inst_sym->exported_count++] = exp;
                }
                else if (decl->kind == NK_EXCEPTION_DECL) {
                  for (uint32_t j = 0; j < decl->exception_decl.names.count; j++) {
                    Syntax_Node *nm = decl->exception_decl.names.items[j];
                    Symbol *exp = Symbol_New (SYMBOL_EXCEPTION,
                      nm->string_val.text, nm->location);
                    exp->parent = inst_sym;
                    inst_sym->exported[inst_sym->exported_count++] = exp;
                  }
                }
                else if (decl->kind == NK_OBJECT_DECL) {

                  // Export object declarations (including renames)
                  Type_Info *obj_type = NULL;
                  if (decl->object_decl.object_type) {
                    obj_type = decl->object_decl.object_type->type;

                    // If type not resolved (generic template), look up by name
                    if (not obj_type and decl->object_decl.object_type->kind == NK_IDENTIFIER) {
                      String_Slice type_name = decl->object_decl.object_type->string_val.text;
                      Symbol *type_sym = Symbol_Find (type_name);
                      if (type_sym and type_sym->type)
                        obj_type = type_sym->type;
                    }

                    // Substitute generic formals with actuals
                    SUBSTITUTE_TYPE (obj_type);
                  }
                  for (uint32_t j = 0; j < decl->object_decl.names.count; j++) {
                    Syntax_Node *nm = decl->object_decl.names.items[j];
                    Symbol_Kind sk = decl->object_decl.is_constant ?
                      SYMBOL_CONSTANT : SYMBOL_VARIABLE;
                    Symbol *exp = Symbol_New (sk, nm->string_val.text, nm->location);
                    exp->type = obj_type;
                    exp->parent = inst_sym;

                    // For renames, store the renamed object expression
                    if (decl->object_decl.is_rename and decl->object_decl.init) {
                      exp->renamed_object = decl->object_decl.init;
                    }
                    inst_sym->exported[inst_sym->exported_count++] = exp;
                  }
                }
                else if (decl->kind == NK_TASK_SPEC) {

                  // Export task declarations from generic instantiation.                           
                  // Create a type symbol with TYPE_TASK and populate its                           
                  // entries from the task spec's entry declarations.                               
                  //                                                                                
                  String_Slice name = decl->task_spec.name;
                  Symbol *exp = Symbol_New (SYMBOL_TYPE, name, decl->location);
                  Type_Info *type = Type_New (TYPE_TASK, name);
                  exp->type = type;
                  type->defining_symbol = exp;
                  exp->declaration = decl;
                  exp->parent = inst_sym;

                  // Add entries to the task type's exported list
                  for (uint32_t j = 0; j < decl->task_spec.entries.count; j++) {
                    Syntax_Node *entry = decl->task_spec.entries.items[j];
                    if (entry->kind == NK_ENTRY_DECL) {
                      Symbol *entry_sym = Symbol_New (SYMBOL_ENTRY,
                        entry->entry_decl.name, entry->location);
                      entry_sym->declaration = entry;
                      entry_sym->parent = exp;
                      entry_sym->entry_index = j;
                      if (not exp->exported) {
                        exp->exported = Arena_Allocate (100 * sizeof (Symbol*));
                      }
                      exp->exported[exp->exported_count++] = entry_sym;
                    }
                  }
                  inst_sym->exported[inst_sym->exported_count++] = exp;

                  // For single task declarations (not task types),
                  // also create an object variable (RM 9.1)
                  if (not decl->task_spec.is_type) {
                    Symbol *obj = Symbol_New (SYMBOL_VARIABLE,
                      name, decl->location);
                    obj->type = type;
                    obj->declaration = decl;
                    obj->parent = inst_sym;
                    inst_sym->exported[inst_sym->exported_count++] = obj;
                  }
                }
              }
              #undef SUBSTITUTE_TYPE
            }
          }
        }
        Symbol_Add (inst_sym);
        node->symbol = inst_sym;

        // For package instantiations, expand the generic now
        // so we have expanded_spec/expanded_body for codegen
        if (node->generic_inst.unit_kind == TK_PACKAGE) {
          Expand_Generic_Package (inst_sym);
        }
      }
      break;

    // Resolve entity name
    case NK_REPRESENTATION_CLAUSE:

      // Representation clause: FOR T'SIZE USE 32; or FOR T USE RECORD ...
      {
        if (node->rep_clause.entity_name) {
          Resolve_Expression (node->rep_clause.entity_name);

          // Get target type or symbol
          Symbol *target_sym = NULL;
          if (node->rep_clause.entity_name->kind == NK_IDENTIFIER) {
            target_sym = Symbol_Find (node->rep_clause.entity_name->string_val.text);
          } else if (node->rep_clause.entity_name->symbol) {
            target_sym = node->rep_clause.entity_name->symbol;
          }
          Type_Info *target_type = target_sym ? target_sym->type : NULL;

          // Process attribute clauses: FOR T'SIZE USE 32;
          if (node->rep_clause.attribute.data and target_type) {
            String_Slice attr = node->rep_clause.attribute;
            if (node->rep_clause.expression) {
              Resolve_Expression (node->rep_clause.expression);

              // Evaluate constant expression (handles T'SIZE/2 etc.)
              double dval = Eval_Const_Numeric (node->rep_clause.expression);
              int64_t value = isnan (dval) ? 0 : (int64_t)dval;

              // Apply representation
              // Size in bits - store exact and convert to bytes
              if (Slice_Equal_Ignore_Case (attr, S("SIZE"))) {
                target_type->specified_bit_size = (uint32_t)value;
                target_type->size = (uint32_t)((value + 7) / 8);
              } else if (Slice_Equal_Ignore_Case (attr, S("ALIGNMENT"))) {
                target_type->alignment = (uint32_t)value;
              } else if (Slice_Equal_Ignore_Case (attr, S("STORAGE_SIZE"))) {
                target_type->storage_size = value;

              // For fixed-point: set small value
              } else if (Slice_Equal_Ignore_Case (attr, S("SMALL"))) {
                if (Type_Is_Fixed_Point (target_type) and
                  node->rep_clause.expression->kind == NK_REAL) {
                  target_type->fixed.small =
                    node->rep_clause.expression->real_lit.value;
                }
              }
            }
          }

          // Process record representation: FOR T USE RECORD ...
          if (node->rep_clause.is_record_rep and
            Type_Is_Record (target_type)) {

            // Process alignment clause
            if (node->rep_clause.expression) {
              Resolve_Expression (node->rep_clause.expression);
              if (node->rep_clause.expression->kind == NK_INTEGER) {
                target_type->alignment =
                  (uint32_t)node->rep_clause.expression->integer_lit.value;
              }
            }

            // Process component clauses (record layout)
            // Each clause: component_name AT byte_position [RANGE bits];
            for (uint32_t i = 0; i < node->rep_clause.component_clauses.count; i++) {
              Syntax_Node *cc = node->rep_clause.component_clauses.items[i];
              if (cc->kind == NK_ASSOCIATION and cc->association.choices.count > 0) {
                Syntax_Node *name_node = cc->association.choices.items[0];
                if (name_node and name_node->kind == NK_IDENTIFIER) {
                  String_Slice comp_name = name_node->string_val.text;

                  // Find matching component in record type
                  for (uint32_t j = 0; j < target_type->record.component_count; j++) {
                    Component_Info *comp = &target_type->record.components[j];

                    // Get byte offset from expression
                    if (Slice_Equal_Ignore_Case (comp->name, comp_name)) {
                      Syntax_Node *pos_expr = cc->association.expression;
                      if (pos_expr and pos_expr->kind == NK_INTEGER) {
                        comp->byte_offset = (uint32_t)pos_expr->integer_lit.value;
                      }
                      break;
                    }
                  }
                }
              }
            }
          }

          // Process enumeration representation: FOR T USE (val0, val1, ...);
          if (node->rep_clause.is_enum_rep and
            Type_Is_Enumeration (target_type)) {

            // Store internal representation values for enum literals
            // The component_clauses list contains the values in order
            uint32_t val_count = node->rep_clause.component_clauses.count;

            // Allocate array for representation values
            if (val_count > 0 and val_count <= target_type->enumeration.literal_count) {
              target_type->enumeration.rep_values =
                Arena_Allocate (val_count * sizeof (int64_t));
              for (uint32_t i = 0; i < val_count; i++) {
                Syntax_Node *val_node = node->rep_clause.component_clauses.items[i];
                if (val_node and val_node->kind == NK_INTEGER) {
                  target_type->enumeration.rep_values[i] =
                    val_node->integer_lit.value;

                // Default to position value
                } else {
                  target_type->enumeration.rep_values[i] = (int64_t)i;
                }
              }
            }
          }
        }
      }
      break;
    default:
      break;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §12.4 Compilation Unit Resolution                                                                
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Resolve_Compilation_Unit (Syntax_Node *node) {
  if (not node) return;

  // Load WITH'd packages from include paths
  if (node->compilation_unit.context) {
    Syntax_Node *ctx = node->compilation_unit.context;
    for (uint32_t i = 0; i < ctx->context.with_clauses.count; i++) {
      Syntax_Node *with_node = ctx->context.with_clauses.items[i];

      // WITH clause contains a list of package names
      for (uint32_t j = 0; j < with_node->use_clause.names.count; j++) {
        Syntax_Node *pkg_name = with_node->use_clause.names.items[j];
        if (pkg_name->kind == NK_IDENTIFIER) {
          char *pkg_src = Lookup_Path (pkg_name->string_val.text);
          if (pkg_src) {
            Load_Package_Spec (pkg_name->string_val.text, pkg_src);
          }

          // Resolve the identifier to the package symbol
          Resolve_Expression (pkg_name);
        }
      }
    }

    // Resolve USE clauses (make package contents visible)
    for (uint32_t i = 0; i < ctx->context.use_clauses.count; i++) {
      Resolve_Declaration (ctx->context.use_clauses.items[i]);
    }
  }

  // Handle separate subunits (SEPARATE (parent) ...)
  Symbol *parent_sym = NULL;
  if (node->compilation_unit.separate_parent) {
    Syntax_Node *parent = node->compilation_unit.separate_parent;
    String_Slice parent_name = {0};
    if (parent->kind == NK_IDENTIFIER) {
      parent_name = parent->string_val.text;

    // Handle qualified names like A.B - use the selector (rightmost part)
    } else if (parent->kind == NK_SELECTED) {
      parent_name = parent->selected.selector;
    }
    if (parent_name.length > 0) {
      parent_sym = Symbol_Find (parent_name);

      // Push the parent's actual scope so we can find the stub symbol.
      // This reuses the scope where the stub was declared.
      if (parent_sym and parent_sym->scope) {
        Symbol_Manager_Push_Existing_Scope (parent_sym->scope);
      }
    }
  }

  // Resolve main unit
  if (node->compilation_unit.unit) {
    Resolve_Declaration (node->compilation_unit.unit);
  }

  // Pop parent scope if we pushed it
  if (parent_sym and parent_sym->scope) {
    Symbol_Manager_Pop_Scope ();
  }
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §13. LLVM IR CODE GENERATION                                                                     
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1 Code Generator State                                                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Code_Generator *cg;
void Code_Generator_Init (FILE *output) {
  cg                        = Arena_Allocate (sizeof (Code_Generator));
  cg->output                = output;
  cg->temp_id               = 1;
  cg->label_id              = 1;
  cg->global_id             = 1;
  cg->string_id             = 1;
  cg->deferred_count        = 0;
  cg->string_const_capacity = 4096;
  cg->string_const_buffer   = Arena_Allocate (cg->string_const_capacity);
  cg->string_const_size     = 0;
}

// Record the actual LLVM type for a generated temp register
void Temp_Set_Type (uint32_t temp_id, const char *llvm_type) {
  uint32_t idx              = temp_id % TEMP_TYPE_CAPACITY;
  cg->temp_type_keys[idx]  = temp_id;
  cg->temp_types[idx]      = llvm_type;
}

// Get the actual LLVM type for a temp register (returns NULL if unknown).
// Verifies that the stored key matches to avoid hash collisions.
const char *Temp_Get_Type (uint32_t temp_id) {
  uint32_t idx = temp_id % TEMP_TYPE_CAPACITY;
  if (cg->temp_type_keys[idx] == temp_id)
    return cg->temp_types[idx];
  return NULL;
}

// Mark/query a temp as a fat-pointer alloca (separate from its LLVM type)
void Temp_Mark_Fat_Alloca (uint32_t temp_id) {
  cg->temp_is_fat_alloca[temp_id % TEMP_TYPE_CAPACITY] = 1;
}
bool Temp_Is_Fat_Alloca (uint32_t temp_id) {
  uint32_t idx = temp_id % TEMP_TYPE_CAPACITY;
  return cg->temp_is_fat_alloca[idx] and cg->temp_type_keys[idx] == temp_id;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1.1 Type-Derived Codegen Helpers                                                             
//                                                                                                  
// Standard never hardcodes type widths - it derives them from the front-end                        
// type system (see GL_Type, Bound_Sub_GT).  These helpers do the same: they                        
// query sm to derive LLVM IR types from the Ada type system at the point                           
// of emission, so every codegen site gets its types from a single source.                          
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Derive the LLVM bound type for STRING from the type system.                                      
// Follows: STRING > index_type (POSITIVE) > Type_To_Llvm > Llvm_Int_Type.                          
// This is Standard's Bound_Sub_GT path.                                                            
//                                                                                                  
const char *String_Bound_Type (void) {
  return Array_Bound_Llvm_Type (sm->type_string);
}

// Derive the LLVM bounds struct type for STRING: "{ bt, bt }".
const char *String_Bounds_Struct (void) {
  return Bounds_Type_For (String_Bound_Type ());
}

// Derive the LLVM allocation size for STRING bounds struct.
int String_Bounds_Alloc (void) {
  return Bounds_Alloc_Size (String_Bound_Type ());
}

// Derive the LLVM type for Standard.INTEGER (the universal arithmetic type).                       
// In Standard, integer computation uses the GL_Type of Standard.Integer.                           
// This replaces hardcoded "i64" in expression evaluation paths.                                    
//                                                                                                  
const char *Integer_Arith_Type (void) {
  return Type_To_Llvm (sm->type_integer);
}

// Emit to string constant buffer instead of main output
void Emit_String_Const (const char *format, ...) {
  va_list args;
  va_start (args, format);
  char temp[1024];
  int written = vsnprintf (temp, sizeof (temp), format, args);
  va_end (args);
  if (written < 0) return;  // Format error
  size_t length = (size_t) written;

  // Expand buffer if needed
  while (cg->string_const_size + length + 1 > cg->string_const_capacity) {
    size_t new_cap = cg->string_const_capacity * 2;
    char  *new_buf = Arena_Allocate (new_cap);
    memcpy (new_buf, cg->string_const_buffer, cg->string_const_size);
    cg->string_const_buffer   = new_buf;
    cg->string_const_capacity = new_cap;
  }
  memcpy (cg->string_const_buffer + cg->string_const_size, temp, length);
  cg->string_const_size += length;
  cg->string_const_buffer[cg->string_const_size] = '\0';
}

// Emit a single char to string constant buffer
void Emit_String_Const_Char (char ch) {
  if (cg->string_const_size + 2 > cg->string_const_capacity) {
    size_t new_cap = cg->string_const_capacity * 2;
    char  *new_buf = Arena_Allocate (new_cap);
    memcpy (new_buf, cg->string_const_buffer, cg->string_const_size);
    cg->string_const_buffer   = new_buf;
    cg->string_const_capacity = new_cap;
  }
  cg->string_const_buffer[cg->string_const_size++] = ch;
  cg->string_const_buffer[cg->string_const_size] = '\0';
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.2 IR Emission Helpers                                                                        
// ─────────────────────────────────────────────────────────────────────────────────────────────────

uint32_t Emit_Temp (void) {
  return cg->temp_id++;
}
uint32_t Emit_Label (void) {
  return cg->label_id++;
}
void Emit (const char *format, ...) {
  va_list args;
  va_start (args, format);
  vfprintf (cg->output, format, args);
  va_end (args);
}

// Emit a source location comment for debugging: ; [filename:line:col]
void Emit_Location (Source_Location location) {
  if (location.line > 0) {
    const char *filename = location.filename ? location.filename : "?";

    // Extract just the basename for brevity
    const char *base = filename;
    for (const char *scan = filename; *scan; scan++) {
      if (*scan == '/' or *scan == '\\') base = scan + 1;
    }
    Emit ("  ; [%s:%u:%u]\n", base, location.line, location.column);
  }
}

// Emit a label, inserting a fallthrough branch if the prior block is open
void Emit_Label_Here (uint32_t label) {
  if (not cg->block_terminated) {
    Emit ("  br label %%L%u\n", label);
  }
  Emit ("L%u:\n", label);
  cg->block_terminated = false;
}

// Emit a branch only if block is not already terminated
void Emit_Branch_If_Needed (uint32_t label) {
  if (not cg->block_terminated) {
    Emit ("  br label %%L%u\n", label);
    cg->block_terminated = true;
  }
}

// Emit a floating-point constant at the correct precision for the given                            
// LLVM float type.  For "float" emits bitcast i32; for "double" emits                              
// fadd double 0.0, 0x<hex>.  Replaces scattered hardcoded "fadd double" emissions.                 
//                                                                                                  
void Emit_Float_Constant (uint32_t    result,
                                 const char *float_type,
                                 double      value,
                                 const char *comment) {

  // LLVM represents float hex as double-precision hex.
  // Convert to float, then back to double to get exact representation.
  if (strcmp (float_type, "float") == 0) {
    float    float_val  = (float) value;
    double   double_val = (double) float_val;
    uint64_t bits;
    memcpy (&bits, &double_val, sizeof (bits));
    Emit ("  %%t%u = fadd float 0.0, 0x%016llX  ; %s\n",
          result, (unsigned long long) bits, comment);
  } else {
    uint64_t bits;
    memcpy (&bits, &value, sizeof (bits));
    Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; %s\n",
          result, (unsigned long long) bits, comment);
  }
}

// Check if symbol is package-level (global storage with @ prefix in LLVM).                         
// A symbol is global only if it's at package level AND no ancestor is a subprogram.                
// This handles nested packages inside subprogram bodies correctly.                                 
//                                                                                                  
bool Symbol_Is_Global (Symbol *sym) {
  if (not sym->parent) return true;  // Top-level

  // Walk up parent chain - if any ancestor is a subprogram, symbol is local.                       
  // Also check for SYMBOL_GENERIC whose generic_unit is a subprogram spec                          
  // (not a package spec) - symbols inside generic subprogram bodies must be                        
  // treated as local when instantiated.  Generic package variables remain                          
  // global because they're allocated as package-level storage.                                     
  //                                                                                                
  Symbol *ancestor = sym->parent;
  while (ancestor) {
    if (ancestor->kind == SYMBOL_FUNCTION or ancestor->kind == SYMBOL_PROCEDURE) {
      return false;  // Inside a subprogram - use local (%) prefix
    }
    if (ancestor->kind == SYMBOL_GENERIC and ancestor->generic_unit and
      ancestor->generic_unit->kind != NK_PACKAGE_SPEC) {
      return false;  // Inside a generic subprogram - local
    }
    ancestor = ancestor->parent;
  }
  return true;  // No subprogram ancestor - use global (@) prefix
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Symbol_Mangle_Name - Unified LLVM-compatible name mangling                                       
//                                                                                                  
// Produces consistent mangled names for:                                                           
//   1. Code generation (LLVM IR identifiers)                                                       
//   2. ALI files (linkage names)                                                                   
//   3. Cross-compilation linking                                                                   
//                                                                                                  
// Format: parent__name[_SN] (all lowercase, special chars as _XX)                                  
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Internal recursive helper                                                                        
// Mangle a name slice into buf at pos: lowercase, escape non-alphanum as _XX.                      
// GNAT convention: operator symbols become _op_, special chars become _HH.                         
//                                                                                                  
size_t Mangle_Slice_Into (char *buf, size_t pos, size_t max, String_Slice name) {
  for (uint32_t i = 0; i < name.length and pos < max - 4; i++) {
    char ch = name.data[i];
    if (ch >= 'A' and ch <= 'Z') ch = ch - 'A' + 'a';
    if ((ch >= 'a' and ch <= 'z') or (ch >= '0' and ch <= '9') or ch == '_') {
      buf[pos++] = ch;
    } else if (ch == '"') {
      if (pos + 4 < max) { buf[pos++] = '_'; buf[pos++] = 'o'; buf[pos++] = 'p'; buf[pos++] = '_'; }
    } else {
      if (pos + 3 < max) {
        buf[pos++] = '_';
        buf[pos++] = "0123456789abcdef"[(ch >> 4) & 0xF];
        buf[pos++] = "0123456789abcdef"[ch & 0xF];
      }
    }
  }
  return pos;
}
size_t Mangle_Into_Buffer (char *buf, size_t pos, size_t max, Symbol *sym) {
  if (not sym) return pos;
  if (sym->parent and sym->parent->kind == SYMBOL_PACKAGE) {
    pos = Mangle_Into_Buffer (buf, pos, max, sym->parent);
    if (pos + 2 < max) { buf[pos++] = '_'; buf[pos++] = '_'; }
  }
  return Mangle_Slice_Into (buf, pos, max, sym->name);
}

// Get mangled name for a symbol (thread-safe via rotation)
String_Slice Symbol_Mangle_Name (Symbol *sym) {
  static char bufs[4][512];
  static int buf_idx = 0;
  char *buf = bufs[buf_idx++ & 3];
  size_t pos = 0;

  // Build mangled name from parent chain
  pos = Mangle_Into_Buffer (buf, 0, 510, sym);

  // Add unique_id suffix for local/overloaded symbols
  if (sym and (not Symbol_Is_Global (sym) or sym->is_overloaded)) {
    pos += snprintf (buf + pos, 512 - pos, "_s%u", sym->unique_id);
  }
  buf[pos] = '\0';
  return (String_Slice){buf, pos};
}

// Get mangled name for parent+name pair (for ALI generation before symbols exist).
// Reuses Mangle_Slice_Into for consistent GNAT-style mangling.
String_Slice Mangle_Qualified_Name (String_Slice parent, String_Slice name) {
  static char bufs[4][512];
  static int buf_idx = 0;
  char *buf = bufs[buf_idx++ & 3];
  size_t pos = Mangle_Slice_Into (buf, 0, 510, parent);
  if (parent.length > 0) { buf[pos++] = '_'; buf[pos++] = '_'; }
  pos = Mangle_Slice_Into (buf, pos, 510, name);
  buf[pos] = '\0';
  return Slice_Duplicate ((String_Slice){buf, pos});
}

// Find the instance counterpart for a template symbol in the current function's                    
// scope.  When generating a generic instance body, the AST references template                     
// symbols (simple names, frame_offset 0) but the function scope contains the                       
// properly-scoped instance symbols (qualified names, correct frame_offset).                        
// Returns NULL if no match is found.                                                               
//                                                                                                  
Symbol *Find_Instance_Local (const Symbol *template_sym) {
  if (not cg->current_instance or not cg->current_function or
    not cg->current_function->scope)
    return NULL;

  // Determine which instance (package) to match against
  Symbol *inst = cg->current_instance;
  if ((inst->kind == SYMBOL_FUNCTION or inst->kind == SYMBOL_PROCEDURE) and
    inst->parent and inst->parent->kind == SYMBOL_PACKAGE and
    inst->parent->generic_template) {
    inst = inst->parent;
  }
  Scope *scope = cg->current_function->scope;
  for (uint32_t i = 0; i < scope->symbol_count; i++) {
    Symbol *candidate = scope->symbols[i];
    if (not candidate or candidate->kind != template_sym->kind) continue;
    if (candidate->name.length != template_sym->name.length) continue;
    if (candidate->parent != inst) continue;
    bool match = true;
    for (uint32_t j = 0; j < candidate->name.length; j++) {
      char left_ch  = candidate->name.data[j];
      char right_ch = template_sym->name.data[j];
      if (left_ch  >= 'A' and left_ch  <= 'Z') left_ch  = left_ch  - 'A' + 'a';
      if (right_ch >= 'A' and right_ch <= 'Z') right_ch = right_ch - 'A' + 'a';
      if (left_ch != right_ch) { match = false; break; }
    }
    if (match) return candidate;
  }

  // Also check frame_vars (for symbols from child scopes)
  for (uint32_t i = 0; i < scope->frame_var_count; i++) {
    Symbol *candidate = scope->frame_vars[i];
    if (not candidate or candidate->kind != template_sym->kind) continue;
    if (candidate->name.length != template_sym->name.length) continue;
    if (candidate->parent != inst) continue;
    bool match = true;
    for (uint32_t j = 0; j < candidate->name.length; j++) {
      char left_ch  = candidate->name.data[j];
      char right_ch = template_sym->name.data[j];
      if (left_ch  >= 'A' and left_ch  <= 'Z') left_ch  = left_ch  - 'A' + 'a';
      if (right_ch >= 'A' and right_ch <= 'Z') right_ch = right_ch - 'A' + 'a';
      if (left_ch != right_ch) { match = false; break; }
    }
    if (match) return candidate;
  }
  return NULL;
}

// Emit symbol name for LLVM identifier (uses unified Symbol_Mangle_Name)
void Emit_Symbol_Name (Symbol *sym) {
  if (not sym) {
    Emit ("unknown");
    return;
  }

  // For imported symbols with external name, use that directly
  if (sym->is_imported and sym->external_name.length > 0) {
    String_Slice name = sym->external_name;

    // Strip quotes if present (from pragma Import)
    if (name.length >= 2 and name.data[0] == '"' and name.data[name.length - 1] == '"') {
      name.data++;
      name.length -= 2;
    }
    for (uint32_t i = 0; i < name.length; i++) {
      fputc (name.data[i], cg->output);
    }
    return;
  }

  // For generic instance code generation: prefix global OBJECT symbols                             
  // (variables) from the template body with the instance name to avoid                             
  // collisions when multiple instances of the same generic are created.                            
  // Do NOT prefix exceptions, types, or subprograms.                                               
  // Find the package instance - current_instance might be a procedure                              
  // within a package, in which case use the procedure's parent package                             
  //                                                                                                
  if (cg->current_instance and sym->kind == SYMBOL_VARIABLE and Symbol_Is_Global (sym)) {
    Symbol *inst = cg->current_instance;
    if ((inst->kind == SYMBOL_FUNCTION or inst->kind == SYMBOL_PROCEDURE) and
      inst->parent and inst->parent->kind == SYMBOL_PACKAGE and
      inst->parent->generic_template) {
      inst = inst->parent;  // Use owning package instance for globals
    }
    Symbol *tmpl = inst->generic_template;
    if (tmpl and sym->parent and sym->parent != inst and
      (sym->parent == tmpl or sym->parent->kind == SYMBOL_GENERIC)) {

      // Emit instance name prefix
      String_Slice inst_mangled = Symbol_Mangle_Name (inst);
      for (uint32_t i = 0; i < inst_mangled.length; i++) {
        fputc (inst_mangled.data[i], cg->output);
      }
      Emit ("__");

      // Emit just the symbol name (not parent chain)
      for (uint32_t i = 0; i < sym->name.length; i++) {
        char ch = sym->name.data[i];
        if (ch >= 'A' and ch <= 'Z') ch = ch - 'A' + 'a';
        fputc (ch, cg->output);
      }
      return;
    }
  }

  // For generic instance bodies: local variables/constants from the template                       
  // need instance-qualified names to avoid collisions between instances.                           
  // Find the instance counterpart in the function scope and use its name.                          
  //                                                                                                
  if (cg->current_instance and
    (sym->kind == SYMBOL_VARIABLE or sym->kind == SYMBOL_CONSTANT or
     sym->kind == SYMBOL_PARAMETER) and
    not sym->is_named_number) {
    Symbol *inst_sym = Find_Instance_Local (sym);
    if (inst_sym) {
      String_Slice mangled = Symbol_Mangle_Name (inst_sym);
      for (uint32_t i = 0; i < mangled.length; i++) {
        fputc (mangled.data[i], cg->output);
      }
      return;
    }
  }

  // Use unified mangling (lowercase, parent__name format, _sN suffix)
  String_Slice mangled = Symbol_Mangle_Name (sym);
  for (uint32_t i = 0; i < mangled.length; i++) {
    fputc (mangled.data[i], cg->output);
  }
}

// Emit symbol reference with appropriate prefix (@ for global, % for local)
void Emit_Symbol_Ref (Symbol *sym) {
  Emit (Symbol_Is_Global (sym) ? "@" : "%%");
  Emit_Symbol_Name (sym);
}

// Emit @__exc.<name> reference for an exception symbol and track the name
// so Generate_Exception_Globals can emit matching definitions.
void Emit_Exception_Ref (Symbol *exc) {

  // Capture the exception name by temporarily redirecting output
  FILE *real_out = cg->output;
  char buf[256];
  FILE *mem = fmemopen (buf, sizeof (buf) - 1, "w");
  cg->output = mem;
  Emit_Symbol_Name (exc);
  fflush (mem);
  long len = ftell (mem);
  fclose (mem);
  buf[len] = '\0';
  cg->output = real_out;

  // Emit the reference
  Emit ("@__exc.%s", buf);

  // Record the name if not already tracked
  for (uint32_t i = 0; i < cg->exc_ref_count; i++) {
    if (strcmp (cg->exc_refs[i], buf) == 0) return;
  }
  if (cg->exc_ref_count < EXC_REF_CAPACITY) {
    cg->exc_refs[cg->exc_ref_count] = strdup (buf);
    cg->exc_ref_count++;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.0.1 Codegen Predicates - Eliminate Duplicated Inline Checks                                  
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Find the nearest enclosing function/procedure by walking up the parent chain.                    
// This handles nested packages: a procedure inside a package inside a procedure                    
// needs access to the outermost procedure's frame.                                                 
// Returns NULL if no enclosing function/procedure found.                                           
//                                                                                                  
Symbol *Find_Enclosing_Subprogram (Symbol *sym) {
  Symbol *ancestor = sym ? sym->parent : NULL;
  while (ancestor) {
    if (ancestor->kind == SYMBOL_FUNCTION or ancestor->kind == SYMBOL_PROCEDURE)
      return ancestor;
    ancestor = ancestor->parent;
  }
  return NULL;
}

// Check if a subprogram needs static chain (is nested transitively inside                          
// another function/procedure). This handles package subprograms:                                   
//   procedure Outer is                                                                             
//     package P is                                                                                 
//       procedure Inner;  -- Inner needs access to Outer's frame                                   
//     end P;                                                                                       
//   ...                                                                                            
//                                                                                                  
bool Subprogram_Needs_Static_Chain (Symbol *sym) {
  return Find_Enclosing_Subprogram (sym) != NULL;
}

// Is sym an uplevel reference requiring access through __parent_frame?
bool Is_Uplevel_Access (const Symbol *sym) {
  if (not cg->current_function or not sym) return false;
  Symbol *owner = sym->defining_scope ? sym->defining_scope->owner : NULL;
  return cg->is_nested and owner and
       owner != cg->current_function and
       owner != cg->current_function->generic_template;
}

// Emit the storage location for a symbol, automatically handling uplevel                           
// access through __frame.                                                                          
// Emits either "%__frame.<name>" (uplevel) or the normal "%<name>"/"@<name>".                      
//                                                                                                  
void Emit_Symbol_Storage (Symbol *sym) {
  if (Is_Uplevel_Access (sym)) {
    Emit ("%%__frame.");
    Emit_Symbol_Name (sym);
  } else {
    Emit_Symbol_Ref (sym);
  }
}

// RM 8.3 Static Chain for Deeply Nested Calls - two-phase design.                                  
//                                                                                                  
// Phase 1 (Precompute_Nested_Frame_Arg): call BEFORE building the call                             
//   instruction.  For depth ≥ 2 it emits GEP/load instructions to chase the                        
//   static chain and returns the temp holding the ancestor frame pointer.                          
//   For depth 0-1 it returns 0 (no precomputation needed).                                         
//                                                                                                  
// Phase 2 (Emit_Nested_Frame_Arg): call INSIDE the call instruction's                              
//   argument list.  Emits "ptr %__frame_base", "ptr %__parent_frame", or                           
//   "ptr %tN" depending on depth.                                                                  
//                                                                                                  
// Each intermediate frame stores its received __parent_frame at byte                               
// offset scope->frame_size (see Generate_Subprogram_Body prologue).                                
//                                                                                                  
int Nested_Frame_Depth (Symbol *proc) {

  // Find the actual enclosing subprogram (skips packages)
  Symbol *target = Find_Enclosing_Subprogram (proc);
  if (not target) return 0;
  int depth = 0;
  for (Symbol *walker = cg->current_function; walker; walker = Find_Enclosing_Subprogram (walker)) {
    if (walker == target) return depth;
    depth++;
  }
  return depth;  // shouldn't happen in valid code
}
uint32_t Precompute_Nested_Frame_Arg (Symbol *proc) {
  int depth = Nested_Frame_Depth (proc);
  if (depth < 2) return 0;  // no precomputation needed
  Symbol *level = cg->current_function->parent;
  uint32_t prev = 0;
  for (int i = 1; i < depth; i++) {
    int64_t off = (level and level->scope) ? level->scope->frame_size : 0;
    uint32_t gep = Emit_Temp ();
    if (prev)
      Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %lld\n",
         gep, prev, (long long)off);
    else
      Emit ("  %%t%u = getelementptr i8, ptr %%__parent_frame, i64 %lld\n",
         gep, (long long)off);
    prev = Emit_Temp ();
    Emit ("  %%t%u = load ptr, ptr %%t%u\n", prev, gep);
    level = level->parent;
  }
  return prev;
}
bool Emit_Nested_Frame_Arg (Symbol *proc, uint32_t precomp) {
  if (not cg->current_function or not proc->parent) return false;
  int depth = Nested_Frame_Depth (proc);
  if (depth == 0)      Emit ("ptr %%__frame_base");
  else if (depth == 1) Emit ("ptr %%__parent_frame");
  else                 Emit ("ptr %%t%u", precomp);
  return true;
}

// Resolve generic formal type > actual type within an instance.                                    
// Called from attribute codegen, SUCC/PRED, and elsewhere.                                         
// Returns the substituted type, or the original if no match.                                       
//                                                                                                  
Type_Info *Resolve_Generic_Actual_Type (Type_Info *type) {
  if (not type or not type->name.data) return type;
  Symbol *holder = cg->current_instance;

  // Walk up: subprogram inside generic package uses package's actuals
  if (holder and not holder->generic_actuals and holder->parent and
    holder->parent->kind == SYMBOL_PACKAGE and holder->parent->generic_actuals)
    holder = holder->parent;
  if (not holder or not holder->generic_actuals) return type;
  for (uint32_t i = 0; i < holder->generic_actual_count; i++)
    if (holder->generic_actuals[i].actual_type and
      Slice_Equal_Ignore_Case (type->name, holder->generic_actuals[i].formal_name))
      return holder->generic_actuals[i].actual_type;
  return type;
}

// Emit raise sequence for a named exception: ptrtoint + call __ada_raise + unreachable.
void Emit_Raise_Exception (const char *exc_name, const char *comment) {
  uint32_t exc = Emit_Temp ();
  Emit ("  %%t%u = ptrtoint ptr @__exc.%s to i64\n", exc, exc_name);
  Emit ("  call void @__ada_raise(i64 %%t%u)  ; %s\n", exc, comment);
  Emit ("  unreachable\n");
  cg->block_terminated = true;  // unreachable terminates block
}
#define Emit_Raise_Constraint_Error(comment) Emit_Raise_Exception ("constraint_error", comment)
#define Emit_Raise_Program_Error(comment)    Emit_Raise_Exception ("program_error", comment)

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1.1a Granular Runtime Check Emission                                                         
//                                                                                                  
// Each Emit_*_Check function:                                                                      
//   1. Consults Check_Is_Suppressed () - returns early if suppressed.                              
//   2. Emits the check logic (comparison + conditional branch).                                    
//   3. On failure, branches to a block that calls Emit_Raise_Constraint_Error.                     
//   4. On success, falls through to a continuation block.                                          
//                                                                                                  
// LLVM IR pattern for every check:                                                                 
//   %cmp = icmp <pred> <type> %val, <bound>                                                        
//   br i1 %cmp, label %raise, label %cont                                                          
//   raise:                                                                                         
//     <raise constraint_error>                                                                     
//   cont:                                                                                          
//     ; continue execution                                                                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Emit_Overflow_Checked_Op - signed integer overflow check via LLVM intrinsics.                    
// Uses llvm.sadd/ssub/smul.with.overflow.iN for signed types.                                      
// Modular (unsigned) types are exempt - they wrap (RM 3.5.4).                                      
// Returns the temp ID holding the checked result.                                                  
//                                                                                                  
uint32_t Emit_Overflow_Checked_Op (
  uint32_t left, uint32_t right,
  const char *op,  // "add", "sub", "mul"
  const char *llvm_type,  // "i8", "i16", "i32", "i64", "i128"
  Type_Info *result_type)
{
  bool is_unsigned = Type_Is_Unsigned (result_type);
  bool suppressed = Check_Is_Suppressed (result_type, NULL, CHK_OVERFLOW);

  // Plain operation (wraps for modular, or check suppressed)
  if (is_unsigned or suppressed) {
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = %s %s %%t%u, %%t%u\n", t, op, llvm_type, left, right);
    Temp_Set_Type (t, llvm_type);

    // Modular non-power-of-2: caller handles urem wrapping
    return t;
  }

  // Signed overflow check via LLVM intrinsic
  const char *intrinsic_op;
  if      (strcmp (op, "add") == 0) intrinsic_op = "sadd";
  else if (strcmp (op, "sub") == 0) intrinsic_op = "ssub";
  else if (strcmp (op, "mul") == 0) intrinsic_op = "smul";
  else {

    // No intrinsic for div/rem - fall back to plain
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = %s %s %%t%u, %%t%u\n", t, op, llvm_type, left, right);
    Temp_Set_Type (t, llvm_type);
    return t;
  }

  // Emit: %pair = call {iN, i1} @llvm.sOP.with.overflow.iN(iN %left, iN %right)
  uint32_t pair = Emit_Temp ();
  Emit ("  %%t%u = call {%s, i1} @llvm.%s.with.overflow.%s(%s %%t%u, %s %%t%u)\n",
     pair, llvm_type, intrinsic_op, llvm_type, llvm_type, left, llvm_type, right);

  // Extract result and overflow flag
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = extractvalue {%s, i1} %%t%u, 0\n", result, llvm_type, pair);
  Temp_Set_Type (result, llvm_type);
  uint32_t ovf = Emit_Temp ();
  Emit ("  %%t%u = extractvalue {%s, i1} %%t%u, 1\n", ovf, llvm_type, pair);

  // Branch on overflow
  Emit_Check_With_Raise (ovf, true, "arithmetic overflow");
  return result;
}

// Emit_Division_Check - division by zero check (RM 4.5.5).                                         
// Also checks for signed MIN_INT / -1 overflow on two's complement.                                
// Emits before sdiv/udiv/srem/urem.                                                                
//                                                                                                  
void Emit_Division_Check (uint32_t divisor,
                 const char *llvm_type, Type_Info *type) {
  if (Check_Is_Suppressed (type, NULL, CHK_DIVISION)) return;

  // Check divisor == 0
  uint32_t cmp = Emit_Temp ();
  Emit ("  %%t%u = icmp eq %s %%t%u, 0\n", cmp, llvm_type, divisor);
  Emit_Check_With_Raise (cmp, true, "division by zero");
}

// Emit_Signed_Division_Overflow_Check - check for MIN_INT / -1.                                    
// Only for signed types: Integer'First / (-1) overflows on two's complement.                       
// Emits after the division-by-zero check, before the actual sdiv.                                  
//                                                                                                  
void Emit_Signed_Division_Overflow_Check (uint32_t dividend, uint32_t divisor,
                          const char *llvm_type, Type_Info *type) {
  if (Type_Is_Unsigned (type)) return;
  if (Check_Is_Suppressed (type, NULL, CHK_OVERFLOW)) return;

  // Check: divisor == -1 AND dividend == type_min                                                  
  // type_min for iN is -(2^(N-1)) which is the LLVM representation of                              
  // the minimum signed value.  We use a two-step check.                                            
  //                                                                                                
  int bits = 0;
  if (llvm_type[0] == 'i') bits = atoi (llvm_type + 1);
  if (bits == 0) return;

  // Check divisor == -1
  uint32_t cmp_neg1 = Emit_Temp ();
  Emit ("  %%t%u = icmp eq %s %%t%u, -1\n", cmp_neg1, llvm_type, divisor);

  // Get type minimum: for iN, min = -(1 << (N-1)).                                                 
  // In LLVM, the min of i32 is -2147483648, i64 is -9223372036854775808.                           
  // We compute the min using shl + sign.                                                           
  //                                                                                                
  uint32_t min_val = Emit_Temp ();
  Emit ("  %%t%u = shl %s 1, %d  ; type min magnitude\n", min_val, llvm_type, bits - 1);
  uint32_t neg_min = Emit_Temp ();
  Emit ("  %%t%u = sub %s 0, %%t%u  ; negate to get actual min\n", neg_min, llvm_type, min_val);

  // Check dividend == min
  uint32_t cmp_min = Emit_Temp ();
  Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n", cmp_min, llvm_type, dividend, neg_min);

  // Both conditions must be true for overflow
  uint32_t both = Emit_Temp ();
  Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", both, cmp_neg1, cmp_min);
  Emit_Check_With_Raise (both, true, "division overflow (MIN_INT / -1)");
}

// Emit_Index_Check - array index bounds check (RM 4.1.1).                                          
// Checks index against array low and high bounds.                                                  
// Bounds are provided as temp IDs (may be static or dynamic).                                      
//                                                                                                  
uint32_t Emit_Index_Check (uint32_t index,
                  uint32_t low_bound, uint32_t high_bound,
                  const char *index_type, Type_Info *array_type) {
  if (Check_Is_Suppressed (array_type, NULL, CHK_INDEX)) return index;

  // Ensure all operands match index_type
  index = Emit_Coerce (index, index_type);
  low_bound = Emit_Coerce (low_bound, index_type);
  high_bound = Emit_Coerce (high_bound, index_type);
  bool is_unsigned = Type_Is_Unsigned (array_type);
  const char *lt = is_unsigned ? "ult" : "slt";
  const char *gt = is_unsigned ? "ugt" : "sgt";

  // index < low?
  uint32_t cmp_lo = Emit_Temp ();
  Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", cmp_lo, lt, index_type, index, low_bound);

  // index > high?
  uint32_t cmp_hi = Emit_Temp ();
  Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", cmp_hi, gt, index_type, index, high_bound);
  uint32_t out_of_range = Emit_Temp ();
  Emit ("  %%t%u = or i1 %%t%u, %%t%u\n", out_of_range, cmp_lo, cmp_hi);
  Emit_Check_With_Raise (out_of_range, true, "index check failed");
  return index;
}

// Emit_Length_Check - array length mismatch check (RM 4.5.2, 5.2.1).
// Checks that source and destination arrays have matching lengths.
void Emit_Length_Check (uint32_t src_length, uint32_t dst_length,
                 const char *len_type, Type_Info *array_type) {
  if (Check_Is_Suppressed (array_type, NULL, CHK_LENGTH)) return;

  // Ensure both operands match len_type
  src_length = Emit_Coerce (src_length, len_type);
  dst_length = Emit_Coerce (dst_length, len_type);
  uint32_t cmp = Emit_Temp ();
  Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", cmp, len_type, src_length, dst_length);
  Emit_Check_With_Raise (cmp, true, "length check failed");
}

// Emit_Access_Check - null pointer dereference check (RM 4.1).
// Emits before every .ALL dereference and implicit dereference.
void Emit_Access_Check (uint32_t ptr_val, Type_Info *acc_type) {
  if (Check_Is_Suppressed (acc_type, NULL, CHK_ACCESS)) return;

  // If the value is a fat pointer { ptr, ptr }, extract the data pointer first
  const char *actual_ty = Temp_Get_Type (ptr_val);
  if (actual_ty and Llvm_Type_Is_Fat_Pointer (actual_ty)) {
    uint32_t data = Emit_Temp ();
    Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n", data, ptr_val);
    ptr_val = data;
  }
  uint32_t cmp = Emit_Temp ();
  Emit ("  %%t%u = icmp eq ptr %%t%u, null\n", cmp, ptr_val);
  Emit_Check_With_Raise (cmp, true, "access check (null dereference)");
}

// Emit_Discriminant_Check - variant record discriminant check (RM 3.7.1).
// Verifies that a discriminant has the expected value before component access.
void Emit_Discriminant_Check (uint32_t actual, uint32_t expected,
                    const char *disc_type, Type_Info *record_type) {
  if (Check_Is_Suppressed (record_type, NULL, CHK_DISCRIMINANT)) return;
  uint32_t cmp = Emit_Temp ();
  Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", cmp, disc_type, actual, expected);
  Emit_Check_With_Raise (cmp, true, "discriminant check failed");
}

// Parse LLVM type width: "i64" > 64, "float" > 32, "double" > 64
int Type_Bits (const char *llvm_type) {
  if (llvm_type[0] == 'i') return atoi (llvm_type + 1);
  if (llvm_type[0] == 'f') return 32;  // float
  if (llvm_type[0] == 'd') return 64;  // double
  return 64;
}

// Return the wider of two integer LLVM types.                                                      
// Used for binary operations: both operands are widened to the wider type.                         
// Example: Wider_Int_Type ("i8", "i32") > "i32".                                                   
// Both arguments MUST be integer types (i1, i8, i32, i64, etc.).                                   
//                                                                                                  
const char *Wider_Int_Type (const char *left, const char *right) {

  // Non-integer types must not reach here; derive from INTEGER as error path
  if (left[0] != 'i' or right[0] != 'i') {
    fprintf (stderr, "error: Wider_Int_Type called with non-integer type: \"%s\", \"%s\"\n",
             left, right);
    return Integer_Arith_Type ();
  }
  int left_bits  = Type_Bits (left);
  int right_bits = Type_Bits (right);
  return (left_bits >= right_bits) ? left : right;
}

// Check if LLVM type is floating-point
bool Is_Float_Type (const char *llvm_type) {
  return llvm_type and (llvm_type[0] == 'f' or llvm_type[0] == 'd');
}

// Return the LLVM fcmp predicate for a comparison operator.
// Uses ordered predicates for relational ops, unordered for NE (IEEE NaN handling).
const char *Float_Cmp_Predicate (int op) {
  switch (op) {
    case TK_EQ: return "oeq";
    case TK_NE: return "une";
    case TK_LT: return "olt";
    case TK_LE: return "ole";
    case TK_GT: return "ogt";
    case TK_GE: return "oge";
    default:    return "oeq";
  }
}

// Return the LLVM icmp predicate for a comparison operator.
// Selects signed or unsigned predicate based on is_unsigned flag.
const char *Int_Cmp_Predicate (int op, bool is_unsigned) {
  switch (op) {
    case TK_EQ: return "eq";
    case TK_NE: return "ne";
    case TK_LT: return is_unsigned ? "ult" : "slt";
    case TK_LE: return is_unsigned ? "ule" : "sle";
    case TK_GT: return is_unsigned ? "ugt" : "sgt";
    case TK_GE: return is_unsigned ? "uge" : "sge";
    default:    return "eq";
  }
}

// Check if expression produces boolean (i1) result directly.                                       
// Only comparisons and logical operators produce i1 - loaded variables                             
// are widened to i64 even for BOOLEAN type.                                                        
//                                                                                                  
bool Expression_Is_Boolean (Syntax_Node *node) {
  if (not node) return false;
  if (node->kind == NK_BINARY_OP) {
    switch (node->binary.op) {
      case TK_EQ: case TK_NE: case TK_LT: case TK_LE: case TK_GT: case TK_GE:
      case TK_AND: case TK_AND_THEN: case TK_OR: case TK_OR_ELSE: case TK_XOR:
      case TK_IN:  // Membership test
      case TK_NOT:  // NOT IN (binary) is also a membership test
        return true;
      default: break;
    }
  }
  if (node->kind == NK_UNARY_OP and node->unary.op == TK_NOT) {
    Type_Info *operand_type = node->unary.operand ? node->unary.operand->type : NULL;
    if (Type_Is_Boolean (operand_type)) return true;
  }

  // Note: Boolean-valued attributes (CONSTRAINED, CALLABLE, TERMINATED) produce i8
  // (Boolean storage type), not i1.  Only comparisons/logical ops produce i1.
  return false;
}

// Check if expression produces float result
bool Expression_Is_Float (Syntax_Node *node) {
  if (not node) return false;

  // Attributes that return floating-point (double)
  if (node->kind == NK_ATTRIBUTE) {
    String_Slice attr = node->attribute.name;
    if (Slice_Equal_Ignore_Case (attr, S("EPSILON")) or
      Slice_Equal_Ignore_Case (attr, S("SMALL")) or
      Slice_Equal_Ignore_Case (attr, S("LARGE")) or
      Slice_Equal_Ignore_Case (attr, S("SAFE_SMALL")) or
      Slice_Equal_Ignore_Case (attr, S("SAFE_LARGE")) or
      Slice_Equal_Ignore_Case (attr, S("DELTA"))) {
      return true;
    }

    // FIRST/LAST with float prefix type (not fixed-point)
    if (Slice_Equal_Ignore_Case (attr, S("FIRST")) or
      Slice_Equal_Ignore_Case (attr, S("LAST"))) {
      Syntax_Node *prefix = node->attribute.prefix;
      if (prefix and prefix->type and
        not Type_Is_Fixed_Point (prefix->type) and
        (Type_Is_Float_Representation (prefix->type) or
         prefix->type->low_bound.kind == BOUND_FLOAT)) {
        return true;
      }
    }
  }

  // Check node type for general float expressions
  return Type_Is_Float_Representation (node->type);
}

// Get LLVM type string for expression result
const char *Expression_Llvm_Type (Syntax_Node *node) {

  // Boolean expressions (comparisons, logical ops) produce i1.
  // Widening to i8 (Boolean storage type) happens at store boundaries.
  if (Expression_Is_Boolean (node)) {
    return "i1";
  }

  // For float types, return the correct LLVM type based on actual size
  if (node and Type_Is_Float_Representation (node->type)) {
    return Llvm_Float_Type ((uint32_t)To_Bits (node->type->size));
  }

  // Check for pointer/access types.                                                                
  // Access-to-unconstrained arrays use fat pointer { ptr, { bound, bound } }.                      
  // Access-to-constrained or scalar types use plain ptr.                                           
  // Use Type_To_Llvm to get the correct representation.                                            
  //                                                                                                
  if (node and Type_Is_Access (node->type))
    return Type_To_Llvm (node->type);
  if (node and node->kind == NK_ALLOCATOR and node->type)
    return Type_To_Llvm (node->type);
  if (node and node->kind == NK_NULL) return "ptr";

  // Record types and aggregates return pointers (alloca addresses)
  if (node and Type_Is_Record (node->type)) return "ptr";
  if (node and node->kind == NK_AGGREGATE and Type_Is_Record (node->type)) return "ptr";

  // Array aggregates with dynamic bounds produce fat pointers (alloca for
  // { ptr, ptr }); static-bound aggregates return plain ptr (alloca address).
  if (node and node->kind == NK_AGGREGATE and node->type and
    (node->type->kind == TYPE_ARRAY or node->type->kind == TYPE_STRING)) {
    if (Type_Is_Unconstrained_Array (node->type) or
      Type_Has_Dynamic_Bounds (node->type))
      return FAT_PTR_TYPE;
    return "ptr";
  }

  // Slices always produce fat pointers regardless of declared type.
  // Must check before array indexing since both are NK_APPLY.
  if (node and Expression_Is_Slice (node)) {
    return FAT_PTR_TYPE;  // Slices always produce fat pointers
  }

  // Array indexing (NK_APPLY) that returns non-i64 element types.                                  
  // Now preserves native types for ALL element types, not just composites.                         
  // Must exclude function calls where prefix happens to have array-like return type.               
  //                                                                                                
  if (node and node->kind == NK_APPLY and node->apply.prefix and
    node->apply.prefix->type and
    Type_Is_Array_Like (node->apply.prefix->type) and
    not (node->apply.prefix->symbol and
      (node->apply.prefix->symbol->kind == SYMBOL_FUNCTION or
       node->apply.prefix->symbol->kind == SYMBOL_PROCEDURE))) {
    Type_Info *elem_type = node->type;
    if (Type_Is_Record (elem_type) or Type_Is_String (elem_type) or
      Type_Is_Constrained_Array (elem_type)) {
      return "ptr";  // Composite elements return ptr
    }
    if (Type_Is_Access (elem_type)) {
      return "ptr";  // Access elements loaded as ptr
    }
    if (elem_type) return Type_To_Llvm (elem_type);
  }

  // Check for string literals and unconstrained string types (fat pointers).
  // Constrained STRING subtypes (e.g., STRING (1..6)) are flat arrays > ptr.
  if (node and node->kind == NK_STRING) return FAT_PTR_TYPE;
  if (node and Type_Is_String (node->type) and not Type_Is_Constrained_Array (node->type))
    return FAT_PTR_TYPE;

  // Constrained arrays with dynamic bounds are stored as fat pointers
  if (node and node->kind != NK_AGGREGATE and node->type and
    Type_Is_Constrained_Array (node->type) and Type_Has_Dynamic_Bounds (node->type))
    return FAT_PTR_TYPE;

  // Identifiers stored as fat pointers (dynamic bounds at declaration time)
  if (node and node->kind == NK_IDENTIFIER and node->symbol and
    node->symbol->needs_fat_ptr_storage)
    return FAT_PTR_TYPE;

  // Check for unconstrained array types (fat pointers) - for variable references
  if (node and node->kind != NK_AGGREGATE and
    Type_Is_Unconstrained_Array (node->type)) {
    return FAT_PTR_TYPE;
  }

  // Binary integer arithmetic codegen produces result at                                           
  // Wider_Int_Type (left, right), which may differ from Type_To_Llvm (node->type)                  
  // when the type resolver assigns a wider base type (e.g., INTEGER) to the                        
  // expression while operands are narrower (e.g., i8 for RANGE -10..10).                           
  // Must match Generate_Binary_Op's actual output type.                                            
  //                                                                                                
  if (node and node->kind == NK_BINARY_OP and not Expression_Is_Boolean (node) and
    node->type and not Type_Is_Float_Representation (node->type) and
    not Type_Is_Fixed_Point (node->type)) {
    Token_Kind op = node->binary.op;
    if (op == TK_PLUS or op == TK_MINUS or op == TK_STAR or
      op == TK_SLASH or op == TK_MOD or op == TK_REM) {
      const char *left_type = node->binary.left ?
        Expression_Llvm_Type (node->binary.left) : Integer_Arith_Type ();
      const char *right_type = node->binary.right ?
        Expression_Llvm_Type (node->binary.right) : Integer_Arith_Type ();
      if (left_type[0] == 'i' and right_type[0] == 'i')
        return Wider_Int_Type (left_type, right_type);
    }
  }

  // User-defined operator (unary or binary): return type matches function signature
  if (node and node->symbol and node->symbol->kind == SYMBOL_FUNCTION and
    not node->symbol->is_predefined and node->symbol->return_type) {
    return Type_To_Llvm (node->symbol->return_type);
  }

  // Unary integer arithmetic codegen uses operand's native type.
  if (node and node->kind == NK_UNARY_OP and
    node->type and not Type_Is_Float_Representation (node->type) and
    (node->unary.op == TK_MINUS or node->unary.op == TK_PLUS) and
    node->unary.operand) {
    return Expression_Llvm_Type (node->unary.operand);
  }

  // return the native LLVM type for the node's Ada type.                                           
  // No widening to INTEGER - expressions stay at their natural width.                              
  // Only widen at explicit Ada type conversions and LLVM intrinsic boundaries.                     
  // Resolve generic formal types to their actuals so the predicted type                            
  // matches what codegen actually emits.                                                           
  //                                                                                                
  if (node and node->type) {
    Type_Info *resolved = node->type;
    if (cg->current_instance)
      resolved = Resolve_Generic_Actual_Type (resolved);
    return Type_To_Llvm (resolved);
  }
  return Integer_Arith_Type ();
}

// Emit type conversion if needed.                                                                  
// is_unsigned: when true, integer extensions use zext instead of sext,                             
//              and float↔int conversions use unsigned variants (fptoui/uitofp).                    
//              This is required for Ada modular (unsigned) types (RM 3.5.4).                       
//                                                                                                  
uint32_t Emit_Convert_Ext (uint32_t src, const char *src_type,
                 const char *dst_type, bool is_unsigned) {

  // Check tracked actual type - Expression_Llvm_Type may disagree with                             
  // the actual generated type (e.g. after 'VAL, 'POS, arithmetic).                                 
  // Use the tracked type if available to avoid invalid IR.                                         
  //                                                                                                
  const char *actual = Temp_Get_Type (src);
  if (actual and actual[0] != '\0' and strcmp (actual, src_type) != 0)
    src_type = actual;
  if (strcmp (src_type, dst_type) == 0) return src;
  bool src_is_float = Is_Float_Type (src_type);
  bool dst_is_float = Is_Float_Type (dst_type);
  int src_bits = Type_Bits (src_type), dst_bits = Type_Bits (dst_type);
  uint32_t t = Emit_Temp ();
  if (src_is_float and dst_is_float) {

    // float ↔ double
    if (dst_bits > src_bits) {
      Emit ("  %%t%u = fpext %s %%t%u to %s\n", t, src_type, src, dst_type);
    } else {
      Emit ("  %%t%u = fptrunc %s %%t%u to %s\n", t, src_type, src, dst_type);
    }
  } else if (src_is_float and not dst_is_float) {

    // float/double > ptr: fptosi to i64, then inttoptr
    if (Llvm_Type_Is_Pointer (dst_type)) {
      uint32_t int_temp = Emit_Temp ();
      Emit ("  %%t%u = fptosi %s %%t%u to i64\n", int_temp, src_type, src);
      Emit ("  %%t%u = inttoptr i64 %%t%u to ptr\n", t, int_temp);

    // float/double > integer: round then fptosi (Ada RM 4.6)
    } else {
      uint32_t rounded = Emit_Temp ();
      Emit ("  %%t%u = call %s @llvm.round.%s(%s %%t%u)\n",
         rounded, src_type, src_type, src_type, src);
      Emit ("  %%t%u = %s %s %%t%u to %s\n", t,
         is_unsigned ? "fptoui" : "fptosi", src_type, rounded, dst_type);
    }
  } else if (not src_is_float and dst_is_float) {

    // ptr > float/double: ptrtoint to i64, then sitofp
    if (Llvm_Type_Is_Pointer (src_type)) {
      uint32_t int_temp = Emit_Temp ();
      Emit ("  %%t%u = ptrtoint ptr %%t%u to i64\n", int_temp, src);
      Emit ("  %%t%u = %s i64 %%t%u to %s\n", t,
         is_unsigned ? "uitofp" : "sitofp", int_temp, dst_type);

    // integer > float/double: sitofp for signed, uitofp for unsigned
    } else {
      Emit ("  %%t%u = %s %s %%t%u to %s\n", t,
         is_unsigned ? "uitofp" : "sitofp", src_type, src, dst_type);
    }

  // ptr > ptr: no conversion needed
  } else if (Llvm_Type_Is_Pointer (src_type) and Llvm_Type_Is_Pointer (dst_type)) {
    return src;

  // ptr > integer: ptrtoint
  } else if (Llvm_Type_Is_Pointer (src_type) and dst_type[0] == 'i') {
    Emit ("  %%t%u = ptrtoint ptr %%t%u to %s\n", t, src, dst_type);

  // integer > ptr: inttoptr
  } else if (src_type[0] == 'i' and Llvm_Type_Is_Pointer (dst_type)) {
    Emit ("  %%t%u = inttoptr %s %%t%u to ptr\n", t, src_type, src);

  // fat pointer > fat pointer: no conversion needed
  } else if (Llvm_Type_Is_Fat_Pointer (src_type) and Llvm_Type_Is_Fat_Pointer (dst_type)) {
    return src;

  // fat pointer > ptr: extract data pointer (field 0).                                             
  // Note: this loses bounds information, used when passing to constrained params                   
  // or assigning to access types                                                                   
  //                                                                                                
  } else if (Llvm_Type_Is_Fat_Pointer (src_type) and Llvm_Type_Is_Pointer (dst_type)) {
    Emit ("  %%t%u = extractvalue %s %%t%u, 0\n", t, src_type, src);

  // ptr > fat pointer: pointer to unconstrained array storage, load it
  } else if (Llvm_Type_Is_Pointer (src_type) and Llvm_Type_Is_Fat_Pointer (dst_type)) {
    Emit ("  %%t%u = load %s, ptr %%t%u\n", t, dst_type, src);

  // fat pointer > integer: extract data pointer then ptrtoint
  } else if (Llvm_Type_Is_Fat_Pointer (src_type) and dst_type[0] == 'i') {
    uint32_t data = Emit_Temp ();
    Emit ("  %%t%u = extractvalue %s %%t%u, 0\n", data, src_type, src);
    Emit ("  %%t%u = ptrtoint ptr %%t%u to %s\n", t, data, dst_type);

  // integer > fat pointer: likely an access value, inttoptr then load
  } else if (src_type[0] == 'i' and Llvm_Type_Is_Fat_Pointer (dst_type)) {
    uint32_t ptr_temp = Emit_Temp ();
    Emit ("  %%t%u = inttoptr %s %%t%u to ptr\n", ptr_temp, src_type, src);
    Emit ("  %%t%u = load %s, ptr %%t%u\n", t, dst_type, ptr_temp);

  // One is fat pointer, other is something else - best effort
  } else if (Llvm_Type_Is_Fat_Pointer (src_type) or Llvm_Type_Is_Fat_Pointer (dst_type)) {
    return src;

  // integer conversions
  } else {
    if (src_bits == dst_bits) return src;

    // Use zext for boolean (i1) and unsigned/modular types,
    // sext for signed integer extensions
    if (dst_bits > src_bits) {
      if (src_bits == 1 or is_unsigned) {
        Emit ("  %%t%u = zext %s %%t%u to %s\n", t, src_type, src, dst_type);
      } else {
        Emit ("  %%t%u = sext %s %%t%u to %s\n", t, src_type, src, dst_type);
      }

    // Boolean: icmp ne 0 preserves semantics (any non-zero > true)
    } else if (dst_bits == 1) {
      Emit ("  %%t%u = icmp ne %s %%t%u, 0\n", t, src_type, src);
    } else {
      Emit ("  %%t%u = trunc %s %%t%u to %s\n", t, src_type, src, dst_type);
    }
  }
  Temp_Set_Type (t, dst_type);
  return t;
}

// Signed (default) conversion - backward-compatible wrapper
uint32_t Emit_Convert (uint32_t src,
                  const char *src_type, const char *dst_type) {
  return Emit_Convert_Ext (src, src_type, dst_type, false);
}

// Coerce a temp to the desired LLVM type, using Temp_Get_Type to discover its current type.
// If the type is already correct or unknown, returns the temp unchanged.
uint32_t Emit_Coerce (uint32_t temp,
                  const char *desired_type) {
  const char *cur = Temp_Get_Type (temp);
  if (not cur or cur[0] == '\0') return temp;  // Unknown type - no conversion
  if (strcmp (cur, desired_type) == 0) return temp;
  return Emit_Convert (temp, cur, desired_type);
}

// Like Emit_Coerce, but assumes Integer_Arith_Type when the temp's type is                         
// unknown.  Use only in contexts where untracked temps are known to be i32                         
// (e.g. bound arithmetic).                                                                         
//                                                                                                  
uint32_t Emit_Coerce_Default_Int (uint32_t temp,
                        const char *desired_type) {
  const char *cur = Temp_Get_Type (temp);
  if (not cur or cur[0] == '\0') cur = Integer_Arith_Type ();
  if (strcmp (cur, desired_type) == 0) return temp;
  return Emit_Convert (temp, cur, desired_type);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.1.2 Constraint Checks                                                                        
//                                                                                                  
// Per RM 3.5.4: CONSTRAINT_ERROR raised when value falls outside subtype range.                    
// Handles both static (BOUND_INTEGER) and dynamic (BOUND_EXPR) bounds.                             
// Generates: if (val < low or val > high) __ada_raise(CONSTRAINT_ERROR)                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

uint32_t Emit_Bound_Value_Typed (Type_Bound *bound,
                     const char **out_type) {
  const char *iat = Integer_Arith_Type ();

  // If a pre-evaluated cached temp exists, use it directly instead of                              
  // re-generating the expression.  This prevents side-effectful bound                              
  // expressions (function calls) from being evaluated multiple times                               
  // during constraint checking.  RM 3.2.2(5).                                                      
  //                                                                                                
  if (bound->cached_temp != 0) {
    if (out_type) {
      const char *cached_type = Temp_Get_Type (bound->cached_temp);
      *out_type = (cached_type and cached_type[0]) ? cached_type : iat;
    }
    return bound->cached_temp;
  }

  // Use i64 if the value exceeds i32 range
  if (bound->kind == BOUND_INTEGER) {
    const char *bound_type = iat;
    int128_t   value       = bound->int_value;
    if (value < (int128_t)INT32_MIN or value > (int128_t)INT32_MAX) bound_type = "i64";
    uint32_t result = Emit_Temp ();
    Emit ("  %%t%u = add %s 0, %s  ; literal bound\n",
       result, bound_type, I128_Decimal (value));
    Temp_Set_Type (result, bound_type);
    if (out_type) *out_type = bound_type;
    return result;
  } else if (bound->kind == BOUND_FLOAT) {
    int128_t   int_val    = (int128_t)bound->float_value;
    const char *bound_type = iat;
    if (int_val < (int128_t)INT32_MIN or int_val > (int128_t)INT32_MAX) bound_type = "i64";
    uint32_t result = Emit_Temp ();
    Emit ("  %%t%u = add %s 0, %s  ; float-to-int bound (%g)\n",
       result, bound_type, I128_Decimal (int_val), bound->float_value);
    Temp_Set_Type (result, bound_type);
    if (out_type) *out_type = bound_type;
    return result;

  // BOUND_EXPR: generate the expression and report its actual LLVM type.
  // Do NOT convert here - the caller will unify types.
  } else if (bound->kind == BOUND_EXPR and bound->expr) {
    uint32_t val = Generate_Expression (bound->expr);
    if (out_type) *out_type = Expression_Llvm_Type (bound->expr);
    return val;
  }
  if (out_type) *out_type = iat;
  return 0;  // Cannot determine bound
}
uint32_t Emit_Bound_Value (Type_Bound *bound) {
  return Emit_Bound_Value_Typed (bound, NULL);
}

// RM 3.5: General scalar constraint check - dispatches to integer or                               
// float path based on the target type's kind.  Covers:                                             
//   Integer, enumeration, character > icmp slt/sgt on native type (Integer_Arith_Type)             
//   Float, fixed, universal_real    > fcmp olt/ogt on double                                       
//                                                                                                  
uint32_t Emit_Constraint_Check_With_Type (uint32_t val,
                     Type_Info *target, Type_Info *source,
                     const char *actual_val_type) {
  if (not target) return val;

  // Respect pragma Suppress (Range_Check) on the target type
  if (Check_Is_Suppressed (target, NULL, CHK_RANGE)) return val;
  bool is_int_like = (target->kind == TYPE_INTEGER or
            target->kind == TYPE_MODULAR or
            target->kind == TYPE_ENUMERATION or
            target->kind == TYPE_BOOLEAN or
            target->kind == TYPE_CHARACTER or
            target->kind == TYPE_FIXED);  // scaled integer repr
  bool is_flt_like = (target->kind == TYPE_FLOAT);
  if (not is_int_like and not is_flt_like) return val;

  // Elide check when static bounds cover the full representation range.                            
  // Common case: INTEGER element in array aggregates has bounds                                    
  // -2147483648..2147483647 which any i32 value trivially satisfies.                               
  // This avoids 3 basic blocks per element - critical for JIT perf.                                
  //                                                                                                
  if (is_int_like and
    target->low_bound.kind == BOUND_INTEGER and
    target->high_bound.kind == BOUND_INTEGER) {
    uint32_t bits = (uint32_t)To_Bits (target->size);
    if (bits == 0) bits = 32;
    bool full_range;
    if (Type_Is_Unsigned (target)) {
      full_range = (target->low_bound.int_value == 0 and
              target->high_bound.int_value == ((int128_t)1 << bits) - 1);
    } else {
      full_range = (target->low_bound.int_value == -((int128_t)1 << (bits - 1)) and
              target->high_bound.int_value == ((int128_t)1 << (bits - 1)) - 1);
    }

    // Also elide if source fits entirely within target bounds
    if (not full_range and source and
      source->low_bound.kind == BOUND_INTEGER and
      source->high_bound.kind == BOUND_INTEGER) {
      full_range = (source->low_bound.int_value >= target->low_bound.int_value and
              source->high_bound.int_value <= target->high_bound.int_value);
    }
    if (full_range) return val;
  }

  // Need either static or dynamic bounds
  bool lo_ok = (target->low_bound.kind == BOUND_INTEGER or
          target->low_bound.kind == BOUND_FLOAT or
          target->low_bound.kind == BOUND_EXPR);
  bool hi_ok = (target->high_bound.kind == BOUND_INTEGER or
          target->high_bound.kind == BOUND_FLOAT or
          target->high_bound.kind == BOUND_EXPR);
  if (not lo_ok or not hi_ok) return val;
  uint32_t raise_label = cg->label_id++;
  uint32_t ok_label    = cg->label_id++;
  uint32_t cont_label  = cg->label_id++;

  // Float/fixed path - compare at the target's native float type
  if (is_flt_like) {
    const char *flt_type = Float_Llvm_Type_Of (target);

    // Convert value to comparison type if needed.
    // Use actual_val_type when provided (value may already be converted).
    const char *src_flt = actual_val_type ? actual_val_type :
                (source ? Float_Llvm_Type_Of (source) : flt_type);
    if (strcmp (src_flt, flt_type) != 0) {
      val = Emit_Convert (val, src_flt, flt_type);
    }
    uint32_t lo = 0, hi = 0;
    if (target->low_bound.kind == BOUND_FLOAT) {
      double   float_val = target->low_bound.float_value;
      uint64_t raw_bits;
      memcpy (&raw_bits, &float_val, sizeof (raw_bits));
      lo = Emit_Temp ();
      if (strcmp (flt_type, "float") == 0) {
        float    single = (float)float_val;
        uint32_t single_bits;
        memcpy (&single_bits, &single, sizeof (single_bits));
        Emit ("  %%t%u = bitcast i32 %u to float  ; low bound %g\n",
           lo, single_bits, float_val);
      } else {
        Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; low bound %g\n",
           lo, (unsigned long long)raw_bits, float_val);
      }
    } else if (target->low_bound.kind == BOUND_INTEGER) {
      lo = Emit_Temp ();
      Emit ("  %%t%u = sitofp %s %s to %s  ; low bound\n",
         lo, Integer_Arith_Type (), I128_Decimal (target->low_bound.int_value), flt_type);
    } else {
      if (target->low_bound.cached_temp) {
        lo = target->low_bound.cached_temp;
      } else {
        lo = Generate_Expression (target->low_bound.expr);
      }
      const char *lo_expr_ty = Expression_Llvm_Type (target->low_bound.expr);
      lo = Emit_Convert (lo, lo_expr_ty, flt_type);
    }
    if (target->high_bound.kind == BOUND_FLOAT) {
      double   float_val = target->high_bound.float_value;
      uint64_t raw_bits;
      memcpy (&raw_bits, &float_val, sizeof (raw_bits));
      hi = Emit_Temp ();
      if (strcmp (flt_type, "float") == 0) {
        float    single = (float)float_val;
        uint32_t single_bits;
        memcpy (&single_bits, &single, sizeof (single_bits));
        Emit ("  %%t%u = bitcast i32 %u to float  ; high bound %g\n",
           hi, single_bits, float_val);
      } else {
        Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; high bound %g\n",
           hi, (unsigned long long)raw_bits, float_val);
      }
    } else if (target->high_bound.kind == BOUND_INTEGER) {
      hi = Emit_Temp ();
      Emit ("  %%t%u = sitofp %s %s to %s  ; high bound\n",
         hi, Integer_Arith_Type (), I128_Decimal (target->high_bound.int_value), flt_type);
    } else {
      if (target->high_bound.cached_temp) {
        hi = target->high_bound.cached_temp;
      } else {
        hi = Generate_Expression (target->high_bound.expr);
      }
      const char *hi_expr_ty = Expression_Llvm_Type (target->high_bound.expr);
      hi = Emit_Convert (hi, hi_expr_ty, flt_type);
    }
    if (not lo or not hi) return val;

    // val < low?
    uint32_t cmp_lo = Emit_Temp ();
    Emit ("  %%t%u = fcmp olt %s %%t%u, %%t%u\n", cmp_lo, flt_type, val, lo);
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp_lo, raise_label, ok_label);
    cg->block_terminated = true;
    Emit_Label_Here (ok_label);

    // val > high?
    uint32_t cmp_hi = Emit_Temp ();
    Emit ("  %%t%u = fcmp ogt %s %%t%u, %%t%u\n", cmp_hi, flt_type, val, hi);
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp_hi, raise_label, cont_label);
    cg->block_terminated = true;

  // Integer/enum/char/fixed path - all compare as i64.                                             
  // For TYPE_FIXED, bounds may be BOUND_FLOAT or BOUND_EXPR                                        
  // (e.g. subtype F is Fix range -101.0..0.0).  Scale float                                        
  // bounds to match the mantissa repr: scaled = bound / small.                                     
  // BOUND_EXPR bounds are generated as double then fptosi'd.                                       
  //                                                                                                
  } else {
    uint32_t low_bound, high_bound;
    const char *lo_type = NULL, *hi_type = NULL;
    if (target->kind == TYPE_FIXED) {
      double small = target->fixed.small;
      if (small <= 0) small = target->fixed.delta > 0 ? target->fixed.delta : 1.0;

      // Fixed-point bounds must use the fixed type's native width (i64)                            
      // to avoid overflow when mantissa approaches MAX_MANTISSA.                                   
      // Integer_Arith_Type is only i32 which overflows at 2^31.                                    
      //                                                                                            
      const char *fix_bnd_ty = Type_To_Llvm (target);
      if (not fix_bnd_ty or fix_bnd_ty[0] != 'i') fix_bnd_ty = "i64";
      #define FIXED_BOUND_TO_I64(bnd, out) do {                     \
        if ((bnd)->cached_temp) {                                 \
          (out) = (bnd)->cached_temp;                           \
          const char *_ct = Temp_Get_Type ((out));           \
          /* Float cached temp: apply fixed-point scaling */\
          if (_ct and Is_Float_Type (_ct)) {                      \
            uint64_t _sb; memcpy (&_sb, &small, sizeof (_sb)); \
            uint32_t _st = Emit_Temp ();                    \
            Emit ("  %%t%u = fadd double 0.0, 0x%016llX\n",\
               _st, (unsigned long long)_sb);               \
            uint32_t _dv = Emit_Temp ();                    \
            Emit ("  %%t%u = fdiv double %%t%u, %%t%u\n", \
               _dv, (out), _st);                            \
            (out) = Emit_Temp ();                            \
            Emit ("  %%t%u = fptosi double %%t%u to %s\n",\
               (out), _dv, fix_bnd_ty);                     \
          } else if (_ct and _ct[0] == 'i' and                    \
            strcmp (_ct, fix_bnd_ty) != 0)                     \
            (out) = Emit_Convert ((out), _ct, fix_bnd_ty);\
        } else if ((bnd)->kind == BOUND_FLOAT) {                  \
          int128_t iv = (int128_t)((bnd)->float_value / small); \
          (out) = Emit_Temp ();                                \
          Emit ("  %%t%u = add %s 0, %s  ; fixed bound"     \
             " (%g/small)\n", (out), fix_bnd_ty,              \
             I128_Decimal (iv), (bnd)->float_value);           \
        } else if ((bnd)->kind == BOUND_EXPR) {                   \
          uint32_t fv = Generate_Expression ((bnd)->expr);   \
          const char *fv_ty = Temp_Get_Type (fv);            \
          bool fv_float = (fv_ty and Is_Float_Type (fv_ty)) or    \
            Type_Is_Float_Representation ((bnd)->expr->type) or\
            ((bnd)->expr->type and                             \
             (bnd)->expr->type->kind == TYPE_UNIVERSAL_REAL); \
          /* Float expression: divide by SMALL, fptosi */   \
          if (fv_float) {                                       \
            uint64_t sb; memcpy (&sb, &small, sizeof (sb));     \
            uint32_t st = Emit_Temp ();                      \
            Emit ("  %%t%u = fadd double 0.0, 0x%016llX\n",\
               st, (unsigned long long)sb);                 \
            uint32_t dv = Emit_Temp ();                      \
            Emit ("  %%t%u = fdiv double %%t%u, %%t%u\n", \
               dv, fv, st);                                 \
            (out) = Emit_Temp ();                            \
            Emit ("  %%t%u = fptosi double %%t%u to %s\n",\
               (out), dv, fix_bnd_ty);                      \
          /* Already scaled integer (fixed-point value) */  \
          } else {                                              \
            const char *fv_src = fv_ty ? fv_ty :              \
              Type_To_Llvm ((bnd)->expr->type);              \
            (out) = Emit_Convert (fv,                      \
              fv_src, fix_bnd_ty);                          \
          }                                                     \
          Temp_Set_Type ((out), fix_bnd_ty);                 \
        } else {                                                  \
          (out) = Emit_Bound_Value ((bnd));                  \
        }                                                         \
      } while (0)
      FIXED_BOUND_TO_I64 (&target->low_bound, low_bound);
      FIXED_BOUND_TO_I64 (&target->high_bound, high_bound);
      #undef FIXED_BOUND_TO_I64
      lo_type = fix_bnd_ty;
      hi_type = fix_bnd_ty;
    } else {
      low_bound  = Emit_Bound_Value_Typed (&target->low_bound, &lo_type);
      high_bound = Emit_Bound_Value_Typed (&target->high_bound, &hi_type);
    }
    if (not low_bound or not high_bound) return val;

    // val < low? - Compare at wider of source/target type width.
    // Modular (unsigned) types use unsigned predicates (RM 3.5.4).
    const char *target_llvm = Type_To_Llvm (target);
    const char *source_llvm = source ? Type_To_Llvm (source) : target_llvm;

    // Use actual LLVM type if provided (may differ from Ada type info)
    const char *effective_source = actual_val_type ? actual_val_type : source_llvm;
    const char *chk_type;
    if (target_llvm[0] == 'i' and effective_source[0] == 'i') {
      chk_type = Wider_Int_Type (target_llvm, effective_source);
    } else {
      chk_type = Integer_Arith_Type ();
    }
    bool chk_unsigned = Type_Is_Unsigned (target);
    const char *lt_pred = chk_unsigned ? "ult" : "slt";
    const char *gt_pred = chk_unsigned ? "ugt" : "sgt";

    // Widen val and bounds to chk_type for comparison only.                                        
    // Use temporary widened values - original val is returned unchanged                            
    // so the caller's value stays at its original LLVM type.                                       
    //                                                                                              
    const char *val_type = actual_val_type ? actual_val_type : source_llvm;

    // Determine actual bound types.  BOUND_INTEGER/BOUND_FLOAT always produce                      
    // at Integer_Arith_Type.  BOUND_EXPR produces at Expression_Llvm_Type.                         
    // lo_type/hi_type are set above for the non-FIXED path; for FIXED they                         
    // come from the macro which always uses Integer_Arith_Type.                                    
    //                                                                                              
    if (not lo_type) lo_type = Integer_Arith_Type ();
    if (not hi_type) hi_type = Integer_Arith_Type ();

    // Ensure chk_type is wide enough for value and both bounds
    if (chk_type[0] == 'i' and lo_type[0] == 'i')
      chk_type = Wider_Int_Type (chk_type, lo_type);
    if (chk_type[0] == 'i' and hi_type[0] == 'i')
      chk_type = Wider_Int_Type (chk_type, hi_type);
    uint32_t wval, wlo, whi;
    if (chk_unsigned) {
      wval = Emit_Convert_Ext (val, val_type, chk_type, true);
      wlo  = Emit_Convert_Ext (low_bound, lo_type, chk_type, true);
      whi  = Emit_Convert_Ext (high_bound, hi_type, chk_type, true);
    } else {
      wval = Emit_Convert (val, val_type, chk_type);
      wlo  = Emit_Convert (low_bound, lo_type, chk_type);
      whi  = Emit_Convert (high_bound, hi_type, chk_type);
    }
    uint32_t cmp_lo = Emit_Temp ();
    Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", cmp_lo, lt_pred, chk_type, wval, wlo);
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp_lo, raise_label, ok_label);
    cg->block_terminated = true;
    Emit_Label_Here (ok_label);

    // val > high?
    uint32_t cmp_hi = Emit_Temp ();
    Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", cmp_hi, gt_pred, chk_type, wval, whi);
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp_hi, raise_label, cont_label);
    cg->block_terminated = true;
  }
  Emit_Label_Here (raise_label);  // raise CONSTRAINT_ERROR
  Emit_Raise_Constraint_Error ("bound check");
  Emit_Label_Here (cont_label);
  cg->block_terminated = false;
  return val;
}

// Backward-compatible wrapper - derives val_type from source Type_Info
uint32_t Emit_Constraint_Check (uint32_t val,
                     Type_Info *target, Type_Info *source) {
  return Emit_Constraint_Check_With_Type (val, target, source, NULL);
}

// RM 3.5(3): When a subtype_indication includes a range constraint                                 
// (e.g. I5 RANGE 0..P), the constraint bounds must lie within the                                  
// base type's range.  Emit runtime checks for any BOUND_EXPR bound.                                
//                                                                                                  
void Emit_Subtype_Constraint_Compat_Check (Type_Info *subtype) {
  if (not subtype or not subtype->base_type or not Type_Is_Scalar (subtype)) return;
  Type_Info *base = subtype->base_type;
  if (Check_Is_Suppressed (subtype, NULL, CHK_RANGE)) return;
  bool lo_dyn = (subtype->low_bound.kind == BOUND_EXPR);
  bool hi_dyn = (subtype->high_bound.kind == BOUND_EXPR);
  if (not lo_dyn and not hi_dyn) return;  // purely static - checked at compile time
  bool base_lo_ok = (base->low_bound.kind == BOUND_INTEGER or
             base->low_bound.kind == BOUND_EXPR);
  bool base_hi_ok = (base->high_bound.kind == BOUND_INTEGER or
             base->high_bound.kind == BOUND_EXPR);
  if (not base_lo_ok or not base_hi_ok) return;
  const char *iat = Integer_Arith_Type ();

  // Emit base type bounds - widen to iat for comparison
  const char *base_low_type = NULL, *base_high_type = NULL;
  uint32_t blo = Emit_Bound_Value_Typed (&base->low_bound,  &base_low_type);
  uint32_t bhi = Emit_Bound_Value_Typed (&base->high_bound, &base_high_type);
  if (not base_low_type)  base_low_type  = iat;
  if (not base_high_type) base_high_type = iat;
  blo = Emit_Convert (blo, base_low_type,  iat);
  bhi = Emit_Convert (bhi, base_high_type, iat);

  // Check each dynamic constraint bound against base range
  if (lo_dyn) {
    const char *constraint_low_type = NULL;
    uint32_t clo = Emit_Bound_Value_Typed (&subtype->low_bound, &constraint_low_type);
    if (not constraint_low_type) constraint_low_type = iat;
    clo = Emit_Convert (clo, constraint_low_type, iat);
    uint32_t cmp_result = Emit_Temp ();
    Emit ("  %%t%u = icmp slt %s %%t%u, %%t%u\n", cmp_result, iat, clo, blo);
    Emit_Check_With_Raise (cmp_result, true, "subtype low bound");
  }
  if (hi_dyn) {
    const char *constraint_high_type = NULL;
    uint32_t chi = Emit_Bound_Value_Typed (&subtype->high_bound, &constraint_high_type);
    if (not constraint_high_type) constraint_high_type = iat;
    chi = Emit_Convert (chi, constraint_high_type, iat);
    uint32_t cmp_result = Emit_Temp ();
    Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n", cmp_result, iat, chi, bhi);
    Emit_Check_With_Raise (cmp_result, true, "subtype high bound");
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.2.1 Fat Pointer Support for Unconstrained Arrays                                             
//                                                                                                  
// fat pointer = { data_ptr, bounds_ptr } = { ptr, ptr }.                                           
// Bounds live behind the second pointer as a struct { bt, bt } where                               
// bt = native index type (i32 for STRING, i8 for CHARACTER, etc.).                                 
//                                                                                                  
// All helpers take a `bt` (bound type) parameter - the LLVM type string                            
// for the bounds (e.g., "i32", "i8").  The bounds struct type is derived                           
// from bt via Bounds_Type_For ().                                                                  
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Create a fat pointer from data pointer and constant bounds.                                      
// bt = bound LLVM type (e.g., "i32").                                                              
// Allocates bounds struct on stack, stores lo/hi, builds { ptr, ptr }.                             
//                                                                                                  
uint32_t Emit_Fat_Pointer (uint32_t data_ptr,
                  int128_t low, int128_t high, const char *bt) {
  const char *bounds_struct = Bounds_Type_For (bt);

  // Allocate bounds struct { bt, bt } on stack - native type
  uint32_t bounds_alloca = Emit_Temp ();
  Emit ("  %%t%u = alloca %s\n", bounds_alloca, bounds_struct);

  // Store low bound in native bt
  uint32_t low_gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 0, i32 0\n",
     low_gep, bounds_struct, bounds_alloca);
  Emit ("  store %s %s, ptr %%t%u\n", bt, I128_Decimal (low), low_gep);

  // Store high bound in native bt
  uint32_t high_gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 0, i32 1\n",
     high_gep, bounds_struct, bounds_alloca);
  Emit ("  store %s %s, ptr %%t%u\n", bt, I128_Decimal (high), high_gep);

  // Build fat pointer { ptr, ptr } via insertvalue
  uint32_t partial = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " undef, ptr %%t%u, 0\n",
     partial, data_ptr);
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " %%t%u, ptr %%t%u, 1\n",
     result, partial, bounds_alloca);
  Temp_Set_Type (result, FAT_PTR_TYPE);
  return result;
}

// Widen a value to INTEGER width (Integer_Arith_Type) for use at                                   
// LLVM intrinsic boundaries (memcpy length, alloca size, malloc size, RTS ABI).                    
// Uses sext for signed types, zext for unsigned (modular) types.                                   
// No-op if from_type is already at INTEGER width.                                                  
//                                                                                                  
uint32_t Emit_Widen_For_Intrinsic (uint32_t val,
                  const char *from_type) {
  const char *actual = Temp_Get_Type (val);
  if (actual and actual[0] != '\0') from_type = actual;
  const char *iat = Integer_Arith_Type ();
  if (strcmp (from_type, iat) == 0) return val;
  uint32_t widened = Emit_Temp ();
  Emit ("  %%t%u = sext %s %%t%u to %s\n", widened, from_type, val, iat);
  Temp_Set_Type (widened, iat);
  return widened;
}

// Extend value to i64 for C-level operations (memcpy, alloca, malloc, sec_stack_alloc).
// These always require i64 on 64-bit targets regardless of Ada INTEGER width.
uint32_t Emit_Extend_To_I64 (uint32_t val, const char *from_type) {
  const char *actual = Temp_Get_Type (val);
  if (actual and actual[0] != '\0') from_type = actual;
  if (strcmp (from_type, "i64") == 0) return val;
  uint32_t extended = Emit_Temp ();
  Emit ("  %%t%u = sext %s %%t%u to i64\n", extended, from_type, val);
  Temp_Set_Type (extended, "i64");
  return extended;
}

// Ensure fat_ptr is an SSA {ptr,ptr} value.  If it's an alloca-backed                              
// fat pointer (marked via Temp_Mark_Fat_Alloca), emit a load first.                                
// Conservative: only loads when the temp is KNOWN to be an alloca.                                 
//                                                                                                  
uint32_t Fat_Ptr_As_Value (uint32_t fat_ptr) {
  if (not Temp_Is_Fat_Alloca (fat_ptr)) return fat_ptr;
  uint32_t loaded = Emit_Temp ();
  Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n", loaded, fat_ptr);
  Temp_Set_Type (loaded, FAT_PTR_TYPE);
  return loaded;
}

// Extract data pointer from fat pointer.
// bt parameter retained for API compatibility but unused.
uint32_t Emit_Fat_Pointer_Data (uint32_t fat_ptr,
                     const char *bt) {
  (void)bt;
  fat_ptr = Fat_Ptr_As_Value (fat_ptr);
  uint32_t data = Emit_Temp ();
  Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n", data, fat_ptr);
  Temp_Set_Type (data, "ptr");
  return data;
}

// Extract bound at field_index from fat pointer's bounds struct.                                   
// Parametric over index: 0 = low, 1 = high.  Handles both SSA and                                  
// alloca-style fat pointers transparently via Fat_Ptr_As_Value.                                    
//                                                                                                  
uint32_t Emit_Fat_Pointer_Bound (uint32_t fat_ptr,
                    const char *bt, uint32_t field_index) {
  const char *bounds_struct = Bounds_Type_For (bt);
  fat_ptr = Fat_Ptr_As_Value (fat_ptr);
  uint32_t bptr = Emit_Temp ();
  Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n", bptr, fat_ptr);
  uint32_t gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 0, i32 %u\n",
     gep, bounds_struct, bptr, field_index);
  uint32_t val = Emit_Temp ();
  Emit ("  %%t%u = load %s, ptr %%t%u\n", val, bt, gep);
  Temp_Set_Type (val, bt);
  return val;
}
#define Emit_Fat_Pointer_Low(fp, bt)  Emit_Fat_Pointer_Bound ((fp), (bt), 0)
#define Emit_Fat_Pointer_High(fp, bt) Emit_Fat_Pointer_Bound ((fp), (bt), 1)

// Extract low bound for dimension `dim` from fat pointer.                                          
// Bounds are stored as flat pairs: [low0, high0, low1, high1, ...].                                
// Uses flat GEP with index 2*dim for low, 2*dim+1 for high.                                        
//                                                                                                  
uint32_t Emit_Fat_Pointer_Low_Dim (uint32_t fat_ptr,
                      const char *bt, uint32_t dim) {
  fat_ptr = Fat_Ptr_As_Value (fat_ptr);
  uint32_t bptr = Emit_Temp ();
  Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n", bptr, fat_ptr);
  uint32_t gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
     gep, bt, bptr, dim * 2);
  uint32_t val = Emit_Temp ();
  Emit ("  %%t%u = load %s, ptr %%t%u\n", val, bt, gep);
  Temp_Set_Type (val, bt);
  return val;
}
uint32_t Emit_Fat_Pointer_High_Dim (uint32_t fat_ptr,
                      const char *bt, uint32_t dim) {
  fat_ptr = Fat_Ptr_As_Value (fat_ptr);
  uint32_t bptr = Emit_Temp ();
  Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n", bptr, fat_ptr);
  uint32_t gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
     gep, bt, bptr, dim * 2 + 1);
  uint32_t val = Emit_Temp ();
  Emit ("  %%t%u = load %s, ptr %%t%u\n", val, bt, gep);
  Temp_Set_Type (val, bt);
  return val;
}

// Compute length for dimension `dim` from fat pointer: high - low + 1
// with null-array clamping (RM 3.6.2).
uint32_t Emit_Fat_Pointer_Length_Dim (uint32_t fat_ptr,
                       const char *bt, uint32_t dim) {
  uint32_t low = Emit_Fat_Pointer_Low_Dim (fat_ptr, bt, dim);
  uint32_t high = Emit_Fat_Pointer_High_Dim (fat_ptr, bt, dim);
  return Emit_Length_Clamped (low, high, bt);
}

// Allocate a multi-dimension bounds block on the stack.                                            
// Layout: [low0, high0, low1, high1, ...] as flat array of 2*ndims values.                         
// bounds_lo/bounds_hi are arrays of ndims temp IDs.                                                
// Returns the alloca temp ID (ptr to the bounds memory).                                           
//                                                                                                  
uint32_t Emit_Alloc_Bounds_MultiDim (uint32_t *bounds_lo, uint32_t *bounds_hi, uint32_t ndims, const char *bt)
{
  uint32_t bounds_alloca = Emit_Temp ();
  Emit ("  %%t%u = alloca [%u x %s]  ; bounds for %u dims\n",
     bounds_alloca, ndims * 2, bt, ndims);
  for (uint32_t dim = 0; dim < ndims; dim++) {
    uint32_t lo_coerced = Emit_Coerce (bounds_lo[dim], bt);
    uint32_t hi_coerced = Emit_Coerce (bounds_hi[dim], bt);
    uint32_t lo_gep = Emit_Temp ();
    Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
       lo_gep, bt, bounds_alloca, dim * 2);
    Emit ("  store %s %%t%u, ptr %%t%u\n", bt, lo_coerced, lo_gep);
    uint32_t hi_gep = Emit_Temp ();
    Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
       hi_gep, bt, bounds_alloca, dim * 2 + 1);
    Emit ("  store %s %%t%u, ptr %%t%u\n", bt, hi_coerced, hi_gep);
  }
  return bounds_alloca;
}

// Create a fat pointer with multi-dim bounds.
// bounds_lo/bounds_hi are arrays of ndims temp IDs.
uint32_t Emit_Fat_Pointer_MultiDim (uint32_t data_ptr,
  uint32_t *bounds_lo, uint32_t *bounds_hi, uint32_t ndims, const char *bt)
{
  uint32_t bounds_alloca = Emit_Alloc_Bounds_MultiDim (bounds_lo, bounds_hi, ndims, bt);
  uint32_t partial = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " undef, ptr %%t%u, 0\n",
     partial, data_ptr);
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " %%t%u, ptr %%t%u, 1\n",
     result, partial, bounds_alloca);
  Temp_Set_Type (result, FAT_PTR_TYPE);
  return result;
}

// Create a fat pointer from data pointer and dynamic bounds (temp IDs).                            
// bt = bound LLVM type of the input temps (already in native type).                                
// Allocates bounds struct on stack, stores in native bt, builds { ptr, ptr }.                      
// Allocate a bounds struct { bt, bt } on the stack and store lo/hi into it.                        
// Returns the alloca temp ID (a ptr to the bounds struct).                                         
//                                                                                                  
uint32_t Emit_Alloc_Bounds_Struct (uint32_t low_temp, uint32_t high_temp, const char *bt)
{
  const char *bounds_struct = Bounds_Type_For (bt);
  uint32_t bounds_alloca = Emit_Temp ();
  Emit ("  %%t%u = alloca %s\n", bounds_alloca, bounds_struct);

  // Convert bounds to target type if needed
  low_temp  = Emit_Coerce (low_temp, bt);
  high_temp = Emit_Coerce (high_temp, bt);
  uint32_t low_gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 0, i32 0\n",
     low_gep, bounds_struct, bounds_alloca);
  Emit ("  store %s %%t%u, ptr %%t%u\n", bt, low_temp, low_gep);
  uint32_t high_gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 0, i32 1\n",
     high_gep, bounds_struct, bounds_alloca);
  Emit ("  store %s %%t%u, ptr %%t%u\n", bt, high_temp, high_gep);
  return bounds_alloca;
}
uint32_t Emit_Fat_Pointer_Dynamic (uint32_t data_ptr,
                      uint32_t low_temp, uint32_t high_temp,
                      const char *bt) {
  uint32_t bounds_alloca = Emit_Alloc_Bounds_Struct (low_temp, high_temp, bt);
  uint32_t partial = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " undef, ptr %%t%u, 0\n",
     partial, data_ptr);
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " %%t%u, ptr %%t%u, 1\n",
     result, partial, bounds_alloca);
  Temp_Set_Type (result, FAT_PTR_TYPE);
  return result;
}

// Like Emit_Alloc_Bounds_Struct but uses malloc instead of alloca.                                 
// Necessary for allocator (NEW) results where the fat pointer crosses                              
// function boundaries and the bounds must outlive the creating function.                           
//                                                                                                  
uint32_t Emit_Heap_Bounds_Struct (uint32_t low_temp, uint32_t high_temp, const char *bt)
{
  const char *bounds_struct = Bounds_Type_For (bt);

  // Compute size of bounds struct: 2 * sizeof (bound_type)
  int bit_width = 32;
  if (bt and bt[0] == 'i') bit_width = atoi (bt + 1);
  uint64_t bounds_size = (uint64_t)(bit_width / 8) * 2;
  if (bounds_size == 0) bounds_size = 8;
  uint32_t bounds_ptr = Emit_Temp ();
  Emit ("  %%t%u = call ptr @malloc (i64 %llu)\n",
     bounds_ptr, (unsigned long long)bounds_size);
  low_temp  = Emit_Coerce (low_temp, bt);
  high_temp = Emit_Coerce (high_temp, bt);
  uint32_t low_gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 0, i32 0\n",
     low_gep, bounds_struct, bounds_ptr);
  Emit ("  store %s %%t%u, ptr %%t%u\n", bt, low_temp, low_gep);
  uint32_t high_gep = Emit_Temp ();
  Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 0, i32 1\n",
     high_gep, bounds_struct, bounds_ptr);
  Emit ("  store %s %%t%u, ptr %%t%u\n", bt, high_temp, high_gep);
  return bounds_ptr;
}

// Like Emit_Fat_Pointer_Dynamic but heap-allocates the bounds struct.
// Used for allocator (NEW) results that must survive across function returns.
uint32_t Emit_Fat_Pointer_Heap (uint32_t data_ptr,
                     uint32_t low_temp, uint32_t high_temp,
                     const char *bt) {
  uint32_t bounds_ptr = Emit_Heap_Bounds_Struct (low_temp, high_temp, bt);
  uint32_t partial = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " undef, ptr %%t%u, 0\n",
     partial, data_ptr);
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " %%t%u, ptr %%t%u, 1\n",
     result, partial, bounds_ptr);
  Temp_Set_Type (result, FAT_PTR_TYPE);
  return result;
}

// Compute length from fat pointer bounds: high - low + 1
// Returns temp ID holding the length in native bound type (bt).
uint32_t Emit_Fat_Pointer_Length (uint32_t fat_ptr,
                     const char *bt) {
  uint32_t low = Emit_Fat_Pointer_Low (fat_ptr, bt);
  uint32_t high = Emit_Fat_Pointer_High (fat_ptr, bt);
  uint32_t diff = Emit_Temp ();
  Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", diff, bt, high, low);
  Temp_Set_Type (diff, bt);
  uint32_t len = Emit_Temp ();
  Emit ("  %%t%u = add %s %%t%u, 1\n", len, bt, diff);
  Temp_Set_Type (len, bt);
  return len;
}

// Emit length from two temp IDs (raw bounds, not from fat pointer):                                
//   result = max(high - low + 1, 0)                                                                
// Null ranges (high < low) yield 0, not a negative / wrapped value.                                
// Lighter than Emit_Fat_Pointer_Length when you already have low/high.                             
//                                                                                                  
uint32_t Emit_Length_From_Bounds (uint32_t low, uint32_t high, const char *bt)
{
  uint32_t diff = Emit_Temp ();
  Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", diff, bt, high, low);
  uint32_t raw = Emit_Temp ();
  Emit ("  %%t%u = add %s %%t%u, 1\n", raw, bt, diff);
  uint32_t neg = Emit_Temp ();
  Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", neg, bt, raw);
  uint32_t len = Emit_Temp ();
  Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n", len, neg, bt, bt, raw);
  Temp_Set_Type (len, bt);
  return len;
}

// Emit_Fat_To_Array_Memcpy: Copy data from a fat pointer source to a destination.                  
// Extracts data pointer, computes byte length from bounds * element size, emits memcpy.            
// Used when assigning fat pointer (unconstrained string/array) to constrained component.           
//                                                                                                  
void Emit_Fat_To_Array_Memcpy (uint32_t fat_val,
                    uint32_t dest_ptr, Type_Info *array_type) {
  const char *bnd_type = Array_Bound_Llvm_Type (array_type);
  uint32_t data_ptr = Emit_Fat_Pointer_Data (fat_val, bnd_type);
  uint32_t fat_lo = Emit_Fat_Pointer_Low (fat_val, bnd_type);
  uint32_t fat_hi = Emit_Fat_Pointer_High (fat_val, bnd_type);
  uint32_t len = Emit_Length_From_Bounds (fat_lo, fat_hi, bnd_type);
  uint32_t element_size = (array_type->array.element_type and
               array_type->array.element_type->size > 0) ?
               array_type->array.element_type->size : 1;
  uint32_t byte_len = len;
  if (element_size > 1) {
    byte_len = Emit_Temp ();
    Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_len, bnd_type, len, element_size);
  }
  uint32_t byte_len_64 = Emit_Extend_To_I64 (byte_len, bnd_type);
  Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
     dest_ptr, data_ptr, byte_len_64);
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §13.2.1 Consolidated Helpers                                                                     
//                                                                                                  
// Length computation with null-array clamping (RM 3.6.2), static integer constant emission,        
// memcmp-based array comparison, and conditional check + raise sequences.                          
// ═════════════════════════════════════════════════════════════════════════════════════════════════

// Emit length from bounds with null-array clamping (RM 3.6.2):                                     
//   result = (low > high) ? 0 : (high - low + 1)                                                   
// Handles the Ada rule that arrays with first > last have length 0.                                
//                                                                                                  
uint32_t Emit_Length_Clamped (uint32_t low, uint32_t high, const char *bt)
{
  uint32_t diff = Emit_Temp ();
  Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", diff, bt, high, low);
  uint32_t unclamped = Emit_Temp ();
  Emit ("  %%t%u = add %s %%t%u, 1\n", unclamped, bt, diff);
  uint32_t is_null = Emit_Temp ();
  Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n", is_null, bt, low, high);
  uint32_t len = Emit_Temp ();
  Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n", len, is_null, bt, bt, unclamped);
  Temp_Set_Type (len, bt);
  return len;
}

// Emit a static integer constant using the add idiom:                                              
//   %tN = add <type> 0, <value>                                                                    
// This is the standard LLVM IR pattern for loading constants.                                      
//                                                                                                  
uint32_t Emit_Static_Int (int128_t value, const char *ty) {
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = add %s 0, %s\n", result, ty, I128_Decimal (value));
  Temp_Set_Type (result, ty);
  return result;
}

// Emit memcmp and return i1 equality result (true if arrays equal).
// Handles both static (literal size) and dynamic (temp size) cases.
uint32_t Emit_Memcmp_Eq (uint32_t left_ptr, uint32_t right_ptr, uint32_t byte_size_temp,
  int64_t byte_size_static, bool is_dynamic)
{
  uint32_t memcmp_res = Emit_Temp ();
  if (is_dynamic) {
    Emit ("  %%t%u = call i32 @memcmp (ptr %%t%u, ptr %%t%u, i64 %%t%u)\n",
       memcmp_res, left_ptr, right_ptr, byte_size_temp);
  } else {
    Emit ("  %%t%u = call i32 @memcmp (ptr %%t%u, ptr %%t%u, i64 %lld)\n",
       memcmp_res, left_ptr, right_ptr, (long long)byte_size_static);
  }
  uint32_t eq = Emit_Temp ();
  Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", eq, memcmp_res);
  Temp_Set_Type (eq, "i1");
  return eq;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.2.2 Bound Extraction Helpers                                                                 
//                                                                                                  
// Ada types carry bounds as either:                                                                
//   - BOUND_INTEGER: compile-time constant                                                         
//   - BOUND_EXPR: runtime expression (dynamic subtypes)                                            
//   - BOUND_FLOAT: floating-point constant (for float types)                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Bound_Temps: Holds emitted temps for a dimension's low and high bounds.
// The bound_type field records the LLVM type used (e.g., "i32", "i64").

// Emit a single bound (low or high) from a Type_Bound structure.
// Handles all three bound kinds: INTEGER, EXPR, FLOAT.
uint32_t Emit_Single_Bound (Type_Bound *bound,
                   const char *target_type) {
  if (bound->kind == BOUND_INTEGER) {
    return Emit_Static_Int (bound->int_value, target_type);
  } else if (bound->kind == BOUND_FLOAT) {
    return Emit_Static_Int ((int128_t)bound->float_value, target_type);

  // RM 3.3.1: Use cached temp if pre-evaluated, to avoid re-calling
  // side-effectful expressions (function calls) during aggregate gen.
  } else if (bound->kind == BOUND_EXPR and bound->expr) {
    uint32_t val = bound->cached_temp ? bound->cached_temp
      : Generate_Expression (bound->expr);
    const char *expr_ty = bound->cached_temp
      ? Temp_Get_Type (bound->cached_temp)
      : Expression_Llvm_Type (bound->expr);
    if (not expr_ty or not expr_ty[0]) expr_ty = target_type;
    if (strcmp (expr_ty, target_type) != 0 and
      expr_ty[0] == 'i' and target_type[0] == 'i') {
      val = Emit_Convert (val, expr_ty, target_type);
    }
    return val;
  }
  return Emit_Static_Int (0, target_type);  // Fallback for unset bounds
}

// Emit_Bounds: Extract both bounds from a Type_Info for a specific dimension.                      
// For arrays: uses indices[dim].low_bound/high_bound                                               
// For scalars: uses type->low_bound/high_bound (dim ignored)                                       
// Returns Bound_Temps with both temps and the bound type string.                                   
//                                                                                                  
Bound_Temps Emit_Bounds (Type_Info *type, uint32_t dim) {
  Bound_Temps result = {0, 0, Integer_Arith_Type ()};
  if (not type) return result;
  Type_Bound *lb = &type->low_bound;
  Type_Bound *hb = &type->high_bound;

  // For array types, use the appropriate dimension's bounds
  if (Type_Is_Array_Like (type) and dim < type->array.index_count) {
    lb = &type->array.indices[dim].low_bound;
    hb = &type->array.indices[dim].high_bound;

    // Determine bound type from index type if available
    if (type->array.indices[dim].index_type) {
      const char *idx_llvm = Type_To_Llvm (type->array.indices[dim].index_type);
      if (idx_llvm and idx_llvm[0] == 'i') result.bound_type = idx_llvm;
    }
  }

  // Use array bound LLVM type if this is an array
  if (Type_Is_Array_Like (type)) {
    const char *array_bound_ty = Array_Bound_Llvm_Type (type);
    if (array_bound_ty and array_bound_ty[0] == 'i') result.bound_type = array_bound_ty;
  }
  result.low_temp = Emit_Single_Bound (lb, result.bound_type);
  result.high_temp = Emit_Single_Bound (hb, result.bound_type);
  return result;
}

// Emit_Bounds_From_Fat: Extract bounds from a fat pointer value.                                   
// Fat pointers have structure { ptr data, ptr bounds } where bounds                                
// points to a { low, high } pair. Uses existing Emit_Fat_Pointer_Low/High.                         
//                                                                                                  
Bound_Temps Emit_Bounds_From_Fat (uint32_t fat_ptr,
                     const char *bt) {
  Bound_Temps result;
  result.bound_type = bt;
  result.low_temp = Emit_Fat_Pointer_Low (fat_ptr, bt);
  result.high_temp = Emit_Fat_Pointer_High (fat_ptr, bt);
  return result;
}

// Emit_Bounds_From_Fat_Dim: Extract bounds from fat pointer for specific dimension.
// Supports multi-dimensional unconstrained arrays (RM 3.6.1).
Bound_Temps Emit_Bounds_From_Fat_Dim (uint32_t fat_ptr,
                       const char *bt, uint32_t dim) {
  Bound_Temps result;
  result.bound_type = bt;
  result.low_temp = Emit_Fat_Pointer_Low_Dim (fat_ptr, bt, dim);
  result.high_temp = Emit_Fat_Pointer_High_Dim (fat_ptr, bt, dim);
  return result;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.2.4 Conditional Check + Raise                                                                
//                                                                                                  
// Ada runtime checks follow a common pattern:                                                      
//   1. Emit comparison (icmp/fcmp)                                                                 
//   2. Branch to raise block if check fails                                                        
//   3. Raise CONSTRAINT_ERROR (or other exception)                                                 
//   4. Continue to next instruction                                                                
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Emit_Check_With_Raise: Conditional branch + raise + continue.                                    
// If raise_on_true is true: raises when cond is true, continues when false.                        
// If raise_on_true is false: raises when cond is false, continues when true.                       
//                                                                                                  
void Emit_Check_With_Raise (uint32_t cond,
                   bool raise_on_true, const char *comment) {
  uint32_t raise_label = cg->label_id++;
  uint32_t cont_label  = cg->label_id++;
  if (raise_on_true) {
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
       cond, raise_label, cont_label);
  } else {
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
       cond, cont_label, raise_label);
  }
  cg->block_terminated = true;  // conditional branch terminates block
  Emit_Label_Here (raise_label);
  Emit_Raise_Constraint_Error (comment);
  Emit_Label_Here (cont_label);
  cg->block_terminated = false;
}

// Emit_Range_Check_With_Raise: Checks val in [lo_val, hi_val], raises if not.
// Emits two icmp + br with shared raise label for efficiency.
void Emit_Range_Check_With_Raise (uint32_t val,
                     int64_t lo_val, int64_t hi_val,
                     const char *type, const char *comment) {
  uint32_t lo_ok = cg->label_id++;
  uint32_t hi_ok = cg->label_id++;
  uint32_t raise_label = cg->label_id++;
  uint32_t cmp_lo = Emit_Temp ();
  Emit ("  %%t%u = icmp slt %s %%t%u, %lld\n", cmp_lo, type, val, (long long)lo_val);
  Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp_lo, raise_label, lo_ok);
  cg->block_terminated = true;
  Emit_Label_Here (lo_ok);
  uint32_t cmp_hi = Emit_Temp ();
  Emit ("  %%t%u = icmp sgt %s %%t%u, %lld\n", cmp_hi, type, val, (long long)hi_val);
  Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp_hi, raise_label, hi_ok);
  cg->block_terminated = true;
  Emit_Label_Here (raise_label);
  Emit_Raise_Constraint_Error (comment);
  Emit_Label_Here (hi_ok);
  cg->block_terminated = false;
}

// Emit a type bound as a temp: handles BOUND_INTEGER, BOUND_EXPR, BOUND_FLOAT.
// Returns temp ID with value at specified type width.
uint32_t Emit_Type_Bound (Type_Bound *bound, const char *ty) {
  if (bound->kind == BOUND_INTEGER) {
    return Emit_Static_Int (bound->int_value, ty);
  } else if (bound->kind == BOUND_FLOAT) {
    return Emit_Static_Int ((int128_t)bound->float_value, ty);
  } else if (bound->kind == BOUND_EXPR and bound->expr) {
    uint32_t val = Generate_Expression (bound->expr);
    const char *expr_ty = Expression_Llvm_Type (bound->expr);
    if (strcmp (expr_ty, ty) != 0 and expr_ty[0] == 'i' and ty[0] == 'i') {
      val = Emit_Convert (val, expr_ty, ty);
    }
    return val;
  }
  return Emit_Static_Int (0, ty);  // Fallback
}

// Copy data from fat pointer to a named destination.
// Emits: memcpy (dst, src_data, length).  bt = bound type.
void Emit_Fat_Pointer_Copy_To_Name (uint32_t fat_ptr,
                      Symbol *dst, const char *bt) {
  uint32_t src_ptr = Emit_Fat_Pointer_Data (fat_ptr, bt);
  uint32_t len = Emit_Fat_Pointer_Length (fat_ptr, bt);
  uint32_t len64 = Emit_Extend_To_I64 (len, bt);
  Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
  Emit_Symbol_Name (dst);
  Emit (", ptr %%t%u, i64 %%t%u, i1 false)\n", src_ptr, len64);
}

// Load fat pointer from a symbol's storage.  bt = bound type (unused for load).
uint32_t Emit_Load_Fat_Pointer (Symbol *sym, const char *bt) {
  (void)bt;
  uint32_t fat = Emit_Temp ();
  Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr ", fat);
  Emit_Symbol_Storage (sym);
  Emit ("\n");
  Temp_Set_Type (fat, FAT_PTR_TYPE);
  return fat;
}

// Load fat pointer from a temp pointer (%%t<N>).  bt = bound type (unused for load).
uint32_t Emit_Load_Fat_Pointer_From_Temp (uint32_t ptr_temp, const char *bt) {
  (void)bt;
  uint32_t fat = Emit_Temp ();
  Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n", fat, ptr_temp);
  Temp_Set_Type (fat, FAT_PTR_TYPE);
  return fat;
}

// Store a fat pointer value into a symbol's storage.  bt = bound type (unused).
void Emit_Store_Fat_Pointer_To_Symbol (uint32_t fat_val, Symbol *sym,
                        const char *bt) {
  (void)bt;
  Emit ("  store " FAT_PTR_TYPE " %%t%u, ptr ", fat_val);
  Emit_Symbol_Storage (sym);
  Emit ("\n");
}

// Store fat pointer fields (data ptr, low, high) into a symbol using GEP+store.                    
// Allocates bounds struct, stores lo/hi, then stores { ptr, ptr } fields.                          
// bt = bound type.                                                                                 
//                                                                                                  
void Emit_Store_Fat_Pointer_Fields_To_Symbol
  (uint32_t data_ptr, uint32_t low_temp, uint32_t high_temp, Symbol *sym, const char *bt)
{
  uint32_t bounds_alloca = Emit_Alloc_Bounds_Struct (low_temp, high_temp, bt);

  // Store data ptr (field 0 of { ptr, ptr })
  uint32_t data_slot = Emit_Temp ();
  Emit ("  %%t%u = getelementptr " FAT_PTR_TYPE ", ptr ", data_slot);
  Emit_Symbol_Storage (sym);
  Emit (", i32 0, i32 0\n");
  Emit ("  store ptr %%t%u, ptr %%t%u\n", data_ptr, data_slot);

  // Store bounds ptr (field 1 of { ptr, ptr })
  uint32_t bounds_slot = Emit_Temp ();
  Emit ("  %%t%u = getelementptr " FAT_PTR_TYPE ", ptr ", bounds_slot);
  Emit_Symbol_Storage (sym);
  Emit (", i32 0, i32 1\n");
  Emit ("  store ptr %%t%u, ptr %%t%u\n", bounds_alloca, bounds_slot);
}

// Store fat pointer fields (data ptr, low, high) into a temp alloca using GEP+store.
// bt = bound type.
void Emit_Store_Fat_Pointer_Fields_To_Temp (uint32_t data_ptr, uint32_t low_temp, uint32_t high_temp,
  uint32_t fat_alloca, const char *bt)
{
  uint32_t bounds_alloca = Emit_Alloc_Bounds_Struct (low_temp, high_temp, bt);

  // Store data ptr (field 0 of { ptr, ptr })
  uint32_t data_slot = Emit_Temp ();
  Emit ("  %%t%u = getelementptr " FAT_PTR_TYPE ", ptr %%t%u, i32 0, i32 0\n",
     data_slot, fat_alloca);
  Emit ("  store ptr %%t%u, ptr %%t%u\n", data_ptr, data_slot);

  // Store bounds ptr (field 1 of { ptr, ptr })
  uint32_t bounds_slot = Emit_Temp ();
  Emit ("  %%t%u = getelementptr " FAT_PTR_TYPE ", ptr %%t%u, i32 0, i32 1\n",
     bounds_slot, fat_alloca);
  Emit ("  store ptr %%t%u, ptr %%t%u\n", bounds_alloca, bounds_slot);
}

// Compare two fat pointers for identity equality (data ptr + both bounds).
// Returns temp ID holding i1 result.  bt = bound type.
uint32_t Emit_Fat_Pointer_Compare (uint32_t left_fat, uint32_t right_fat, const char *bt)
{
  uint32_t left_ptr  = Emit_Fat_Pointer_Data (left_fat, bt);
  uint32_t right_ptr = Emit_Fat_Pointer_Data (right_fat, bt);
  uint32_t left_low  = Emit_Fat_Pointer_Low (left_fat, bt);
  uint32_t right_low = Emit_Fat_Pointer_Low (right_fat, bt);
  uint32_t left_high  = Emit_Fat_Pointer_High (left_fat, bt);
  uint32_t right_high = Emit_Fat_Pointer_High (right_fat, bt);

  uint32_t ptr_eq = Emit_Temp ();
  Emit ("  %%t%u = icmp eq ptr %%t%u, %%t%u\n", ptr_eq, left_ptr, right_ptr);
  uint32_t low_eq = Emit_Temp ();
  Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n", low_eq, bt, left_low, right_low);
  uint32_t high_eq = Emit_Temp ();
  Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n", high_eq, bt, left_high, right_high);

  uint32_t partial = Emit_Temp ();
  Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", partial, ptr_eq, low_eq);
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", result, partial, high_eq);
  return result;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §13.2.3 Additional Helpers                                                                       
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// Emit minimum of two values:  result = (left < right) ? left : right
uint32_t Emit_Min_Value (uint32_t left, uint32_t right, const char *ty) {
  uint32_t cmp = Emit_Temp ();
  Emit ("  %%t%u = icmp slt %s %%t%u, %%t%u\n", cmp, ty, left, right);
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
     result, cmp, ty, left, ty, right);
  Temp_Set_Type (result, ty);
  return result;
}

// Structure returned by Emit_Exception_Handler_Setup

// Setup exception handler with setjmp. Emits alloca, push_handler, setjmp, branch.                 
// Caller must emit labels and code for normal/handler paths.                                       
// Returns structure with handler_frame, jmp_buf temps, and label IDs.                              
//                                                                                                  
Exception_Setup Emit_Exception_Handler_Setup (void) {
  Exception_Setup setup;
  setup.handler_frame = Emit_Temp ();
  Emit ("  %%t%u = alloca { ptr, [200 x i8] }, align 16  ; handler frame\n", setup.handler_frame);
  Emit ("  call void @__ada_push_handler(ptr %%t%u)\n", setup.handler_frame);
  setup.jmp_buf = Emit_Temp ();
  Emit ("  %%t%u = getelementptr { ptr, [200 x i8] }, ptr %%t%u, i32 0, i32 1\n",
     setup.jmp_buf, setup.handler_frame);
  uint32_t setjmp_result = Emit_Temp ();
  Emit ("  %%t%u = call i32 @setjmp(ptr %%t%u)\n", setjmp_result, setup.jmp_buf);
  uint32_t is_normal = Emit_Temp ();
  Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", is_normal, setjmp_result);
  setup.normal_label  = Emit_Label ();
  setup.handler_label = Emit_Label ();
  Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
     is_normal, setup.normal_label, setup.handler_label);
  cg->block_terminated = true;  // conditional branch terminates block
  return setup;
}

// Emit code to get current exception identity (after exception raised).
// Returns temp holding i64 exception ID.
uint32_t Emit_Current_Exception_Id (void) {
  uint32_t exc_id = Emit_Temp ();
  Emit ("  %%t%u = call i64 @__ada_current_exception()\n", exc_id);
  return exc_id;
}

// Generate exception handler dispatch code for a list of handlers.                                 
// exc_id = temp holding current exception identity (i64)                                           
// end_label = label to branch to after handler completes                                           
// handlers = list of NK_EXCEPTION_HANDLER nodes                                                    
//                                                                                                  
void Generate_Exception_Dispatch (Node_List *handlers,
                     uint32_t exc_id, uint32_t end_label)
{
  uint32_t next_handler = 0;
  for (uint32_t i = 0; i < handlers->count; i++) {
    Syntax_Node *handler = handlers->items[i];
    if (not handler) continue;
    if (next_handler != 0) {
      Emit_Label_Here (next_handler);
      cg->block_terminated = false;
    }
    next_handler = Emit_Label ();
    uint32_t handler_body = Emit_Label ();

    // Check each exception name in the handler
    bool has_others = false;
    for (uint32_t j = 0; j < handler->handler.exceptions.count; j++) {
      Syntax_Node *exc_name = handler->handler.exceptions.items[j];
      if (exc_name->kind == NK_OTHERS) {
        has_others = true;
        break;
      }
    }
    if (has_others) {
      Emit ("  br label %%L%u\n", handler_body);
    } else {
      for (uint32_t j = 0; j < handler->handler.exceptions.count; j++) {
        Syntax_Node *exc_name = handler->handler.exceptions.items[j];
        if (exc_name->symbol) {
          uint32_t exc_ptr = Emit_Temp ();
          Emit ("  %%t%u = ptrtoint ptr ", exc_ptr);
          Emit_Exception_Ref (exc_name->symbol);
          Emit (" to i64\n");
          uint32_t match = Emit_Temp ();
          Emit ("  %%t%u = icmp eq i64 %%t%u, %%t%u\n", match, exc_id, exc_ptr);
          bool is_last = true;
          for (uint32_t k = j + 1; k < handler->handler.exceptions.count; k++) {
            if (handler->handler.exceptions.items[k]->symbol) { is_last = false; break; }
          }
          uint32_t fail_label = is_last ? next_handler : Emit_Label ();
          Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", match, handler_body, fail_label);
          if (not is_last) {
            Emit_Label_Here (fail_label);
            cg->block_terminated = false;
          }
        }
      }
    }
    cg->block_terminated = true;

    // Handler body
    Emit_Label_Here (handler_body);
    cg->block_terminated = false;
    Generate_Statement_List (&handler->handler.statements);
    Emit_Branch_If_Needed (end_label);
  }

  // If no handler matched, reraise
  if (next_handler != 0) {
    Emit_Label_Here (next_handler);
    cg->block_terminated = false;
    Emit ("  call void @__ada_reraise()\n");
    Emit ("  unreachable\n");
    cg->block_terminated = true;
  }
}

// Emit lexicographic array comparison for unconstrained arrays.                                    
// Compares common prefix via memcmp, then compares lengths.                                        
// Returns i32 result: <0 if left<right, 0 if equal, >0 if left>right.                              
// left_ptr/right_ptr are fat pointers, bt = bound type.                                            
//                                                                                                  
uint32_t Emit_Array_Lex_Compare (uint32_t left_ptr, uint32_t right_ptr, uint32_t elem_size, const char *bt)
{

  // Extract bounds and data pointers
  uint32_t left_low = Emit_Fat_Pointer_Low (left_ptr, bt);
  uint32_t left_high = Emit_Fat_Pointer_High (left_ptr, bt);
  uint32_t right_low = Emit_Fat_Pointer_Low (right_ptr, bt);
  uint32_t right_high = Emit_Fat_Pointer_High (right_ptr, bt);
  uint32_t left_len = Emit_Length_From_Bounds (left_low, left_high, bt);
  uint32_t right_len = Emit_Length_From_Bounds (right_low, right_high, bt);
  uint32_t left_data = Emit_Fat_Pointer_Data (left_ptr, bt);
  uint32_t right_data = Emit_Fat_Pointer_Data (right_ptr, bt);

  // min_len = min(left_len, right_len)
  uint32_t min_len = Emit_Min_Value (left_len, right_len, bt);

  // byte_size = min_len * elem_size
  uint32_t byte_size_nat = Emit_Temp ();
  Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_size_nat, bt, min_len, elem_size);
  uint32_t byte_size = Emit_Extend_To_I64 (byte_size_nat, bt);

  // prefix_cmp = memcmp (left_data, right_data, byte_size)
  uint32_t prefix_cmp = Emit_Temp ();
  Emit ("  %%t%u = call i32 @memcmp (ptr %%t%u, ptr %%t%u, i64 %%t%u)\n",
     prefix_cmp, left_data, right_data, byte_size);

  // If prefix differs, return prefix_cmp; otherwise return len_diff
  uint32_t len_diff = Emit_Temp ();
  Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", len_diff, bt, left_len, right_len);
  uint32_t len_diff32 = Emit_Temp ();
  Emit ("  %%t%u = trunc %s %%t%u to i32\n", len_diff32, bt, len_diff);
  uint32_t prefix_zero = Emit_Temp ();
  Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", prefix_zero, prefix_cmp);
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = select i1 %%t%u, i32 %%t%u, i32 %%t%u\n",
     result, prefix_zero, len_diff32, prefix_cmp);
  return result;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.2.2 Named-SSA Fat Pointer Helpers for RTS Functions                                          
//                                                                                                  
// RTS function bodies (emitted as LLVM IR text with named registers like                           
// %fat1, %data, etc.) cannot use the temp-ID helpers above.  These helpers                         
// emit the same patterns but with caller-supplied named SSA prefixes.                              
//                                                                                                  
// With { ptr, ptr } layout, bounds must be allocated and filled first,                             
// then the fat pointer is built as { data_ptr, bounds_ptr }.                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Build a fat pointer { ptr, ptr } from named SSA values.                                          
// Allocates a bounds struct on the secondary stack (not alloca) so the                             
// bounds pointer remains valid after the function returns.  This is critical                       
// for RTS functions that return fat pointers.                                                      
// Emits: %<prefix>_bnd = call ptr @__ada_sec_stack_alloc(i64 N)                                    
//        %<prefix>_lo_gep, %<prefix>_hi_gep = GEP + store                                          
//        %<prefix>1 = insertvalue { ptr, ptr } undef, <data_expr>, 0                               
//        %<prefix>2 = insertvalue { ptr, ptr } %<prefix>1, ptr %<prefix>_bnd, 1                    
// The result value is %<prefix>2.                                                                  
//                                                                                                  
void Emit_Fat_Pointer_Insertvalue_Named (const char *prefix, const char *data_expr,
  const char *low_expr, const char *high_expr, const char *bt)
{
  const char *bounds_struct = Bounds_Type_For (bt);

  // Allocate bounds struct { bt, bt } on secondary stack (survives function return)
  Emit ("  %%%s_bnd = call ptr @__ada_sec_stack_alloc(i64 %d)\n",
     prefix, Bounds_Alloc_Size (bt));

  // Store low bound in native bt
  Emit ("  %%%s_lo_gep = getelementptr %s, ptr %%%s_bnd, i32 0, i32 0\n",
     prefix, bounds_struct, prefix);
  Emit ("  store %s, ptr %%%s_lo_gep\n", low_expr, prefix);

  // Store high bound in native bt
  Emit ("  %%%s_hi_gep = getelementptr %s, ptr %%%s_bnd, i32 0, i32 1\n",
     prefix, bounds_struct, prefix);
  Emit ("  store %s, ptr %%%s_hi_gep\n", high_expr, prefix);

  // Build { ptr, ptr } via insertvalue
  Emit ("  %%%s1 = insertvalue " FAT_PTR_TYPE " undef, %s, 0\n", prefix, data_expr);
  Emit ("  %%%s2 = insertvalue " FAT_PTR_TYPE " %%%s1, ptr %%%s_bnd, 1\n",
     prefix, prefix, prefix);
}

// Extract data pointer, low bound, and high bound from a named SSA fat pointer.                    
// Extracts bounds_ptr (field 1), then GEP+load for low and high.                                   
// src_name:       name of the source fat pointer SSA value  (e.g., "str" for %str)                 
// data_name:      name for extracted data pointer            (e.g., "data")                        
// low_name:       name for extracted low bound               (e.g., "low32")                       
// high_name:      name for extracted high bound              (e.g., "high32")                      
// bt:             bound type string                           (e.g., "i32")                        
//                                                                                                  
void Emit_Fat_Pointer_Extractvalue_Named (const char *src_name, const char *data_name,
  const char *low_name, const char *high_name, const char *bt)
{
  const char *bounds_struct = Bounds_Type_For (bt);

  // Extract data pointer (field 0)
  Emit ("  %%%s = extractvalue " FAT_PTR_TYPE " %%%s, 0\n", data_name, src_name);

  // Extract bounds pointer (field 1)
  Emit ("  %%%s_bptr = extractvalue " FAT_PTR_TYPE " %%%s, 1\n", src_name, src_name);

  // GEP + load low bound in native bt
  Emit ("  %%%s_gep = getelementptr %s, ptr %%%s_bptr, i32 0, i32 0\n",
     low_name, bounds_struct, src_name);
  Emit ("  %%%s = load %s, ptr %%%s_gep\n", low_name, bt, low_name);

  // GEP + load high bound in native bt
  Emit ("  %%%s_gep = getelementptr %s, ptr %%%s_bptr, i32 0, i32 1\n",
     high_name, bounds_struct, src_name);
  Emit ("  %%%s = load %s, ptr %%%s_gep\n", high_name, bt, high_name);
}

// Emit a named-SSA widen from bound type to INTEGER width for intrinsic/RTS use.                   
// If bt is already at INTEGER width, emits a no-op copy (add 0).                                   
// src_name:  name of the source SSA value (in bt)                                                  
// dst_name:  name for the widened value                                                            
// bt:        the bound type string                                                                 
// Uses sext (signed extension) - for unsigned types use the _Unsigned variant.                     
//                                                                                                  
void Emit_Widen_Named_For_Intrinsic (const char *src_name, const char *dst_name, const char *bt)
{
  const char *iat = Integer_Arith_Type ();

  // Already at INTEGER width - emit a trivial add-0 to create the alias
  if (strcmp (bt, iat) == 0) {
    Emit ("  %%%s = add %s %%%s, 0\n", dst_name, iat, src_name);
  } else {
    Emit ("  %%%s = sext %s %%%s to %s\n", dst_name, bt, src_name, iat);
  }
}

// Emit a named-SSA narrow from INTEGER width to bound type after intrinsic/RTS.                    
// If bt is already at INTEGER width, emits a no-op copy.                                           
// src_name:  name of the source SSA value (INTEGER width)                                          
// dst_name:  name for the narrowed bt value                                                        
// bt:        the target bound type string                                                          
//                                                                                                  
void Emit_Narrow_Named_From_Intrinsic (const char *src_name, const char *dst_name, const char *bt)
{
  const char *iat = Integer_Arith_Type ();
  if (strcmp (bt, iat) == 0) {
    Emit ("  %%%s = add %s %%%s, 0\n", dst_name, iat, src_name);
  } else {
    Emit ("  %%%s = trunc %s %%%s to %s\n", dst_name, iat, src_name, bt);
  }
}

// Build a null fat pointer value: { ptr null, ptr null }.
// Returns temp ID of the constructed value.  bt = bound type (unused).
uint32_t Emit_Fat_Pointer_Null (const char *bt) {
  (void)bt;
  uint32_t partial = Emit_Temp ();
  uint32_t result  = Emit_Temp ();
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " undef, ptr null, 0\n", partial);
  Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " %%t%u, ptr null, 1\n", result, partial);
  return result;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.3 Expression Code Generation                                                                 
//                                                                                                  
// Returns the LLVM SSA value ID holding the expression result.                                     
// Every expression yields a value, and in SSA form every value has one definition.                 
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Generate_Lvalue - Return a ptr to the storage location of an lvalue.                             
//                                                                                                  
// Handles:                                                                                         
//   NK_IDENTIFIER   - symbol storage (local, global, or uplevel)                                   
//   NK_SELECTED     - record field offset from base address                                        
//   NK_APPLY        - array element address (indexed component)                                    
//   NK_UNARY_OP/ALL - .ALL dereference (load pointer value)                                        
//                                                                                                  
// Returns a temp holding ptr to the storage.  Caller can then:                                     
//   - load from it  (expression context)                                                           
//   - store to it   (assignment context)                                                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

uint32_t Generate_Lvalue (Syntax_Node *node) {
  if (not node) return 0;

  // NK_IDENTIFIER: return ptr to the symbol's storage
  if (node->kind == NK_IDENTIFIER) {
    Symbol *sym = node->symbol;
    if (not sym) return 0;

    // For RENAMES: redirect to renamed object
    if (sym->renamed_object) {
      return Generate_Lvalue (sym->renamed_object);
    }
    uint32_t addr = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr ", addr);
    Emit_Symbol_Storage (sym);
    Emit (", i64 0  ; lvalue of %.*s\n",
       (int)sym->name.length, sym->name.data);
    return addr;
  }

  // NK_SELECTED: record field access - compute base + field offset
  if (node->kind == NK_SELECTED) {
    Type_Info *prefix_type = node->selected.prefix ? node->selected.prefix->type : NULL;

    // Explicit .ALL dereference: load pointer value
    if (Type_Is_Access (prefix_type) and
      Slice_Equal_Ignore_Case (node->selected.selector, S("ALL"))) {

      // The pointer IS the lvalue target
      uint32_t ptr = Generate_Expression (node->selected.prefix);
      Emit_Access_Check (ptr, prefix_type);
      return ptr;
    }

    // Determine effective record type (handle implicit dereference)
    Type_Info *record_type = prefix_type;
    bool implicit_deref = false;
    if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type) {
      Type_Info *designated = prefix_type->access.designated_type;
      if (Type_Is_Record (designated) or Type_Is_Private (designated)) {
        record_type = designated;
        implicit_deref = true;
      }
    }

    // Resolve generic formal private types to actual types (RM 12.3)
    for (int depth = 0; depth < 10 and record_type and not Type_Is_Record (record_type); depth++) {
      if (Type_Is_Private (record_type) and not record_type->parent_type and
        g_generic_type_map.count > 0 and record_type->name.data) {
        for (uint32_t i = 0; i < g_generic_type_map.count; i++) {
          if (g_generic_type_map.mappings[i].actual_type and
            Slice_Equal_Ignore_Case (record_type->name,
                        g_generic_type_map.mappings[i].formal_name)) {
            record_type = g_generic_type_map.mappings[i].actual_type;
            break;
          }
        }
      } else if ((Type_Is_Private (record_type) or record_type->kind == TYPE_INCOMPLETE) and
             record_type->parent_type) {
        record_type = record_type->parent_type;
      } else {
        break;
      }
    }

    // Find field offset and component index
    if (Type_Is_Record (record_type)) {
      uint32_t byte_offset = 0;
      uint32_t comp_idx = 0;
      for (uint32_t i = 0; i < record_type->record.component_count; i++) {
        if (Slice_Equal_Ignore_Case (
            record_type->record.components[i].name,
            node->selected.selector)) {
          byte_offset = record_type->record.components[i].byte_offset;
          comp_idx = i;
          break;
        }
      }

      // Get base pointer
      uint32_t base;
      if (implicit_deref) {
        base = Generate_Expression (node->selected.prefix);
        Emit_Access_Check (base, prefix_type);
      } else {
        base = Generate_Lvalue (node->selected.prefix);
      }

      // GEP to field offset - use runtime offset for dynamic records
      uint32_t field_ptr = Emit_Temp ();

      // Walk base/parent chain to find rt_global_id (covers subtypes)
      uint32_t rec_rt_id = record_type->rt_global_id;
      if (rec_rt_id == 0) {
        Type_Info *walk = record_type->base_type ? record_type->base_type
                             : record_type->parent_type;
        for (int depth = 0; walk and depth < 10; depth++) {
          if (walk->rt_global_id > 0) { rec_rt_id = walk->rt_global_id; break; }
          walk = walk->base_type ? walk->base_type : walk->parent_type;
        }
      }
      if (rec_rt_id > 0) {
        uint32_t runtime_offset = Emit_Temp ();
        Emit ("  %%t%u = load i64, ptr @__rt_rec_%u_off%u\n",
           runtime_offset, rec_rt_id, comp_idx);
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %%t%u"
           "  ; rt field lvalue\n", field_ptr, base, runtime_offset);
      } else {
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u"
           "  ; field lvalue\n", field_ptr, base, byte_offset);
      }
      return field_ptr;
    }

    // Package-qualified name: return lvalue (address) of the resolved symbol.                      
    // E.g. PACK1.ARG1 where PACK1 is a package and ARG1 is a variable inside it.                   
    // We must NOT fall through to Generate_Expression which would load the value.                  
    //                                                                                              
    if (node->symbol) {
      uint32_t addr = Emit_Temp ();
      Emit ("  %%t%u = getelementptr i8, ptr ", addr);
      Emit_Symbol_Storage (node->symbol);
      Emit (", i64 0  ; lvalue of pkg-qualified %.*s\n",
         (int)node->symbol->name.length, node->symbol->name.data);
      return addr;
    }
  }

  // NK_UNARY_OP with TK_ALL: .ALL dereference - load the pointer value
  if (node->kind == NK_UNARY_OP and node->unary.op == TK_ALL) {
    uint32_t ptr = Generate_Expression (node->unary.operand);
    return ptr;
  }

  // NK_APPLY: array indexed component - compute element address
  if (node->kind == NK_APPLY and node->apply.prefix) {
    Type_Info *prefix_type = node->apply.prefix->type;
    bool implicit_access_deref = false;
    if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type) {
      Type_Info *designated = prefix_type->access.designated_type;
      if (designated->kind == TYPE_ARRAY or designated->kind == TYPE_STRING) {
        implicit_access_deref = true;
        prefix_type = designated;
      }
    }
    if (prefix_type and (prefix_type->kind == TYPE_ARRAY or
              prefix_type->kind == TYPE_STRING)) {

      // Get array base pointer
      Symbol *array_sym = node->apply.prefix->symbol;
      uint32_t base;
      bool has_dynamic_low = false;
      uint32_t dynamic_low = 0, dyn_fat = 0;
      const char *dyn_lv_bt = NULL;
      uint32_t dynamic_high = 0;

      // Access-to-array: load the access value to get array data pointer
      if (implicit_access_deref) {
        base = Generate_Expression (node->apply.prefix);
      } else if (array_sym and (Type_Is_Unconstrained_Array (prefix_type) or
                Type_Has_Dynamic_Bounds (prefix_type))) {

        // Unconstrained array: load fat pointer, extract data ptr
        const char *bt = Array_Bound_Llvm_Type (prefix_type);
        dyn_lv_bt = bt;
        uint32_t fat = Emit_Load_Fat_Pointer (array_sym, bt);
        dyn_fat = fat;
        base = Emit_Fat_Pointer_Data (fat, bt);
        dynamic_low = Emit_Fat_Pointer_Low (fat, bt);
        dynamic_high = Emit_Fat_Pointer_High (fat, bt);
        has_dynamic_low = true;
      } else if (array_sym) {
        base = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr ", base);
        Emit_Symbol_Storage (array_sym);
        Emit (", i64 0  ; array base\n");
      } else {
        base = Generate_Expression (node->apply.prefix);
      }

      // Generate index expression (supports multi-dimensional arrays)
      if (node->apply.arguments.count > 0) {
        const char *idx_lv_iat = Integer_Arith_Type ();
        uint32_t ndims = prefix_type->array.index_count;
        uint32_t nargs = node->apply.arguments.count;
        uint32_t flat_idx;

        // Multi-dimensional: linearize indices.                                                    
        // flat_idx = sum over d of (idx[d] - lo[d]) * stride[d]                                    
        // where stride[d] = product of lengths of dims d+1..ndims-1                                
        //                                                                                          
        if (ndims > 1 and nargs >= ndims) {
          for (uint32_t d = 0; d < ndims; d++) {
            Syntax_Node *arg_d = node->apply.arguments.items[d];
            uint32_t dim_idx = Generate_Expression (arg_d);
            const char *dim_src = Expression_Llvm_Type (arg_d);
            if (dim_src and dim_src[0] == 'i' and strcmp (dim_src, idx_lv_iat) != 0)
              dim_idx = Emit_Convert (dim_idx, dim_src, idx_lv_iat);

            // Subtract low bound for dimension d
            if (has_dynamic_low and dyn_fat) {
              uint32_t lo_d = Emit_Fat_Pointer_Low_Dim (dyn_fat, dyn_lv_bt, d);
              lo_d = Emit_Convert (lo_d, dyn_lv_bt, idx_lv_iat);
              uint32_t adj = Emit_Temp ();
              Emit ("  %%t%u = sub %s %%t%u, %%t%u  ; lv dim %u dyn low adj\n",
                 adj, idx_lv_iat, dim_idx, lo_d, d);
              dim_idx = adj;
            } else {
              int128_t lo_d = Type_Bound_Value (prefix_type->array.indices[d].low_bound);
              if (lo_d != 0) {
                uint32_t adj = Emit_Temp ();
                Emit ("  %%t%u = sub %s %%t%u, %s  ; lv dim %u low adj\n",
                   adj, idx_lv_iat, dim_idx, I128_Decimal (lo_d), d);
                dim_idx = adj;
              }
            }

            // Compute stride: product of inner dimension lengths
            if (has_dynamic_low and dyn_fat) {
              uint32_t stride_val = 0;
              for (uint32_t d2 = d + 1; d2 < ndims; d2++) {
                uint32_t len_d2 = Emit_Fat_Pointer_Length_Dim (dyn_fat, dyn_lv_bt, d2);
                len_d2 = Emit_Convert (len_d2, dyn_lv_bt, idx_lv_iat);
                if (stride_val == 0) {
                  stride_val = len_d2;
                } else {
                  uint32_t product = Emit_Temp ();
                  Emit ("  %%t%u = mul %s %%t%u, %%t%u\n",
                     product, idx_lv_iat, stride_val, len_d2);
                  stride_val = product;
                }
              }
              if (stride_val != 0) {
                uint32_t scaled = Emit_Temp ();
                Emit ("  %%t%u = mul %s %%t%u, %%t%u  ; lv dim %u dyn stride\n",
                   scaled, idx_lv_iat, dim_idx, stride_val, d);
                dim_idx = scaled;
              }
            } else {
              uint32_t stride = 1;
              for (uint32_t d2 = d + 1; d2 < ndims; d2++) {
                int128_t lo2 = Type_Bound_Value (prefix_type->array.indices[d2].low_bound);
                int128_t hi2 = Type_Bound_Value (prefix_type->array.indices[d2].high_bound);
                int128_t cnt2 = hi2 - lo2 + 1;
                if (cnt2 > 0) stride *= (uint32_t)cnt2;
              }
              if (stride > 1) {
                uint32_t scaled = Emit_Temp ();
                Emit ("  %%t%u = mul %s %%t%u, %u  ; lv dim %u stride\n",
                   scaled, idx_lv_iat, dim_idx, stride, d);
                dim_idx = scaled;
              }
            }
            if (d == 0) {
              flat_idx = dim_idx;
            } else {
              uint32_t sum = Emit_Temp ();
              Emit ("  %%t%u = add %s %%t%u, %%t%u\n",
                 sum, idx_lv_iat, flat_idx, dim_idx);
              flat_idx = sum;
            }
          }

        // Single-dimension indexing
        } else {
          Syntax_Node *arg = node->apply.arguments.items[0];
          uint32_t idx = Generate_Expression (arg);
          const char *idx_src = Expression_Llvm_Type (arg);
          if (idx_src and idx_src[0] == 'i' and strcmp (idx_src, idx_lv_iat) != 0)
            idx = Emit_Convert (idx, idx_src, idx_lv_iat);
          if (has_dynamic_low and dynamic_high) {
            uint32_t lo_chk = Emit_Convert (dynamic_low, dyn_lv_bt, idx_lv_iat);
            uint32_t hi_chk = Emit_Convert (dynamic_high, dyn_lv_bt, idx_lv_iat);
            idx = Emit_Index_Check (idx, lo_chk, hi_chk, idx_lv_iat, prefix_type);
          } else if (prefix_type->array.index_count > 0) {
            int128_t lo = Type_Bound_Value (prefix_type->array.indices[0].low_bound);
            int128_t hi = Type_Bound_Value (prefix_type->array.indices[0].high_bound);
            if (lo != hi or lo != 0) {
              uint32_t lo_t = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %s  ; lv low bound\n", lo_t, idx_lv_iat, I128_Decimal (lo));
              uint32_t hi_t = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %s  ; lv high bound\n", hi_t, idx_lv_iat, I128_Decimal (hi));
              idx = Emit_Index_Check (idx, lo_t, hi_t, idx_lv_iat, prefix_type);
            }
          }
          if (has_dynamic_low) {
            uint32_t dynamic_low_conv = Emit_Convert (dynamic_low, dyn_lv_bt, idx_lv_iat);
            uint32_t adj = Emit_Temp ();
            Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", adj, idx_lv_iat, idx, dynamic_low_conv);
            idx = adj;
          } else {
            int128_t low_bound = Array_Low_Bound (prefix_type);
            if (low_bound != 0) {
              uint32_t adj = Emit_Temp ();
              Emit ("  %%t%u = sub %s %%t%u, %s\n",
                 adj, idx_lv_iat, idx, I128_Decimal (low_bound));
              idx = adj;
            }
          }
          flat_idx = idx;
        }

        // Compute element address
        Type_Info *elem_type_info = prefix_type->array.element_type;
        const char *elem_type = Type_To_Llvm (elem_type_info);
        uint32_t ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr %s, ptr %%t%u, %s %%t%u  ; elem lvalue\n",
           ptr, elem_type, base, idx_lv_iat, flat_idx);
        return ptr;
      }
    }
  }

  // Fallback: treat expression result as pointer (for composite types
  // that Generate_Expression already returns as ptr)
  return Generate_Expression (node);
}

// Generate code to evaluate a type bound at runtime.
// Returns the temp register containing the bound value.
uint32_t Generate_Bound_Value (Type_Bound bound, const char *target_type) {
  uint32_t   result     = cg->temp_id++;
  const char *bound_type = target_type ? target_type : Integer_Arith_Type ();

  if (bound.kind == BOUND_INTEGER) {
    Emit ("  %%t%u = add %s 0, %s  ; bound (integer)\n",
       result, bound_type, I128_Decimal (bound.int_value));
    Temp_Set_Type (result, bound_type);
    return result;
  }

  // Note: "double" here is intentional - %e produces a double-precision                            
  // LLVM IR literal, matching the fptosi source type.  This converts a                             
  // real-valued bound constant to integer, not a typed expression.                                 
  //                                                                                                
  if (bound.kind == BOUND_FLOAT) {
    Emit ("  %%t%u = fptosi double %e to %s  ; bound (float)\n",
       result, bound.float_value, bound_type);
    Temp_Set_Type (result, bound_type);
    return result;
  }

  // First try compile-time evaluation
  if (bound.kind == BOUND_EXPR and bound.expr) {
    double val = Eval_Const_Numeric (bound.expr);
    if (val == val) {  // Not NaN
      Emit ("  %%t%u = add %s 0, %lld  ; bound (const expr)\n",
         result, bound_type, (long long)val);
      Temp_Set_Type (result, bound_type);
      return result;
    }

    // Must evaluate expression at runtime
    uint32_t expr_result = Generate_Expression (bound.expr);
    if (bound_type) {
      const char *expr_llvm = Expression_Llvm_Type (bound.expr);
      if (expr_llvm and strcmp (expr_llvm, bound_type) != 0)
        expr_result = Emit_Convert (expr_result, expr_llvm, bound_type);
      Temp_Set_Type (expr_result, bound_type);
    }
    return expr_result;
  }
  Emit ("  %%t%u = add %s 0, 0  ; bound (unknown)\n", result, bound_type);
  Temp_Set_Type (result, bound_type);
  return result;
}
uint32_t Generate_Integer_Literal (Syntax_Node *node) {
  uint32_t result = Emit_Temp ();

  // Emit literal at the expression's native type width
  const char *lit_type = node->type ? Type_To_Llvm (node->type) : Integer_Arith_Type ();

  // Value may exceed int64_t range - use Big_Integer_To_Int128
  if (node->integer_lit.big_value) {
    int128_t val128;
    if (Big_Integer_To_Int128 (node->integer_lit.big_value, &val128)) {
      Emit ("  %%t%u = add %s 0, %s\n", result, lit_type, I128_Decimal (val128));

    // Value doesn't fit in int128 - fall back to int64
    } else {
      Emit ("  %%t%u = add %s 0, %lld\n", result, lit_type, (long long)node->integer_lit.value);
    }
  } else {
    Emit ("  %%t%u = add %s 0, %lld\n", result, lit_type, (long long)node->integer_lit.value);
  }
  Temp_Set_Type (result, lit_type);
  return result;
}
uint32_t Generate_Real_Literal (Syntax_Node *node) {
  uint32_t result = Emit_Temp ();

  // Use IEEE 754 hex encoding for full precision.                                                  
  // This preserves all 53 bits of mantissa (vs %f which loses precision).                          
  // The double value was computed from Big_Real during parsing.                                    
  //                                                                                                
  double   value = node->real_lit.value;
  uint64_t raw_bits;
  memcpy (&raw_bits, &value, sizeof (raw_bits));
  Emit ("  %%t%u = fadd double 0.0, 0x%016llX\n", result, (unsigned long long)raw_bits);
  Temp_Set_Type (result, "double");
  return result;
}
uint32_t Generate_String_Literal (Syntax_Node *node) {

  // Allocate string constant
  uint32_t str_id = cg->string_id++;
  uint32_t len = node->string_val.text.length;

  // Generate global constant to buffer (without null terminator for Ada strings)
  // Use linkonce_odr to allow merging of duplicate string constants across units
  Emit_String_Const ("@.str%u = linkonce_odr unnamed_addr constant [%u x i8] c\"", str_id, len);
  for (uint32_t i = 0; i < len; i++) {
    char ch = node->string_val.text.data[i];
    if (ch >= 32 and ch < 127 and ch != '"' and ch != '\\') {
      Emit_String_Const_Char (ch);
    } else {
      Emit_String_Const ("\\%02X", (unsigned char)ch);
    }
  }
  Emit_String_Const ("\"\n");

  // Get pointer to string data
  uint32_t data_ptr = Emit_Temp ();
  Emit ("  %%t%u = getelementptr [%u x i8], ptr @.str%u, i64 0, i64 0\n",
     data_ptr, len, str_id);

  // Return fat pointer with Ada STRING bounds.                                                     
  // Default is 1..length (RM 4.2(9) - POSITIVE'FIRST).                                             
  // When the applicable index constraint specifies different bounds                                
  // (e.g. STRING (3..5)), use those bounds instead.                                                
  //                                                                                                
  int128_t lo = 1, hi = (int128_t)len;
  Type_Info *string_type = node->type;
  if (string_type and string_type->array.is_constrained and string_type->array.index_count > 0) {
    Type_Bound lb = string_type->array.indices[0].low_bound;
    Type_Bound hb = string_type->array.indices[0].high_bound;
    if (lb.kind == BOUND_INTEGER and hb.kind == BOUND_INTEGER) {
      lo = lb.int_value;
      hi = hb.int_value;
    }
  }
  return Emit_Fat_Pointer (data_ptr, lo, hi, Array_Bound_Llvm_Type (node->type));
}
uint32_t Generate_Identifier (Syntax_Node *node) {
  Symbol *sym = node->symbol;
  if (not sym) {
    Report_Error (node->location, "unresolved identifier in codegen");
    return 0;
  }

  // Generic formal object substitution: if this is a formal object inside
  // a generic instantiation, generate code for the actual expression.
  if (cg->current_instance and cg->current_instance->generic_actuals) {
    for (uint32_t i = 0; i < cg->current_instance->generic_actual_count; i++) {
      if (cg->current_instance->generic_actuals[i].actual_expr and
        Slice_Equal_Ignore_Case (sym->name,
          cg->current_instance->generic_actuals[i].formal_name)) {
        return Generate_Expression (cg->current_instance->generic_actuals[i].actual_expr);
      }
    }
  }

  // For RENAMES: redirect to the renamed object
  if (sym->renamed_object) {
    return Generate_Expression (sym->renamed_object);
  }
  uint32_t t = Emit_Temp ();
  Type_Info *ty = sym->type;

  // Resolve generic formal types to their actuals for correct LLVM type sizing.
  // E.g. generic formal T (<>) mapped to COLOR (i8) - must load as i8, not i32.
  if (cg->current_instance and ty)
    ty = Resolve_Generic_Actual_Type (ty);
  switch (sym->kind) {

    // Discriminant with aggregate-local temp: load from temp alloca
    case SYMBOL_VARIABLE:
    case SYMBOL_PARAMETER:
    case SYMBOL_DISCRIMINANT: {
      if (sym->kind == SYMBOL_DISCRIMINANT and sym->disc_agg_temp > 0) {
        const char *type_str = Type_To_Llvm (ty);
        Emit ("  %%t%u = load %s, ptr %%t%u\n", t, type_str, sym->disc_agg_temp);
        Temp_Set_Type (t, type_str);
        break;
      }

      // Check if symbol is stored as a fat pointer (dynamic/unconstrained arrays).                 
      // This flag is set at symbol creation time when bounds were dynamic.                         
      // Must take priority over Type_Is_Constrained_Array which may change                         
      // after bounds are resolved to static values.                                                
      //                                                                                            
      if (sym->needs_fat_ptr_storage) {
        const char *dbt = Array_Bound_Llvm_Type (ty);
        uint32_t fat = Emit_Load_Fat_Pointer (sym, dbt);
        return fat;
      }

      // Also check at codegen time in case symbol was created via a path                           
      // that doesn't go through Symbol_Add (e.g., semantic pass creates                            
      // constrained subtypes with dynamic bounds).                                                 
      //                                                                                            
      if (ty and (Type_Has_Dynamic_Bounds (ty) or Type_Is_Unconstrained_Array (ty)) and
        (sym->kind == SYMBOL_VARIABLE or sym->kind == SYMBOL_PARAMETER)) {
        const char *dbt = Array_Bound_Llvm_Type (ty);
        uint32_t fat = Emit_Load_Fat_Pointer (sym, dbt);
        return fat;
      }

      // Constrained arrays with static bounds are flat allocas.                                    
      // The alloca address IS the value - no load needed.                                          
      // Return pointer to the alloca directly (no load)                                            
      //                                                                                            
      if (Type_Is_Constrained_Array (ty)) {
        Emit ("  %%t%u = getelementptr i8, ptr ", t);
        Emit_Symbol_Storage (sym);
        Emit (", i64 0  ; constrained array ref\n");
        break;
      }

      // Records are composite types stored as [N x i8] allocas.                                    
      // Like constrained arrays, the alloca address IS the value -                                 
      // no load needed.                                                                            
      //                                                                                            
      if (Type_Is_Record (ty)) {
        Emit ("  %%t%u = getelementptr i8, ptr ", t);
        Emit_Symbol_Storage (sym);
        Emit (", i64 0  ; record ref\n");
        break;
      }
      const char *type_str = Type_To_Llvm (ty);
      Emit ("  %%t%u = load %s, ptr ", t, type_str);
      Emit_Symbol_Storage (sym);
      Emit ("\n");
      Temp_Set_Type (t, type_str);

      // No widening - value stays at native type width.                                            
      // Expression_Llvm_Type returns the native type, and Emit_Convert                             
      // handles any needed conversions at use sites.                                               
      //                                                                                            
    } break;

    // Find position in enumeration
    case SYMBOL_CONSTANT:
    case SYMBOL_LITERAL:

      // Enumeration literal or constant
      if (sym->kind == SYMBOL_LITERAL and Type_Is_Enumeration (sym->type)) {
        int64_t pos = 0;
        for (uint32_t i = 0; i < sym->type->enumeration.literal_count; i++) {
          if (Slice_Equal_Ignore_Case (sym->type->enumeration.literals[i], sym->name)) {
            pos = i;
            break;
          }
        }
        { const char *ety = Type_To_Llvm (sym->type);
        Emit ("  %%t%u = add %s 0, %lld\n", t, ety, (long long)pos);
        Temp_Set_Type (t, ety); }

      // Boolean literal: TRUE=1, FALSE=0 - native type width
      } else if (sym->kind == SYMBOL_LITERAL and Type_Is_Boolean (ty)) {
        int64_t pos = Slice_Equal_Ignore_Case (sym->name, S("TRUE")) ? 1 : 0;
        { const char *bty = Type_To_Llvm (ty);
        Emit ("  %%t%u = add %s 0, %lld\n", t, bty, (long long)pos);
        Temp_Set_Type (t, bty); }

      // Character literal - native type width
      } else if (sym->kind == SYMBOL_LITERAL and Type_Is_Character (ty)) {
        { const char *cty = Type_To_Llvm (ty);
        Emit ("  %%t%u = add %s 0, 0  ; character literal\n", t, cty);
        Temp_Set_Type (t, cty); }

      // Constant stored as fat pointer (dynamic bounds at declaration)
      } else if (sym->needs_fat_ptr_storage) {
        const char *dbt = Array_Bound_Llvm_Type (ty);
        uint32_t fat = Emit_Load_Fat_Pointer (sym, dbt);
        return fat;

      // Constrained array constant - return pointer to data.
      // Fat pointer wrapping is handled at call sites when needed.
      } else if (Type_Is_Constrained_Array (ty)) {
        Emit ("  %%t%u = getelementptr i8, ptr ", t);
        Emit_Symbol_Storage (sym);
        Emit (", i64 0\n");
        return t;

      // Named number (constant without explicit type) - evaluate initializer                       
      // Named numbers in Ada are compile-time constants with no storage.                           
      // Per RM 3.2.2: "A named number provides a name for a numeric value                          
      // known at compile time."                                                                    
      //                                                                                            
      } else if (sym->kind == SYMBOL_CONSTANT and sym->is_named_number) {
        Syntax_Node *decl = sym->declaration;

        // Try compile-time evaluation first.  Named numbers are                                    
        // UNIVERSAL_INTEGER / UNIVERSAL_REAL and may involve                                       
        // expressions like 2**31 that overflow i32 at runtime.                                     
        //                                                                                          
        if (decl and decl->kind == NK_OBJECT_DECL and decl->object_decl.init) {
          double cv = Eval_Const_Numeric (decl->object_decl.init);
          const char *named_t = ty ? Type_To_Llvm (ty) : Integer_Arith_Type ();
          if (cv == cv) {  // not NaN - constant evaluation succeeded
            if (Type_Is_Float_Representation (ty)) {
              uint64_t bits; memcpy (&bits, &cv, sizeof (bits));
              Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; named number %g\n",
                 t, (unsigned long long)bits, cv);
              Temp_Set_Type (t, "double");
            } else {
              Emit ("  %%t%u = add %s 0, %lld  ; named number\n",
                 t, named_t, (long long)(int64_t)cv);
              Temp_Set_Type (t, named_t);
            }

          // Fallback to codegen for initializer expression
          } else {
            uint32_t result = Generate_Expression (decl->object_decl.init);
            const char *init_t = Expression_Llvm_Type (decl->object_decl.init);
            return Emit_Convert (result, init_t, named_t);
          }

        // Predefined constant (e.g. ASCII.NUL) with value in frame_offset
        } else if (sym->frame_offset != 0 or Type_Is_Character (ty)) {
          const char *named_t = ty ? Type_To_Llvm (ty) : Integer_Arith_Type ();
          Emit ("  %%t%u = add %s 0, %lld  ; predefined constant\n",
             t, named_t, (long long)sym->frame_offset);
          Temp_Set_Type (t, named_t);

        // Fallback: no initializer found for named number
        } else {
          Emit ("  %%t%u = add %s 0, 0  ; named number without init\n", t, Integer_Arith_Type ());
          Temp_Set_Type (t, Integer_Arith_Type ());
        }
      } else if (sym->kind == SYMBOL_CONSTANT and not sym->is_named_number and
             Type_Is_Record (ty)) {

        // Record constant: return pointer to storage (no load)
        Emit ("  %%t%u = getelementptr i8, ptr ", t);
        Emit_Symbol_Storage (sym);
        Emit (", i64 0  ; record constant ref\n");

      // Typed constant: try compile-time evaluation first.                                         
      // Constants from WITHed packages (e.g. TEXT_IO.UNBOUNDED)                                    
      // may not have global definitions if the package body was                                    
      // separately compiled.  Inlining avoids unresolved symbols.                                  
      //                                                                                            
      } else if (sym->kind == SYMBOL_CONSTANT and not sym->is_named_number) {
        Syntax_Node *cdecl = sym->declaration;
        double cv = (cdecl and cdecl->kind == NK_OBJECT_DECL and cdecl->object_decl.init)
          ? Eval_Const_Numeric (cdecl->object_decl.init) : __builtin_nan("");
        const char *type_str = Type_To_Llvm (ty);

        // Integer constant with known value - inline it
        if (cv == cv and not Type_Is_Float_Representation (ty)) {
          Emit ("  %%t%u = add %s 0, %lld  ; typed constant\n",
             t, type_str, (long long)(int64_t)cv);
          Temp_Set_Type (t, type_str);
        } else if (cv == cv and Type_Is_Float_Representation (ty)) {
          const char *ftype = type_str;  // float or double from Type_To_Llvm
          char comment[64];
          snprintf (comment, sizeof (comment), "typed constant %g", cv);
          Emit_Float_Constant (t, ftype, cv, comment);
          Temp_Set_Type (t, ftype);

        // Cannot evaluate at compile time - load from storage
        } else {
          Emit ("  %%t%u = load %s, ptr ", t, type_str);
          Emit_Symbol_Storage (sym);
          Emit ("\n");
          Temp_Set_Type (t, type_str);
        }

      // Unknown literal type - emit zero as fallback
      } else {
        Emit ("  %%t%u = add %s 0, 0  ; unknown literal\n", t, Integer_Arith_Type ());
      }
      break;

    // Parameterless function call: F is syntactically an identifier                                
    // but semantically a function call with zero arguments.                                        
    // Generic formal subprogram substitution: if this is a formal subprogram                       
    // inside a generic instantiation, substitute with actual.                                      
    //                                                                                              
    case SYMBOL_FUNCTION: {
      Symbol *actual = sym;
      if (cg->current_instance and cg->current_instance->generic_actuals) {
        for (uint32_t i = 0; i < cg->current_instance->generic_actual_count; i++) {
          if (cg->current_instance->generic_actuals[i].actual_subprogram and
            Slice_Equal_Ignore_Case (sym->name,
              cg->current_instance->generic_actuals[i].formal_name)) {
            actual = cg->current_instance->generic_actuals[i].actual_subprogram;
            break;
          }
        }
      }

      // Check if actual is an enumeration literal (e.g., RED, YELLOW)
      if (actual->kind == SYMBOL_LITERAL and Type_Is_Enumeration (actual->type)) {
        int64_t pos = 0;
        for (uint32_t i = 0; i < actual->type->enumeration.literal_count; i++) {
          if (Slice_Equal_Ignore_Case (actual->type->enumeration.literals[i],
                        actual->name)) {
            pos = i;
            break;
          }
        }
        { const char *efty = Type_To_Llvm (actual->type);
        Emit ("  %%t%u = add %s 0, %lld  ; enum literal as function\n",
           t, efty, (long long)pos);
        Temp_Set_Type (t, efty); }

      // Generate actual function call
      } else if (actual->kind == SYMBOL_FUNCTION) {
        const char *ret_type = actual->return_type ?
          Type_To_Llvm_Sig (actual->return_type) : Integer_Arith_Type ();
        bool callee_is_nested = Subprogram_Needs_Static_Chain (actual);
        uint32_t frame_pre = callee_is_nested ?
          Precompute_Nested_Frame_Arg (actual) : 0;
        Emit ("  %%t%u = call %s @", t, ret_type);
        Emit_Symbol_Name (actual);
        if (callee_is_nested) {
          Emit ("(");
          Emit_Nested_Frame_Arg (actual, frame_pre);
          Emit (")\n");
        } else {
          Emit ("()\n");
        }

        // No widening - return value stays at native type width.
        Temp_Set_Type (t, ret_type);

      // Fallback for other symbol kinds
      } else {
        Emit ("  %%t%u = add %s 0, 0  ; unhandled function symbol\n", t, Integer_Arith_Type ());
      }
    } break;
    default:

      // Unhandled symbol kind - emit zero as fallback
      Emit ("  %%t%u = add %s 0, 0  ; unhandled symbol kind\n", t, Integer_Arith_Type ());
  }
  return t;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.3.1 Implicit Operators for Composite Types                                                   
//                                                                                                  
// Ada requires equality operators for all non-limited types. For composite                         
// types (records, arrays), equality is defined component-wise.                                     
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Generate equality comparison for record types (component-by-component)
uint32_t Generate_Record_Equality (uint32_t left_ptr,
                     uint32_t right_ptr, Type_Info *record_type) {
  if (not Type_Is_Record (record_type) or
    record_type->record.component_count == 0) {

    // Empty record or invalid - always equal
    if (not Type_Is_Record (record_type))
      fprintf (stderr, "warning: record equality called on non-record type\n");
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = add i1 0, 1  ; empty record equality\n", t);
    return t;
  }
  uint32_t result = 0;
  for (uint32_t i = 0; i < record_type->record.component_count; i++) {
    Component_Info *comp = &record_type->record.components[i];
    const char *comp_llvm_type = Type_To_Llvm (comp->component_type);

    // Get pointers to components
    uint32_t left_gep = Emit_Temp ();
    uint32_t right_gep = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
       left_gep, left_ptr, comp->byte_offset);
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
       right_gep, right_ptr, comp->byte_offset);

    // Compare component - handle arrays/strings specially
    uint32_t cmp;
    Type_Info *comp_type = comp->component_type;
    bool is_fat_ptr_access = Type_Is_Access (comp_type) and
      Type_Needs_Fat_Pointer (comp_type);
    if (Type_Is_Unconstrained_Array (comp_type) or
      (not Type_Is_Constrained_Array (comp_type) and Type_Is_String (comp_type))) {

      // Unconstrained array/string - load fat pointer values from storage
      const char *comp_bt = Array_Bound_Llvm_Type (comp_type);
      uint32_t left_fat = Emit_Load_Fat_Pointer_From_Temp (left_gep, comp_bt);
      uint32_t right_fat = Emit_Load_Fat_Pointer_From_Temp (right_gep, comp_bt);
      cmp = Generate_Array_Equality (left_fat, right_fat, comp_type);

    // Constrained array with dynamic bounds (discriminant-dependent).                              
    // Data stored inline; compute runtime byte size from discriminant.                             
    // For ARRAY (LOW..DISC) OF ELEM: size = max(0, disc - low + 1) * elem_size                     
    //                                                                                              
    } else if (Type_Is_Constrained_Array (comp_type) and Type_Has_Dynamic_Bounds (comp_type)) {
      uint32_t elem_size = comp_type->array.element_type ? comp_type->array.element_type->size : 1;
      if (elem_size == 0) elem_size = 1;
      int64_t low_val = 1;
      if (comp_type->array.index_count > 0 and
        comp_type->array.indices[0].low_bound.kind == BOUND_INTEGER)
        low_val = (int64_t)comp_type->array.indices[0].low_bound.int_value;

      // Find the discriminant that controls the high bound.
      // Match the bound expression's symbol against the record's discriminant components.
      uint32_t disc_offset = 0;
      const char *disc_llvm = "i32";
      if (comp_type->array.index_count > 0 and
        comp_type->array.indices[0].high_bound.kind == BOUND_EXPR and
        comp_type->array.indices[0].high_bound.expr) {
        Symbol *bsym = comp_type->array.indices[0].high_bound.expr->symbol;
        for (uint32_t d = 0; d < record_type->record.discriminant_count; d++) {
          Component_Info *dc = &record_type->record.components[d];
          if (bsym and Slice_Equal_Ignore_Case (dc->name, bsym->name)) {
            disc_offset = dc->byte_offset;
            disc_llvm = Type_To_Llvm (dc->component_type);
            if (not disc_llvm) disc_llvm = "i32";
            break;
          }
        }
      }

      // Load discriminant from left record and compute memcmp size
      uint32_t disc_gep  = Emit_Temp ();
      uint32_t disc_val  = Emit_Temp ();
      Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
         disc_gep, left_ptr, disc_offset);
      Emit ("  %%t%u = load %s, ptr %%t%u\n", disc_val, disc_llvm, disc_gep);
      uint32_t extent = Emit_Temp ();
      uint32_t size64 = Emit_Temp ();
      Emit ("  %%t%u = sub %s %%t%u, %lld\n",
         extent, disc_llvm, disc_val, (long long)(low_val - 1));
      if (elem_size > 1) {
        uint32_t mul = Emit_Temp ();
        Emit ("  %%t%u = mul %s %%t%u, %u\n", mul, disc_llvm, extent, elem_size);
        extent = mul;
      }

      // Clamp to 0 for null arrays
      uint32_t is_neg  = Emit_Temp ();
      uint32_t clamped = Emit_Temp ();
      Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", is_neg, disc_llvm, extent);
      Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
         clamped, is_neg, disc_llvm, disc_llvm, extent);
      Emit ("  %%t%u = sext %s %%t%u to i64\n", size64, disc_llvm, clamped);
      uint32_t memcmp_result = Emit_Temp ();
      uint32_t memcmp_eq     = Emit_Temp ();
      Emit ("  %%t%u = call i32 @memcmp (ptr %%t%u, ptr %%t%u, i64 %%t%u)\n",
         memcmp_result, left_gep, right_gep, size64);
      Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", memcmp_eq, memcmp_result);
      cmp = memcmp_eq;

    // Constrained array with static bounds - use array equality directly on pointers
    } else if (Type_Is_Constrained_Array (comp_type)) {
      cmp = Generate_Array_Equality (left_gep, right_gep, comp_type);

    // Nested record - recurse
    } else if (Type_Is_Record (comp_type)) {
      cmp = Generate_Record_Equality (left_gep, right_gep, comp_type);

    // ACCESS to unconstrained array - compare fat pointer identity
    } else if (is_fat_ptr_access) {
      const char *acc_bt = Array_Bound_Llvm_Type (comp_type->access.designated_type);
      uint32_t left_val = Emit_Load_Fat_Pointer_From_Temp (left_gep, acc_bt);
      uint32_t right_val = Emit_Load_Fat_Pointer_From_Temp (right_gep, acc_bt);
      cmp = Emit_Fat_Pointer_Compare (left_val, right_val, acc_bt);

    // Scalar type - load and compare
    } else {
      uint32_t left_val = Emit_Temp ();
      uint32_t right_val = Emit_Temp ();
      Emit ("  %%t%u = load %s, ptr %%t%u\n", left_val, comp_llvm_type, left_gep);
      Emit ("  %%t%u = load %s, ptr %%t%u\n", right_val, comp_llvm_type, right_gep);
      cmp = Emit_Temp ();
      if (Type_Is_Float_Representation (comp_type)) {
        Emit ("  %%t%u = fcmp oeq %s %%t%u, %%t%u\n",
           cmp, comp_llvm_type, left_val, right_val);
      } else {
        Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n",
           cmp, comp_llvm_type, left_val, right_val);
      }
    }

    // AND with previous results
    if (i == 0) {
      result = cmp;
    } else {
      uint32_t and_result = Emit_Temp ();
      Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", and_result, result, cmp);
      result = and_result;
    }
  }
  return result;
}

// Generate equality comparison for constrained array types (element-by-element)
uint32_t Generate_Array_Equality (uint32_t left_ptr,
                    uint32_t right_ptr, Type_Info *array_type) {
  if (not Type_Is_Array_Like (array_type)) {
    fprintf (stderr, "warning: array equality called on non-array type\n");
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = add i1 0, 1  ; invalid array equality\n", t);
    return t;
  }

  // For constrained arrays with static bounds, use memcmp.                                         
  // Dynamic-bounds constrained arrays use fat pointers at runtime,                                 
  // so fall through to the unconstrained path.                                                     
  //                                                                                                
  if (array_type->array.is_constrained and not Type_Has_Dynamic_Bounds (array_type)) {
    int128_t count = Array_Element_Count (array_type);
    uint32_t elem_size = array_type->array.element_type ?
               array_type->array.element_type->size : 4;
    int64_t total_size = count * elem_size;

    // If inputs are actually fat pointers, extract data pointers
    const char *lty = Temp_Get_Type (left_ptr);
    if (lty and Llvm_Type_Is_Fat_Pointer (lty)) {
      const char *bt = Array_Bound_Llvm_Type (array_type);
      left_ptr = Emit_Fat_Pointer_Data (left_ptr, bt);
    }
    const char *rty = Temp_Get_Type (right_ptr);
    if (rty and Llvm_Type_Is_Fat_Pointer (rty)) {
      const char *bt = Array_Bound_Llvm_Type (array_type);
      right_ptr = Emit_Fat_Pointer_Data (right_ptr, bt);
    }

    // Use memcmp for byte-by-byte comparison
    return Emit_Memcmp_Eq (left_ptr, right_ptr, 0, total_size, false);
  }

  // Unconstrained array equality (per RM 4.5.2):                                                   
  // Two arrays are equal iff they have the same length and matching components.                    
  // Bounds themselves need not match-only length and content.                                      
  //                                                                                                
  // For fat pointers: compare lengths, then data if lengths match.                                 
  // Use select instead of phi to avoid block label complications.                                  
  //                                                                                                

  // For unconstrained arrays, left_ptr/right_ptr are fat pointer VALUES                            
  // (not pointers to storage).  All callers must ensure they pass loaded                           
  // fat pointer values: { ptr, { bound, bound } }.                                                 
  //                                                                                                

  // Extract bounds and compute lengths for ALL dimensions (RM 4.5.2).                              
  // For multidimensional arrays, each dimension's length must match and                            
  // the total byte count is the product of all dimension lengths × elem_size.                      
  //                                                                                                
  const char *aeq_bt = Array_Bound_Llvm_Type (array_type);
  const char *aeq_iat = Integer_Arith_Type ();
  uint32_t ndims = array_type->array.index_count;
  if (ndims < 1) ndims = 1;

  // Compare lengths for each dimension and accumulate total element count
  uint32_t all_len_eq = 0;  // AND of all dimension length comparisons
  uint32_t total_elems = 0;  // product of all dimension lengths
  for (uint32_t d = 0; d < ndims; d++) {
    uint32_t ll = Emit_Fat_Pointer_Low_Dim (left_ptr, aeq_bt, d);
    uint32_t lh = Emit_Fat_Pointer_High_Dim (left_ptr, aeq_bt, d);
    uint32_t rl = Emit_Fat_Pointer_Low_Dim (right_ptr, aeq_bt, d);
    uint32_t rh = Emit_Fat_Pointer_High_Dim (right_ptr, aeq_bt, d);
    uint32_t l_len = Emit_Length_From_Bounds (ll, lh, aeq_bt);
    uint32_t r_len = Emit_Length_From_Bounds (rl, rh, aeq_bt);
    uint32_t dim_eq = Emit_Temp ();
    Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n", dim_eq, aeq_bt, l_len, r_len);
    if (d == 0) {
      all_len_eq = dim_eq;
      total_elems = Emit_Convert (l_len, aeq_bt, aeq_iat);
    } else {
      uint32_t merged = Emit_Temp ();
      Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", merged, all_len_eq, dim_eq);
      all_len_eq = merged;
      uint32_t l_conv = Emit_Convert (l_len, aeq_bt, aeq_iat);
      uint32_t prod = Emit_Temp ();
      Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", prod, aeq_iat, total_elems, l_conv);
      total_elems = prod;
    }
  }

  // Extract data pointers
  uint32_t left_data = Emit_Fat_Pointer_Data (left_ptr, aeq_bt);
  uint32_t right_data = Emit_Fat_Pointer_Data (right_ptr, aeq_bt);

  // Compute byte size for memcmp: total_elems * scalar_elem_size
  uint32_t elem_size = array_type->array.element_type ?
             array_type->array.element_type->size : 1;
  uint32_t byte_size = Emit_Temp ();
  Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_size, aeq_iat, total_elems, elem_size);
  uint32_t byte_size_64 = Emit_Extend_To_I64 (byte_size, aeq_iat);

  // Call memcmp
  uint32_t data_eq = Emit_Memcmp_Eq (left_data, right_data, byte_size_64, 0, true);

  // Result: all dimension lengths match AND data matches
  uint32_t result = Emit_Temp ();
  Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", result, all_len_eq, data_eq);
  return result;
}

// Generate the address of a composite type expression (for equality comparison)
uint32_t Generate_Composite_Address (Syntax_Node *node) {

  // Generate the ADDRESS of an lvalue (identifier, selected, indexed)
  if (node->kind == NK_IDENTIFIER) {
    Symbol *sym = node->symbol;
    if (sym) {
      uint32_t t = Emit_Temp ();
      Emit ("  %%t%u = getelementptr i8, ptr ", t);
      Emit_Symbol_Storage (sym);
      Emit (", i64 0\n");
      return t;
    }
  }

  // Field access: Rec.Field - compute address of field
  if (node->kind == NK_SELECTED) {
    Type_Info *prefix_type = node->selected.prefix ? node->selected.prefix->type : NULL;

    // Handle implicit dereference: U.A where U is access-to-record.
    // First load the pointer, then compute field address.
    Type_Info *record_type = prefix_type;
    bool implicit_deref = false;
    if (Type_Is_Access (prefix_type) and
      Type_Is_Record (prefix_type->access.designated_type)) {
      record_type = prefix_type->access.designated_type;
      implicit_deref = true;
    }
    if (Type_Is_Record (record_type)) {
      uint32_t base;

      // Load the access value (pointer to record)
      if (implicit_deref) {
        Symbol *access_sym = node->selected.prefix->symbol;
        base = Emit_Temp ();
        if (access_sym) {
          Emit ("  %%t%u = load ptr, ptr ", base);
          Emit_Symbol_Storage (access_sym);
          Emit ("  ; implicit deref for field address\n");
        } else {
          uint32_t ptr = Generate_Expression (node->selected.prefix);
          base = ptr;  // Already a ptr from access expr
        }
        Emit_Access_Check (base, prefix_type);
      } else {
        base = Generate_Composite_Address (node->selected.prefix);
      }

      // Find field offset and component index
      uint32_t offset = 0;
      uint32_t comp_idx = 0;
      for (uint32_t i = 0; i < record_type->record.component_count; i++) {
        if (Slice_Equal_Ignore_Case (
            record_type->record.components[i].name, node->selected.selector)) {
          offset = record_type->record.components[i].byte_offset;
          comp_idx = i;
          break;
        }
      }

      // Walk base/parent chain to find rt_global_id (covers subtypes)
      uint32_t rec_rt_id = record_type->rt_global_id;
      if (rec_rt_id == 0) {
        Type_Info *walk = record_type->base_type ? record_type->base_type
                             : record_type->parent_type;
        for (int depth = 0; walk and depth < 10; depth++) {
          if (walk->rt_global_id > 0) { rec_rt_id = walk->rt_global_id; break; }
          walk = walk->base_type ? walk->base_type : walk->parent_type;
        }
      }
      uint32_t addr = Emit_Temp ();
      if (rec_rt_id > 0) {
        uint32_t off_r = Emit_Temp ();
        Emit ("  %%t%u = load i64, ptr @__rt_rec_%u_off%u\n",
           off_r, rec_rt_id, comp_idx);
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %%t%u"
           "  ; rt composite addr\n", addr, base, off_r);
      } else {
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n", addr, base, offset);
      }
      return addr;
    }
  }

  // .ALL dereference: address of P.ALL = value of P (the pointer itself)
  if (node->kind == NK_UNARY_OP and node->unary.op == TK_ALL and node->unary.operand) {
    Symbol *access_sym = node->unary.operand->symbol;
    if (access_sym) {
      uint32_t t = Emit_Temp ();
      Emit ("  %%t%u = load ptr, ptr ", t);
      Emit_Symbol_Storage (access_sym);
      Emit ("  ; .ALL address (deref access)\n");
      return t;
    }

    // For non-symbol expressions, evaluate to get the pointer
    return Generate_Expression (node->unary.operand);
  }

  // Array indexing or slice: Arr (I) or Arr (low..high)
  if (node->kind == NK_APPLY and node->apply.arguments.count == 1) {
    Type_Info *prefix_type = node->apply.prefix ? node->apply.prefix->type : NULL;

    // Handle implicit dereference: PT (I) where PT is access-to-array (RM 4.1)
    Type_Info *array_type = prefix_type;
    bool access_to_array = false;
    if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type and
      prefix_type->access.designated_type->kind == TYPE_ARRAY) {
      array_type = prefix_type->access.designated_type;
      access_to_array = true;
    }
    if (array_type and array_type->kind == TYPE_ARRAY) {
      uint32_t elem_size = array_type->array.element_type
                 ? array_type->array.element_type->size : 1;
      if (elem_size == 0) elem_size = 1;
      int128_t low = Array_Low_Bound (array_type);

      // Slice: ARR (low..high) - return address at slice start (RM 4.1.2)
      Syntax_Node *arg0 = node->apply.arguments.items[0];
      if (arg0->kind == NK_RANGE) {
        uint32_t base;
        if (access_to_array) {
          base = Emit_Temp ();
          Emit ("  %%t%u = load ptr, ptr ", base);
          Emit_Symbol_Storage (node->apply.prefix->symbol);
          Emit ("  ; deref access-to-array for slice\n");
        } else {
          base = Generate_Composite_Address (node->apply.prefix);
        }
        uint32_t slice_low = Generate_Expression (arg0->range.low);
        uint32_t adj = slice_low;
        const char *slice_idx_t = Integer_Arith_Type ();
        if (low != 0) {
          adj = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %s\n", adj, slice_idx_t, slice_low, I128_Decimal (low));
        }
        uint32_t byte_off = adj;
        if (elem_size != 1) {
          byte_off = Emit_Temp ();
          Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_off, slice_idx_t, adj, elem_size);
        }
        uint32_t addr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n", addr, base, slice_idx_t, byte_off);
        return addr;
      }
      uint32_t base;

      // Dereference access-to-array: load pointer then index
      if (access_to_array) {
        base = Emit_Temp ();
        Emit ("  %%t%u = load ptr, ptr ", base);
        Emit_Symbol_Storage (node->apply.prefix->symbol);
        Emit ("  ; deref access-to-array\n");
      } else {
        base = Generate_Composite_Address (node->apply.prefix);
      }
      uint32_t idx = Generate_Expression (arg0);

      // Adjust index: byte_offset = (idx - low) * elem_size
      const char *comp_idx_t = Integer_Arith_Type ();

      // Widen index to native integer (enum/bool indices may be i8/i1)
      {
        const char *idx_src = Expression_Llvm_Type (arg0);
        if (idx_src and idx_src[0] == 'i' and strcmp (idx_src, comp_idx_t) != 0)
          idx = Emit_Convert (idx, idx_src, comp_idx_t);
      }
      uint32_t adj_idx = idx;
      if (low != 0) {
        adj_idx = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %lld\n", adj_idx, comp_idx_t, idx, (long long)low);
      }
      uint32_t byte_off = Emit_Temp ();
      if (elem_size == 1) {
        byte_off = adj_idx;
      } else {
        Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_off, comp_idx_t, adj_idx, elem_size);
      }
      uint32_t addr = Emit_Temp ();
      Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n", addr, base, comp_idx_t, byte_off);
      return addr;
    }

    // Handle unconstrained arrays passed as fat pointers
    // Fat pointer - extract data and compute element address
    if (not Type_Is_Constrained_Array (prefix_type) and Type_Is_String (prefix_type)) {
      const char *str_bt = Array_Bound_Llvm_Type (prefix_type);
      uint32_t fat = Generate_Expression (node->apply.prefix);
      uint32_t data = Emit_Fat_Pointer_Data (fat, str_bt);
      uint32_t low = Emit_Fat_Pointer_Low (fat, str_bt);
      const char *str_idx_t = Integer_Arith_Type ();
      uint32_t low_conv = Emit_Convert (low, str_bt, str_idx_t);
      uint32_t idx = Generate_Expression (node->apply.arguments.items[0]);
      uint32_t adj_idx = Emit_Temp ();
      Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", adj_idx, str_idx_t, idx, low_conv);
      uint32_t addr = Emit_Temp ();
      Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n", addr, data, str_idx_t, adj_idx);
      return addr;
    }
  }

  // Fallback: for expressions that return a pointer (like composite values)
  return Generate_Expression (node);
}

// ─── Boolean Array Elementwise Op (RM 4.5.1) ─────────────────────────────────────────────────────
// Unrolled loop applying `ir_op` (and/or/xor) byte-by-byte.                                        
// Returns alloca ptr to result.  Binary variant (two operands).                                    
//                                                                                                  
uint32_t Emit_Bool_Array_Binop (uint32_t left, uint32_t right, Type_Info *result_type, const char *ir_op)
{
  int128_t count = Array_Element_Count (result_type);
  uint32_t extent = (count > 0) ? (uint32_t)count
                  : (result_type->size > 0 ? result_type->size : 1);
  uint32_t dst = Emit_Temp ();
  Emit ("  %%t%u = alloca [%u x i8]  ; bool array %s result\n", dst, extent, ir_op);
  for (uint32_t i = 0; i < extent; i++) {
    uint32_t lp = Emit_Temp (), rp = Emit_Temp ();
    uint32_t lv = Emit_Temp (), rv = Emit_Temp ();
    uint32_t ov = Emit_Temp (), dp = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i32 %u\n", lp, left, i);
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i32 %u\n", rp, right, i);
    Emit ("  %%t%u = load i8, ptr %%t%u\n", lv, lp);
    Emit ("  %%t%u = load i8, ptr %%t%u\n", rv, rp);
    Emit ("  %%t%u = %s i8 %%t%u, %%t%u\n", ov, ir_op, lv, rv);
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i32 %u\n", dp, dst, i);
    Emit ("  store i8 %%t%u, ptr %%t%u\n", ov, dp);
  }
  Temp_Set_Type (dst, "ptr");
  return dst;
}

// Unary NOT for boolean arrays: xor each byte with 1.
uint32_t Emit_Bool_Array_Not (uint32_t operand, Type_Info *result_type)
{
  int128_t count = Array_Element_Count (result_type);
  uint32_t extent = (count > 0) ? (uint32_t)count
                  : (result_type->size > 0 ? result_type->size : 1);
  uint32_t dst = Emit_Temp ();
  Emit ("  %%t%u = alloca [%u x i8]  ; bool array NOT result\n", dst, extent);
  for (uint32_t i = 0; i < extent; i++) {
    uint32_t sp = Emit_Temp (), sv = Emit_Temp ();
    uint32_t nv = Emit_Temp (), dp = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i32 %u\n", sp, operand, i);
    Emit ("  %%t%u = load i8, ptr %%t%u\n", sv, sp);
    Emit ("  %%t%u = xor i8 %%t%u, 1\n", nv, sv);
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i32 %u\n", dp, dst, i);
    Emit ("  store i8 %%t%u, ptr %%t%u\n", nv, dp);
  }
  Temp_Set_Type (dst, "ptr");
  return dst;
}

// Test whether a type is a boolean array (for AND/OR/XOR/NOT dispatch).
bool Type_Is_Bool_Array (const Type_Info *t) {
  return t and Type_Is_Array_Like (t) and t->array.element_type
       and Type_Is_Boolean (t->array.element_type);
}

// ─── Normalize Expression to Fat Pointer ─────────────────────────────────────────────────────────
// Given an expression and its type, produce a fat pointer value.                                   
// Handles: already-fat, CHARACTER (alloca+store+wrap), constrained array (wrap).                   
//                                                                                                  
uint32_t Normalize_To_Fat_Pointer (Syntax_Node *expr, uint32_t raw, Type_Info *type, const char *bt)
{
  if (Expression_Produces_Fat_Pointer (expr, type))
    return raw;
  if (Type_Is_Character (type)) {
    uint32_t ca = Emit_Temp ();
    Emit ("  %%t%u = alloca i8\n", ca);
    uint32_t ct = Emit_Convert (raw, Integer_Arith_Type (), "i8");
    Emit ("  store i8 %%t%u, ptr %%t%u\n", ct, ca);
    return Emit_Fat_Pointer (ca, 1, 1, bt);
  }
  if (Type_Is_Constrained_Array (type) and type->array.index_count > 0) {
    int128_t lo = Type_Bound_Value (type->array.indices[0].low_bound);
    int128_t hi = Type_Bound_Value (type->array.indices[0].high_bound);

    // Discriminant-dependent bounds: Type_Bound_Value returns 0 for                                
    // BOUND_EXPR.  For aggregates, derive bounds from positional count.                            
    // For non-aggregates, load the discriminant at runtime. (RM 3.7.1)                             
    //                                                                                              
    if (Type_Has_Dynamic_Bounds (type) and expr and
      expr->kind == NK_AGGREGATE) {
      uint32_t n_pos = 0;
      for (uint32_t ai = 0; ai < expr->aggregate.items.count; ai++) {
        if (expr->aggregate.items.items[ai]->kind != NK_ASSOCIATION)
          n_pos++;
      }
      if (n_pos > 0) hi = lo + (int128_t)n_pos - 1;
    }
    return Emit_Fat_Pointer (raw, lo, hi, bt);
  }
  return raw;  // fallback: assume already fat
}

// Check whether a positional aggregate of the given constrained array type                         
// will produce a fat pointer from Generate_Aggregate.  Generate_Aggregate                          
// overrides dim_hi[d] with a BOUND_INTEGER when dim_lo[d] is BOUND_INTEGER,                        
// collapsing dynamic bounds to static.  So an aggregate only stays dynamic                         
// when at least one dimension has a BOUND_EXPR lower bound (typically from                         
// runtime expressions like IDENT_INT, NOT from discriminant references whose                       
// low bound is a literal).  Returns false for non-array / non-dynamic types.                       
//                                                                                                  
bool Aggregate_Produces_Fat_Pointer (const Type_Info *t) {
  if (not t or not Type_Has_Dynamic_Bounds (t) or not Type_Is_Array_Like (t))
    return false;
  for (uint32_t d = 0; d < t->array.index_count; d++) {
    Type_Bound lo = t->array.indices[d].low_bound;
    Type_Bound hi = t->array.indices[d].high_bound;
    if (lo.kind == BOUND_NONE and t->array.indices[d].index_type)
      lo = t->array.indices[d].index_type->low_bound;
    if (hi.kind == BOUND_NONE and t->array.indices[d].index_type)
      hi = t->array.indices[d].index_type->high_bound;
    if (lo.kind == BOUND_EXPR or hi.kind == BOUND_EXPR)
      return true;
  }
  return false;
}

// Assign a runtime elaboration ID to a constrained array type with                                 
// BOUND_EXPR bounds and emit its LLVM globals (@__rt_type_<id>_size,                               
// per-dimension _lo/_hi).  Idempotent: returns immediately if ID                                   
// already assigned or if the type has only static bounds.                                          
//                                                                                                  
void Ensure_Runtime_Type_Globals (Type_Info *t) {
  if (not t or t->rt_global_id > 0) return;
  if (not Type_Is_Array_Like (t) or not t->array.is_constrained) return;
  bool needs = false;
  for (uint32_t d = 0; d < t->array.index_count; d++) {
    Type_Bound lo = t->array.indices[d].low_bound;
    Type_Bound hi = t->array.indices[d].high_bound;
    if (lo.kind == BOUND_NONE and t->array.indices[d].index_type)
      lo = t->array.indices[d].index_type->low_bound;
    if (hi.kind == BOUND_NONE and t->array.indices[d].index_type)
      hi = t->array.indices[d].index_type->high_bound;

    // Only trigger for non-discriminant BOUND_EXPR (function calls like                            
    // IDENT_INT (-3)).  Discriminant-dependent bounds (expr->symbol points                         
    // to a discriminant) are handled by the existing disc_dep path.                                
    //                                                                                              
    bool lo_rt = (lo.kind == BOUND_EXPR and
            not (lo.expr and lo.expr->kind == NK_IDENTIFIER and lo.expr->symbol));
    bool hi_rt = (hi.kind == BOUND_EXPR and
            not (hi.expr and hi.expr->kind == NK_IDENTIFIER and hi.expr->symbol));
    if (lo_rt or hi_rt)
      { needs = true; break; }
  }
  if (not needs) return;
  uint32_t id = ++cg->rt_type_counter;
  t->rt_global_id = id;
  Emit_String_Const ("@__rt_type_%u_size = internal global i64 0\n", id);
  for (uint32_t d = 0; d < t->array.index_count; d++) {
    Emit_String_Const ("@__rt_type_%u_lo%u = internal global i64 0\n", id, d);
    Emit_String_Const ("@__rt_type_%u_hi%u = internal global i64 0\n", id, d);
  }
}

// Generate expression and wrap as fat pointer if needed.                                           
// Like Normalize_To_Fat_Pointer but generates the expression internally.                           
// Handles aggregates of unconstrained types specially (load pre-built fat ptr).                    
//                                                                                                  
uint32_t Wrap_Constrained_As_Fat (Syntax_Node *expr, Type_Info *type, const char *bt)
{
  if (Expression_Produces_Fat_Pointer (expr, type))
    return Generate_Expression (expr);

  // Aggregates of unconstrained or truly-dynamic array types already build                         
  // their own fat pointer alloca in Generate_Aggregate.  Load it.                                  
  // NOTE: Generate_Aggregate overrides dim_hi from positional element count                        
  // when dim_lo is BOUND_INTEGER, converting bounds to static.  So only                            
  // aggregates whose type has a BOUND_EXPR *lower* bound (in any dimension)                        
  // actually produce a fat pointer.  Discriminant-dependent bounds like                            
  // TB (1..A) have static lower bound → positional override → static array.                        
  //                                                                                                
  if (expr->kind == NK_AGGREGATE and type and Type_Is_Array_Like (type) and
    (Type_Is_Unconstrained_Array (type) or Aggregate_Produces_Fat_Pointer (type))) {
    uint32_t agg_ptr = Generate_Expression (expr);
    uint32_t loaded = Emit_Temp ();
    Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u  ; load agg fat ptr\n",
       loaded, agg_ptr);
    return loaded;
  }
  uint32_t ptr = Generate_Composite_Address (expr);
  int128_t lo = 1, hi = 0;
  if (Type_Is_Array_Like (type) and type->array.index_count > 0) {
    lo = Type_Bound_Value (type->array.indices[0].low_bound);
    hi = Type_Bound_Value (type->array.indices[0].high_bound);

    // Discriminant-dependent bounds: for aggregates, use positional count
    if (Type_Has_Dynamic_Bounds (type) and expr and
      expr->kind == NK_AGGREGATE) {
      uint32_t n_pos = 0;
      for (uint32_t ai = 0; ai < expr->aggregate.items.count; ai++) {
        if (expr->aggregate.items.items[ai]->kind != NK_ASSOCIATION)
          n_pos++;
      }
      if (n_pos > 0) hi = lo + (int128_t)n_pos - 1;
    }
  }
  return Emit_Fat_Pointer (ptr, lo, hi, bt);
}

// ─── Convert Universal_Real to Fixed-Point Scaled Integer ────────────────────────────────────────
// For fixed type with small S, value V converts to: fptosi(V / S).
uint32_t Convert_Real_To_Fixed (uint32_t val, double small, const char *fix_type)
{
  uint64_t small_bits;
  memcpy (&small_bits, &small, sizeof (small_bits));
  uint32_t small_val = Emit_Temp ();
  Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small=%g\n",
     small_val, (unsigned long long)small_bits, small);
  uint32_t divided = Emit_Temp ();
  Emit ("  %%t%u = fdiv double %%t%u, %%t%u\n", divided, val, small_val);
  uint32_t scaled = Emit_Temp ();
  Emit ("  %%t%u = fptosi double %%t%u to %s\n", scaled, divided, fix_type);
  Temp_Set_Type (scaled, fix_type);
  return scaled;
}
uint32_t Generate_Binary_Op (Syntax_Node *node) {

  // User-defined operator: generate function call (RM 6.7)
  if (node->symbol and node->symbol->kind == SYMBOL_FUNCTION and
    not node->symbol->is_predefined) {
    uint32_t    left       = Generate_Expression  (node->binary.left);
    uint32_t    right      = Generate_Expression  (node->binary.right);
    const char *left_llvm  = Expression_Llvm_Type (node->binary.left);
    const char *right_llvm = Expression_Llvm_Type (node->binary.right);
    Type_Info *p0_type = (node->symbol->parameter_count > 0) ?
      node->symbol->parameters[0].param_type : NULL;
    Type_Info *p1_type = (node->symbol->parameter_count > 1) ?
      node->symbol->parameters[1].param_type : NULL;
    const char *p0_llvm = p0_type ? Type_To_Llvm (p0_type) : left_llvm;
    const char *p1_llvm = p1_type ? Type_To_Llvm (p1_type) : right_llvm;
    left = Emit_Convert (left, left_llvm, p0_llvm);
    right = Emit_Convert (right, right_llvm, p1_llvm);
    const char *ret_type = node->symbol->return_type ?
      Type_To_Llvm (node->symbol->return_type) : "i32";
    bool callee_is_nested = Subprogram_Needs_Static_Chain (node->symbol);
    uint32_t frame_pre = callee_is_nested ?
      Precompute_Nested_Frame_Arg (node->symbol) : 0;
    uint32_t result = Emit_Temp ();
    Emit ("  %%t%u = call %s @", result, ret_type);
    Emit_Symbol_Name (node->symbol);
    Emit ("(");
    if (callee_is_nested) {
      Emit_Nested_Frame_Arg (node->symbol, frame_pre);
      Emit (", ");
    }
    Emit ("%s %%t%u, %s %%t%u)\n", p0_llvm, left, p1_llvm, right);
    return result;
  }

  // Check if this is equality/inequality on composite types
  Type_Info *left_type = node->binary.left ? node->binary.left->type : NULL;
  if ((node->binary.op == TK_EQ or node->binary.op == TK_NE) and
    left_type and Type_Is_Composite (left_type)) {

    // Composite type comparison
    uint32_t eq_result;
    bool left_is_slice = Expression_Is_Slice (node->binary.left);
    bool right_is_slice = Expression_Is_Slice (node->binary.right);
    Type_Info *right_type = node->binary.right ? node->binary.right->type : NULL;
    bool left_is_fat = Expression_Produces_Fat_Pointer (node->binary.left, left_type);
    bool right_is_fat = Expression_Produces_Fat_Pointer (node->binary.right, right_type);
    bool is_unconstrained = left_is_fat or right_is_fat;

    // Handle slice comparisons specially - they have mixed representations
    // At least one slice with constrained type - generate inline comparison
    if ((left_is_slice or right_is_slice) and not is_unconstrained) {
      uint32_t left_data, right_data;
      uint32_t left_low, left_high, right_low, right_high;
      uint32_t elem_size = left_type->array.element_type ?
                 left_type->array.element_type->size : 8;
      const char *slice_bt = Array_Bound_Llvm_Type (left_type);

      // Generate left operand
      if (left_is_slice) {
        uint32_t left_fat = Generate_Expression (node->binary.left);
        left_data = Emit_Fat_Pointer_Data (left_fat, slice_bt);
        Bound_Temps lb = Emit_Bounds_From_Fat (left_fat, slice_bt);
        left_low = lb.low_temp;
        left_high = lb.high_temp;
      } else {
        left_data = Generate_Composite_Address (node->binary.left);
        Bound_Temps lb = Emit_Bounds (left_type, 0);
        left_low = lb.low_temp;
        left_high = lb.high_temp;
      }

      // Generate right operand
      if (right_is_slice) {
        uint32_t right_fat = Generate_Expression (node->binary.right);
        right_data = Emit_Fat_Pointer_Data (right_fat, slice_bt);
        Bound_Temps rb = Emit_Bounds_From_Fat (right_fat, slice_bt);
        right_low = rb.low_temp;
        right_high = rb.high_temp;
      } else {
        right_data = Generate_Composite_Address (node->binary.right);
        Type_Info *rtype = node->binary.right->type ? node->binary.right->type : left_type;
        Bound_Temps rb = Emit_Bounds (rtype, 0);
        right_low = rb.low_temp;
        right_high = rb.high_temp;
      }

      // Compute lengths and compare
      uint32_t left_len = Emit_Length_From_Bounds (left_low, left_high, slice_bt);
      uint32_t right_len = Emit_Length_From_Bounds (right_low, right_high, slice_bt);
      uint32_t len_eq = Emit_Temp ();
      Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n", len_eq, slice_bt, left_len, right_len);

      // Compare data with memcmp
      uint32_t byte_size_nat = Emit_Temp ();
      Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_size_nat, slice_bt, left_len, elem_size);
      uint32_t byte_size = Emit_Extend_To_I64 (byte_size_nat, slice_bt);
      uint32_t data_eq = Emit_Memcmp_Eq (left_data, right_data, byte_size, 0, true);

      // Result: lengths match AND data matches
      eq_result = Emit_Temp ();
      Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", eq_result, len_eq, data_eq);

    // At least one operand produces a fat pointer.
    // Normalize both to fat pointers for uniform comparison.
    } else if (left_is_fat or right_is_fat) {
      uint32_t left_val, right_val;
      const char *eq_bt = Array_Bound_Llvm_Type (left_type);

      // Normalize both operands to fat pointers for uniform comparison.
      // Constrained arrays get wrapped with static bounds.
      left_val  = Wrap_Constrained_As_Fat (node->binary.left,  left_type, eq_bt);
      right_val = Wrap_Constrained_As_Fat (node->binary.right,
        node->binary.right->type ? node->binary.right->type : left_type, eq_bt);

      // Use the unconstrained array equality path (compares lengths then data)
      Type_Info *cmp_type = left_type;
      if (Type_Is_Constrained_Array (cmp_type) and (Type_Is_String (right_type) or
        Type_Is_Unconstrained_Array (right_type))) {
        cmp_type = right_type;  // Use unconstrained type for comparison
      }

      // Ensure comparison uses unconstrained path
      eq_result = Generate_Array_Equality (left_val, right_val, cmp_type);

    // Standard path: both operands are constrained (same representation).                          
    // However, some expressions (concatenation, function calls) may still                          
    // produce fat pointers.  Extract data ptr if needed.                                           
    //                                                                                              
    } else {

      // If both are constrained arrays but with different static sizes,
      // they cannot be equal (RM 4.5.2(9)).  Emit constant false.
      if (right_type and Type_Is_Array_Like (left_type) and Type_Is_Array_Like (right_type) and
        left_type->size > 0 and right_type->size > 0 and
        left_type->size != right_type->size) {
        eq_result = Emit_Temp ();
        Emit ("  %%t%u = add i1 0, 0  ; arrays of different size\n", eq_result);
        goto eq_done;
      }
      uint32_t left_ptr, right_ptr;
      bool l_produces_fat = Expression_Produces_Fat_Pointer (node->binary.left, left_type);
      bool r_produces_fat = Expression_Produces_Fat_Pointer (node->binary.right, right_type);
      if (l_produces_fat) {
        uint32_t lfat = Generate_Expression (node->binary.left);
        const char *lbt = Array_Bound_Llvm_Type (left_type);
        left_ptr = Emit_Fat_Pointer_Data (lfat, lbt);
      } else {
        left_ptr = Generate_Composite_Address (node->binary.left);
      }
      if (r_produces_fat) {
        uint32_t rfat = Generate_Expression (node->binary.right);
        const char *rbt = Array_Bound_Llvm_Type (right_type ? right_type : left_type);
        right_ptr = Emit_Fat_Pointer_Data (rfat, rbt);
      } else {
        right_ptr = Generate_Composite_Address (node->binary.right);
      }
      eq_result = Emit_Temp ();
      if (left_type->equality_func_name) {
        const char *arg_type = Type_To_Llvm (left_type);
        Emit ("  %%t%u = call i1 @%s(%s %%t%u, %s %%t%u)\n",
           eq_result, left_type->equality_func_name, arg_type, left_ptr, arg_type, right_ptr);
      } else {
        if (Type_Is_Record (left_type)) {
          eq_result = Generate_Record_Equality (left_ptr, right_ptr, left_type);
        } else {
          eq_result = Generate_Array_Equality (left_ptr, right_ptr, left_type);
        }
      }
    }
    eq_done:

    // For /= operator, negate the result
    if (node->binary.op == TK_NE) {
      uint32_t ne_result = Emit_Temp ();
      Emit ("  %%t%u = xor i1 %%t%u, 1\n", ne_result, eq_result);
      eq_result = ne_result;
    }

    // Comparisons stay as i1 (Boolean_Data relationship)
    return eq_result;
  }

  // Array relational comparisons (lexicographic)
  if ((node->binary.op == TK_LT or node->binary.op == TK_LE or
     node->binary.op == TK_GT or node->binary.op == TK_GE) and
    Type_Is_Array_Like (left_type)) {

    // Get addresses of both arrays for comparison
    uint32_t left_ptr, right_ptr;
    Type_Info *rhs_cmp_type = node->binary.right ? node->binary.right->type : NULL;

    // An aggregate of an unconstrained type already builds its own fat                             
    // pointer alloca inside Generate_Aggregate.  Detect this so we can                             
    // just load it instead of double-wrapping with wrong bounds.                                   
    //                                                                                              
    bool l_is_uncon_agg = (node->binary.left->kind == NK_AGGREGATE and
      node->binary.left->type and Type_Is_Unconstrained_Array (node->binary.left->type));
    bool r_is_uncon_agg = (node->binary.right and
      node->binary.right->kind == NK_AGGREGATE and
      node->binary.right->type and Type_Is_Unconstrained_Array (node->binary.right->type));
    bool is_unconstrained = Expression_Produces_Fat_Pointer (node->binary.left, left_type) or
                Expression_Produces_Fat_Pointer (node->binary.right, rhs_cmp_type) or
                l_is_uncon_agg or r_is_uncon_agg;
    const char *rel_bt = Array_Bound_Llvm_Type (left_type);

    // Generate each operand as fat pointer, wrapping constrained if needed
    if (is_unconstrained) {
      left_ptr = Wrap_Constrained_As_Fat (node->binary.left, left_type, rel_bt);
      right_ptr = Wrap_Constrained_As_Fat (node->binary.right, rhs_cmp_type, rel_bt);
    } else {
      left_ptr = Generate_Composite_Address (node->binary.left);
      right_ptr = Generate_Composite_Address (node->binary.right);
    }
    uint32_t memcmp_result, cmp_result;

    // Constrained array: same size, use memcmp directly
    if (left_type->array.is_constrained and not is_unconstrained) {
      int128_t count = Array_Element_Count (left_type);
      uint32_t elem_size = left_type->array.element_type ?
                 left_type->array.element_type->size : 1;
      int64_t total_size = count * elem_size;
      memcmp_result = Emit_Temp ();
      Emit ("  %%t%u = call i32 @memcmp (ptr %%t%u, ptr %%t%u, i64 %lld)\n",
         memcmp_result, left_ptr, right_ptr, (long long)total_size);

    // Unconstrained array: lexicographic comparison via helper
    } else {
      uint32_t elem_size = left_type->array.element_type ?
                 left_type->array.element_type->size : 1;
      memcmp_result = Emit_Array_Lex_Compare (left_ptr, right_ptr, elem_size, rel_bt);
    }

    // Compare memcmp result with 0 based on operator
    cmp_result = Emit_Temp ();
    switch (node->binary.op) {

      // AND THEN: if left is false, result is false (don't evaluate right)
      //           if left is true, result is right
      case TK_LT:
        Emit ("  %%t%u = icmp slt i32 %%t%u, 0\n", cmp_result, memcmp_result);
        break;
      case TK_LE:
        Emit ("  %%t%u = icmp sle i32 %%t%u, 0\n", cmp_result, memcmp_result);
        break;
      case TK_GT:
        Emit ("  %%t%u = icmp sgt i32 %%t%u, 0\n", cmp_result, memcmp_result);
        break;
      case TK_GE:
        Emit ("  %%t%u = icmp sge i32 %%t%u, 0\n", cmp_result, memcmp_result);
        break;
      default:
        fprintf (stderr, "warning: unhandled array comparison operator, defaulting to equality\n");
        Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", cmp_result, memcmp_result);
    }

    // Comparisons stay as i1
    return cmp_result;
  }

  // Short-circuit boolean operators: AND THEN, OR ELSE                                             
  // These must NOT evaluate the right operand if the left operand                                  
  // determines the result (Ada RM 4.5.1).                                                          
  //                                                                                                
  if (node->binary.op == TK_AND_THEN) {
    uint32_t left = Generate_Expression (node->binary.left);
    const char *left_llvm = Expression_Llvm_Type (node->binary.left);
    uint32_t left_i1 = Emit_Convert (left, left_llvm, "i1");
    uint32_t eval_right_label = cg->label_id++;
    uint32_t done_label = cg->label_id++;
    uint32_t left_block_label = cg->label_id++;

    // Save current block for phi
    Emit ("  br label %%Landthen_check%u\n", left_block_label);
    Emit ("Landthen_check%u:\n", left_block_label);
    Emit ("  br i1 %%t%u, label %%Landthen_right%u, label %%Landthen_done%u\n",
       left_i1, eval_right_label, done_label);

    // Evaluate right if left was true
    Emit ("Landthen_right%u:\n", eval_right_label);
    uint32_t right = Generate_Expression (node->binary.right);
    const char *right_llvm = Expression_Llvm_Type (node->binary.right);
    uint32_t right_i1 = Emit_Convert (right, right_llvm, "i1");
    uint32_t right_done_label = cg->label_id++;
    Emit ("  br label %%Landthen_merge%u\n", right_done_label);
    Emit ("Landthen_merge%u:\n", right_done_label);
    Emit ("  br label %%Landthen_done%u\n", done_label);

    // Merge point
    Emit ("Landthen_done%u:\n", done_label);
    uint32_t phi = Emit_Temp ();
    Emit ("  %%t%u = phi i1 [ false, %%Landthen_check%u ], [ %%t%u, %%Landthen_merge%u ]\n",
       phi, left_block_label, right_i1, right_done_label);
    return phi;  // Stays as i1
  }

  // OR ELSE: if left is true, result is true (don't evaluate right)
  //          if left is false, result is right
  if (node->binary.op == TK_OR_ELSE) {
    uint32_t left = Generate_Expression (node->binary.left);
    const char *left_llvm = Expression_Llvm_Type (node->binary.left);
    uint32_t left_i1 = Emit_Convert (left, left_llvm, "i1");
    uint32_t eval_right_label = cg->label_id++;
    uint32_t done_label = cg->label_id++;
    uint32_t left_block_label = cg->label_id++;

    // Save current block for phi
    Emit ("  br label %%Lorelse_check%u\n", left_block_label);
    Emit ("Lorelse_check%u:\n", left_block_label);
    Emit ("  br i1 %%t%u, label %%Lorelse_done%u, label %%Lorelse_right%u\n",
       left_i1, done_label, eval_right_label);

    // Evaluate right if left was false
    Emit ("Lorelse_right%u:\n", eval_right_label);
    uint32_t right = Generate_Expression (node->binary.right);
    const char *right_llvm = Expression_Llvm_Type (node->binary.right);
    uint32_t right_i1 = Emit_Convert (right, right_llvm, "i1");
    uint32_t right_done_label = cg->label_id++;
    Emit ("  br label %%Lorelse_merge%u\n", right_done_label);
    Emit ("Lorelse_merge%u:\n", right_done_label);
    Emit ("  br label %%Lorelse_done%u\n", done_label);

    // Merge point
    Emit ("Lorelse_done%u:\n", done_label);
    uint32_t phi = Emit_Temp ();
    Emit ("  %%t%u = phi i1 [ true, %%Lorelse_check%u ], [ %%t%u, %%Lorelse_merge%u ]\n",
       phi, left_block_label, right_i1, right_done_label);
    return phi;  // stays as i1
  }

  // String/array concatenation
  if (node->binary.op == TK_AMPERSAND and
    (Type_Is_Array_Like (left_type) or Type_Is_Array_Like (node->type) or
     Type_Is_String (left_type) or Type_Is_String (node->type))) {

    // Generate both operands.                                                                      
    // Each operand may be: fat pointer (STRING/unconstrained/literal/slice/concat),                
    // ptr (constrained array), or i64 (CHARACTER).                                                 
    // We normalize each to a fat pointer before proceeding.                                        
    //                                                                                              
    uint32_t left_raw = Generate_Expression (node->binary.left);
    uint32_t right_raw = Generate_Expression (node->binary.right);

    // Normalize both operands to fat pointers (handles character, constrained, already-fat)
    const char *cat_bt = Array_Bound_Llvm_Type (left_type);
    Type_Info *rhs_type = node->binary.right ? node->binary.right->type : NULL;
    uint32_t left_fat  = Normalize_To_Fat_Pointer (node->binary.left,  left_raw, left_type, cat_bt);
    uint32_t right_fat = Normalize_To_Fat_Pointer (node->binary.right, right_raw, rhs_type, cat_bt);

    // Extract data pointers and bounds
    uint32_t left_data = Emit_Fat_Pointer_Data (left_fat, cat_bt);
    uint32_t left_low = Emit_Fat_Pointer_Low (left_fat, cat_bt);
    uint32_t left_high = Emit_Fat_Pointer_High (left_fat, cat_bt);
    uint32_t right_data = Emit_Fat_Pointer_Data (right_fat, cat_bt);
    uint32_t right_low = Emit_Fat_Pointer_Low (right_fat, cat_bt);
    uint32_t right_high = Emit_Fat_Pointer_High (right_fat, cat_bt);

    // Calculate lengths: high - low + 1
    uint32_t left_len1  = Emit_Length_From_Bounds (left_low,  left_high,  cat_bt);
    uint32_t right_len1 = Emit_Length_From_Bounds (right_low, right_high, cat_bt);

    // Total length
    uint32_t total_len = Emit_Temp ();
    Emit ("  %%t%u = add %s %%t%u, %%t%u\n", total_len, cat_bt, left_len1, right_len1);

    // Check result length against index SUBTYPE bounds (RM 4.5.3(7)).                              
    // The check is against the index subtype (e.g., POSITIVE for STRING),                          
    // NOT the specific constraint on a variable. For STRING, the index                             
    // subtype is POSITIVE (1..INTEGER'LAST), so almost any length is valid.                        
    //                                                                                              
    Type_Info *result_type = node->type;
    if (result_type and (result_type->kind == TYPE_ARRAY or result_type->kind == TYPE_STRING) and
      result_type->array.index_count > 0) {

      // Always use index_type bounds (the index subtype), not Index_Info
      // bounds which represent a specific constraint on a variable.
      Index_Info *idx_info = &result_type->array.indices[0];
      Type_Bound low_b = {0}, high_b = {0};
      if (idx_info->index_type) {
        low_b = idx_info->index_type->low_bound;
        high_b = idx_info->index_type->high_bound;
      }

      // Max length = index_high - index_low + 1
      if (low_b.kind == BOUND_INTEGER and high_b.kind == BOUND_INTEGER) {
        int128_t max_len = high_b.int_value - low_b.int_value + 1;
        if (max_len > 0) {
          uint32_t max_const = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %lld  ; max index length\n", max_const, cat_bt, (long long)max_len);
          uint32_t overflow = Emit_Temp ();
          Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n", overflow, cat_bt, total_len, max_const);
          Emit_Check_With_Raise (overflow, true, "concatenation length exceeds index subtype");
        }
      }
    }

    // Extend to i64 for C-level calls (memcpy, sec_stack_alloc)
    uint32_t total_len_64 = Emit_Extend_To_I64 (total_len, cat_bt);
    uint32_t left_len1_64 = Emit_Extend_To_I64 (left_len1, cat_bt);
    uint32_t right_len1_64 = Emit_Extend_To_I64 (right_len1, cat_bt);

    // Allocate space on secondary stack
    uint32_t result_data = Emit_Temp ();
    Emit ("  %%t%u = call ptr @__ada_sec_stack_alloc(i64 %%t%u)\n",
       result_data, total_len_64);

    // Copy left string using llvm.memcpy
    Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
       result_data, left_data, left_len1_64);

    // Calculate destination for right string
    uint32_t right_dest = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %%t%u\n",
       right_dest, result_data, left_len1_64);

    // Copy right string
    Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
       right_dest, right_data, right_len1_64);

    // Result bounds: INDEX_SUBTYPE'FIRST .. FIRST+total_len-1 (RM 4.5.3(8)).                       
    // For STRING the index subtype is POSITIVE with FIRST=1; for custom                            
    // array types the FIRST may differ (e.g. STE'FIRST=2).                                         
    //                                                                                              
    int128_t idx_first = 1;  // default for STRING (POSITIVE'FIRST)
    if (result_type and (result_type->kind == TYPE_ARRAY or result_type->kind == TYPE_STRING) and
      result_type->array.index_count > 0 and result_type->array.indices) {
      Type_Info *idx_ty = result_type->array.indices[0].index_type;
      if (idx_ty and idx_ty->low_bound.kind == BOUND_INTEGER)
        idx_first = idx_ty->low_bound.int_value;
    }
    uint32_t res_lo = Emit_Temp ();
    Emit ("  %%t%u = add %s 0, %s\n", res_lo, cat_bt, I128_Decimal (idx_first));
    uint32_t res_hi = Emit_Temp ();
    Emit ("  %%t%u = add %s %%t%u, %s  ; first + total_len - 1\n",
       res_hi, cat_bt, total_len, I128_Decimal (idx_first - 1));

    // Return fat pointer to result
    return Emit_Fat_Pointer_Dynamic (result_data, res_lo, res_hi, cat_bt);
  }
  uint32_t left = Generate_Expression (node->binary.left);

  // NK_RANGE right operand is generated inside the IN/NOT IN handler.                              
  // For membership tests (IN/NOT IN), type names are also handled specially                        
  // and should not be evaluated as expressions.                                                    
  //                                                                                                
  bool right_is_range = node->binary.right and node->binary.right->kind == NK_RANGE;
  bool is_membership = (node->binary.op == TK_IN) or
             (node->binary.op == TK_NOT and node->binary.right and
              (node->binary.right->kind == NK_IDENTIFIER or
               node->binary.right->kind == NK_QUALIFIED) and
              node->binary.right->symbol and
              node->binary.right->symbol->kind == SYMBOL_TYPE);
  uint32_t right = (right_is_range or is_membership) ? 0 : Generate_Expression (node->binary.right);
  uint32_t t = Emit_Temp ();
  const char *op;
  Type_Info *result_type = node->type;

  // track actual LLVM types for native-width integer operations.
  const char *left_int_type = Expression_Llvm_Type (node->binary.left);
  const char *right_int_type = (right_is_range or is_membership) ? Integer_Arith_Type () :
                  Expression_Llvm_Type (node->binary.right);
  Type_Info *lhs_type = node->binary.left ? node->binary.left->type : NULL;
  Type_Info *rhs_type = node->binary.right ? node->binary.right->type : NULL;
  bool is_float = Type_Is_Float_Representation (result_type);
  bool is_fixed = Type_Is_Fixed_Point (result_type);

  // Determine LLVM float type from result type (float vs double)
  const char *float_type_str = Float_Llvm_Type_Of (result_type);

  // Mixed-mode arithmetic: when result is float but operands are integer,
  // convert integer operands to float for proper arithmetic (RM 4.5.5)
  if (is_float) {
    bool lhs_is_float = Type_Is_Float_Representation (lhs_type);
    bool rhs_is_float = Type_Is_Float_Representation (rhs_type);

    // Determine actual types for left and right operands
    const char *lhs_float_type = Float_Llvm_Type_Of (lhs_type);
    const char *rhs_float_type = Float_Llvm_Type_Of (rhs_type);

    // Integer > float: use uitofp for modular (unsigned) types
    if (not lhs_is_float) {
      uint32_t conv = Emit_Temp ();
      const char *itof = Type_Is_Unsigned (lhs_type) ? "uitofp" : "sitofp";
      Emit ("  %%t%u = %s %s %%t%u to %s\n", conv, itof, left_int_type, left, float_type_str);
      left = conv;

    // Convert left operand to result float type
    } else if (strcmp (lhs_float_type, float_type_str) != 0) {
      left = Emit_Convert (left, lhs_float_type, float_type_str);
    }

    // For exponentiation, skip RHS conversion - TK_EXPON handles it
    // Integer > float: use uitofp for modular (unsigned) types
    if (not rhs_is_float and node->binary.op != TK_EXPON) {
      uint32_t conv = Emit_Temp ();
      const char *itof = Type_Is_Unsigned (rhs_type) ? "uitofp" : "sitofp";
      Emit ("  %%t%u = %s %s %%t%u to %s\n", conv, itof, right_int_type, right, float_type_str);
      right = conv;
    } else if (rhs_is_float and strcmp (rhs_float_type, float_type_str) != 0 and
           node->binary.op != TK_EXPON) {

      // Convert right operand to result float type
      right = Emit_Convert (right, rhs_float_type, float_type_str);
    }
  }

  // Fixed-point uses scaled integer representation at the                                          
  // result type's native width.  Widen operands to match.                                          
  // Skip universal_real operands - they'll be handled below via                                    
  // the fdiv/fptosi path which produces the correct scaled integer.                                
  //                                                                                                
  if (is_fixed and not is_float) {
    const char *fixed_arith = Type_To_Llvm (result_type);
    if (not Type_Is_Universal_Real (lhs_type))
      left = Emit_Convert (left, left_int_type, fixed_arith);
    if (not right_is_range and not is_membership and not Type_Is_Universal_Real (rhs_type))
      right = Emit_Convert (right, right_int_type, fixed_arith);
    left_int_type = fixed_arith;
    right_int_type = fixed_arith;
  }

  // Mixed fixed-point / universal_real arithmetic (RM 4.5.5, 4.10):                                
  // When result is fixed-point but an operand is universal_real, convert                           
  // the universal_real to the fixed-point's scaled integer representation.                         
  // For fixed type with small S, value V converts to: floor (V / S)                                
  // Skip for exponentiation which has its own special handling.                                    
  //                                                                                                
  if (is_fixed and node->binary.op != TK_EXPON) {
    double small = result_type->fixed.small;
    if (small <= 0) small = result_type->fixed.delta > 0 ? result_type->fixed.delta : 1.0;
    const char *fix_arith = Type_To_Llvm (result_type);
    if (Type_Is_Universal_Real (rhs_type))
      right = Convert_Real_To_Fixed (right, small, fix_arith);
    if (Type_Is_Universal_Real (lhs_type))
      left = Convert_Real_To_Fixed (left, small, fix_arith);
  }

  // Fixed-point multiplication/division needs scaling (RM 4.5.5)                                   
  // Only when BOTH operands are fixed-point. Integer × Fixed (or v.v.)                             
  // already yields a correctly scaled result - no shift needed.                                    
  //                                                                                                
  bool both_fixed = is_fixed
    and Type_Is_Fixed_Point (lhs_type) and Type_Is_Fixed_Point (rhs_type);
  if (both_fixed and (node->binary.op == TK_STAR or node->binary.op == TK_SLASH)) {
    const char *fix_type = Type_To_Llvm (result_type);
    int scale = result_type->fixed.scale;

    // Fixed * Fixed: result = (a * b) >> abs(scale)
    if (node->binary.op == TK_STAR) {
      uint32_t mul = Emit_Temp ();
      Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", mul, fix_type, left, right);

      // Negative scale = right shift by |scale|
      if (scale < 0) {
        Emit ("  %%t%u = ashr %s %%t%u, %d\n", t, fix_type, mul, -scale);
        Temp_Set_Type (t, fix_type);

      // Positive scale = left shift (uncommon)
      } else if (scale > 0) {
        Emit ("  %%t%u = shl %s %%t%u, %d\n", t, fix_type, mul, scale);
        Temp_Set_Type (t, fix_type);
      } else {
        t = mul;
        Temp_Set_Type (t, fix_type);
      }
      return t;

    // Fixed / Fixed: result = (a << abs(scale)) / b
    } else {
      uint32_t shifted = Emit_Temp ();
      if (scale < 0) {
        Emit ("  %%t%u = shl %s %%t%u, %d\n", shifted, fix_type, left, -scale);
      } else if (scale > 0) {
        Emit ("  %%t%u = ashr %s %%t%u, %d\n", shifted, fix_type, left, scale);
      } else {
        shifted = left;
      }
      Emit ("  %%t%u = sdiv %s %%t%u, %%t%u\n", t, fix_type, shifted, right);
      Temp_Set_Type (t, fix_type);
      return t;
    }
  }

  // modular (unsigned) types use unsigned division/remainder.                                      
  // Ada MOD vs REM differ for signed types (RM 4.5.5), but for modular                             
  // types both map to urem since all values are non-negative.                                      
  //                                                                                                
  bool lhs_unsigned = Type_Is_Unsigned (lhs_type);
  switch (node->binary.op) {
    case TK_PLUS:  op = is_float ? "fadd" : "add"; break;
    case TK_MINUS: op = is_float ? "fsub" : "sub"; break;
    case TK_STAR:  op = is_float ? "fmul" : "mul"; break;
    case TK_SLASH: op = is_float ? "fdiv" : (lhs_unsigned ? "udiv" : "sdiv"); break;
    case TK_MOD:   op = lhs_unsigned ? "urem" : "srem"; break;
    case TK_REM:   op = lhs_unsigned ? "urem" : "srem"; break;
    case TK_EXPON:

      // Exponentiation: base ** exponent                                                           
      // For floating-point: use llvm.pow.f64 intrinsic (requires double)                           
      // For integer: use __ada_integer_pow                                                         
      //                                                                                            
      {
        bool left_is_float = Type_Is_Float_Representation (left_type);

        // Float ** Integer: use native-precision pow intrinsic.                                    
        // LLVM provides llvm.pow.f32 and llvm.pow.f64.                                             
        // RM 4.5.6(12): 0.0 ** negative must raise CONSTRAINT_ERROR.                               
        //                                                                                          
        if (left_is_float) {
          const char *lhs_ftype = Float_Llvm_Type_Of (left_type);
          const char *pow_intrinsic = (lhs_ftype[0] == 'f')
            ? "llvm.pow.f32" : "llvm.pow.f64";

          // Check: if base == 0.0 and exponent < 0, raise CONSTRAINT_ERROR
          uint32_t is_zero = Emit_Temp ();
          Emit ("  %%t%u = fcmp oeq %s %%t%u, 0.0\n",
             is_zero, lhs_ftype, left);
          uint32_t is_neg = Emit_Temp ();
          Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", is_neg, right_int_type, right);
          uint32_t bad = Emit_Temp ();
          Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", bad, is_zero, is_neg);
          uint32_t ok_label = Emit_Label ();
          uint32_t bad_label = Emit_Label ();
          Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", bad, bad_label, ok_label);
          cg->block_terminated = true;
          Emit_Label_Here (bad_label);
          cg->block_terminated = false;
          Emit_Raise_Constraint_Error ("0.0 ** negative (RM 4.5.6)");
          Emit_Label_Here (ok_label);
          cg->block_terminated = false;

          // Convert integer exponent to matching float type
          uint32_t exp_float = Emit_Temp ();
          Emit ("  %%t%u = sitofp %s %%t%u to %s\n",
             exp_float, right_int_type, right, lhs_ftype);
          Emit ("  %%t%u = call %s @%s(%s %%t%u, %s %%t%u)\n",
             t, lhs_ftype, pow_intrinsic, lhs_ftype, left, lhs_ftype, exp_float);

        // Integer ** Integer: use integer power function.                                          
        // Signed types use overflow-checked __ada_integer_pow.                                     
        // Modular types use wrapping __ada_modular_pow (RM 3.5.4).                                 
        //                                                                                          
        } else {
          const char *iat = Integer_Arith_Type ();
          left = Emit_Convert (left, left_int_type, iat);
          right = Emit_Convert (right, right_int_type, iat);
          bool pow_unsigned = Type_Is_Unsigned (result_type);
          bool pow_suppressed = Check_Is_Suppressed (result_type, NULL, CHK_OVERFLOW);
          const char *pow_fn = (pow_unsigned or pow_suppressed)
            ? "__ada_modular_pow" : "__ada_integer_pow";
          Emit ("  %%t%u = call %s @%s(%s %%t%u, %s %%t%u)\n",
             t, iat, pow_fn, iat, left, iat, right);
          Temp_Set_Type (t, iat);
        }
        return t;
      }

    // Bitwise/logical operations (RM 4.5.1):                                                       
    // - Modular types: bitwise at native width                                                     
    // - Boolean arrays: element-wise                                                               
    // - Boolean scalars: convert to i1, operate, widen back                                        
    //                                                                                              
    case TK_AND:
    case TK_AND_THEN:
    case TK_OR:
    case TK_OR_ELSE:
    case TK_XOR:
      {
        const char *llvm_op = (node->binary.op == TK_AND or node->binary.op == TK_AND_THEN) ? "and" :
                    (node->binary.op == TK_OR  or node->binary.op == TK_OR_ELSE)  ? "or" : "xor";
        if (Type_Is_Bool_Array (result_type)) {
          return Emit_Bool_Array_Binop (left, right, result_type, llvm_op);
        } else if (Type_Is_Unsigned (result_type)) {
          const char *common_t = Wider_Int_Type (left_int_type, right_int_type);
          left = Emit_Convert_Ext (left, left_int_type, common_t, true);
          right = Emit_Convert_Ext (right, right_int_type, common_t, true);
          Emit ("  %%t%u = %s %s %%t%u, %%t%u\n", t, llvm_op, common_t, left, right);
        } else {
          left = Emit_Convert (left, Expression_Llvm_Type (node->binary.left), "i1");
          right = Emit_Convert (right, Expression_Llvm_Type (node->binary.right), "i1");
          Emit ("  %%t%u = %s i1 %%t%u, %%t%u\n", t, llvm_op, left, right);
          Temp_Set_Type (t, "i1");
        }
        return t;
      }

    // Check operand types for typed comparisons
    case TK_EQ:
    case TK_NE:
    case TK_LT:
    case TK_LE:
    case TK_GT:
    case TK_GE:
      {
        Type_Info *right_type = node->binary.right ? node->binary.right->type : NULL;
        bool left_is_float = Type_Is_Float_Representation (left_type);
        bool right_is_float = Type_Is_Float_Representation (right_type);
        bool left_is_bool = Type_Is_Boolean (left_type);
        bool right_is_bool = Type_Is_Boolean (right_type);

        // Determine actual LLVM types of operands for type-safe comparison.                        
        // Access types produce ptr, arrays produce ptr or fat_ptr,                                 
        // integers/enums produce i64. Use Expression_Llvm_Type to                                  
        // get the actual type each operand produces.                                               
        //                                                                                          
        const char *left_llvm_type = Expression_Llvm_Type (node->binary.left);
        const char *right_llvm_type = Expression_Llvm_Type (node->binary.right);

        // Boolean sub-expressions may produce i1 (raw comparisons) or                              
        // i64 (widened booleans).  Use actual expression type to avoid                             
        // double-widening when the value is already i64.                                           
        //                                                                                          
        if (not left_is_float and not right_is_float) {
          const char *int_arith = Integer_Arith_Type ();
          if (Expression_Is_Boolean (node->binary.left)) {
            left = Emit_Convert (left, left_llvm_type, int_arith);
            left_llvm_type = int_arith;
          }
          if (Expression_Is_Boolean (node->binary.right)) {
            right = Emit_Convert (right, right_llvm_type, int_arith);
            right_llvm_type = int_arith;
          }
        }

        // For non-float, non-boolean: ensure operands are same type.                               
        // If one is ptr and other is i64, convert to common type.                                  
        // Fat pointers: extract data pointer for comparison.                                       
        //                                                                                          
        if (not left_is_float and not right_is_float and
          not left_is_bool and not right_is_bool) {

          // Handle fat pointer operands - extract data pointer
          if (Llvm_Type_Is_Fat_Pointer (left_llvm_type)) {
            left = Emit_Convert (left, left_llvm_type, "ptr");
            left_llvm_type = "ptr";
          }
          if (Llvm_Type_Is_Fat_Pointer (right_llvm_type)) {
            right = Emit_Convert (right, right_llvm_type, "ptr");
            right_llvm_type = "ptr";
          }

          // Normalize: if one is ptr and other is integer, convert ptr to integer.
          // integer side may be native type (i8/i16/i32/i64).
          if (Llvm_Type_Is_Pointer (left_llvm_type) and
            right_llvm_type[0] == 'i') {
            const char *ptr_int = Integer_Arith_Type ();
            left = Emit_Convert (left, "ptr", ptr_int);
            left_llvm_type = ptr_int;
          } else if (left_llvm_type[0] == 'i' and
                 Llvm_Type_Is_Pointer (right_llvm_type)) {
            const char *ptr_int = Integer_Arith_Type ();
            right = Emit_Convert (right, "ptr", ptr_int);
            right_llvm_type = ptr_int;
          }

          // Both ptr: will use icmp eq ptr below
          // Both integer: will use common type below
        }

        // Determine float type based on left operand
        const char *float_type = Float_Llvm_Type_Of (left_type);

        // Get the right operand's float type (if it is float)
        const char *right_float_type = Float_Llvm_Type_Of (right_type);

        // UNIVERSAL_REAL always uses double (Generate_Real_Literal produces double)

        // Convert operands to same type if needed                                                  
        // Convert right to float. If it's fixed-point, multiply by SMALL.                          
        // use actual integer type; uitofp for unsigned.                                            
        //                                                                                          
        if (left_is_float and not right_is_float) {
          uint32_t conv = Emit_Temp ();
          const char *itof_cmp = Type_Is_Unsigned (right_type) ? "uitofp" : "sitofp";
          Emit ("  %%t%u = %s %s %%t%u to %s\n", conv, itof_cmp, right_llvm_type, right, float_type);
          right = conv;

          // Fixed-point: scale by SMALL to get actual value.
          // Resolve generic formal type to get actual SMALL.
          if (Type_Is_Fixed_Point (right_type)) {
            Type_Info *resolved_right = cg->current_instance ?
              Resolve_Generic_Actual_Type (right_type) : right_type;
            double small = resolved_right->fixed.small;
            if (small <= 0) small = resolved_right->fixed.delta > 0 ? resolved_right->fixed.delta : 1.0;
            uint64_t bits; memcpy (&bits, &small, sizeof (bits));
            uint32_t small_t = Emit_Temp ();
            Emit ("  %%t%u = fadd %s 0.0, 0x%016llX\n", small_t, float_type, (unsigned long long)bits);
            uint32_t scaled = Emit_Temp ();
            Emit ("  %%t%u = fmul %s %%t%u, %%t%u\n", scaled, float_type, right, small_t);
            right = scaled;
          }
          right_is_float = true;

        // Convert right float to integer for fixed-point comparison.
        // If left is fixed-point, divide by SMALL first
        } else if (not left_is_float and right_is_float) {

          // Resolve generic formal type to get actual SMALL value
          if (Type_Is_Fixed_Point (left_type)) {
            Type_Info *resolved_left = cg->current_instance ?
              Resolve_Generic_Actual_Type (left_type) : left_type;
            double small = resolved_left->fixed.small;
            if (small <= 0) small = resolved_left->fixed.delta > 0 ? resolved_left->fixed.delta : 1.0;
            uint64_t bits; memcpy (&bits, &small, sizeof (bits));
            uint32_t small_t = Emit_Temp ();
            Emit ("  %%t%u = fadd %s 0.0, 0x%016llX\n", small_t, right_float_type, (unsigned long long)bits);
            uint32_t div_t = Emit_Temp ();
            Emit ("  %%t%u = fdiv %s %%t%u, %%t%u\n", div_t, right_float_type, right, small_t);
            right = div_t;
          }
          uint32_t conv = Emit_Temp ();
          const char *ftoi_cmp = Type_Is_Unsigned (left_type) ? "fptoui" : "fptosi";
          Emit ("  %%t%u = %s %s %%t%u to %s\n", conv, ftoi_cmp, right_float_type, right, Integer_Arith_Type ());
          right = conv;
          right_is_float = false;
          right_llvm_type = Integer_Arith_Type ();

        // Both floats - use Emit_Convert which handles fpext/fptrunc
        // based on bit widths, no hardcoded type string matching needed
        } else if (left_is_float and right_is_float) {
          if (strcmp (float_type, right_float_type) != 0) {
            right = Emit_Convert (right, right_float_type, float_type);
          }
        }

        // RM 4.10: If both sides are static universal_real, fold
        // the comparison at compile time using exact rationals.
        if (left_is_float and right_is_float) {
          Rational lq, rq;
          bool l_ok = Eval_Const_Rational (node->binary.left, &lq);
          bool r_ok = Eval_Const_Rational (node->binary.right, &rq);
          if (l_ok and r_ok) {
            int cmp = Rational_Compare (lq, rq);
            bool result;
            switch (node->binary.op) {
              case TK_EQ: result = (cmp == 0); break;
              case TK_NE: result = (cmp != 0); break;
              case TK_LT: result = (cmp < 0);  break;
              case TK_LE: result = (cmp <= 0); break;
              case TK_GT: result = (cmp > 0);  break;
              case TK_GE: result = (cmp >= 0); break;
              default:    goto no_fold;
            }
            Emit ("  %%t%u = add i1 0, %d  ; folded universal_real cmp\n",
               t, result ? 1 : 0);
            Temp_Set_Type (t, "i1");
            return t;
          }
        }
        no_fold: ;
        const char *cmp_op;
        char cmp_buf[64];
        if (left_is_float and right_is_float) {
          snprintf (cmp_buf, sizeof (cmp_buf), "fcmp %s %s",
               Float_Cmp_Predicate (node->binary.op), float_type);
          cmp_op = cmp_buf;
        } else if (Llvm_Type_Is_Pointer (left_llvm_type) and
               Llvm_Type_Is_Pointer (right_llvm_type)) {
          if (node->binary.op == TK_EQ or node->binary.op == TK_NE) {
            snprintf (cmp_buf, sizeof (cmp_buf), "icmp %s ptr",
                 Int_Cmp_Predicate (node->binary.op, false));
            cmp_op = cmp_buf;

          // Ordered comparisons on pointers: convert to integer first
          } else {
            const char *iat = Integer_Arith_Type ();
            left = Emit_Convert (left, "ptr", iat);
            right = Emit_Convert (right, "ptr", iat);
            snprintf (cmp_buf, sizeof (cmp_buf), "icmp %s %s",
                 Int_Cmp_Predicate (node->binary.op, false), iat);
            cmp_op = cmp_buf;
          }

        // Float comparison that wasn't caught by left_is_float/right_is_float
        // (e.g., derived float types, universal real)
        } else if (Is_Float_Type (left_llvm_type) or Is_Float_Type (right_llvm_type)) {
          const char *fty = Is_Float_Type (left_llvm_type) ? left_llvm_type : right_llvm_type;
          if (not Is_Float_Type (left_llvm_type)) {
            uint32_t cv = Emit_Temp ();
            Emit ("  %%t%u = sitofp %s %%t%u to %s\n", cv, left_llvm_type, left, fty);
            left = cv;
          }
          if (not Is_Float_Type (right_llvm_type)) {
            uint32_t cv = Emit_Temp ();
            Emit ("  %%t%u = sitofp %s %%t%u to %s\n", cv, right_llvm_type, right, fty);
            right = cv;
          }
          snprintf (cmp_buf, sizeof (cmp_buf), "fcmp %s %s",
               Float_Cmp_Predicate (node->binary.op), fty);
          cmp_op = cmp_buf;

        // integer comparison using common native type.
        // Modular (unsigned) types use unsigned predicates.
        } else {
          const char *cmp_int_t = Wider_Int_Type (left_llvm_type, right_llvm_type);
          bool cmp_unsigned = Type_Is_Unsigned (left_type) or Type_Is_Unsigned (right_type);
          left = Emit_Convert_Ext (left, left_llvm_type, cmp_int_t, cmp_unsigned);
          right = Emit_Convert_Ext (right, right_llvm_type, cmp_int_t, cmp_unsigned);
          snprintf (cmp_buf, sizeof (cmp_buf), "icmp %s %s",
               Int_Cmp_Predicate (node->binary.op, cmp_unsigned), cmp_int_t);
          cmp_op = cmp_buf;
        }
        Emit ("  %%t%u = %s %%t%u, %%t%u\n", t, cmp_op, left, right);
        Temp_Set_Type (t, "i1");

        // comparison result stays as i1; widened at store boundary
        return t;
      }

    // Membership test - two forms:                                                                 
    //   X IN  low .. high   >  low <= X <= high                                                    
    //   X IN  T             >  T'FIRST <= X <= T'LAST                                              
    //                                                                                              
    case TK_IN:
    case TK_NOT:  // NOT IN encoded as TK_NOT binary (RM 4.4)
      {
        bool negate = (node->binary.op == TK_NOT);
        bool left_is_flt = Type_Is_Float_Representation (lhs_type);
        const char *mem_float_type = Float_Llvm_Type_Of (lhs_type);

        // Dynamic range: generate both bounds from AST
        if (node->binary.right and node->binary.right->kind == NK_RANGE) {
          uint32_t lo = Generate_Expression (node->binary.right->range.low);
          uint32_t hi = Generate_Expression (node->binary.right->range.high);
          uint32_t ge = Emit_Temp (), le = Emit_Temp (), in_range = Emit_Temp ();

          // Ensure all operands have the same float type.                                          
          // Use Expression_Llvm_Type to get the actual LLVM type, which                            
          // accounts for any conversions done during expression generation.                        
          //                                                                                        
          if (left_is_flt) {
            const char *lo_ftype = Expression_Llvm_Type (node->binary.right->range.low);
            const char *hi_ftype = Expression_Llvm_Type (node->binary.right->range.high);

            // Convert bounds to match left operand type if different
            if (strcmp (lo_ftype, mem_float_type) != 0) {
              lo = Emit_Convert (lo, lo_ftype, mem_float_type);
            }
            if (strcmp (hi_ftype, mem_float_type) != 0) {
              hi = Emit_Convert (hi, hi_ftype, mem_float_type);
            }
            Emit ("  %%t%u = fcmp oge %s %%t%u, %%t%u\n", ge, mem_float_type, left, lo);
            Emit ("  %%t%u = fcmp ole %s %%t%u, %%t%u\n", le, mem_float_type, left, hi);

          // Float membership: left is float, use fcmp
          } else if (Is_Float_Type (left_int_type)) {
            const char *lo_type = Expression_Llvm_Type (node->binary.right->range.low);
            const char *hi_type = Expression_Llvm_Type (node->binary.right->range.high);
            if (not Is_Float_Type (lo_type)) {
              uint32_t cv = Emit_Temp ();
              Emit ("  %%t%u = sitofp %s %%t%u to %s\n", cv, lo_type, lo, left_int_type);
              lo = cv;
            } else if (strcmp (lo_type, left_int_type) != 0) {
              lo = Emit_Convert (lo, lo_type, left_int_type);
            }
            if (not Is_Float_Type (hi_type)) {
              uint32_t cv = Emit_Temp ();
              Emit ("  %%t%u = sitofp %s %%t%u to %s\n", cv, hi_type, hi, left_int_type);
              hi = cv;
            } else if (strcmp (hi_type, left_int_type) != 0) {
              hi = Emit_Convert (hi, hi_type, left_int_type);
            }
            Emit ("  %%t%u = fcmp oge %s %%t%u, %%t%u\n", ge, left_int_type, left, lo);
            Emit ("  %%t%u = fcmp ole %s %%t%u, %%t%u\n", le, left_int_type, left, hi);

          // use common native type for integer membership.
          // Modular (unsigned) types use unsigned predicates.
          } else {
            bool mem_unsigned = Type_Is_Unsigned (lhs_type);
            const char *lo_type = Expression_Llvm_Type (node->binary.right->range.low);
            const char *hi_type = Expression_Llvm_Type (node->binary.right->range.high);

            // Guard: if any types are float, convert to integer first.                             
            // For fixed-point types, float bounds must be divided by                               
            // SMALL before fptosi to match the scaled representation                               
            // (RM 3.5.9: fixed_value = mantissa * SMALL).                                          
            //                                                                                      
            bool fp_scale = (lhs_type and lhs_type->kind == TYPE_FIXED);
            double fp_small = 1.0;
            if (fp_scale) {
              fp_small = lhs_type->fixed.small;
              if (fp_small <= 0) fp_small = lhs_type->fixed.delta > 0
                            ? lhs_type->fixed.delta : 1.0;
            }
            if (Is_Float_Type (lo_type)) {
              uint32_t cv = Emit_Temp (); const char *iat2 = Integer_Arith_Type ();
              if (fp_scale) {
                uint64_t sb; memcpy (&sb, &fp_small, sizeof (sb));
                uint32_t st = Emit_Temp ();
                Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small\n",
                   st, (unsigned long long)sb);
                uint32_t dv = Emit_Temp ();
                Emit ("  %%t%u = fdiv %s %%t%u, %%t%u  ; bound/small\n",
                   dv, lo_type, lo, st);
                Emit ("  %%t%u = fptosi %s %%t%u to %s\n", cv, lo_type, dv, iat2);
              } else {
                Emit ("  %%t%u = fptosi %s %%t%u to %s\n", cv, lo_type, lo, iat2);
              }
              lo = cv; lo_type = iat2;
            }
            if (Is_Float_Type (hi_type)) {
              uint32_t cv = Emit_Temp (); const char *iat2 = Integer_Arith_Type ();
              if (fp_scale) {
                uint64_t sb; memcpy (&sb, &fp_small, sizeof (sb));
                uint32_t st = Emit_Temp ();
                Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small\n",
                   st, (unsigned long long)sb);
                uint32_t dv = Emit_Temp ();
                Emit ("  %%t%u = fdiv %s %%t%u, %%t%u  ; bound/small\n",
                   dv, hi_type, hi, st);
                Emit ("  %%t%u = fptosi %s %%t%u to %s\n", cv, hi_type, dv, iat2);
              } else {
                Emit ("  %%t%u = fptosi %s %%t%u to %s\n", cv, hi_type, hi, iat2);
              }
              hi = cv; hi_type = iat2;
            }
            if (Is_Float_Type (left_int_type)) {
              uint32_t cv = Emit_Temp (); const char *iat2 = Integer_Arith_Type ();
              Emit ("  %%t%u = fptosi %s %%t%u to %s\n", cv, left_int_type, left, iat2);
              left = cv; left_int_type = iat2;
            }
            const char *mem_ct = Wider_Int_Type (left_int_type, Wider_Int_Type (lo_type, hi_type));
            uint32_t ml = Emit_Convert_Ext (left, left_int_type, mem_ct, mem_unsigned);
            lo = Emit_Convert_Ext (lo, lo_type, mem_ct, mem_unsigned);
            hi = Emit_Convert_Ext (hi, hi_type, mem_ct, mem_unsigned);
            const char *ge_pred = mem_unsigned ? "uge" : "sge";
            const char *le_pred = mem_unsigned ? "ule" : "sle";
            Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", ge, ge_pred, mem_ct, ml, lo);
            Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", le, le_pred, mem_ct, ml, hi);
          }
          Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", in_range, ge, le);
          if (negate) { Emit ("  %%t%u = xor i1 %%t%u, 1\n", t, in_range); }
          else        { t = in_range; }
        } else if (node->binary.right and node->binary.right->kind == NK_ATTRIBUTE and
               Slice_Equal_Ignore_Case (node->binary.right->attribute.name, S("RANGE"))) {

          // X IN A'RANGE or X IN A'RANGE (N) - expand to A'FIRST (N) <= X <= A'LAST (N)
          Syntax_Node *attr_node = node->binary.right;
          Type_Info *arr_type = attr_node->attribute.prefix->type;
          Symbol *arr_sym = attr_node->attribute.prefix->symbol;
          Syntax_Node *range_dim_arg = attr_node->attribute.arguments.count > 0
                         ? attr_node->attribute.arguments.items[0] : NULL;
          uint32_t rdim = Get_Dimension_Index (range_dim_arg);
          uint32_t range_low, range_high;
          bool arr_needs_rt = false;
          if (arr_type and (Type_Is_Unconstrained_Array (arr_type) or Type_Has_Dynamic_Bounds (arr_type)) and
            arr_sym and (arr_sym->kind == SYMBOL_PARAMETER or arr_sym->kind == SYMBOL_VARIABLE or
                  arr_sym->kind == SYMBOL_CONSTANT or arr_sym->kind == SYMBOL_DISCRIMINANT))
            arr_needs_rt = true;
          if (arr_needs_rt) {
            const char *rbt = Array_Bound_Llvm_Type (arr_type);
            uint32_t fat = Emit_Load_Fat_Pointer (arr_sym, rbt);
            range_low  = Emit_Fat_Pointer_Low_Dim (fat, rbt, rdim);
            range_high = Emit_Fat_Pointer_High_Dim (fat, rbt, rdim);
          } else if (arr_type and Type_Is_Array_Like (arr_type) and rdim < arr_type->array.index_count) {
            const char *iat = Integer_Arith_Type ();
            Type_Bound lb = arr_type->array.indices[rdim].low_bound;
            Type_Bound hb = arr_type->array.indices[rdim].high_bound;

            // If bounds not set on array, derive from index type
            if (lb.kind != BOUND_INTEGER and lb.kind != BOUND_EXPR and
              arr_type->array.indices[rdim].index_type) {
              Type_Info *idx_ty = arr_type->array.indices[rdim].index_type;
              lb = idx_ty->low_bound;
              hb = idx_ty->high_bound;
            }
            range_low  = Emit_Single_Bound (&lb, iat);
            range_high = Emit_Single_Bound (&hb, iat);

          // Fallback - can't determine bounds
          } else {
            uint32_t always = Emit_Temp ();
            Emit ("  %%t%u = add i1 0, 1  ; range membership fallback\n", always);
            if (negate) { Emit ("  %%t%u = xor i1 %%t%u, 1\n", t, always); }
            else        { t = always; }
            return t;
          }

          // Compare left IN lo..hi
          const char *rng_iat = Integer_Arith_Type ();
          uint32_t ml = Emit_Convert (left, left_int_type, rng_iat);
          range_low  = Emit_Convert (range_low, rng_iat, rng_iat);
          range_high = Emit_Convert (range_high, rng_iat, rng_iat);
          uint32_t ge = Emit_Temp (), le = Emit_Temp (), in_range = Emit_Temp ();
          Emit ("  %%t%u = icmp sge %s %%t%u, %%t%u\n", ge, rng_iat, ml, range_low);
          Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", le, rng_iat, ml, range_high);
          Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", in_range, ge, le);
          if (negate) { Emit ("  %%t%u = xor i1 %%t%u, 1\n", t, in_range); }
          else        { t = in_range; }

        // Type or subtype name: generate bounds at runtime (RM 4.4)
        } else {
          Type_Info *range_type = node->binary.right ? node->binary.right->type : NULL;

          // Composite types (records, arrays) and access/task types:                               
          // membership is always TRUE since the value is already of                                
          // that type (RM 4.5.2). Access types have no range.                                      
          //                                                                                        
          if (range_type and (Type_Is_Composite (range_type) or Type_Is_Access (range_type) or
                range_type->kind == TYPE_TASK)) {
            uint32_t always = Emit_Temp ();
            Emit ("  %%t%u = add i1 0, 1  ; composite IN is always true\n", always);
            if (negate) { Emit ("  %%t%u = xor i1 %%t%u, 1\n", t, always); }
            else        { t = always; }
            return t;
          }
          if (range_type and (range_type->low_bound.kind == BOUND_INTEGER or range_type->low_bound.kind == BOUND_EXPR) and
                (range_type->high_bound.kind == BOUND_INTEGER or range_type->high_bound.kind == BOUND_EXPR)) {
            const char *lo_bt = NULL, *hi_bt = NULL;
            uint32_t bound_low  = Emit_Bound_Value_Typed (&range_type->low_bound, &lo_bt);
            uint32_t bound_high = Emit_Bound_Value_Typed (&range_type->high_bound, &hi_bt);

            // Bounds could not be determined - assume always in range
            if (bound_low == 0 or bound_high == 0) {
              uint32_t always = Emit_Temp ();
              Emit ("  %%t%u = add i1 0, 1  ; membership fallback (no bounds)\n", always);
              if (negate) { Emit ("  %%t%u = xor i1 %%t%u, 1\n", t, always); }
              else        { t = always; }
              return t;
            }
            if (not lo_bt) lo_bt = Integer_Arith_Type ();
            if (not hi_bt) hi_bt = Integer_Arith_Type ();
            uint32_t ge = Emit_Temp (), le = Emit_Temp (), in_range = Emit_Temp ();

            // For float membership tests, determine bound types and convert if needed.
            // BOUND_INTEGER may produce i32 or i64, BOUND_EXPR for float types produces float.
            if (left_is_flt) {
              const char *lo_src_type = lo_bt;
              const char *hi_src_type = hi_bt;

              // Convert bounds to match left operand's float type
              uint32_t lo_f = bound_low, hi_f = bound_high;
              if (strcmp (lo_src_type, mem_float_type) != 0) {
                lo_f = Emit_Convert (bound_low, lo_src_type, mem_float_type);
              }
              if (strcmp (hi_src_type, mem_float_type) != 0) {
                hi_f = Emit_Convert (bound_high, hi_src_type, mem_float_type);
              }
              Emit ("  %%t%u = fcmp oge %s %%t%u, %%t%u\n", ge, mem_float_type, left, lo_f);
              Emit ("  %%t%u = fcmp ole %s %%t%u, %%t%u\n", le, mem_float_type, left, hi_f);

            // widen left and bounds to widest type for comparison.
            // Modular (unsigned) types use unsigned predicates.
            } else {
              bool mem_u = Type_Is_Unsigned (lhs_type);
              const char *cmp_t = Integer_Arith_Type ();
              if (left_int_type[0] == 'i') cmp_t = Wider_Int_Type (cmp_t, left_int_type);
              if (lo_bt[0] == 'i') cmp_t = Wider_Int_Type (cmp_t, lo_bt);
              if (hi_bt[0] == 'i') cmp_t = Wider_Int_Type (cmp_t, hi_bt);
              uint32_t ml = Emit_Convert_Ext (left, left_int_type, cmp_t, mem_u);
              uint32_t wlo = Emit_Convert_Ext (bound_low, lo_bt, cmp_t, mem_u);
              uint32_t whi = Emit_Convert_Ext (bound_high, hi_bt, cmp_t, mem_u);
              const char *ge_p = mem_u ? "uge" : "sge";
              const char *le_p = mem_u ? "ule" : "sle";
              Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", ge, ge_p, cmp_t, ml, wlo);
              Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u\n", le, le_p, cmp_t, ml, whi);
            }
            Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", in_range, ge, le);
            if (negate) { Emit ("  %%t%u = xor i1 %%t%u, 1\n", t, in_range); }
            else        { t = in_range; }

          // Fallback: equality with right operand value
          } else {
            if (left_is_flt) {
              Emit ("  %%t%u = fcmp oeq %s %%t%u, %%t%u\n", t, mem_float_type, left, right);

            // use common native type for equality.
            } else {
              const char *fb_ct = Wider_Int_Type (left_int_type, right_int_type);
              uint32_t ml = Emit_Convert (left, left_int_type, fb_ct);
              uint32_t mr = Emit_Convert (right, right_int_type, fb_ct);
              Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n", t, fb_ct, ml, mr);
            }
            if (negate) {
              uint32_t neg = Emit_Temp ();
              Emit ("  %%t%u = xor i1 %%t%u, 1\n", neg, t);
              t = neg;
            }
          }
        }

        // membership result stays as i1; widened at store boundary
        Temp_Set_Type (t, "i1");
        return t;
      }

    // Type system said not float, but LLVM types are float (derived float).
    // Promote to float arithmetic.
    default:
      fprintf (stderr, "internal error: unhandled binary operator %d in codegen\n",
          node->binary.op);
      abort ();
  }
  if (not is_float and (Is_Float_Type (left_int_type) or Is_Float_Type (right_int_type))) {
    is_float = true;
    float_type_str = Is_Float_Type (left_int_type) ? left_int_type : right_int_type;
    if (not Is_Float_Type (left_int_type)) {
      uint32_t cv = Emit_Temp ();
      Emit ("  %%t%u = sitofp %s %%t%u to %s\n", cv, left_int_type, left, float_type_str);
      left = cv;
    }
    if (not Is_Float_Type (right_int_type)) {
      uint32_t cv = Emit_Temp ();
      Emit ("  %%t%u = sitofp %s %%t%u to %s\n", cv, right_int_type, right, float_type_str);
      right = cv;
    }
  }

  // use common native integer type for arithmetic.
  if (not is_float) {
    const char *common_t = Wider_Int_Type (left_int_type, right_int_type);
    left = Emit_Convert (left, left_int_type, common_t);
    right = Emit_Convert (right, right_int_type, common_t);

    // Division/remainder: emit division-by-zero check and MIN_INT/-1 check
    // before the actual sdiv/udiv/srem/urem (RM 4.5.5).
    Token_Kind binop = node->binary.op;
    if (binop == TK_SLASH or binop == TK_MOD or binop == TK_REM) {
      Emit_Division_Check (right, common_t, result_type);
      if (binop == TK_SLASH and not lhs_unsigned) {
        Emit_Signed_Division_Overflow_Check (left, right, common_t, result_type);
      }
      Emit ("  %%t%u = %s %s %%t%u, %%t%u\n", t, op, common_t, left, right);
      Temp_Set_Type (t, common_t);

      // Ada MOD correction: MOD result has sign of divisor (RM 4.5.5)
      // srem result has sign of dividend. When signs differ, add divisor.
      if (binop == TK_MOD and not lhs_unsigned) {
        uint32_t r_ne_zero = Emit_Temp ();
        Emit ("  %%t%u = icmp ne %s %%t%u, 0\n", r_ne_zero, common_t, t);
        uint32_t r_xor_b = Emit_Temp ();
        Emit ("  %%t%u = xor %s %%t%u, %%t%u\n", r_xor_b, common_t, t, right);
        uint32_t signs_differ = Emit_Temp ();
        Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", signs_differ, common_t, r_xor_b);
        uint32_t need_fix = Emit_Temp ();
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", need_fix, r_ne_zero, signs_differ);
        uint32_t r_plus_b = Emit_Temp ();
        Emit ("  %%t%u = add %s %%t%u, %%t%u\n", r_plus_b, common_t, t, right);
        uint32_t mod_result = Emit_Temp ();
        Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
           mod_result, need_fix, common_t, r_plus_b, common_t, t);
        t = mod_result;
        Temp_Set_Type (t, common_t);
      }
    }

    // Add/sub/mul: use overflow-checked intrinsics for signed types (RM 4.5).
    // Modular types wrap naturally; Emit_Overflow_Checked_Op handles this.
    else if (binop == TK_PLUS or binop == TK_MINUS or binop == TK_STAR) {
      t = Emit_Overflow_Checked_Op (left, right, op, common_t, result_type);
    } else {
      Emit ("  %%t%u = %s %s %%t%u, %%t%u\n", t, op, common_t, left, right);
      Temp_Set_Type (t, common_t);
    }

    // Modular wrapping: Ada modular arithmetic wraps modulo M (RM 4.5.3).                          
    // For power-of-2 moduli, LLVM's natural integer wrapping suffices.                             
    // For non-power-of-2 moduli (e.g. mod 100), emit: urem result, modulus.                        
    // Only applies to add, sub, mul - div/rem already produce in-range values.                     
    //                                                                                              
    if (result_type and result_type->kind == TYPE_MODULAR and result_type->modulus > 0) {
      uint128_t m = result_type->modulus;

      // Check if modulus is a power of 2: if so, no masking needed
      if ((m & (m - 1)) != 0) {
        if (binop == TK_PLUS or binop == TK_MINUS or binop == TK_STAR) {
          uint32_t mod_val = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %s  ; modulus\n", mod_val, common_t,
             U128_Decimal (m));
          uint32_t wrapped = Emit_Temp ();
          Emit ("  %%t%u = urem %s %%t%u, %%t%u\n", wrapped, common_t, t, mod_val);
          t = wrapped;
        }
      }
    }

  // RM 4.5.5: float division by zero raises CONSTRAINT_ERROR
  } else {
    if (node->binary.op == TK_SLASH and
      not Check_Is_Suppressed (result_type, NULL, CHK_DIVISION)) {
      uint32_t fz = Emit_Temp ();
      Emit ("  %%t%u = fcmp oeq %s %%t%u, 0.0\n", fz, float_type_str, right);
      uint32_t ok = Emit_Label (), bad = Emit_Label ();
      Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", fz, bad, ok);
      cg->block_terminated = true;
      Emit_Label_Here (bad);
      cg->block_terminated = false;
      Emit_Raise_Constraint_Error ("float division by zero");
      Emit_Label_Here (ok);
      cg->block_terminated = false;
    }
    Emit ("  %%t%u = %s %s %%t%u, %%t%u\n", t, op, float_type_str, left, right);
  }
  return t;
}
uint32_t Generate_Unary_Op (Syntax_Node *node) {

  // User-defined unary operator: generate function call (RM 6.7)
  if (node->symbol and node->symbol->kind == SYMBOL_FUNCTION and
    not node->symbol->is_predefined) {

    // Resolve through parent_operation for derived operators (RM 3.4)
    Symbol *call_target = node->symbol->parent_operation ?
      node->symbol->parent_operation : node->symbol;
    uint32_t operand = Generate_Expression (node->unary.operand);
    const char *op_llvm = Expression_Llvm_Type (node->unary.operand);
    Type_Info *p0_type = (call_target->parameter_count > 0) ?
      call_target->parameters[0].param_type : NULL;
    const char *p0_llvm = p0_type ? Type_To_Llvm (p0_type) : op_llvm;
    operand = Emit_Convert (operand, op_llvm, p0_llvm);
    const char *ret_type = call_target->return_type ?
      Type_To_Llvm (call_target->return_type) : "i32";
    bool callee_is_nested = Subprogram_Needs_Static_Chain (call_target);
    uint32_t frame_pre = callee_is_nested ?
      Precompute_Nested_Frame_Arg (call_target) : 0;
    uint32_t result = Emit_Temp ();
    Emit ("  %%t%u = call %s @", result, ret_type);
    Emit_Symbol_Name (call_target);
    Emit ("(");
    if (callee_is_nested) {
      Emit_Nested_Frame_Arg (call_target, frame_pre);
      Emit (", ");
    }
    Emit ("%s %%t%u)\n", p0_llvm, operand);
    Temp_Set_Type (result, ret_type);
    return result;
  }
  uint32_t operand = Generate_Expression (node->unary.operand);
  uint32_t t = Emit_Temp ();
  Type_Info *op_type_info = node->unary.operand->type;
  bool is_float = Type_Is_Float_Representation (op_type_info);

  // Determine LLVM float type from operand type
  const char *float_type = Float_Llvm_Type_Of (op_type_info);

  // determine native integer type for unary operations.
  // Fixed-point types use integer representation at LLVM level.
  const char *unary_int_type = is_float ? Integer_Arith_Type () : Expression_Llvm_Type (node->unary.operand);
  switch (node->unary.op) {
    case TK_MINUS:
      if (is_float) {
        Emit ("  %%t%u = fsub %s 0.0, %%t%u\n", t, float_type, operand);
        Temp_Set_Type (t, float_type);

      // Unary negation: 0 - operand.  Overflow check for signed types:
      // -Integer'First overflows on two's complement (RM 4.5).
      } else {
        Type_Info *res_type = node->type ? node->type : op_type_info;
        uint32_t zero = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, 0\n", zero, unary_int_type);
        t = Emit_Overflow_Checked_Op (zero, operand, "sub", unary_int_type, res_type);

        // Modular wrapping for unary minus (RM 4.5.3): -x = modulus - x.                           
        // For power-of-2 moduli, the sub already wraps correctly.                                  
        // For non-power-of-2, emit urem to wrap into 0..M-1.                                       
        //                                                                                          
        if (res_type and res_type->kind == TYPE_MODULAR and res_type->modulus > 0) {
          uint128_t m = res_type->modulus;
          if ((m & (m - 1)) != 0) {
            uint32_t mod_val = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %s  ; modulus\n", mod_val, unary_int_type,
               U128_Decimal (m));
            uint32_t wrapped = Emit_Temp ();
            Emit ("  %%t%u = urem %s %%t%u, %%t%u\n", wrapped, unary_int_type, t, mod_val);
            t = wrapped;
          }
        }
      }
      break;
    case TK_PLUS:
      return operand;
    case TK_NOT:
      {
        Type_Info *res_type = node->type ? node->type : op_type_info;

        // Modular NOT is bitwise complement modulo M (RM 4.5.6).                                   
        // For power-of-2 moduli: XOR with (M-1) gives correct masking.                             
        // For non-power-of-2: same - XOR with (M-1) is the Ada definition.                         
        //                                                                                          
        if (res_type and res_type->kind == TYPE_MODULAR) {
          uint128_t mask = (res_type->modulus > 0) ? res_type->modulus - 1 : (uint128_t)~0ULL;
          Emit ("  %%t%u = xor %s %%t%u, %s  ; modular NOT\n",
             t, unary_int_type, operand, U128_Decimal (mask));
          Temp_Set_Type (t, unary_int_type);
        } else if (Type_Is_Bool_Array (res_type)) {
          t = Emit_Bool_Array_Not (operand, res_type);

        // Boolean NOT: convert to i1, flip - result stays i1 (Standard)
        } else {
          const char *op_type = Expression_Llvm_Type (node->unary.operand);
          operand = Emit_Convert (operand, op_type, "i1");
          Emit ("  %%t%u = xor i1 %%t%u, 1\n", t, operand);
          Temp_Set_Type (t, "i1");

          // result stays as i1; widened at store boundary
        }
      }
      break;
    case TK_ABS:
      {
        if (is_float) {
          uint32_t neg = Emit_Temp ();
          uint32_t cmp = Emit_Temp ();
          Emit ("  %%t%u = fsub %s 0.0, %%t%u\n", neg, float_type, operand);
          Emit ("  %%t%u = fcmp olt %s %%t%u, 0.0\n", cmp, float_type, operand);
          Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
             t, cmp, float_type, neg, float_type, operand);
          Temp_Set_Type (t, float_type);

        // Integer ABS with overflow check: ABS (MIN_INT) overflows.
        // Use Emit_Overflow_Checked_Op for negation, then select.
        } else {
          Type_Info *res_type = node->type ? node->type : op_type_info;
          uint32_t zero = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, 0\n", zero, unary_int_type);
          uint32_t neg = Emit_Overflow_Checked_Op (zero, operand, "sub", unary_int_type, res_type);
          uint32_t cmp = Emit_Temp ();
          Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", cmp, unary_int_type, operand);
          Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
             t, cmp, unary_int_type, neg, unary_int_type, operand);
          Temp_Set_Type (t, unary_int_type);
        }
      }
      break;

    // .ALL dereference: operand is pointer, load the value
    case TK_ALL:
      {
        Emit_Access_Check (operand, node->unary.operand->type);
        Type_Info *designated = node->type;  // Set by resolution

        // For composite types, pointer is the value
        if (Type_Is_Composite (designated)) {
          return operand;
        }

        // For scalar types, load the value from the pointer
        const char *type_str = Type_To_Llvm (designated);
        Emit ("  %%t%u = load %s, ptr %%t%u  ; .ALL dereference\n",
           t, type_str, operand);
        Temp_Set_Type (t, type_str);

        // no widening at load - value stays at native type width.
        // Conversions happen at use sites via Emit_Convert.
      }
      break;
    default:
      fprintf (stderr, "internal error: unhandled unary operator %d in codegen\n",
          node->unary.op);
      abort ();
  }
  return t;
}
uint32_t Generate_Apply (Syntax_Node *node) {
  Symbol *sym = node->apply.prefix->symbol;

  // Emit call/apply comment if symbol is known
  if (sym) {
    Emit ("  ; apply %.*s (", (int)sym->name.length, sym->name.data);
    if (sym->kind == SYMBOL_FUNCTION)
      Emit ("function");
    else if (sym->kind == SYMBOL_PROCEDURE)
      Emit ("procedure");
    else if (sym->kind == SYMBOL_TYPE or sym->kind == SYMBOL_SUBTYPE)
      Emit ("type conversion");
    else
      Emit ("symbol");
    Emit (")\n");
  }

  // Follow rename chain to get actual target symbol for code generation.
  // Renames don't generate their own function body - they call the target.
  while (sym and sym->renamed_object and
       (sym->kind == SYMBOL_FUNCTION or sym->kind == SYMBOL_PROCEDURE)) {
    Symbol *target = (Symbol *)sym->renamed_object;
    if (target->kind == SYMBOL_FUNCTION or target->kind == SYMBOL_PROCEDURE) {
      sym = target;
    } else {
      break;
    }
  }

  // Predefined operator called as function: P."="(X,Y) or "NOT"(X) etc.                            
  // These have is_predefined set and no body, or the prefix is an operator                         
  // symbol identifier with no symbol (resolved in semantic analysis).                              
  //                                                                                                
  bool is_operator_symbol = (sym and sym->is_predefined) or
    (not sym and node->apply.prefix->kind == NK_IDENTIFIER and
     node->apply.prefix->string_val.text.length <= 3);
  if (is_operator_symbol and node->apply.arguments.count >= 1) {
    String_Slice op_name = sym ? sym->name : node->apply.prefix->string_val.text;
    uint32_t argc = (uint32_t)node->apply.arguments.count;

    // Get first argument
    Syntax_Node *arg0 = node->apply.arguments.items[0];
    if (arg0->kind == NK_ASSOCIATION) arg0 = arg0->association.expression;
    uint32_t v0 = Generate_Expression (arg0);
    const char *t0 = Expression_Llvm_Type (arg0);
    Type_Info *ty0 = arg0->type;
    if (argc == 2) {
      Syntax_Node *arg1 = node->apply.arguments.items[1];
      if (arg1->kind == NK_ASSOCIATION) arg1 = arg1->association.expression;
      uint32_t v1 = Generate_Expression (arg1);
      const char *t1 = Expression_Llvm_Type (arg1);

      // Composite equality/inequality: dispatch to record/array equality
      // before scalar widening (RM 4.5.2)
      if (ty0 and (Type_Is_Record (ty0) or Type_Is_Array_Like (ty0)) and
        (Slice_Equal_Ignore_Case (op_name, S("=")) or
         Slice_Equal_Ignore_Case (op_name, S("/=")))) {
        uint32_t eq_result;
        if (Type_Is_Record (ty0)) {
          eq_result = Generate_Record_Equality (v0, v1, ty0);
        } else {
          eq_result = Generate_Array_Equality (v0, v1, ty0);
        }
        if (Slice_Equal_Ignore_Case (op_name, S("/="))) {
          uint32_t ne_r = Emit_Temp ();
          Emit ("  %%t%u = xor i1 %%t%u, 1\n", ne_r, eq_result);
          eq_result = ne_r;
        }

        // Named operator returns BOOLEAN (i8), not bare i1
        const char *bool_t = (sym and sym->return_type) ? Type_To_Llvm (sym->return_type) : "i8";
        if (bool_t[0] == 'i' and bool_t[1] != '1') {
          uint32_t ext = Emit_Temp ();
          Emit ("  %%t%u = zext i1 %%t%u to %s\n", ext, eq_result, bool_t);
          eq_result = ext;
          Temp_Set_Type (eq_result, bool_t);
        }
        return eq_result;
      }

      // Widen to common type (scalar operands only)
      const char *ct = Wider_Int_Type (t0, t1);
      bool uns = Type_Is_Unsigned (ty0);
      v0 = Emit_Convert_Ext (v0, t0, ct, uns);
      v1 = Emit_Convert_Ext (v1, t1, ct, uns);

      // Comparison operators
      int cmp_tk = -1;
      if (Slice_Equal_Ignore_Case (op_name, S("=")))  cmp_tk = TK_EQ;
      else if (Slice_Equal_Ignore_Case (op_name, S("/="))) cmp_tk = TK_NE;
      else if (Slice_Equal_Ignore_Case (op_name, S("<")))  cmp_tk = TK_LT;
      else if (Slice_Equal_Ignore_Case (op_name, S("<="))) cmp_tk = TK_LE;
      else if (Slice_Equal_Ignore_Case (op_name, S(">")))  cmp_tk = TK_GT;
      else if (Slice_Equal_Ignore_Case (op_name, S(">="))) cmp_tk = TK_GE;
      if (cmp_tk >= 0) {
        uint32_t result = Emit_Temp ();
        Emit ("  %%t%u = icmp %s %s %%t%u, %%t%u  ; predef %.*s\n",
           result, Int_Cmp_Predicate (cmp_tk, uns), ct, v0, v1,
           (int)op_name.length, op_name.data);

        // Widen i1 > i8 for Ada BOOLEAN
        uint32_t widened = Emit_Temp ();
        Emit ("  %%t%u = zext i1 %%t%u to i8\n", widened, result);
        Temp_Set_Type (widened, "i8");
        return widened;
      }

      // Arithmetic operators
      const char *arith_op = NULL;
      if (Slice_Equal_Ignore_Case (op_name, S("+")))   arith_op = "add";
      else if (Slice_Equal_Ignore_Case (op_name, S("-")))   arith_op = "sub";
      else if (Slice_Equal_Ignore_Case (op_name, S("*")))   arith_op = "mul";
      else if (Slice_Equal_Ignore_Case (op_name, S("/")))   arith_op = uns ? "udiv" : "sdiv";
      else if (Slice_Equal_Ignore_Case (op_name, S("mod"))) arith_op = uns ? "urem" : "srem";
      else if (Slice_Equal_Ignore_Case (op_name, S("rem"))) arith_op = uns ? "urem" : "srem";
      if (arith_op) {
        uint32_t result = Emit_Temp ();
        Emit ("  %%t%u = %s %s %%t%u, %%t%u  ; predef %.*s\n",
           result, arith_op, ct, v0, v1,
           (int)op_name.length, op_name.data);

        // Ada MOD correction for named operator
        if (Slice_Equal_Ignore_Case (op_name, S("mod")) and not uns) {
          uint32_t nonzero   = Emit_Temp ();
          Emit ("  %%t%u = icmp ne %s %%t%u, 0\n", nonzero, ct, result);
          uint32_t sign_xor  = Emit_Temp ();
          Emit ("  %%t%u = xor %s %%t%u, %%t%u\n", sign_xor, ct, result, v1);
          uint32_t sign_diff = Emit_Temp ();
          Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", sign_diff, ct, sign_xor);
          uint32_t need_fix  = Emit_Temp ();
          Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", need_fix, nonzero, sign_diff);
          uint32_t adjusted  = Emit_Temp ();
          Emit ("  %%t%u = add %s %%t%u, %%t%u\n", adjusted, ct, result, v1);
          uint32_t corrected = Emit_Temp ();
          Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
             corrected, need_fix, ct, adjusted, ct, result);
          result = corrected;
        }
        return result;
      }

      // Exponentiation: "**"(base, exp)
      if (Slice_Equal_Ignore_Case (op_name, S("**"))) {
        const char *iat = Integer_Arith_Type ();
        v0 = Emit_Convert (v0, t0, iat);
        v1 = Emit_Convert (v1, t1, iat);
        uint32_t result = Emit_Temp ();
        Emit ("  %%t%u = call %s @__ada_integer_pow (%s %%t%u, %s %%t%u)\n",
           result, iat, iat, v0, iat, v1);
        Temp_Set_Type (result, iat);
        return result;
      }

      // Boolean operators
      if (Slice_Equal_Ignore_Case (op_name, S("and")) or
        Slice_Equal_Ignore_Case (op_name, S("or")) or
        Slice_Equal_Ignore_Case (op_name, S("xor"))) {
        const char *bool_op = Slice_Equal_Ignore_Case (op_name, S("and")) ? "and" :
                    Slice_Equal_Ignore_Case (op_name, S("or")) ? "or" : "xor";
        v0 = Emit_Convert (v0, t0, "i1");
        v1 = Emit_Convert (v1, t1, "i1");
        uint32_t result = Emit_Temp ();
        Emit ("  %%t%u = %s i1 %%t%u, %%t%u\n", result, bool_op, v0, v1);
        uint32_t widened = Emit_Temp ();
        Emit ("  %%t%u = zext i1 %%t%u to i8\n", widened, result);
        Temp_Set_Type (widened, "i8");
        return widened;
      }

    // Unary operators: abs, not, unary -
    } else if (argc == 1) {

      // ABS with overflow check: ABS (MIN_INT) overflows
      if (Slice_Equal_Ignore_Case (op_name, S("abs"))) {
        uint32_t zero = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, 0\n", zero, t0);
        uint32_t neg = Emit_Overflow_Checked_Op (zero, v0, "sub", t0, ty0);
        uint32_t cmp = Emit_Temp ();
        Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", cmp, t0, v0);
        uint32_t result = Emit_Temp ();
        Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
           result, cmp, t0, neg, t0, v0);
        return result;
      }
      if (Slice_Equal_Ignore_Case (op_name, S("not"))) {
        v0 = Emit_Convert (v0, t0, "i1");
        uint32_t result = Emit_Temp ();
        Emit ("  %%t%u = xor i1 %%t%u, true\n", result, v0);
        uint32_t widened = Emit_Temp ();
        Emit ("  %%t%u = zext i1 %%t%u to i8\n", widened, result);
        Temp_Set_Type (widened, "i8");
        return widened;
      }
      if (Slice_Equal_Ignore_Case (op_name, S("-"))) {
        uint32_t result = Emit_Temp ();
        Emit ("  %%t%u = sub %s 0, %%t%u\n", result, t0, v0);
        return result;
      }
    }

    // Fall through to regular call for unhandled operators
  }

  // Generic formal subprogram substitution: if calling a formal subprogram                         
  // inside a generic instantiation, substitute with the actual subprogram                          
  // or generate inline code for built-in operators.                                                
  // For subprograms exported from generic packages, the actuals are on the                         
  // parent package instance, not on the subprogram itself.                                         
  //                                                                                                
  Symbol *actuals_holder = cg->current_instance;
  if (actuals_holder and not actuals_holder->generic_actuals and actuals_holder->parent and
    actuals_holder->parent->kind == SYMBOL_PACKAGE and actuals_holder->parent->generic_actuals) {
    actuals_holder = actuals_holder->parent;  // Use package's generic_actuals
  }
  if (sym and actuals_holder and actuals_holder->generic_actuals) {
    for (uint32_t i = 0; i < actuals_holder->generic_actual_count; i++) {
      if (Slice_Equal_Ignore_Case (sym->name,
          actuals_holder->generic_actuals[i].formal_name)) {
        if (actuals_holder->generic_actuals[i].actual_subprogram) {
          sym = actuals_holder->generic_actuals[i].actual_subprogram;

        // Built-in operator - generate inline
        } else if (actuals_holder->generic_actuals[i].builtin_operator) {
          Token_Kind op = cg->current_instance->generic_actuals[i].builtin_operator;

          // String/array concatenation
          if (op == TK_AMPERSAND and node->apply.arguments.count == 2) {
            Syntax_Node *left_arg = node->apply.arguments.items[0];
            Syntax_Node *right_arg = node->apply.arguments.items[1];
            if (left_arg->kind == NK_ASSOCIATION)
              left_arg = left_arg->association.expression;
            if (right_arg->kind == NK_ASSOCIATION)
              right_arg = right_arg->association.expression;

            // Get parameter types from formal subprogram symbol
            Type_Info *left_type = (sym and sym->parameter_count > 0) ?
              sym->parameters[0].param_type : left_arg->type;
            Type_Info *right_type = (sym and sym->parameter_count > 1) ?
              sym->parameters[1].param_type : right_arg->type;

            // Substitute generic formal types with actual types
            if (actuals_holder and actuals_holder->generic_actuals) {
              for (uint32_t k = 0; k < actuals_holder->generic_actual_count; k++) {
                if (left_type and left_type->name.data and
                  Slice_Equal_Ignore_Case (left_type->name,
                    actuals_holder->generic_actuals[k].formal_name) and
                  actuals_holder->generic_actuals[k].actual_type) {
                  left_type = actuals_holder->generic_actuals[k].actual_type;
                }
                if (right_type and right_type->name.data and
                  Slice_Equal_Ignore_Case (right_type->name,
                    actuals_holder->generic_actuals[k].formal_name) and
                  actuals_holder->generic_actuals[k].actual_type) {
                  right_type = actuals_holder->generic_actuals[k].actual_type;
                }
              }
            }

            // Check if first arg is CHARACTER (single byte)
            bool left_is_char = Type_Is_Character (left_type);
            bool right_is_string = Type_Is_Unconstrained_Array (right_type) or
                         (not Type_Is_Constrained_Array (right_type) and Type_Is_String (right_type));
            uint32_t left_val = Generate_Expression (left_arg);
            uint32_t right_val = Generate_Expression (right_arg);
            const char *concat_bt = Array_Bound_Llvm_Type (right_type);

            // CHARACTER & STRING concatenation
            if (left_is_char and right_is_string) {

              // Wrap character in single-element fat pointer
              uint32_t char_alloc = Emit_Temp ();
              Emit ("  %%t%u = alloca i8\n", char_alloc);
              const char *char_src_type = Expression_Llvm_Type (left_arg);
              uint32_t char_trunc = Emit_Convert (left_val, char_src_type, "i8");
              Emit ("  store i8 %%t%u, ptr %%t%u\n", char_trunc, char_alloc);
              uint32_t one = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, 1\n", one, concat_bt);

              // Extract right string bounds and data
              uint32_t right_data = Emit_Fat_Pointer_Data (right_val, concat_bt);
              uint32_t right_low = Emit_Fat_Pointer_Low (right_val, concat_bt);
              uint32_t right_high = Emit_Fat_Pointer_High (right_val, concat_bt);
              uint32_t right_len1 = Emit_Length_From_Bounds (right_low, right_high, concat_bt);

              // Total length = 1 + right_len
              uint32_t total_len = Emit_Temp ();
              Emit ("  %%t%u = add %s 1, %%t%u\n", total_len, concat_bt, right_len1);

              // Extend to i64 for C-level calls
              uint32_t total_len_64 = Emit_Extend_To_I64 (total_len, concat_bt);
              uint32_t right_len1_64 = Emit_Extend_To_I64 (right_len1, concat_bt);

              // Allocate result buffer
              uint32_t result_data = Emit_Temp ();
              Emit ("  %%t%u = call ptr @__ada_sec_stack_alloc(i64 %%t%u)\n",
                 result_data, total_len_64);

              // Store character at first position
              Emit ("  store i8 %%t%u, ptr %%t%u\n", char_trunc, result_data);

              // Copy right string after character
              uint32_t dest = Emit_Temp ();
              Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 1\n", dest, result_data);
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
                 dest, right_data, right_len1_64);

              // Return fat pointer
              return Emit_Fat_Pointer_Dynamic (result_data, one, total_len, concat_bt);

            // STRING & STRING concatenation
            } else {
              uint32_t left_fat = left_val;
              uint32_t right_fat = right_val;
              uint32_t left_data = Emit_Fat_Pointer_Data (left_fat, concat_bt);
              uint32_t left_low = Emit_Fat_Pointer_Low (left_fat, concat_bt);
              uint32_t left_high = Emit_Fat_Pointer_High (left_fat, concat_bt);
              uint32_t right_data = Emit_Fat_Pointer_Data (right_fat, concat_bt);
              uint32_t right_low = Emit_Fat_Pointer_Low (right_fat, concat_bt);
              uint32_t right_high = Emit_Fat_Pointer_High (right_fat, concat_bt);
              uint32_t left_len1 = Emit_Length_From_Bounds (left_low, left_high, concat_bt);
              uint32_t right_len1 = Emit_Length_From_Bounds (right_low, right_high, concat_bt);
              uint32_t total_len = Emit_Temp ();
              Emit ("  %%t%u = add %s %%t%u, %%t%u\n", total_len, concat_bt, left_len1, right_len1);

              // Extend to i64 for C-level calls
              uint32_t total_len_64 = Emit_Extend_To_I64 (total_len, concat_bt);
              uint32_t left_len1_64 = Emit_Extend_To_I64 (left_len1, concat_bt);
              uint32_t right_len1_64 = Emit_Extend_To_I64 (right_len1, concat_bt);
              uint32_t result_data = Emit_Temp ();
              Emit ("  %%t%u = call ptr @__ada_sec_stack_alloc(i64 %%t%u)\n",
                 result_data, total_len_64);
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
                 result_data, left_data, left_len1_64);
              uint32_t right_dest = Emit_Temp ();
              Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %%t%u\n",
                 right_dest, result_data, left_len1_64);
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
                 right_dest, right_data, right_len1_64);
              uint32_t one = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, 1\n", one, concat_bt);
              return Emit_Fat_Pointer_Dynamic (result_data, one, total_len, concat_bt);
            }
          }

          // Inline arithmetic operators (+, -, *, /)
          if ((op == TK_PLUS or op == TK_MINUS or op == TK_STAR or op == TK_SLASH)
            and node->apply.arguments.count == 2) {
            Syntax_Node *left_arg = node->apply.arguments.items[0];
            Syntax_Node *right_arg = node->apply.arguments.items[1];
            if (left_arg->kind == NK_ASSOCIATION)
              left_arg = left_arg->association.expression;
            if (right_arg->kind == NK_ASSOCIATION)
              right_arg = right_arg->association.expression;

            // Find the actual type for the first type formal
            Type_Info *elem_type = NULL;
            for (uint32_t k = 0; k < actuals_holder->generic_actual_count; k++) {
              if (actuals_holder->generic_actuals[k].actual_type) {
                elem_type = actuals_holder->generic_actuals[k].actual_type;
                break;
              }
            }
            const char *arith_type = elem_type ? Type_To_Llvm (elem_type)
                               : Integer_Arith_Type ();
            uint32_t lv = Generate_Expression (left_arg);
            uint32_t rv = Generate_Expression (right_arg);
            const char *lt = Expression_Llvm_Type (left_arg);
            const char *rt = Expression_Llvm_Type (right_arg);
            lv = Emit_Convert (lv, lt, arith_type);
            rv = Emit_Convert (rv, rt, arith_type);
            uint32_t result = Emit_Temp ();
            const char *ir_op = (op == TK_PLUS) ? "add" :
                      (op == TK_MINUS) ? "sub" :
                      (op == TK_STAR) ? "mul" : "sdiv";
            Emit ("  %%t%u = %s %s %%t%u, %%t%u\n",
               result, ir_op, arith_type, lv, rv);
            Temp_Set_Type (result, arith_type);
            return result;
          }
        }
        break;
      }
    }
  }

  // Slice on expression result: prefix(low..high).  NK_RANGE as argument                           
  // ALWAYS means slice in Ada - never a function parameter (RM 4.1.2).                             
  // Resolve array type, obtain ptr to array data, compute fat pointer.                             
  //                                                                                                
  // Generate_Expression returns different LLVM types for different sources:                        
  //   variable/param of ptr type > ptr  (not widened)                                              
  //   function call returning ptr > i64 (ptrtoint)                                                 
  //   fat pointer (unconstrained) > { ptr, { bound, bound } }                                      
  // We use Generate_Composite_Address (returns ptr) for lvalues and                                
  // Generate_Expression + inttoptr for function-call results.                                      
  //                                                                                                
  if (node->apply.arguments.count > 0 and
    node->apply.arguments.items[0]->kind == NK_RANGE) {
    Type_Info *at = node->apply.prefix->type;

    // Fallback type resolution for overloaded functions (RM 6.6)
    if (not at and sym and sym->kind == SYMBOL_FUNCTION) at = sym->return_type;
    if (not at) at = node->type;

    // Implicit dereference: access-to-array (RM 4.1(3))
    bool access_deref = false;
    if (Type_Is_Access (at) and Type_Is_Array_Like (at->access.designated_type)) {
      at = at->access.designated_type;
      access_deref = true;
    }
    if (at and Type_Is_Array_Like (at)) {
      Syntax_Node *rng = node->apply.arguments.items[0];
      uint32_t base, low_bound_val = 0;
      bool dyn_low = false;
      const char *dyn_low_bt = NULL;
      const char *repr = Type_To_Llvm (at);
      bool is_fat = Llvm_Type_Is_Fat_Pointer (repr) or
              Type_Needs_Fat_Pointer_Load (at);

      // Access-to-array: evaluate prefix > access value (i64) > ptr
      if (access_deref) {
        uint32_t access_val = Generate_Expression (node->apply.prefix);
        uint32_t ptr = Emit_Temp ();
        Emit ("  %%t%u = inttoptr i64 %%t%u to ptr\n",
           ptr, access_val);
        const char *at_bt = Array_Bound_Llvm_Type (at);

        // Unconstrained designated: load fat pointer from heap
        if (is_fat) {
          uint32_t fat = Emit_Temp ();
          Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n",
             fat, ptr);
          base = Emit_Fat_Pointer_Data (fat, at_bt);
          low_bound_val = Emit_Fat_Pointer_Low (fat, at_bt);
          dyn_low = true;
          dyn_low_bt = at_bt;
        } else {
          base = ptr;
        }
      } else if (is_fat) {
        const char *at_bt = Array_Bound_Llvm_Type (at);

        // Unconstrained/string: Generate_Expression > fat pointer
        uint32_t pv = Generate_Expression (node->apply.prefix);
        base = Emit_Fat_Pointer_Data (pv, at_bt);
        low_bound_val = Emit_Fat_Pointer_Low (pv, at_bt);
        dyn_low = true;
        dyn_low_bt = at_bt;

      // Constrained array: need ptr to array data.                                                 
      // For lvalues (variable, param, field) use Generate_Composite_Address                        
      // which always returns ptr.  For function results, Generate_Expression                       
      // returns i64 (ptrtoint from ptr), so convert back.                                          
      //                                                                                            
      } else {
        bool is_lvalue = false;
        if (node->apply.prefix->kind == NK_IDENTIFIER) {
          Symbol *ps = node->apply.prefix->symbol;
          is_lvalue = ps and ps->kind != SYMBOL_FUNCTION;
        } else if (node->apply.prefix->kind == NK_SELECTED) {
          is_lvalue = true;
        }
        if (is_lvalue) {
          base = Generate_Composite_Address (node->apply.prefix);
        } else {
          uint32_t pv = Generate_Expression (node->apply.prefix);
          base = Emit_Temp ();
          Emit ("  %%t%u = inttoptr i64 %%t%u to ptr\n",
             base, pv);
        }
      }
      Type_Info *elem = at->array.element_type;
      uint32_t esz = elem ? elem->size : 1;
      if (esz == 0) esz = 1;
      uint32_t slo = Generate_Expression (rng->range.low);
      uint32_t shi = Generate_Expression (rng->range.high);
      const char *slice_iat = Integer_Arith_Type ();
      uint32_t off;
      if (dyn_low) {
        uint32_t low_bound_conv = Emit_Convert (low_bound_val, dyn_low_bt, slice_iat);
        off = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", off, slice_iat, slo, low_bound_conv);
      } else {
        int128_t al = Array_Low_Bound (at);
        if (al != 0) {
          off = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %s\n", off, slice_iat, slo, I128_Decimal (al));
        } else off = slo;
      }
      uint32_t dp = Emit_Temp ();
      if (esz == 1) {
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n", dp, base, slice_iat, off);
      } else {
        uint32_t bo = Emit_Temp ();
        const char *iat_gep = Integer_Arith_Type ();
        Emit ("  %%t%u = mul %s %%t%u, %u\n", bo, iat_gep, off, esz);
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n", dp, base, iat_gep, bo);
      }
      {
        const char *slice_bt = Array_Bound_Llvm_Type (at);
        uint32_t slo_bt = Emit_Convert (slo, Integer_Arith_Type (), slice_bt);
        uint32_t shi_bt = Emit_Convert (shi, Integer_Arith_Type (), slice_bt);
        return Emit_Fat_Pointer_Dynamic (dp, slo_bt, shi_bt, slice_bt);
      }
    }
  }

  // RM 12.3(17): redirect generic recursive calls to current instance
  if (sym and sym->kind == SYMBOL_GENERIC and cg->current_instance)
    sym = cg->current_instance;

  // Function call - generate arguments                                                             
  // For OUT/IN OUT parameters, we need to pass the ADDRESS, not the value.                         
  // RM 6.2: Scalar and access types are passed by copy (copy-in/copy-out).                         
  // This prevents aliasing - P(I, I, I) uses independent copies.                                   
  //                                                                                                
  if (sym and (sym->kind == SYMBOL_FUNCTION or sym->kind == SYMBOL_PROCEDURE)) {
    uint32_t *args = Arena_Allocate (node->apply.arguments.count * sizeof (uint32_t));
    bool *is_byref = Arena_Allocate (node->apply.arguments.count * sizeof (bool));

    // Track copy-back info for scalar/access OUT/IN OUT params
    uint32_t *copyback_addr = Arena_Allocate (node->apply.arguments.count * sizeof (uint32_t));
    const char **copyback_llvm = Arena_Allocate (node->apply.arguments.count * sizeof (const char*));
    memset (copyback_addr, 0, node->apply.arguments.count * sizeof (uint32_t));
    memset (copyback_llvm, 0, node->apply.arguments.count * sizeof (const char*));
    for (uint32_t i = 0; i < node->apply.arguments.count; i++) {
      Syntax_Node *arg_node = node->apply.arguments.items[i];
      Syntax_Node *arg = arg_node;  // Actual expression to evaluate
      uint32_t param_idx = i;  // Index into sym->parameters[]

      // Handle named association: PARAM_NAME => expression
      if (arg_node->kind == NK_ASSOCIATION) {
        arg = arg_node->association.expression;

        // Look up formal parameter by name to get correct index
        if (arg_node->association.choices.count > 0 and sym->parameters) {
          Syntax_Node *name_node = arg_node->association.choices.items[0];
          if (name_node and name_node->kind == NK_IDENTIFIER) {
            String_Slice param_name = name_node->string_val.text;
            for (uint32_t p = 0; p < sym->parameter_count; p++) {
              if (Slice_Equal_Ignore_Case (sym->parameters[p].name, param_name)) {
                param_idx = p;
                break;
              }
            }
          }
        }
      }

      // For ALI-loaded symbols, parameters may be NULL - default to pass by value
      bool byref = sym->parameters and param_idx < sym->parameter_count and
             Param_Is_By_Reference (sym->parameters[param_idx].mode);
      is_byref[i] = byref;

      // OUT/IN OUT: pass address of variable
      if (byref) {
        Parameter_Mode pmode = sym->parameters[param_idx].mode;
        Type_Info *formal_type = sym->parameters[param_idx].param_type;

        // Get the actual's address
        uint32_t actual_addr;
        if (arg->kind == NK_IDENTIFIER and arg->symbol) {
          actual_addr = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%", actual_addr);
          Emit_Symbol_Name (arg->symbol);
          Emit (", i64 0  ; address for OUT/IN OUT\n");
        } else {
          actual_addr = Generate_Composite_Address (arg);
        }

        // RM 6.2: scalar and access types use copy-in/copy-out
        bool copy_semantics = formal_type and
          (Type_Is_Scalar (formal_type) or Type_Is_Access (formal_type));
        if (copy_semantics) {
          const char *ld_ty = Type_To_Llvm (formal_type);

          // Alloca temp for isolated copy
          uint32_t temp = Emit_Temp ();
          Emit ("  %%t%u = alloca %s  ; copy-in/copy-out temp\n", temp, ld_ty);

          // Copy-in: load from actual, check constraint, store to temp
          if (pmode == PARAM_IN_OUT) {
            uint32_t val = Emit_Temp ();
            Emit ("  %%t%u = load %s, ptr %%t%u\n", val, ld_ty, actual_addr);
            Emit_Constraint_Check (val, formal_type, arg->type);
            Emit ("  store %s %%t%u, ptr %%t%u  ; copy-in\n", ld_ty, val, temp);
          }
          args[i] = temp;
          copyback_addr[i] = actual_addr;
          copyback_llvm[i] = ld_ty;

        // Composite: pass actual address directly (by reference)
        } else {
          args[i] = actual_addr;

          // IN OUT constraint check for composites
          if (pmode == PARAM_IN_OUT and formal_type and
            Type_Is_Scalar (formal_type)) {
            const char *ld_ty = Type_To_Llvm (formal_type);
            uint32_t cur_val = Emit_Temp ();
            Emit ("  %%t%u = load %s, ptr %%t%u\n", cur_val, ld_ty, args[i]);
            Emit_Constraint_Check (cur_val, formal_type, arg->type);
          }
        }
      } else {
        args[i] = Generate_Expression (arg);

        // IN parameter: check constraint before call (RM 4.6)
        if (sym->parameters and param_idx < sym->parameter_count and
          sym->parameters[param_idx].param_type) {
          Type_Info *formal_type = sym->parameters[param_idx].param_type;
          Type_Info *actual_type = arg->type;
          const char *arg_llvm = Expression_Llvm_Type (arg);
          args[i] = Emit_Constraint_Check_With_Type (args[i], formal_type, actual_type, arg_llvm);

          // Discriminant constraint check for constrained record                                   
          // subtypes (RM 3.7.2(3)).  When a formal has a constrained                               
          // discriminated record subtype like S_TRUE IS VAR_REC (TRUE),                            
          // verify that the actual's discriminant matches.                                         
          //                                                                                        
          if (Type_Is_Record (formal_type) and
            formal_type->record.has_disc_constraints and
            formal_type->record.discriminant_count > 0 and
            formal_type->record.disc_constraint_values) {
            const char *iat = Integer_Arith_Type ();
            for (uint32_t di = 0; di < formal_type->record.discriminant_count; di++) {
              Component_Info *dc = &formal_type->record.components[di];
              if (not dc->component_type) continue;
              const char *disc_llvm = Type_To_Llvm (dc->component_type);
              uint32_t disc_ptr = Emit_Temp ();
              Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u"
                   "  ; disc constraint addr\n",
                 disc_ptr, args[i], dc->byte_offset);
              uint32_t disc_val = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u"
                   "  ; load actual discriminant\n",
                 disc_val, disc_llvm, disc_ptr);
              if (strcmp (disc_llvm, iat) != 0)
                disc_val = Emit_Convert (disc_val, disc_llvm, iat);
              uint32_t expected = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld"
                   "  ; expected disc value\n",
                 expected, iat,
                 (long long)formal_type->record.disc_constraint_values[di]);
              Emit_Discriminant_Check (disc_val, expected,
                          iat, formal_type);
            }
          }

          // Constrained array > unconstrained formal: build fat pointer (RM 6.4.1)                 
          // When passing a constrained array to an unconstrained formal, we must                   
          // create a fat pointer with the constrained type's bounds.                               
          //                                                                                        
          bool formal_needs_fat = Type_Is_Unconstrained_Array (formal_type) or
                      Type_Is_String (formal_type) or
                      (Type_Is_Constrained_Array (formal_type) and
                       Type_Has_Dynamic_Bounds (formal_type));
          bool actual_is_constrained =
            Type_Is_Constrained_Array (actual_type) and
            not Type_Has_Dynamic_Bounds (actual_type) and
            actual_type->array.index_count > 0;

          // Constrained array to unconstrained formal: build fat pointer.                          
          // Generate_Expression returns ptr for constrained arrays; wrap                           
          // with the type's static bounds for the unconstrained formal.                            
          // But if the expression already produces a fat pointer (e.g.                             
          // string literals, slices), skip the wrapping.                                           
          //                                                                                        
          if (formal_needs_fat and actual_is_constrained) {
            const char *arg_llvm = Expression_Llvm_Type (arg);
            if (not Llvm_Type_Is_Fat_Pointer (arg_llvm)) {
              const char *abt = Array_Bound_Llvm_Type (actual_type);
              uint32_t ndims_a = actual_type->array.index_count;

              // Multi-dim: build fat pointer with all dimension bounds
              if (ndims_a > 1) {
                uint32_t mlo[8], mhi[8];
                if (ndims_a > 8) ndims_a = 8;
                for (uint32_t d = 0; d < ndims_a; d++) {
                  int128_t lo_d = Type_Bound_Value (actual_type->array.indices[d].low_bound);
                  int128_t hi_d = Type_Bound_Value (actual_type->array.indices[d].high_bound);
                  mlo[d] = Emit_Temp ();
                  Emit ("  %%t%u = add %s 0, %s  ; dim%u lo\n", mlo[d], abt, I128_Decimal (lo_d), d);
                  mhi[d] = Emit_Temp ();
                  Emit ("  %%t%u = add %s 0, %s  ; dim%u hi\n", mhi[d], abt, I128_Decimal (hi_d), d);
                }
                args[i] = Emit_Fat_Pointer_MultiDim (args[i], mlo, mhi, ndims_a, abt);
              } else {
                int128_t lo = Type_Bound_Value (actual_type->array.indices[0].low_bound);
                int128_t hi = Type_Bound_Value (actual_type->array.indices[0].high_bound);
                args[i] = Emit_Fat_Pointer (args[i], lo, hi, abt);
              }
            }
          } else if (Expression_Produces_Fat_Pointer (arg, actual_type) and
                 formal_type->array.is_constrained and
                 formal_type->array.index_count > 0) {

            // Fat pointer actual (string literal, slice) to constrained                            
            // formal: rebuild fat pointer with the formal type's bounds.                           
            // E.g. "ABCDE" passed to STRING (11..15) - bounds must be                              
            // 11..15, not the literal's default 1..5.  (RM 4.3.2)                                  
            //                                                                                      
            const char *abt = Array_Bound_Llvm_Type (formal_type);
            uint32_t data_ptr = Emit_Fat_Pointer_Data (args[i], abt);
            uint32_t lo = Emit_Single_Bound (&formal_type->array.indices[0].low_bound, abt);
            uint32_t hi = Emit_Single_Bound (&formal_type->array.indices[0].high_bound, abt);
            args[i] = Emit_Fat_Pointer_Dynamic (data_ptr, lo, hi, abt);
          } else {
            const char *param_type = Type_To_Llvm_Sig (formal_type);
            const char *arg_type = Expression_Llvm_Type (arg);

            // Real literal > fixed-point param: scale by 1/SMALL
            if (Type_Is_Fixed_Point (formal_type) and arg_type and
              arg_type[0] != 'i' and arg_type[0] != 'p') {
              double small = formal_type->fixed.small;
              if (small <= 0) small = formal_type->fixed.delta > 0
                          ? formal_type->fixed.delta : 1.0;
              args[i] = Convert_Real_To_Fixed (args[i], small, param_type);
            } else {
              args[i] = Emit_Convert (args[i], arg_type, param_type);
            }
          }
        }
      }
    }

    // For derived type operations (RM 3.4), emit direct call to parent.                            
    // Derived types have identical representation to parent in Ada 83,                             
    // so no wrapper needed - just call the parent's implementation directly.                       
    // This is the GNAT-style optimization.                                                         
    //                                                                                              
    Symbol *call_target = sym->parent_operation ? sym->parent_operation : sym;

    // RM 12.3(17): recursive call within a generic body > call current                             
    // instance.  Redirect SYMBOL_GENERIC to cg->current_instance so the                            
    // emitted name, return type, and parameter list are correct.                                   
    // GNAT: Expand_N_Subprogram_Call maps generic to current instance.                             
    //                                                                                              
    if (call_target->kind == SYMBOL_GENERIC and cg->current_instance) {
      sym = cg->current_instance;
      call_target = sym;
    }

    // Check if calling a nested function (transitively inside another subprogram)
    bool callee_is_nested = Subprogram_Needs_Static_Chain (call_target);

    // Precompute static chain pointer before building call (RM 8.3)
    uint32_t frame_pre = callee_is_nested ?
      Precompute_Nested_Frame_Arg (call_target) : 0;

    // Check if callee is a BIP function (returns limited type)
    bool callee_is_bip = BIP_Is_BIP_Function (call_target);
    uint32_t bip_dest = 0;  // Destination for BIP result

    // For BIP functions, allocate space for the result
    if (callee_is_bip and sym->return_type) {
      uint32_t type_size = sym->return_type->size;
      if (type_size == 0) type_size = 8;  // Default for opaque types
      bip_dest = Emit_Temp ();
      Emit ("  %%t%u = alloca [%u x i8]  ; BIP result space\n",
         bip_dest, type_size);
    }
    uint32_t t = Emit_Temp ();

    // BIP functions return void - result is built into destination
    if (callee_is_bip) {
      Emit ("  call void @");
    } else if (sym->return_type) {
      Emit ("  %%t%u = call %s @", t, Type_To_Llvm_Sig (sym->return_type));
    } else {
      Emit ("  call void @");
    }
    Emit_Symbol_Name (call_target);
    Emit ("(");
    bool need_comma = false;

    // Pass frame pointer to nested functions (RM 8.3 static chain)
    if (callee_is_nested) {
      if (Emit_Nested_Frame_Arg (call_target, frame_pre)) {
        need_comma = true;
      }
    }

    // BIP extra arguments: allocation form and destination pointer
    if (callee_is_bip) {
      if (need_comma) Emit (", ");

      // Determine allocation form from context:                                                    
      // - has_target = true (we allocate stack space above)                                        
      // - is_allocator = false (not a new expression)                                              
      // - in_return_stmt = false (not in return context)                                           
      //                                                                                            
      BIP_Alloc_Form alloc_form = BIP_Determine_Alloc_Form (

        false, false, true);  // is_allocator, in_return_stmt, has_target
      Emit ("i32 %d, ptr %%t%u", alloc_form, bip_dest);
      need_comma = true;

      // Pass task formals if callee's return type has task components
      uint32_t bip_count = BIP_Extra_Formal_Count (call_target);

      // Pass master and chain (0 for now - full tasking support later)
      if (bip_count > 2) {
        Emit (", i32 0, ptr null");
      }
    }

    // Regular arguments
    for (uint32_t i = 0; i < node->apply.arguments.count; i++) {
      if (need_comma) Emit (", ");
      need_comma = true;

      // OUT/IN OUT: pass as pointer
      if (is_byref[i]) {
        Emit ("ptr %%t%u", args[i]);
      } else {
        const char *param_type = (sym->parameters and i < sym->parameter_count and
                      sym->parameters[i].param_type)
          ? Type_To_Llvm_Sig (sym->parameters[i].param_type) : Integer_Arith_Type ();
        Emit ("%s %%t%u", param_type, args[i]);
      }
    }
    Emit (")\n");

    // For BIP functions, the result is now at bip_dest - keep as ptr
    // Result is directly at bip_dest, just use it as ptr
    if (callee_is_bip and sym->return_type) {
      Temp_Set_Type (bip_dest, "ptr");
      t = bip_dest;
    }

    // Copy-out: for scalar/access OUT/IN OUT, copy from temp back to actual.
    // This only runs on normal return - exceptions skip via longjmp (RM 6.2).
    for (uint32_t i = 0; i < node->apply.arguments.count; i++) {

      // Scalar/access copy-back
      if (copyback_addr[i]) {
        uint32_t ret_val = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u  ; copy-out\n",
           ret_val, copyback_llvm[i], args[i]);
        Emit ("  store %s %%t%u, ptr %%t%u  ; copy-back to actual\n",
           copyback_llvm[i], ret_val, copyback_addr[i]);

      // Composite by-ref: check constraint on returned value
      } else if (is_byref[i]) {
        Syntax_Node *arg_node = node->apply.arguments.items[i];
        Syntax_Node *arg = (arg_node->kind == NK_ASSOCIATION) ?
          arg_node->association.expression : arg_node;
        Type_Info *actual_type = arg->type;
        if (not actual_type or not Type_Is_Scalar (actual_type)) continue;
        const char *ld_ty = Type_To_Llvm (actual_type);
        uint32_t ret_val = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u  ; OUT/INOUT result\n", ret_val, ld_ty, args[i]);
        Emit_Constraint_Check (ret_val, actual_type, NULL);
      }
    }

    // return value stays at native type width.                                                     
    // No widening to INTEGER - callers use Emit_Convert at use sites.                              
    // Track the actual LLVM type of the call result so that Emit_Convert                           
    // at use sites (e.g. return statements) knows the true generated type.                         
    //                                                                                              
    if (sym->return_type) {
      Temp_Set_Type (t, Type_To_Llvm_Sig (sym->return_type));

      // For fat pointer returns (dynamic array), copy the data from the                            
      // callee's stack to a local alloca so it survives.  The callee's                             
      // alloca is freed on return, making the data pointer dangling.                               
      //                                                                                            
      Type_Info *rt = sym->return_type;
      if (not callee_is_bip and Type_Is_Array_Like (rt) and
        Type_Has_Dynamic_Bounds (rt)) {
        const char *rt_bt = Array_Bound_Llvm_Type (rt);

        // Extract data and bounds pointers
        uint32_t old_data = Emit_Fat_Pointer_Data (t, rt_bt);

        // Compute data size from bounds
        uint32_t ndims = rt->array.index_count;
        uint32_t elem_sz = rt->array.element_type
          ? rt->array.element_type->size : 1;
        if (elem_sz == 0) elem_sz = 1;
        uint32_t total_sz = 0;
        if (ndims == 1) {
          uint32_t len = Emit_Fat_Pointer_Length (t, rt_bt);
          total_sz = Emit_Temp ();
          Emit ("  %%t%u = mul i32 %%t%u, %u\n", total_sz, len, elem_sz);

        // Multi-dim: multiply lengths of all dimensions
        } else {
          uint32_t prod = 0;
          for (uint32_t d = 0; d < ndims and d < 8; d++) {
            uint32_t lo_d = Emit_Fat_Pointer_Low_Dim (t, rt_bt, d);
            uint32_t hi_d = Emit_Fat_Pointer_High_Dim (t, rt_bt, d);
            uint32_t diff = Emit_Temp ();
            Emit ("  %%t%u = sub i32 %%t%u, %%t%u\n", diff, hi_d, lo_d);
            uint32_t len_d = Emit_Temp ();
            Emit ("  %%t%u = add i32 %%t%u, 1\n", len_d, diff);
            if (d == 0) prod = len_d;
            else {
              uint32_t np = Emit_Temp ();
              Emit ("  %%t%u = mul i32 %%t%u, %%t%u\n", np, prod, len_d);
              prod = np;
            }
          }
          total_sz = Emit_Temp ();
          Emit ("  %%t%u = mul i32 %%t%u, %u\n", total_sz, prod, elem_sz);
        }

        // Clamp size to >= 0
        uint32_t neg_chk = Emit_Temp ();
        Emit ("  %%t%u = icmp slt i32 %%t%u, 0\n", neg_chk, total_sz);
        uint32_t clamped = Emit_Temp ();
        Emit ("  %%t%u = select i1 %%t%u, i32 0, i32 %%t%u\n",
           clamped, neg_chk, total_sz);

        // Allocate local buffer and copy data
        uint32_t local_buf = Emit_Temp ();
        Emit ("  %%t%u = alloca i8, i32 %%t%u  ; copy func return data\n",
           local_buf, clamped);
        uint32_t sz64 = Emit_Temp ();
        Emit ("  %%t%u = sext i32 %%t%u to i64\n", sz64, clamped);
        Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)  ; copy return data to caller\n",
           local_buf, old_data, sz64);

        // Also copy bounds to a local alloca
        uint32_t old_bounds = Emit_Temp ();
        Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n",
           old_bounds, t);
        uint32_t bounds_sz = ndims * 2 * 4;  // 2 bounds per dim, i32 each
        uint32_t local_bounds = Emit_Temp ();
        Emit ("  %%t%u = alloca [%u x i8]  ; local bounds copy\n",
           local_bounds, bounds_sz);
        Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)  ; copy return bounds\n",
           local_bounds, old_bounds, bounds_sz);

        // Rebuild fat pointer with local copies
        uint32_t new_fat1 = Emit_Temp ();
        Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " undef, ptr %%t%u, 0\n",
           new_fat1, local_buf);
        uint32_t new_fat2 = Emit_Temp ();
        Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " %%t%u, ptr %%t%u, 1\n",
           new_fat2, new_fat1, local_bounds);
        Temp_Set_Type (new_fat2, FAT_PTR_TYPE);
        t = new_fat2;
      }
      return t;
    }
    return 0;
  }

  // Entry call (task rendezvous)
  // Entry call: pack parameters, call entry, wait for accept completion
  if (sym and sym->kind == SYMBOL_ENTRY) {
    Emit ("  ; Entry call: %.*s\n",
       (int)sym->name.length, sym->name.data);

    // Check if this is an entry family - first argument is family index, not a parameter
    bool is_entry_family = sym->declaration and sym->declaration->kind == NK_ENTRY_DECL and
                 sym->declaration->entry_decl.index_constraints.count > 0;
    uint32_t family_idx_temp = 0;
    uint32_t first_param_idx = 0;  // Index of first actual parameter in arguments

    // First argument is the family index - widen to i32 for index arithmetic
    if (is_entry_family and node->apply.arguments.count > 0) {
      family_idx_temp = Generate_Expression (node->apply.arguments.items[0]);
      const char *fam_t = Expression_Llvm_Type (node->apply.arguments.items[0]);
      family_idx_temp = Emit_Convert (family_idx_temp, fam_t, Integer_Arith_Type ());
      first_param_idx = 1;  // Skip family index when processing parameters
    }

    // Allocate parameter block (excluding family index for entry families)
    uint32_t param_count = node->apply.arguments.count - first_param_idx;
    uint32_t param_block = Emit_Temp ();
    if (param_count > 0) {
      Emit ("  %%t%u = alloca [%u x i64]  ; entry call parameters\n",
         param_block, param_count);
    } else {
      Emit ("  %%t%u = inttoptr i64 0 to ptr  ; no parameters\n", param_block);
    }

    // Store arguments into parameter block (skip family index for entry families)
    for (uint32_t i = first_param_idx; i < node->apply.arguments.count; i++) {
      uint32_t arg_val = Generate_Expression (node->apply.arguments.items[i]);

      // Widen argument to i64 for the parameter block ABI
      const char *arg_t = Expression_Llvm_Type (node->apply.arguments.items[i]);
      arg_val = Emit_Convert (arg_val, arg_t, "i64");
      uint32_t arg_ptr = Emit_Temp ();
      Emit ("  %%t%u = getelementptr [%u x i64], ptr %%t%u, i64 0, i64 %u\n",
         arg_ptr, param_count, param_block, i - first_param_idx);
      Emit ("  store i64 %%t%u, ptr %%t%u\n", arg_val, arg_ptr);
    }

    // Get task object (from prefix if it's a selected component like Task_Obj.Entry).              
    // For access-to-task (P.E1), load the pointer to get the designated task.                      
    // For .ALL dereference (P.ALL.E1), unwrap NK_UNARY_OP (TK_ALL) to get access var.              
    //                                                                                              
    uint32_t task_ptr = 0;
    Syntax_Node *prefix = node->apply.prefix;
    if (prefix->kind == NK_SELECTED) {
      Symbol *task_sym = prefix->selected.prefix->symbol;

      // Handle explicit .ALL: prefix->selected.prefix is NK_UNARY_OP (TK_ALL)
      if (not task_sym and prefix->selected.prefix->kind == NK_UNARY_OP and
        prefix->selected.prefix->unary.op == TK_ALL and
        prefix->selected.prefix->unary.operand) {
        task_sym = prefix->selected.prefix->unary.operand->symbol;
      }
      if (not task_sym) goto entry_no_task;
      task_ptr = Emit_Temp ();

      // Access-to-task: load the pointer value (implicit dereference)
      if (task_sym->type and Type_Is_Access (task_sym->type)) {
        Emit ("  %%t%u = load ptr, ptr ", task_ptr);
        Emit_Symbol_Storage (task_sym);
        Emit ("  ; access-to-task deref\n");
      } else {
        Emit ("  %%t%u = getelementptr i8, ptr ", task_ptr);
        Emit_Symbol_Storage (task_sym);
        Emit (", i64 0  ; task object\n");
      }
    } else {
      entry_no_task:
      task_ptr = Emit_Temp ();
      Emit ("  %%t%u = inttoptr i64 0 to ptr  ; current task\n", task_ptr);
    }

    // Get entry index - combine base index with family index for entry families.
    // Formula: entry_idx = base * 1000 + family_arg (matching accept side)
    const char *eidx_t = Integer_Arith_Type ();
    uint32_t entry_idx = Emit_Temp ();
    if (is_entry_family and family_idx_temp) {
      Emit ("  %%t%u = add %s %u, %%t%u  ; entry index (base + family)\n",
         entry_idx, eidx_t, sym->entry_index * 1000, family_idx_temp);
    } else {
      Emit ("  %%t%u = add %s 0, %u  ; entry index (simple entry)\n",
         entry_idx, eidx_t, sym->entry_index * 1000);
    }

    // Call runtime entry call function - extend entry index to i64 for RTS ABI
    uint32_t entry_idx_64 = Emit_Extend_To_I64 (entry_idx, eidx_t);
    Emit ("  call void @__ada_entry_call(ptr %%t%u, i64 %%t%u, ptr %%t%u)\n",
       task_ptr, entry_idx_64, param_block);
    return 0;
  }

  // Type conversion must be checked BEFORE array indexing.                                         
  // PARENT (X) where PARENT is an array type is a type conversion (RM 4.6),                        
  // not an indexed component.  When the prefix symbol is a type or subtype,                        
  // this is always a type conversion, never array indexing.                                        
  //                                                                                                
  if (sym and (sym->kind == SYMBOL_TYPE or sym->kind == SYMBOL_SUBTYPE) and
    node->apply.arguments.count == 1) {

    // Handled below in the type-conversion section
    goto type_conversion;
  }

  // Array indexing (with implicit access dereference per RM 4.1(3))
  Type_Info *prefix_type = node->apply.prefix->type;
  Type_Info *array_type = prefix_type;  // Type to use for indexing
  bool implicit_deref = false;

  // Handle implicit dereference: A(I) where A is access-to-array
  if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type) {
    array_type = prefix_type->access.designated_type;
    implicit_deref = true;
  }
  if (Type_Is_Array_Like (array_type)) {
    Symbol *array_sym = node->apply.prefix->symbol;
    uint32_t base;
    uint32_t low_bound_val = 0, dyn_fat = 0;
    uint32_t high_bound_val = 0;  // For index checks
    bool has_dynamic_low = false;
    const char *dyn_bt = NULL;  // bound type when has_dynamic_low

    // Load the access value then use as base.                                                      
    // For access-to-unconstrained-array, the access value is a fat                                 
    // pointer { ptr, ptr } and we must extract data + bounds.                                      
    //                                                                                              
    if (implicit_deref) {
      bool deref_is_fat = prefix_type and Type_Is_Access (prefix_type) and
                prefix_type->access.designated_type and
                (Type_Is_Unconstrained_Array (prefix_type->access.designated_type) or
                 Type_Is_String (prefix_type->access.designated_type));
      if (deref_is_fat) {
        const char *idx_bt = Array_Bound_Llvm_Type (array_type);
        uint32_t fat;
        if (array_sym) {
          fat = Emit_Temp ();
          Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr ", fat);
          Emit_Symbol_Storage (array_sym);
          Emit ("  ; access-to-unconstrained deref (fat)\n");
          Temp_Set_Type (fat, FAT_PTR_TYPE);
        } else {
          fat = Generate_Expression (node->apply.prefix);
          fat = Fat_Ptr_As_Value (fat);
        }
        dyn_fat = fat;
        base = Emit_Fat_Pointer_Data (fat, idx_bt);
        low_bound_val = Emit_Fat_Pointer_Low (fat, idx_bt);
        high_bound_val = Emit_Fat_Pointer_High (fat, idx_bt);
        has_dynamic_low = true;
        dyn_bt = idx_bt;
        Emit_Access_Check (base, prefix_type);
      } else {
        if (array_sym) {
          base = Emit_Temp ();
          Emit ("  %%t%u = load ptr, ptr ", base);
          Emit_Symbol_Storage (array_sym);
          Emit ("  ; implicit dereference of access\n");
        } else {
          base = Generate_Expression (node->apply.prefix);
        }
        Emit_Access_Check (base, prefix_type);
      }
    }

    // Check if unconstrained array OR constrained array with dynamic bounds
    // needing fat pointer handling. Both are stored as fat pointers.
    else if ((Type_Is_Unconstrained_Array (array_type) or Type_Has_Dynamic_Bounds (array_type)) and
      array_sym and (array_sym->kind == SYMBOL_PARAMETER or array_sym->kind == SYMBOL_VARIABLE or
              array_sym->kind == SYMBOL_CONSTANT or array_sym->kind == SYMBOL_DISCRIMINANT)) {

      // Load fat pointer and extract data pointer and low bound
      Emit ("  ; DEBUG ARRAY INDEX: using fat pointer path (unconstrained=%d, dynamic=%d)\n",
         Type_Is_Unconstrained_Array (array_type), Type_Has_Dynamic_Bounds (array_type));
      const char *idx_bt = Array_Bound_Llvm_Type (array_type);
      uint32_t fat = Emit_Load_Fat_Pointer (array_sym, idx_bt);
      dyn_fat = fat;
      base = Emit_Fat_Pointer_Data (fat, idx_bt);
      low_bound_val = Emit_Fat_Pointer_Low (fat, idx_bt);
      high_bound_val = Emit_Fat_Pointer_High (fat, idx_bt);
      has_dynamic_low = true;
      dyn_bt = idx_bt;

    // Constrained array - get direct pointer to data
    } else if (array_sym) {
      Emit ("  ; DEBUG ARRAY INDEX: using constrained path (sym_kind=%d)\n", array_sym->kind);
      base = Emit_Temp ();
      Emit ("  %%t%u = getelementptr i8, ptr ", base);
      Emit_Symbol_Storage (array_sym);
      Emit (", i64 0\n");

    // Complex prefix (e.g., array field of indexed record)
    } else {
      uint32_t prefix_val = Generate_Expression (node->apply.prefix);

      // If the array is unconstrained or has dynamic bounds, the expression                        
      // returns a fat pointer struct { ptr, { bound, bound } }. We need to extract                 
      // the data pointer and low bound from it.                                                    
      // Extract data pointer from fat pointer value                                                
      //                                                                                            
      if (Type_Is_Unconstrained_Array (array_type) or Type_Has_Dynamic_Bounds (array_type)) {
        const char *pfx_bt = Array_Bound_Llvm_Type (array_type);
        dyn_fat = prefix_val;
        base = Emit_Fat_Pointer_Data (prefix_val, pfx_bt);
        low_bound_val = Emit_Fat_Pointer_Low (prefix_val, pfx_bt);
        high_bound_val = Emit_Fat_Pointer_High (prefix_val, pfx_bt);
        has_dynamic_low = true;
        dyn_bt = pfx_bt;

      // Constrained array - the expression result is the base pointer
      } else {
        base = prefix_val;
      }
    }

    // Check for slice: ARR (low..high)
    Syntax_Node *arg0 = node->apply.arguments.items[0];

    // Array slice - return fat pointer {ptr, {low, high}}                                          
    // Slice bounds are absolute indices into the source array.                                     
    // The fat pointer stores: {data_ptr_at_slice_start, {slice_low, slice_high}}                   
    //                                                                                              
    if (arg0->kind == NK_RANGE) {
      Type_Info *elem_type = array_type->array.element_type;
      uint32_t elem_size = elem_type ? elem_type->size : 1;
      if (elem_size == 0) elem_size = 1;

      // Generate slice bounds (these are the logical bounds of the slice)
      uint32_t slice_low = Generate_Expression (arg0->range.low);
      uint32_t slice_high = Generate_Expression (arg0->range.high);

      // Compute zero-based offset for slice start
      // offset = (slice_low - array_low_bound) * elem_size
      const char *sl_iat = Integer_Arith_Type ();
      uint32_t offset;
      int128_t array_low = Array_Low_Bound (array_type);
      if (has_dynamic_low) {
        uint32_t low_bound_conv = Emit_Convert (low_bound_val, dyn_bt, sl_iat);
        offset = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", offset, sl_iat, slice_low, low_bound_conv);
      } else if (array_low != 0) {
        offset = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %s\n", offset, sl_iat, slice_low, I128_Decimal (array_low));
      } else {
        offset = slice_low;
      }

      // Compute data pointer at slice start
      uint32_t data_ptr = Emit_Temp ();
      if (elem_size == 1) {
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
           data_ptr, base, sl_iat, offset);
      } else {
        uint32_t byte_off = Emit_Temp ();
        Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_off, sl_iat, offset, elem_size);
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
           data_ptr, base, sl_iat, byte_off);
      }

      // Build fat pointer with slice bounds using helper
      {
        const char *sl_bt = Array_Bound_Llvm_Type (array_type);
        uint32_t sl_low_bt = Emit_Convert (slice_low, Integer_Arith_Type (), sl_bt);
        uint32_t sl_high_bt = Emit_Convert (slice_high, Integer_Arith_Type (), sl_bt);
        return Emit_Fat_Pointer_Dynamic (data_ptr, sl_low_bt, sl_high_bt, sl_bt);
      }
    }

    // Multi-dimensional array indexing: linearize indices into flat offset.                        
    // For ARRAY (1..M, 1..N) OF T, D(I,J) > flat = (I-low0)*N + (J-low1)                           
    // For single-dimension, this reduces to (I - low0).                                            
    //                                                                                              
    const char *idx_iat = Integer_Arith_Type ();
    uint32_t flat_idx = 0;  // linearized zero-based index
    uint32_t ndims = array_type->array.index_count;
    uint32_t nargs = node->apply.arguments.count;

    // Multi-dimensional: compute flat index = sum of (idx[d] - lo[d]) * stride[d]
    // where stride[d] = product of lengths of dims d+1..ndims-1
    if (ndims > 1 and nargs >= ndims) {
      for (uint32_t d = 0; d < ndims; d++) {
        Syntax_Node *arg_d = node->apply.arguments.items[d];
        uint32_t dim_idx = Generate_Expression (arg_d);
        const char *dim_src = Expression_Llvm_Type (arg_d);
        if (dim_src and dim_src[0] == 'i' and strcmp (dim_src, idx_iat) != 0)
          dim_idx = Emit_Convert (dim_idx, dim_src, idx_iat);

        // Subtract low bound for dimension d
        if (has_dynamic_low and dyn_fat) {
          uint32_t lo_d = Emit_Fat_Pointer_Low_Dim (dyn_fat, dyn_bt, d);
          lo_d = Emit_Convert (lo_d, dyn_bt, idx_iat);
          uint32_t adj = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %%t%u  ; dim %u dyn low adj\n",
             adj, idx_iat, dim_idx, lo_d, d);
          dim_idx = adj;
        } else {
          int128_t lo_d = Type_Bound_Value (array_type->array.indices[d].low_bound);
          if (lo_d != 0) {
            uint32_t adj = Emit_Temp ();
            Emit ("  %%t%u = sub %s %%t%u, %s  ; dim %u low adj\n",
               adj, idx_iat, dim_idx, I128_Decimal (lo_d), d);
            dim_idx = adj;
          }
        }

        // Compute stride: product of inner dimension lengths
        if (has_dynamic_low and dyn_fat) {
          uint32_t stride_val = 0;
          for (uint32_t d2 = d + 1; d2 < ndims; d2++) {
            uint32_t len_d2 = Emit_Fat_Pointer_Length_Dim (dyn_fat, dyn_bt, d2);
            len_d2 = Emit_Convert (len_d2, dyn_bt, idx_iat);
            if (stride_val == 0) {
              stride_val = len_d2;
            } else {
              uint32_t product = Emit_Temp ();
              Emit ("  %%t%u = mul %s %%t%u, %%t%u\n",
                 product, idx_iat, stride_val, len_d2);
              stride_val = product;
            }
          }
          if (stride_val != 0) {
            uint32_t scaled = Emit_Temp ();
            Emit ("  %%t%u = mul %s %%t%u, %%t%u  ; dim %u dyn stride\n",
               scaled, idx_iat, dim_idx, stride_val, d);
            dim_idx = scaled;
          }
        } else {
          uint32_t stride = 1;
          for (uint32_t d2 = d + 1; d2 < ndims; d2++) {
            int128_t lo2 = Type_Bound_Value (array_type->array.indices[d2].low_bound);
            int128_t hi2 = Type_Bound_Value (array_type->array.indices[d2].high_bound);
            int128_t cnt2 = hi2 - lo2 + 1;
            if (cnt2 > 0) stride *= (uint32_t)cnt2;
          }
          if (stride > 1) {
            uint32_t scaled = Emit_Temp ();
            Emit ("  %%t%u = mul %s %%t%u, %u  ; dim %u stride\n",
               scaled, idx_iat, dim_idx, stride, d);
            dim_idx = scaled;
          }
        }
        if (d == 0) {
          flat_idx = dim_idx;
        } else {
          uint32_t sum = Emit_Temp ();
          Emit ("  %%t%u = add %s %%t%u, %%t%u  ; accum flat idx\n",
             sum, idx_iat, flat_idx, dim_idx);
          flat_idx = sum;
        }
      }

    // Single-dimension indexing (original path)
    } else {
      uint32_t idx = Generate_Expression (arg0);

      // Widen to native integer type for GEP compatibility
      {
        const char *idx_src_type = Expression_Llvm_Type (arg0);
        if (idx_src_type and idx_src_type[0] == 'i' and strcmp (idx_src_type, idx_iat) != 0)
          idx = Emit_Convert (idx, idx_src_type, idx_iat);
      }
      if (has_dynamic_low and high_bound_val) {
        uint32_t low_chk = Emit_Convert (low_bound_val, dyn_bt, idx_iat);
        uint32_t high_chk = Emit_Convert (high_bound_val, dyn_bt, idx_iat);
        idx = Emit_Index_Check (idx, low_chk, high_chk, idx_iat, array_type);
      } else if (array_type->array.index_count > 0) {
        int128_t lo = Type_Bound_Value (array_type->array.indices[0].low_bound);
        int128_t hi = Type_Bound_Value (array_type->array.indices[0].high_bound);
        if (lo != hi or lo != 0) {
          uint32_t lo_t = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %s  ; low bound\n", lo_t, idx_iat, I128_Decimal (lo));
          uint32_t hi_t = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %s  ; high bound\n", hi_t, idx_iat, I128_Decimal (hi));
          idx = Emit_Index_Check (idx, lo_t, hi_t, idx_iat, array_type);
        }
      }

      // Adjust for array low bound
      if (has_dynamic_low) {
        uint32_t low_bound_conv = Emit_Convert (low_bound_val, dyn_bt, idx_iat);
        uint32_t adj = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u  ; adjust for dynamic low bound\n",
           adj, idx_iat, idx, low_bound_conv);
        idx = adj;
      } else {
        int128_t low_bound = Array_Low_Bound (array_type);
        if (low_bound != 0) {
          uint32_t adj = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %s\n", adj, idx_iat, idx, I128_Decimal (low_bound));
          idx = adj;
        }
      }
      flat_idx = idx;
    }

    // Get pointer to element and load
    Type_Info *elem_type_info = array_type->array.element_type;
    bool elem_is_composite = Type_Is_Record (elem_type_info) or
      Type_Is_Constrained_Array (elem_type_info);
    bool elem_is_fat = Type_Needs_Fat_Pointer_Load (elem_type_info);
    uint32_t elem_size = elem_type_info ? elem_type_info->size : 8;
    const char *elem_type = elem_is_fat ? FAT_PTR_TYPE : Type_To_Llvm (elem_type_info);
    uint32_t ptr = Emit_Temp ();
    uint32_t t;

    // Element stored as fat pointer { ptr, ptr } (dynamic-bound or
    // unconstrained array/string component) - load the pair.
    if (elem_is_fat) {
      const char *iat_idx = Integer_Arith_Type ();
      Emit ("  %%t%u = getelementptr %s, ptr %%t%u, %s %%t%u"
         "  ; &array[idx] (fat elem)\n",
         ptr, FAT_PTR_TYPE, base, iat_idx, flat_idx);
      t = Emit_Temp ();
      Emit ("  %%t%u = load %s, ptr %%t%u  ; array[idx] fat\n", t, FAT_PTR_TYPE, ptr);
      return t;

    // Composite element - use byte array for getelementptr
    } else if (elem_is_composite and elem_size > 0) {
      Emit ("  %%t%u = getelementptr [%u x i8], ptr %%t%u, %s %%t%u"
         "  ; array[idx] (composite elem, size=%u)\n",
         ptr, elem_size, base, idx_iat, flat_idx, elem_size);

      // Return pointer to composite element (don't load)
      return ptr;
    } else {
      t = Emit_Temp ();
      const char *iat_idx = Integer_Arith_Type ();
      Emit ("  %%t%u = getelementptr %s, ptr %%t%u, %s %%t%u"
         "  ; &array[idx] (elem_type=%s)\n",
         ptr, elem_type, base, iat_idx, flat_idx, elem_type);
      Emit ("  %%t%u = load %s, ptr %%t%u  ; array[idx]\n", t, elem_type, ptr);

      // no widening at load - value stays at native type width.
      // Conversions happen at use sites via Emit_Convert.
      return t;
    }
  }

  // Type conversion: Type_Name (Expression)
type_conversion:

  // Type conversion: TYPE_NAME (expression) per RM 4.6.
  // Handles scalar, array, and record conversions.
  if (sym and (sym->kind == SYMBOL_TYPE or sym->kind == SYMBOL_SUBTYPE)) {
    if (node->apply.arguments.count == 1) {
      Syntax_Node *arg = node->apply.arguments.items[0];

      // Get source and destination types
      Type_Info *src_type = arg->type;
      Type_Info *dst_type = sym->type;

      // Array type conversions (RM 4.6(24)):                                                       
      // Constrained>Unconstrained: wrap data+bounds into fat pointer.                              
      // Unconstrained>Constrained: extract data (bounds checked at runtime).                       
      // Same representation: pass through.                                                         
      //                                                                                            
      if (dst_type and src_type and
        Type_Is_Array_Like (dst_type) and Type_Is_Array_Like (src_type)) {
        uint32_t result = Generate_Expression (arg);
        bool dst_unc = Type_Is_Unconstrained_Array (dst_type);

        // Check if the source expression actually produces a fat pointer value.                    
        // This covers: unconstrained parameters/variables, function calls returning                
        // unconstrained arrays, slices, concatenations, string literals, etc.                      
        //                                                                                          
        bool src_is_fat = Expression_Produces_Fat_Pointer (arg, src_type);

        // Constrained/flat storage > Unconstrained: build fat pointer {data, {low, high}}.
        // Source is stored as a flat alloca; bounds come from type info.
        if (dst_unc and not src_is_fat) {
          const char *bt = Array_Bound_Llvm_Type (dst_type);
          int128_t lo = Array_Low_Bound (src_type);
          int128_t hi = (src_type->kind == TYPE_ARRAY and src_type->array.index_count > 0)
            ? Type_Bound_Value (src_type->array.indices[0].high_bound) : lo;
          uint32_t lo_t = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %s  ; array conv low bound\n", lo_t, bt, I128_Decimal (lo));
          uint32_t hi_t = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %s  ; array conv high bound\n", hi_t, bt, I128_Decimal (hi));
          return Emit_Fat_Pointer_Dynamic (result, lo_t, hi_t, bt);

        // Unconstrained fat ptr > Constrained: extract data pointer
        } else if (not dst_unc and src_is_fat) {
          const char *bt = Array_Bound_Llvm_Type (src_type);
          return Emit_Fat_Pointer_Data (result, bt);
        }

        // Same representation: pass through
        return result;
      }
      uint32_t result = Generate_Expression (arg);

      // Special handling for fixed-point conversions (RM 4.6)
      // Fixed-point uses scaled integer representation: value = integer * SMALL
      if (src_type and dst_type and src_type != dst_type) {

        // Fixed > Float: convert integer to float, multiply by SMALL
        if (Type_Is_Fixed_Point (src_type) and Type_Is_Float_Representation (dst_type)) {
          const char *dst_llvm = Type_To_Llvm (dst_type);
          double small = src_type->fixed.small;
          if (small <= 0) small = src_type->fixed.delta > 0 ? src_type->fixed.delta : 1.0;
          uint32_t t1 = Emit_Temp ();
          const char *fix_int_ty = Temp_Get_Type (result);
          if (not fix_int_ty or fix_int_ty[0] == '\0') fix_int_ty = Type_To_Llvm (src_type);
          if (not fix_int_ty or fix_int_ty[0] == '\0') fix_int_ty = Integer_Arith_Type ();
          Emit ("  %%t%u = sitofp %s %%t%u to %s  ; fixed>float\n", t1, fix_int_ty, result, dst_llvm);
          uint32_t t2 = Emit_Temp ();
          uint64_t small_bits;
          memcpy (&small_bits, &small, sizeof (small_bits));
          Emit ("  %%t%u = fmul %s %%t%u, 0x%016llX  ; scale by SMALL\n",
             t2, dst_llvm, t1, (unsigned long long)small_bits);
          return t2;

        // Float > Fixed: divide by SMALL, convert to integer
        } else if (Type_Is_Float_Representation (src_type) and Type_Is_Fixed_Point (dst_type)) {
          const char *src_llvm = Expression_Llvm_Type (arg);
          double small = dst_type->fixed.small;
          if (small <= 0) small = dst_type->fixed.delta > 0 ? dst_type->fixed.delta : 1.0;
          uint32_t t1 = Emit_Temp ();
          uint64_t small_bits;
          memcpy (&small_bits, &small, sizeof (small_bits));
          Emit ("  %%t%u = fdiv %s %%t%u, 0x%016llX  ; divide by SMALL\n",
             t1, src_llvm, result, (unsigned long long)small_bits);
          uint32_t t2 = Emit_Temp ();
          const char *dst_llvm = Type_To_Llvm (dst_type);
          Emit ("  %%t%u = fptosi %s %%t%u to %s  ; float>fixed\n", t2, src_llvm, t1, dst_llvm);
          Temp_Set_Type (t2, dst_llvm);
          return t2;
        }

        // Types differ - need to convert.
        // Use signedness-aware conversion: zext for unsigned src/dst.
        const char *src_llvm = Expression_Llvm_Type (arg);
        const char *dst_llvm = Type_To_Llvm (dst_type);
        bool conv_unsigned = Type_Is_Unsigned (src_type) or Type_Is_Unsigned (dst_type);
        if (strcmp (src_llvm, dst_llvm) != 0) {
          result = Emit_Convert_Ext (result, src_llvm, dst_llvm, conv_unsigned);
        }

        // RM 4.6: Check converted value against target subtype constraint
        if (Type_Is_Scalar (dst_type)) {
          Emit_Constraint_Check_With_Type (result, dst_type, src_type, dst_llvm);
        }
      }
      return result;
    }
  }
  fprintf (stderr, "warning: Generate_Apply: unhandled call expression at %s:%u\n",
      node->location.filename ? node->location.filename : "<unknown>",
      node->location.line);
  return 0;
}
uint32_t Generate_Selected (Syntax_Node *node) {

  // Generate code for A.B (selected component)
  Type_Info *prefix_type = node->selected.prefix->type;
  Type_Info *record_type = prefix_type;
  bool implicit_deref = false;

  // Handle explicit .ALL dereference (RM 4.1)
  if (Type_Is_Access (prefix_type) and
    Slice_Equal_Ignore_Case (node->selected.selector, S("ALL"))) {
    Type_Info *designated = prefix_type->access.designated_type;

    // Generate_Lvalue returns the loaded pointer (address of designated)
    uint32_t ptr = Generate_Lvalue (node);
    Emit_Access_Check (ptr, prefix_type);

    // For composite types (records, arrays), return pointer
    if (Type_Is_Composite (designated)) {
      return ptr;
    }

    // For scalar types, load the value
    const char *type_str = Type_To_Llvm (designated);
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = load %s, ptr %%t%u  ; load via .ALL\n", t, type_str, ptr);

    // no widening at load - value stays at native type width.
    // Conversions happen at use sites via Emit_Convert.
    return t;
  }

  // Handle implicit dereference: R.C where R is access-to-record (RM 4.1(3))
  if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type) {
    Type_Info *designated = prefix_type->access.designated_type;
    if (Type_Is_Record (designated)) {
      record_type = designated;
      implicit_deref = true;
    }
  }

  // Resolve generic formal private types to their actual types (RM 12.3).                          
  // During instantiation, formal TYPE_PRIVATE maps to the actual type                              
  // via g_generic_type_map.  Also unwrap through parent_type chain.                                
  //                                                                                                
  for (int depth = 0; depth < 10 and record_type and not Type_Is_Record (record_type); depth++) {
    if (Type_Is_Private (record_type) and not record_type->parent_type and
      g_generic_type_map.count > 0 and record_type->name.data) {

      // Look up actual type from generic instantiation map
      for (uint32_t i = 0; i < g_generic_type_map.count; i++) {
        if (g_generic_type_map.mappings[i].actual_type and
          Slice_Equal_Ignore_Case (record_type->name,
                      g_generic_type_map.mappings[i].formal_name)) {
          record_type = g_generic_type_map.mappings[i].actual_type;
          break;
        }
      }
    } else if ((Type_Is_Private (record_type) or record_type->kind == TYPE_INCOMPLETE) and
           record_type->parent_type) {
      record_type = record_type->parent_type;
    } else {
      break;
    }
  }

  // Package-qualified name - use the resolved symbol via Generate_Identifier
  // This handles named numbers, constants, variables, and literals properly
  if (not Type_Is_Record (record_type)) {
    Symbol *sym = node->symbol;

    // Create a temporary identifier node to reuse Generate_Identifier logic
    if (sym) {
      Syntax_Node tmp_id = {.kind = NK_IDENTIFIER, .symbol = sym, .location = node->location};
      tmp_id.type = sym->type;
      return Generate_Identifier (&tmp_id);
    }
    fprintf (stderr, "warning: Generate_Selected: '%.*s' not found in package at %s:%u\n",
        (int)node->selected.selector.length, node->selected.selector.data,
        node->location.filename ? node->location.filename : "<unknown>",
        node->location.line);
    return 0;
  }

  // Record field access - find component by name
  uint32_t byte_offset = 0;
  Type_Info *field_type = NULL;
  int32_t field_variant_index = -1;
  for (uint32_t i = 0; i < record_type->record.component_count; i++) {
    if (Slice_Equal_Ignore_Case (
        record_type->record.components[i].name, node->selected.selector)) {
      byte_offset = record_type->record.components[i].byte_offset;
      field_type = record_type->record.components[i].component_type;
      field_variant_index = record_type->record.components[i].variant_index;
      break;
    }
  }
  const char *field_llvm_type = Type_To_Llvm (field_type);

  // Runtime discriminant check for variant component access (RM 3.7.3)                             
  // If accessing a component that belongs to a variant, verify the                                 
  // discriminant value matches the variant's expected value.                                       
  //                                                                                                
  if (field_variant_index >= 0 and record_type->record.has_discriminants and
    record_type->record.variant_count > 0 and
    (uint32_t)field_variant_index < record_type->record.variant_count) {
    Variant_Info *vinfo = &record_type->record.variants[field_variant_index];

    // Load discriminant value from the first discriminant component
    Component_Info *disc_comp = &record_type->record.components[0];
    uint32_t disc_offset = disc_comp->byte_offset;
    const char *disc_llvm = Type_To_Llvm (disc_comp->component_type);

    // Get base address of record for discriminant check.                                           
    // For implicit dereference, load the pointer to get the record address.                        
    // For direct records, get the storage address via Generate_Lvalue.                             
    //                                                                                              
    uint32_t rec_base;
    if (implicit_deref) {
      rec_base = Generate_Expression (node->selected.prefix);
      Emit_Access_Check (rec_base, prefix_type);
    } else {
      rec_base = Generate_Lvalue (node->selected.prefix);
    }
    uint32_t disc_ptr = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u  ; discriminant addr\n",
       disc_ptr, rec_base, disc_offset);
    uint32_t disc_val = Emit_Temp ();
    Emit ("  %%t%u = load %s, ptr %%t%u  ; load discriminant\n",
       disc_val, disc_llvm, disc_ptr);
    const char *iat_disc = Integer_Arith_Type ();
    if (strcmp (disc_llvm, iat_disc) != 0) {
      disc_val = Emit_Convert (disc_val, disc_llvm, iat_disc);
    }
    if (not vinfo->is_others) {

      // Single value: equality check
      if (vinfo->disc_value_low == vinfo->disc_value_high) {
        uint32_t expected = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld  ; expected discriminant\n",
           expected, iat_disc, (long long)vinfo->disc_value_low);
        Emit_Discriminant_Check (disc_val, expected, iat_disc, record_type);

      // Range: check disc_val in [low..high]
      } else {
        uint32_t lo = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", lo, iat_disc,
           (long long)vinfo->disc_value_low);
        uint32_t hi = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", hi, iat_disc,
           (long long)vinfo->disc_value_high);
        uint32_t cmp_lo = Emit_Temp ();
        Emit ("  %%t%u = icmp sge %s %%t%u, %%t%u\n",
           cmp_lo, iat_disc, disc_val, lo);
        uint32_t cmp_hi = Emit_Temp ();
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n",
           cmp_hi, iat_disc, disc_val, hi);
        uint32_t in_range = Emit_Temp ();
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n",
           in_range, cmp_lo, cmp_hi);
        uint32_t not_in_range = Emit_Temp ();
        Emit ("  %%t%u = xor i1 %%t%u, 1\n",
           not_in_range, in_range);
        Emit_Check_With_Raise (not_in_range, true,
                     "discriminant check failed");
      }
    }

    // For WHEN OTHERS variant, any discriminant value is valid - no check needed
  }

  // Compute field address directly using already-resolved record_type.                             
  // We avoid calling Generate_Lvalue (node) here because it would                                  
  // need to re-resolve generic formal types and could lead to infinite                             
  // recursion when the prefix type is a generic formal private type.                               
  //                                                                                                
  uint32_t base;
  if (implicit_deref) {
    base = Generate_Expression (node->selected.prefix);
    Emit_Access_Check (base, prefix_type);
  } else {
    base = Generate_Lvalue (node->selected.prefix);
  }
  uint32_t ptr;

  // Walk base/parent chain to find rt_global_id (covers subtypes)
  uint32_t rec_rt_id = record_type->rt_global_id;
  if (rec_rt_id == 0) {
    Type_Info *walk = record_type->base_type ? record_type->base_type
                         : record_type->parent_type;
    for (int depth = 0; walk and depth < 10; depth++) {
      if (walk->rt_global_id > 0) { rec_rt_id = walk->rt_global_id; break; }
      walk = walk->base_type ? walk->base_type : walk->parent_type;
    }
  }

  // Find component index for runtime offset lookup
  uint32_t comp_idx = 0;
  for (uint32_t i = 0; i < record_type->record.component_count; i++) {
    if (Slice_Equal_Ignore_Case (
        record_type->record.components[i].name, node->selected.selector)) {
      comp_idx = i;
      break;
    }
  }
  if (rec_rt_id > 0) {
    uint32_t off_r = Emit_Temp ();
    Emit ("  %%t%u = load i64, ptr @__rt_rec_%u_off%u\n",
       off_r, rec_rt_id, comp_idx);
    ptr = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %%t%u"
       "  ; rt field addr\n", ptr, base, off_r);
  } else {
    ptr = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u"
       "  ; field addr\n", ptr, base, byte_offset);
  }

  // Discriminant-dependent array/string component: data is stored inline                           
  // in the record, so we must construct a fat pointer from the field                               
  // address and bounds derived from the discriminant (RM 3.7.1).                                   
  //                                                                                                
  if (field_type and Type_Is_Array_Like (field_type) and Type_Needs_Fat_Pointer_Load (field_type)) {
    bool is_disc_dep = false;
    for (uint32_t xi = 0; xi < field_type->array.index_count; xi++) {
      if (field_type->array.indices[xi].high_bound.kind == BOUND_EXPR or
        field_type->array.indices[xi].low_bound.kind == BOUND_EXPR) {
        is_disc_dep = true;
        break;
      }
    }

    // RT-elaborated array in record: build fat ptr from type globals.
    // Multi-dim layout: [lo0, hi0, lo1, hi1, ...]
    if (is_disc_dep and field_type->rt_global_id > 0) {
      const char *bnd_type = Array_Bound_Llvm_Type (field_type);
      uint32_t rtid = field_type->rt_global_id;
      uint32_t ndims = field_type->array.index_count;
      uint32_t blo[8], bhi[8];
      for (uint32_t d = 0; d < ndims and d < 8; d++) {
        uint32_t lr = Emit_Temp ();
        Emit ("  %%t%u = load i64, ptr @__rt_type_%u_lo%u\n", lr, rtid, d);
        blo[d] = (strcmp (bnd_type, "i64") != 0)
          ? Emit_Convert (lr, "i64", bnd_type) : lr;
        uint32_t hr = Emit_Temp ();
        Emit ("  %%t%u = load i64, ptr @__rt_type_%u_hi%u\n", hr, rtid, d);
        bhi[d] = (strcmp (bnd_type, "i64") != 0)
          ? Emit_Convert (hr, "i64", bnd_type) : hr;
      }
      return Emit_Fat_Pointer_MultiDim (ptr, blo, bhi, ndims, bnd_type);
    }

    // Build fat pointer: { ptr data, ptr bounds }
    // Bounds layout: [lo0, hi0, lo1, hi1, ...] as flat array.
    if (is_disc_dep) {
      const char *bnd_type = Array_Bound_Llvm_Type (field_type);
      uint32_t ndims = field_type->array.index_count;
      uint32_t bounds_alloca = Emit_Temp ();
      Emit ("  %%t%u = alloca [%u x %s]\n",
         bounds_alloca, ndims * 2, bnd_type);

      // Get record base by backing up from field pointer by byte_offset
      uint32_t rec_base = ptr;
      if (byte_offset > 0) {
        rec_base = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 -%u  ; record base\n",
           rec_base, ptr, byte_offset);
      }
      for (uint32_t xi = 0; xi < ndims; xi++) {
        Type_Bound *lo_bnd = &field_type->array.indices[xi].low_bound;
        Type_Bound *hi_bnd = &field_type->array.indices[xi].high_bound;

        // Emit low bound
        uint32_t lo_val;
        if (lo_bnd->kind == BOUND_INTEGER) {
          lo_val = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %lld\n",
             lo_val, bnd_type, (long long)lo_bnd->int_value);
        } else if (lo_bnd->kind == BOUND_EXPR and lo_bnd->expr and
               lo_bnd->expr->symbol) {

          // Load discriminant value for low bound
          Symbol *disc = lo_bnd->expr->symbol;
          uint32_t d_off = 0;
          for (uint32_t di = 0; di < record_type->record.component_count; di++) {
            if (Slice_Equal_Ignore_Case (
                record_type->record.components[di].name, disc->name)) {
              d_off = record_type->record.components[di].byte_offset;
              break;
            }
          }
          uint32_t dp = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
             dp, rec_base, d_off);
          lo_val = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u\n", lo_val, bnd_type, dp);
        } else {
          lo_val = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, 1\n", lo_val, bnd_type);
        }

        // Emit high bound
        uint32_t hi_val;
        if (hi_bnd->kind == BOUND_INTEGER) {
          hi_val = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %lld\n",
             hi_val, bnd_type, (long long)hi_bnd->int_value);
        } else if (hi_bnd->kind == BOUND_EXPR and hi_bnd->expr and
               hi_bnd->expr->symbol) {
          Symbol *disc = hi_bnd->expr->symbol;
          uint32_t d_off = 0;
          for (uint32_t di = 0; di < record_type->record.component_count; di++) {
            if (Slice_Equal_Ignore_Case (
                record_type->record.components[di].name, disc->name)) {
              d_off = record_type->record.components[di].byte_offset;
              break;
            }
          }
          uint32_t dp = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
             dp, rec_base, d_off);
          hi_val = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u\n", hi_val, bnd_type, dp);
        } else {
          hi_val = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, 0\n", hi_val, bnd_type);
        }

        // Store low bound at index xi*2
        uint32_t lo_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
           lo_ptr, bnd_type, bounds_alloca, xi * 2);
        Emit ("  store %s %%t%u, ptr %%t%u\n", bnd_type, lo_val, lo_ptr);

        // Store high bound at index xi*2+1
        uint32_t hi_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
           hi_ptr, bnd_type, bounds_alloca, xi * 2 + 1);
        Emit ("  store %s %%t%u, ptr %%t%u\n", bnd_type, hi_val, hi_ptr);
      }

      // Construct fat pointer { data_ptr, bounds_ptr }
      uint32_t fat1 = Emit_Temp ();
      Emit ("  %%t%u = insertvalue { ptr, ptr } undef, ptr %%t%u, 0\n",
         fat1, ptr);
      uint32_t fat2 = Emit_Temp ();
      Emit ("  %%t%u = insertvalue { ptr, ptr } %%t%u, ptr %%t%u, 1\n",
         fat2, fat1, bounds_alloca);
      return fat2;
    }

    // Non-disc-dependent: load fat pointer from memory
    return Emit_Load_Fat_Pointer_From_Temp (ptr, Array_Bound_Llvm_Type (field_type));
  }

  // Other composite types (records, constrained arrays) return ptr
  if (Type_Is_Record (field_type) or (field_type and field_type->kind == TYPE_ARRAY))
    return ptr;

  // For access-type components, load ptr without converting to i64
  if (Type_Is_Access (field_type)) {
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = load ptr, ptr %%t%u  ; .%.*s (access)\n", t, ptr,
       node->selected.selector.length > 20 ? 20 : (int)node->selected.selector.length, node->selected.selector.data);
    return t;
  }
  uint32_t t = Emit_Temp ();
  Emit ("  %%t%u = load %s, ptr %%t%u  ; .%.*s\n", t, field_llvm_type, ptr,
     node->selected.selector.length > 20 ? 20 : (int)node->selected.selector.length, node->selected.selector.data);

  // no widening at load - value stays at native type width.
  // Conversions happen at use sites via Emit_Convert.
  return t;
}

// Check if a type bound can be evaluated at compile time.                                          
// Returns true if the bound is compile-time known, false otherwise.                                
// Per GNAT's sem_eval.ads, compile-time known is broader than static -                             
// it includes values that can be determined at compile time even if                                
// they technically involve non-static expressions.                                                 
//                                                                                                  
bool Type_Bound_Is_Compile_Time_Known (Type_Bound b) {
  if (b.kind == BOUND_INTEGER or b.kind == BOUND_FLOAT) return true;
  if (b.kind == BOUND_EXPR and b.expr) {
    double val = Eval_Const_Numeric (b.expr);
    return val == val;  // Returns true if not NaN
  }
  return false;
}
double Type_Bound_Float_Value (Type_Bound b) {
  if (b.kind == BOUND_FLOAT) return b.float_value;
  if (b.kind == BOUND_INTEGER) return (double)b.int_value;

  // Try to evaluate expression bound at compile time
  if (b.kind == BOUND_EXPR and b.expr) {
    double val = Eval_Const_Numeric (b.expr);
    if (val == val) return val;  // Not NaN
  }
  return 0.0;
}

// Check if a type bound is explicitly set (not default/unset)
bool Type_Bound_Is_Set (Type_Bound b) {
  return b.kind == BOUND_INTEGER or b.kind == BOUND_FLOAT or
       (b.kind == BOUND_EXPR and b.expr != NULL);
}

// Emit implementation-defined float limit: is_low=true > -FLT/DBL_MAX, else +FLT/DBL_MAX.          
// Per Ada RM 3.5.7, unconstrained types use at least ±SAFE_LARGE.                                  
// LLVM requires 64-bit double hex format for float constants.                                      
//                                                                                                  
void Emit_Float_Type_Limit (uint32_t t, Type_Info *type,
                   bool is_low, String_Slice attr) {
  const char *fty = Float_Llvm_Type_Of (type);
  bool is_single = (strcmp (fty, "float") == 0);

  // Single-precision: emit via bitcast i32 > float
  if (is_single) {

    // -FLT_MAX = 0xFF7FFFFF, +FLT_MAX = 0x7F7FFFFF
    uint32_t hex = is_low ? 0xFF7FFFFFu : 0x7F7FFFFFu;
    Emit ("  %%t%u = bitcast i32 %u to float  ; %.*s'%s (unconstrained)\n",
       t, hex, (int)attr.length, attr.data, is_low ? "FIRST" : "LAST");

  // Double-precision: emit via fadd double 0.0, 0x<hex>
  } else {
    static const char *dbl_hex[] = {
      "0xFFEFFFFFFFFFFFFF",  // -DBL_MAX
      "0x7FEFFFFFFFFFFFFF"  // +DBL_MAX
    };
    Emit ("  %%t%u = fadd double 0.0, %s  ; %.*s'%s (unconstrained)\n",
       t, dbl_hex[not is_low],
       (int)attr.length, attr.data, is_low ? "FIRST" : "LAST");
  }
}

// Get dimension index from attribute argument (1-based, default 1)
uint32_t Get_Dimension_Index (Syntax_Node *arg) {
  if (not arg) return 0;  // Default to first dimension
  if (arg->kind == NK_INTEGER) return (uint32_t)(arg->integer_lit.value - 1);
  return 0;
}

// Emit T'FIRST or T'LAST - unified handler for both bound attributes.                              
// is_low=true > FIRST (low_bound), is_low=false > LAST (high_bound).                               
// For arrays: runtime bounds via fat pointer, or static from index_type.                           
// For floats: runtime BOUND_EXPR, static float, or implementation limit.                           
// For scalars: static integer or runtime BOUND_EXPR.                                               
//                                                                                                  
uint32_t Emit_Bound_Attribute (uint32_t t,
    Type_Info *prefix_type, Symbol *prefix_sym, Syntax_Node *prefix_expr,
    bool needs_runtime_bounds, uint32_t dim, bool is_low, String_Slice attr) {
  const char *tag = is_low ? "FIRST" : "LAST";
  if (Type_Is_Array_Like (prefix_type)) {
    if (needs_runtime_bounds) {
      const char *attr_bt = Array_Bound_Llvm_Type (prefix_type);
      uint32_t fat = (prefix_sym and prefix_sym->kind != SYMBOL_FUNCTION
               and prefix_sym->kind != SYMBOL_PROCEDURE)
        ? Emit_Load_Fat_Pointer (prefix_sym, attr_bt)
        : Generate_Expression (prefix_expr);
      {

        // return bound at native type width, no widening.
        // Use dim-aware accessor to support multi-dimensional arrays.
        uint32_t bound = is_low ? Emit_Fat_Pointer_Low_Dim (fat, attr_bt, dim)
                    : Emit_Fat_Pointer_High_Dim (fat, attr_bt, dim);
        return bound;
      }
    } else if (dim < prefix_type->array.index_count) {
      Type_Bound b = is_low ? prefix_type->array.indices[dim].low_bound
                  : prefix_type->array.indices[dim].high_bound;

      // Dynamic bound expression (e.g., TYPE AA IS ARRAY (SNI,..) where SNI has dynamic range)
      if (b.kind == BOUND_EXPR and b.expr) {
        uint32_t v = Generate_Expression (b.expr);
        const char *iat = Integer_Arith_Type ();
        const char *vty = Expression_Llvm_Type (b.expr);
        if (strcmp (vty, iat) != 0 and vty[0] == 'i')
          v = Emit_Convert (v, vty, iat);
        Emit ("  %%t%u = add %s %%t%u, 0  ; %.*s'%s(%u) dynamic\n",
           t, iat, v, (int)attr.length, attr.data, tag, dim+1);
        Temp_Set_Type (t, iat);
      } else if (b.kind == BOUND_INTEGER or Type_Bound_Is_Set (b)) {
        Emit ("  %%t%u = add %s 0, %s  ; %.*s'%s(%u)\n", t, Integer_Arith_Type (),
           I128_Decimal (Type_Bound_Value (b)), (int)attr.length, attr.data, tag, dim+1);
        Temp_Set_Type (t, Integer_Arith_Type ());

      // Bounds not set on array - derive from index type's bounds
      } else if (prefix_type->array.indices[dim].index_type) {
        Type_Info *idx_ty = prefix_type->array.indices[dim].index_type;
        Type_Bound ib = is_low ? idx_ty->low_bound : idx_ty->high_bound;
        if (ib.kind == BOUND_EXPR and ib.expr) {
          uint32_t v = Generate_Expression (ib.expr);
          const char *iat = Integer_Arith_Type ();
          const char *vty = Expression_Llvm_Type (ib.expr);
          if (strcmp (vty, iat) != 0 and vty[0] == 'i')
            v = Emit_Convert (v, vty, iat);
          Emit ("  %%t%u = add %s %%t%u, 0  ; %.*s'%s(%u) from idx_type\n",
             t, iat, v, (int)attr.length, attr.data, tag, dim+1);
          Temp_Set_Type (t, iat);
        } else {
          Emit ("  %%t%u = add %s 0, %s  ; %.*s'%s(%u) from idx_type\n",
             t, Integer_Arith_Type (),
             I128_Decimal (Type_Bound_Value (ib)),
             (int)attr.length, attr.data, tag, dim+1);
          Temp_Set_Type (t, Integer_Arith_Type ());
        }

      // Fallback: emit 0
      } else {
        Emit ("  %%t%u = add %s 0, 0  ; %.*s'%s(%u) unknown\n", t, Integer_Arith_Type (),
           (int)attr.length, attr.data, tag, dim+1);
        Temp_Set_Type (t, Integer_Arith_Type ());
      }
    }
  } else if (prefix_type and not Type_Is_Fixed_Point (prefix_type) and
         (Type_Is_Float (prefix_type) or
        Type_Is_Universal_Real (prefix_type) or
        (is_low  ? prefix_type->low_bound.kind  == BOUND_FLOAT
             : prefix_type->high_bound.kind == BOUND_FLOAT))) {

    // Float type - determine LLVM type, then emit bound
    const char *fty = Float_Llvm_Type_Of (prefix_type);
    Type_Bound b = is_low ? prefix_type->low_bound : prefix_type->high_bound;
    if (b.kind == BOUND_EXPR and b.expr) {
      uint32_t v = Generate_Expression (b.expr);

      // Convert bound expression to target float type if needed
      const char *expr_fty = Expression_Llvm_Type (b.expr);
      if (strcmp (expr_fty, fty) != 0) {
        v = Emit_Convert (v, expr_fty, fty);
      }
      Emit ("  %%t%u = fadd %s %%t%u, 0.0  ; %.*s'%s (runtime)\n",
         t, fty, v, (int)attr.length, attr.data, tag);
    } else if (Type_Bound_Is_Set (b)) {
      Emit ("  %%t%u = fadd %s 0.0, %e  ; %.*s'%s\n",
         t, fty, Type_Bound_Float_Value (b), (int)attr.length, attr.data, tag);
    } else {
      Emit_Float_Type_Limit (t, prefix_type, is_low, attr);
    }
  } else if (prefix_type) {
    Type_Bound b = is_low ? prefix_type->low_bound : prefix_type->high_bound;

    // Use prefix type's LLVM width so codegen matches Expression_Llvm_Type.                        
    // Inside generic instantiations, resolve through the base type chain                           
    // to get the actual type's native width (e.g., BOOLEAN > i8).                                  
    //                                                                                              
    const char *bound_llvm = Type_To_Llvm (prefix_type);
    if (cg->current_instance and prefix_type->base_type) {
      Type_Info *resolved_base = Resolve_Generic_Actual_Type (prefix_type->base_type);
      if (resolved_base != prefix_type->base_type) {
        const char *actual_llvm = Type_To_Llvm (resolved_base);
        if (actual_llvm and actual_llvm[0] == 'i') bound_llvm = actual_llvm;
      }
    }
    if (not bound_llvm or bound_llvm[0] != 'i') bound_llvm = Integer_Arith_Type ();

    // Fixed-point FIRST/LAST must return scaled integer representation.
    // Internal repr = abstract_value / SMALL.
    if (Type_Is_Fixed_Point (prefix_type)) {
      double small = prefix_type->fixed.small;
      if (small <= 0) small = prefix_type->fixed.delta > 0 ? prefix_type->fixed.delta : 1.0;

      // Runtime-computed bound (e.g. IDENT_INT (1) * (-1.0))
      if (b.kind == BOUND_EXPR and b.expr) {
        uint32_t fv = Generate_Expression (b.expr);
        const char *fv_ty = Temp_Get_Type (fv);
        bool fv_float = (fv_ty and Is_Float_Type (fv_ty))
          or Type_Is_Float_Representation (b.expr->type)
          or (b.expr->type and b.expr->type->kind == TYPE_UNIVERSAL_REAL);
        if (fv_float) {
          uint64_t sb; memcpy (&sb, &small, sizeof (sb));
          uint32_t st = Emit_Temp ();
          Emit ("  %%t%u = fadd double 0.0, 0x%016llX\n", st, (unsigned long long)sb);
          uint32_t dv = Emit_Temp ();
          Emit ("  %%t%u = fdiv double %%t%u, %%t%u\n", dv, fv, st);
          Emit ("  %%t%u = fptosi double %%t%u to %s\n", t, dv, bound_llvm);
        } else {
          const char *fv_src = fv_ty ? fv_ty : Type_To_Llvm (b.expr->type);
          if (not fv_src or fv_src[0] != 'i') fv_src = bound_llvm;
          if (strcmp (fv_src, bound_llvm) == 0) {
            Emit ("  %%t%u = add %s 0, %%t%u\n", t, bound_llvm, fv);
          } else {
            Emit ("  %%t%u = sext %s %%t%u to %s\n", t, fv_src, fv, bound_llvm);
          }
        }
        Temp_Set_Type (t, bound_llvm);
      } else {
        double fval = Type_Bound_Float_Value (b);
        int64_t scaled = (int64_t)(fval / small);
        Emit ("  %%t%u = add %s 0, %lld  ; %.*s'%s (fixed scaled %g/small)\n", t,
           bound_llvm, (long long)scaled, (int)attr.length, attr.data, tag, fval);
        Temp_Set_Type (t, bound_llvm);
      }
    } else if (Type_Bound_Is_Compile_Time_Known (b)) {
      Emit ("  %%t%u = add %s 0, %s  ; %.*s'%s\n", t,
         bound_llvm, I128_Decimal (Type_Bound_Value (b)), (int)attr.length, attr.data, tag);
      Temp_Set_Type (t, bound_llvm);
    } else {
      return Generate_Bound_Value (b, bound_llvm);
    }
  } else {
    fprintf (stderr, "warning: attribute '%.*s'%s has no type, defaulting to 0\n",
        (int)attr.length, attr.data, tag);
    Emit ("  %%t%u = add %s 0, 0  ; %.*s'%s (no type)\n", t,
       Integer_Arith_Type (), (int)attr.length, attr.data, tag);
  }
  return t;
}
uint32_t Generate_Attribute (Syntax_Node *node) {
  Type_Info *prefix_type = node->attribute.prefix->type;
  String_Slice attr = node->attribute.name;
  uint32_t t = Emit_Temp ();
  Syntax_Node *first_arg = node->attribute.arguments.count > 0
               ? node->attribute.arguments.items[0] : NULL;
  uint32_t dim = Get_Dimension_Index (first_arg);

  // Resolve generic formal type > actual (RM 12.3), but preserve constrained subtypes
  if (prefix_type and cg->current_instance and not prefix_type->base_type)
    prefix_type = Resolve_Generic_Actual_Type (prefix_type);

  // If the attribute prefix is T'BASE, resolve BASE after generic substitution.                    
  // T'BASE with T=C (constrained subtype) should use C's unconstrained base type.                  
  // Without this, T'BASE'FIRST/LAST incorrectly uses the subtype's bounds (RM 3.3.3).              
  //                                                                                                
  if (node->attribute.prefix->kind == NK_ATTRIBUTE and
    Slice_Equal_Ignore_Case (node->attribute.prefix->attribute.name, S("BASE")) and
    prefix_type and prefix_type->base_type)
    prefix_type = Type_Root (prefix_type);

  // For subtypes of generic formals (SUBTYPE S IS T where T is a formal),                          
  // resolve through the base chain to find the actual type for classification.                     
  // prefix_type keeps subtype bounds (for FIRST/LAST); classify_type has the                       
  // actual type kind (for IMAGE, VALUE, etc.) (RM 12.3).                                           
  //                                                                                                
  Type_Info *classify_type = prefix_type;
  if (cg->current_instance and prefix_type) {
    Type_Info *walk = prefix_type;
    for (int depth = 0; walk and depth < 20; depth++) {
      Type_Info *resolved = Resolve_Generic_Actual_Type (walk);
      if (resolved != walk) { classify_type = resolved; break; }
      walk = walk->base_type ? walk->base_type : walk->parent_type;
    }
  }

  // Implicit dereference for access-to-array (RM 4.1(3))
  // But NOT for type-level attributes like SIZE, STORAGE_SIZE, BASE (RM 13.7.2)
  if (Type_Is_Access (prefix_type) and prefix_type->access.designated_type and
    not Slice_Equal_Ignore_Case (attr, S("SIZE")) and
    not Slice_Equal_Ignore_Case (attr, S("STORAGE_SIZE")) and
    not Slice_Equal_Ignore_Case (attr, S("BASE")) and
    not Slice_Equal_Ignore_Case (attr, S("ADDRESS")))
    prefix_type = prefix_type->access.designated_type;

  // Determine if prefix needs runtime bounds (fat pointer)
  bool needs_runtime_bounds = false;
  Symbol *prefix_sym = node->attribute.prefix->symbol;
  if (not prefix_type and prefix_sym and prefix_sym->type)
    prefix_type = prefix_sym->type;
  if (prefix_type and
    (Type_Is_Unconstrained_Array (prefix_type) or Type_Has_Dynamic_Bounds (prefix_type)))
    if (not prefix_sym or prefix_sym->kind == SYMBOL_PARAMETER or
      prefix_sym->kind == SYMBOL_VARIABLE or prefix_sym->kind == SYMBOL_CONSTANT or
      prefix_sym->kind == SYMBOL_DISCRIMINANT or
      prefix_sym->kind == SYMBOL_FUNCTION or prefix_sym->kind == SYMBOL_PROCEDURE)
      needs_runtime_bounds = true;

  // ─── Array/Scalar Bound Attributes: unified FIRST/LAST ─────────────────────────────────────────

  if (Slice_Equal_Ignore_Case (attr, S("FIRST")))
    return Emit_Bound_Attribute (t, prefix_type, prefix_sym,
           node->attribute.prefix, needs_runtime_bounds, dim, true, attr);
  if (Slice_Equal_Ignore_Case (attr, S("LAST")))
    return Emit_Bound_Attribute (t, prefix_type, prefix_sym,
           node->attribute.prefix, needs_runtime_bounds, dim, false, attr);
  if (Slice_Equal_Ignore_Case (attr, S("LENGTH"))) {
    if (Type_Is_Array_Like (prefix_type)) {
      if (needs_runtime_bounds) {
        const char *len_bt = Array_Bound_Llvm_Type (prefix_type);
        uint32_t fat;
        if (prefix_sym) {
          fat = Emit_Load_Fat_Pointer (prefix_sym, len_bt);

        // Complex prefix expression - generate it to get fat pointer value
        } else {
          fat = Generate_Expression (node->attribute.prefix);
        }
        {

          // return length at native type width, no widening.
          // Use dim-aware accessor for multi-dimensional arrays.
          uint32_t len = Emit_Fat_Pointer_Length_Dim (fat, len_bt, dim);
          return len;
        }
      } else if (dim < prefix_type->array.index_count) {
        Type_Bound lb = prefix_type->array.indices[dim].low_bound;
        Type_Bound hb = prefix_type->array.indices[dim].high_bound;

        // If bounds not set, try deriving from index_type
        if (lb.kind != BOUND_INTEGER and lb.kind != BOUND_EXPR and
          prefix_type->array.indices[dim].index_type) {
          Type_Info *idx_ty = prefix_type->array.indices[dim].index_type;
          lb = idx_ty->low_bound;
          hb = idx_ty->high_bound;
        }

        // Dynamic length: generate high - low + 1 at runtime
        if (lb.kind == BOUND_EXPR or hb.kind == BOUND_EXPR) {
          const char *iat = Integer_Arith_Type ();
          uint32_t lo_t = Emit_Type_Bound (&lb, iat);
          uint32_t hi_t = Emit_Type_Bound (&hb, iat);
          uint32_t len = Emit_Length_Clamped (lo_t, hi_t, iat);

          // Copy to result temp t
          Emit ("  %%t%u = add %s %%t%u, 0  ; 'LENGTH (%u)\n", t, iat, len, dim+1);
          Temp_Set_Type (t, iat);
        } else {
          int128_t low = Type_Bound_Value (lb);
          int128_t high = Type_Bound_Value (hb);
          int128_t length = (high >= low) ? (high - low + 1) : 0;
          Emit ("  %%t%u = add %s 0, %s  ; 'LENGTH (%u)\n", t, Integer_Arith_Type (),
             I128_Decimal (length), dim + 1);
          Temp_Set_Type (t, Integer_Arith_Type ());
        }
      }
    }
    return t;
  }

  // Range attribute - for general expression contexts, return low bound.
  // For loops handle RANGE specially in Generate_For_Loop.
  if (Slice_Equal_Ignore_Case (attr, S("RANGE"))) {
    if (Type_Is_Array_Like (prefix_type)) {
      if (needs_runtime_bounds) {
        const char *rng_bt = Array_Bound_Llvm_Type (prefix_type);
        uint32_t fat;
        if (prefix_sym) {
          fat = Emit_Load_Fat_Pointer (prefix_sym, rng_bt);

        // Complex prefix expression - generate it to get fat pointer value
        } else {
          fat = Generate_Expression (node->attribute.prefix);
        }

        // return RANGE low at native type width, no widening.
        // Use dim-aware accessor for multi-dimensional arrays.
        return Emit_Fat_Pointer_Low_Dim (fat, rng_bt, dim);
      } else if (dim < prefix_type->array.index_count) {
        Type_Bound lb = prefix_type->array.indices[dim].low_bound;

        // If bounds not set, try deriving from index_type
        if (lb.kind != BOUND_INTEGER and lb.kind != BOUND_EXPR and
          prefix_type->array.indices[dim].index_type) {
          lb = prefix_type->array.indices[dim].index_type->low_bound;
        }
        if (lb.kind == BOUND_EXPR and lb.expr) {
          uint32_t v = Generate_Expression (lb.expr);
          const char *iat = Integer_Arith_Type ();
          const char *vty = Expression_Llvm_Type (lb.expr);
          if (strcmp (vty, iat) != 0 and vty[0] == 'i')
            v = Emit_Convert (v, vty, iat);
          Emit ("  %%t%u = add %s %%t%u, 0  ; 'RANGE (%u) low dynamic\n",
             t, iat, v, dim + 1);
          Temp_Set_Type (t, iat);
        } else {
          Emit ("  %%t%u = add %s 0, %s  ; 'RANGE (%u) low\n", t, Integer_Arith_Type (),
             I128_Decimal (Type_Bound_Value (lb)), dim + 1);
        }
      }
    }
    return t;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Size and Representation Attributes                                                             
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // 'SIZE returns size in bits.                                                                    
  // Use specified_bit_size if a SIZE clause was given (exact value),                               
  // otherwise compute from byte size.                                                              
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("SIZE"))) {
    if (not prefix_type)
      fprintf (stderr, "warning: 'SIZE attribute applied to expression with no type\n");
    int64_t bit_size = 0;
    if (prefix_type) {
      if (prefix_type->specified_bit_size > 0)
        bit_size = prefix_type->specified_bit_size;
      else
        bit_size = (int64_t)prefix_type->size * 8;
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'SIZE in bits\n", t, Integer_Arith_Type (),
       (long long)bit_size);
    return t;
  }
  if (Slice_Equal_Ignore_Case (attr, S("ALIGNMENT"))) {
    if (not prefix_type)
      fprintf (stderr, "warning: 'ALIGNMENT attribute applied to expression with no type\n");
    Emit ("  %%t%u = add %s 0, %lld  ; 'ALIGNMENT\n", t, Integer_Arith_Type (),
       (long long)(prefix_type ? prefix_type->alignment : 8));
    return t;
  }
  if (Slice_Equal_Ignore_Case (attr, S("COMPONENT_SIZE"))) {
    if (Type_Is_Array_Like (prefix_type) and prefix_type->array.element_type) {
      Emit ("  %%t%u = add %s 0, %lld  ; 'COMPONENT_SIZE\n", t, Integer_Arith_Type (),
         (long long)(prefix_type->array.element_type->size * 8));
    } else {
      fprintf (stderr, "warning: 'COMPONENT_SIZE applied to non-array type, returning 0\n");
      Emit ("  %%t%u = add %s 0, 0\n", t, Integer_Arith_Type ());
    }
    return t;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Address Attribute                                                                              
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // Generate address of prefix object
  if (Slice_Equal_Ignore_Case (attr, S("ADDRESS"))) {
    Symbol *sym = node->attribute.prefix->symbol;
    if (sym) {

      // Label address - use blockaddress for code location
      if (sym->kind == SYMBOL_LABEL) {
        if (sym->llvm_label_id == 0) {
          sym->llvm_label_id = cg->label_id++;
        }
        Emit ("  %%t%u = ptrtoint ptr blockaddress(@", t);
        Emit_Symbol_Name (cg->current_function);
        Emit (", %%L%u) to i64  ; '%.*s'ADDRESS\n",
           sym->llvm_label_id,
           (int)sym->name.length, sym->name.data);

      // Subprogram address - use function pointer directly
      } else if (sym->kind == SYMBOL_FUNCTION or sym->kind == SYMBOL_PROCEDURE) {
        Emit ("  %%t%u = ptrtoint ptr @", t);
        Emit_Symbol_Name (sym);
        Emit (" to i64  ; '%.*s'ADDRESS (subprogram)\n",
           (int)sym->name.length, sym->name.data);

      // Package/generic address - use a global marker
      } else if (sym->kind == SYMBOL_PACKAGE or sym->kind == SYMBOL_GENERIC) {
        Emit ("  %%t%u = ptrtoint ptr @__addr.", t);
        Emit_Symbol_Name (sym);
        Emit (" to i64  ; '%.*s'ADDRESS (package/generic)\n",
           (int)sym->name.length, sym->name.data);

        // Mark that we need to emit this global marker
        if (not sym->needs_address_marker and cg->address_marker_count < 256) {
          sym->needs_address_marker = true;
          cg->address_markers[cg->address_marker_count++] = sym;
        }

      // Task type address - use a global marker
      } else if (sym->kind == SYMBOL_TYPE and Type_Is_Task (sym->type)) {
        Emit ("  %%t%u = ptrtoint ptr @__addr.", t);
        Emit_Symbol_Name (sym);
        Emit (" to i64  ; '%.*s'ADDRESS (task type)\n",
           (int)sym->name.length, sym->name.data);
        if (not sym->needs_address_marker and cg->address_marker_count < 256) {
          sym->needs_address_marker = true;
          cg->address_markers[cg->address_marker_count++] = sym;
        }

      // Variable/task address - emit via uplevel predicate
      } else {
        Emit ("  %%t%u = ptrtoint ptr ", t);
        Emit_Symbol_Storage (sym);
        Emit (" to i64  ; 'ADDRESS\n");
      }
    } else {
      fprintf (stderr, "warning: 'ADDRESS attribute applied to expression with no symbol\n");
      Emit ("  %%t%u = add i64 0, 0  ; 'ADDRESS (no symbol)\n", t);
    }
    return t;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Enumeration Attributes                                                                         
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // T'POS (x) - position of enumeration value (RM 3.5.5).                                          
  // POS returns universal_integer - convert to Integer_Arith_Type                                  
  // so the result matches Expression_Llvm_Type's prediction.                                       
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("POS"))) {
    if (first_arg) {
      uint32_t val = Generate_Expression (first_arg);
      const char *arg_t = Expression_Llvm_Type (first_arg);
      const char *iat = Integer_Arith_Type ();
      return Emit_Convert (val, arg_t, iat);
    }
    fprintf (stderr, "warning: 'POS attribute requires an argument\n");
    return 0;
  }

  // T'VAL (n) - value at position n (RM 3.5.5(5))
  // Raises CONSTRAINT_ERROR if n is outside T'BASE range
  if (Slice_Equal_Ignore_Case (attr, S("VAL"))) {
    if (first_arg) {
      uint32_t val = Generate_Expression (first_arg);

      // Get BASE TYPE bounds for range check (RM 3.5.5(5))
      Type_Info *val_base = Type_Root (prefix_type);
      if (cg->current_instance)
        val_base = Type_Root (Resolve_Generic_Actual_Type (val_base));
      int64_t lo = 0, hi = 0;
      bool have_bounds = false;
      if (Type_Is_Enumeration (val_base) and
        val_base->enumeration.literal_count > 0) {
        lo = 0;
        hi = (int64_t)(val_base->enumeration.literal_count - 1);
        have_bounds = true;
      } else if (val_base->low_bound.kind == BOUND_INTEGER and
             val_base->high_bound.kind == BOUND_INTEGER) {
        lo = val_base->low_bound.int_value;
        hi = val_base->high_bound.int_value;
        have_bounds = true;
      }

      // Range check at i32 width (argument is integer)
      if (have_bounds) {
        const char *iat = Integer_Arith_Type ();
        const char *arg_t = Expression_Llvm_Type (first_arg);
        uint32_t check_val = Emit_Convert (val, arg_t, iat);
        Emit_Range_Check_With_Raise (check_val, lo, hi, iat, "'VAL");
      }

      // T'VAL returns at type T's width - convert from argument type.
      // Resolve generic formal types to their actuals.
      const char *val_t = Expression_Llvm_Type (first_arg);
      Type_Info *val_result_type = prefix_type;
      if (cg->current_instance)
        val_result_type = Resolve_Generic_Actual_Type (val_result_type);
      const char *result_t = Type_To_Llvm (val_result_type);
      if (result_t and result_t[0] == 'i')
        val = Emit_Convert (val, val_t, result_t);
      return val;
    }
    fprintf (stderr, "warning: 'VAL attribute requires an argument\n");
    return 0;
  }

  // T'SUCC (x) / T'PRED (x) - RM 3.5.5: operates on base type.
  // SUCC adds 1 and checks against high bound; PRED subtracts 1 and checks low.
  if (Slice_Equal_Ignore_Case (attr, S("SUCC")) or
    Slice_Equal_Ignore_Case (attr, S("PRED"))) {
    bool is_succ = (attr.data[0] == 'S' or attr.data[0] == 's');
    if (first_arg) {
      uint32_t val = Generate_Expression (first_arg);

      // Operate in i64 to avoid overflow when SUCC (LAST) or PRED (FIRST)
      // would wrap in the native type width (e.g. i32 INTEGER).
      const char *wide_iat = "i64";
      const char *arg_t = Expression_Llvm_Type (first_arg);
      uint32_t wide_val = Emit_Convert (val, arg_t, wide_iat);
      Emit ("  %%t%u = %s %s %%t%u, 1  ; '%s\n",
         t, is_succ ? "add" : "sub", wide_iat, wide_val, is_succ ? "SUCC" : "PRED");
      Temp_Set_Type (t, wide_iat);

      // Resolve root base type for 'PRED/'SUCC bound check (RM 3.5.5(6)):                          
      // Result must be in T'BASE range, which for derived types                                    
      // includes ALL values of the parent type's base range.                                       
      //                                                                                            
      Type_Info *base = Type_Root (prefix_type);
      if (cg->current_instance)
        base = Type_Root (Resolve_Generic_Actual_Type (base));

      // Bound check: SUCC vs high, PRED vs low
      Type_Bound limit = is_succ ? base->high_bound : base->low_bound;
      if (base and limit.kind == BOUND_INTEGER) {
        uint32_t cmp = Emit_Temp ();
        Emit ("  %%t%u = icmp %s %s %%t%u, %s\n", cmp,
           is_succ ? "sgt" : "slt", wide_iat, t, I128_Decimal (limit.int_value));
        Emit_Check_With_Raise (cmp, true, is_succ ? "'SUCC" : "'PRED");
      }

      // Convert result back to prefix type width.
      // Resolve generic formal types to their actuals.
      Type_Info *result_type = prefix_type;
      if (cg->current_instance)
        result_type = Resolve_Generic_Actual_Type (result_type);
      const char *result_t = Type_To_Llvm (result_type);
      if (result_t and result_t[0] == 'i')
        t = Emit_Convert (t, wide_iat, result_t);
      return t;
    }
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Scalar Type Attributes                                                                         
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // T'MIN (a, b) - minimum of two values
  if (Slice_Equal_Ignore_Case (attr, S("MIN"))) {
    if (node->attribute.arguments.count >= 2) {
      uint32_t left  = Generate_Expression (node->attribute.arguments.items[0]);
      uint32_t right = Generate_Expression (node->attribute.arguments.items[1]);

      // Select minimum using icmp and select
      uint32_t cmp = Emit_Temp ();
      Emit ("  %%t%u = icmp slt %s %%t%u, %%t%u\n", cmp, Integer_Arith_Type (), left, right);
      Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u  ; 'MIN\n", t, cmp, Integer_Arith_Type (), left, Integer_Arith_Type (), right);
      return t;
    }
    fprintf (stderr, "warning: 'MIN attribute requires two arguments\n");
    return 0;
  }

  // T'MAX (a, b) - maximum of two values
  if (Slice_Equal_Ignore_Case (attr, S("MAX"))) {
    if (node->attribute.arguments.count >= 2) {
      uint32_t left  = Generate_Expression (node->attribute.arguments.items[0]);
      uint32_t right = Generate_Expression (node->attribute.arguments.items[1]);

      // Select maximum using icmp and select
      uint32_t cmp = Emit_Temp ();
      Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n", cmp, Integer_Arith_Type (), left, right);
      Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u  ; 'MAX\n", t, cmp, Integer_Arith_Type (), left, Integer_Arith_Type (), right);
      return t;
    }
    fprintf (stderr, "warning: 'MAX attribute requires two arguments\n");
    return 0;
  }

  // T'ABS (x) with overflow check: ABS (MIN_INT) overflows
  if (Slice_Equal_Ignore_Case (attr, S("ABS"))) {
    if (first_arg) {
      uint32_t val = Generate_Expression (first_arg);
      const char *abs_type = Integer_Arith_Type ();
      Type_Info *res_type = first_arg->type;
      uint32_t zero = Emit_Temp ();
      Emit ("  %%t%u = add %s 0, 0\n", zero, abs_type);
      uint32_t neg = Emit_Overflow_Checked_Op (zero, val, "sub", abs_type, res_type);
      uint32_t cmp = Emit_Temp ();
      Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", cmp, abs_type, val);
      Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u  ; 'ABS\n",
         t, cmp, abs_type, neg, abs_type, val);
      return t;
    }
  }

  // T'MOD - returns the modulus of a modular type (RM 3.5.4(17))
  if (Slice_Equal_Ignore_Case (attr, S("MOD"))) {
    if (prefix_type and prefix_type->kind == TYPE_MODULAR and prefix_type->modulus > 0) {
      const char *mod_type = Type_To_Llvm (prefix_type);
      Emit ("  %%t%u = add %s 0, %s  ; 'MOD\n", t, mod_type,
         U128_Decimal (prefix_type->modulus));
      return t;
    }
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // String/Image Attributes                                                                        
  // These call runtime functions for string conversion.                                            
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // T'IMAGE (x) - string representation (RM 3.5.5)
  if (Slice_Equal_Ignore_Case (attr, S("IMAGE"))) {
    if (first_arg) {
      uint32_t arg_val = Generate_Expression (first_arg);
      if (Type_Is_Integer_Like (classify_type) or
        Type_Is_Universal_Integer (classify_type)) {

        // Integer'IMAGE - widen to INTEGER arith type for RTS ABI
        uint32_t arg_w = Emit_Widen_For_Intrinsic (arg_val, Expression_Llvm_Type (first_arg));
        Emit ("  %%t%u = call " FAT_PTR_TYPE " @__ada_integer_image(%s %%t%u)\n",
           t, Integer_Arith_Type (), arg_w);

      // Character'IMAGE - convert to i8 for RTS ABI
      } else if (Type_Is_Character (classify_type)) {
        const char *arg_src_type = Expression_Llvm_Type (first_arg);
        uint32_t char_val = Emit_Convert (arg_val, arg_src_type, "i8");
        Emit ("  %%t%u = call " FAT_PTR_TYPE " @__ada_character_image(i8 %%t%u)\n",
           t, char_val);

      // Boolean'IMAGE - switch on 0>"FALSE", 1>"TRUE" (RM 3.5.5)
      } else if (Type_Is_Boolean (classify_type)) {
        const char *img_bt = String_Bound_Type ();
        uint32_t result_ptr = Emit_Temp ();
        uint32_t result_len = Emit_Temp ();
        Emit ("  %%t%u = alloca ptr\n", result_ptr);
        Emit ("  %%t%u = alloca %s\n", result_len, img_bt);
        uint32_t false_label = cg->label_id++;
        uint32_t true_label = cg->label_id++;
        uint32_t end_label = cg->label_id++;
        const char *arg_t = Expression_Llvm_Type (first_arg);
        uint32_t cmp_val = Emit_Temp ();
        Emit ("  %%t%u = icmp ne %s %%t%u, 0\n", cmp_val, arg_t, arg_val);
        Emit ("  br i1 %%t%u, label %%Lbimg_t%u, label %%Lbimg_f%u\n",
           cmp_val, true_label, false_label);

        // FALSE case
        uint32_t false_str_id = cg->string_id++;
        Emit_String_Const ("@.img_str%u = private unnamed_addr constant [6 x i8] c\"FALSE\\00\"\n", false_str_id);
        Emit ("Lbimg_f%u:\n", false_label);
        Emit ("  store ptr @.img_str%u, ptr %%t%u\n", false_str_id, result_ptr);
        Emit ("  store %s 5, ptr %%t%u\n", img_bt, result_len);
        Emit ("  br label %%Lbimg_end%u\n", end_label);

        // TRUE case
        uint32_t true_str_id = cg->string_id++;
        Emit_String_Const ("@.img_str%u = private unnamed_addr constant [5 x i8] c\"TRUE\\00\"\n", true_str_id);
        Emit ("Lbimg_t%u:\n", true_label);
        Emit ("  store ptr @.img_str%u, ptr %%t%u\n", true_str_id, result_ptr);
        Emit ("  store %s 4, ptr %%t%u\n", img_bt, result_len);
        Emit ("  br label %%Lbimg_end%u\n", end_label);

        // End
        Emit ("Lbimg_end%u:\n", end_label);
        uint32_t ptr_load = Emit_Temp ();
        uint32_t len_load = Emit_Temp ();
        Emit ("  %%t%u = load ptr, ptr %%t%u\n", ptr_load, result_ptr);
        Emit ("  %%t%u = load %s, ptr %%t%u\n", len_load, img_bt, result_len);
        uint32_t low_one = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, 1\n", low_one, img_bt);
        t = Emit_Fat_Pointer_Dynamic (ptr_load, low_one, len_load, img_bt);
      } else if (Type_Is_Float_Representation (classify_type)) {

        // Float'IMAGE
        Emit ("  %%t%u = call " FAT_PTR_TYPE " @__ada_float_image(double %%t%u)\n",
           t, arg_val);

      // Enumeration'IMAGE - return literal name as string
      } else if (Type_Is_Enumeration (classify_type)) {

        // Find root enumeration type with literals
        Type_Info *enum_type = classify_type;
        while (enum_type and not enum_type->enumeration.literals) {
          if (enum_type->parent_type) enum_type = enum_type->parent_type;
          else if (enum_type->base_type) enum_type = enum_type->base_type;
          else break;
        }
        if (enum_type and enum_type->enumeration.literals and
          enum_type->enumeration.literal_count > 0) {

          // Generate inline switch for literal lookup.
          // Length stored in STRING bound type (derived from type system).
          const char *img_bt = String_Bound_Type ();
          uint32_t result_ptr = Emit_Temp ();
          uint32_t result_len = Emit_Temp ();
          Emit ("  %%t%u = alloca ptr\n", result_ptr);
          Emit ("  %%t%u = alloca %s\n", result_len, img_bt);
          uint32_t switch_label = cg->label_id++;
          uint32_t default_label = cg->label_id++;
          uint32_t end_label = cg->label_id++;

          // Widen arg to i64 for switch - arg may be i8 for small enums
          const char *arg_t = Expression_Llvm_Type (first_arg);
          uint32_t sw_val = Emit_Convert (arg_val, arg_t, "i64");
          Emit ("  switch i64 %%t%u, label %%Limg_def%u [\n", sw_val, default_label);
          for (uint32_t i = 0; i < enum_type->enumeration.literal_count; i++) {
            Emit ("    i64 %u, label %%Limg%u_%u\n", i, switch_label, i);
          }
          Emit ("  ]\n");

          // Generate case labels
          for (uint32_t i = 0; i < enum_type->enumeration.literal_count; i++) {
            String_Slice lit = enum_type->enumeration.literals[i];

            // Generate string constant name
            uint32_t str_id = cg->string_id++;
            Emit ("Limg%u_%u:\n", switch_label, i);

            // Character literals: IMAGE returns 'x' with quotes preserved.
            // Identifier literals: IMAGE returns uppercased name. (RM 3.5.5)
            bool is_char_lit = (lit.length >= 3 and lit.data[0] == '\'');

            // Character literal - emit as-is (e.g. 'a')
            if (is_char_lit) {
              Emit_String_Const ("@.img_str%u = private unnamed_addr constant [%u x i8] c\"",
                     str_id, (unsigned)lit.length + 1);
              for (uint32_t j = 0; j < lit.length; j++) {
                Emit_String_Const ("%c", lit.data[j]);
              }
              Emit_String_Const ("\\00\"\n");

            // Identifier - uppercase (Ada identifiers are case-insensitive)
            } else {
              Emit_String_Const ("@.img_str%u = private unnamed_addr constant [%u x i8] c\"",
                     str_id, (unsigned)lit.length + 1);
              for (uint32_t j = 0; j < lit.length; j++) {
                Emit_String_Const ("%c", (char)toupper ((unsigned char)lit.data[j]));
              }
              Emit_String_Const ("\\00\"\n");
            }
            Emit ("  store ptr @.img_str%u, ptr %%t%u\n", str_id, result_ptr);
            Emit ("  store %s %u, ptr %%t%u\n", img_bt, (unsigned)lit.length, result_len);
            Emit ("  br label %%Limg_end%u\n", end_label);
          }

          // Default case - return empty string
          Emit ("Limg_def%u:\n", default_label);
          Emit ("  store ptr null, ptr %%t%u\n", result_ptr);
          Emit ("  store %s 0, ptr %%t%u\n", img_bt, result_len);
          Emit ("  br label %%Limg_end%u\n", end_label);
          Emit ("Limg_end%u:\n", end_label);
          uint32_t ptr_load = Emit_Temp ();
          uint32_t len_load = Emit_Temp ();
          Emit ("  %%t%u = load ptr, ptr %%t%u\n", ptr_load, result_ptr);
          Emit ("  %%t%u = load %s, ptr %%t%u\n", len_load, img_bt, result_len);

          // Build fat pointer result: { ptr_load, { 1, len_load } }
          uint32_t low_one = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, 1\n", low_one, img_bt);
          t = Emit_Fat_Pointer_Dynamic (ptr_load, low_one, len_load, img_bt);

        // No literals found, fallback to integer image
        } else {
          uint32_t arg_w2 = Emit_Widen_For_Intrinsic (arg_val, Expression_Llvm_Type (first_arg));
          Emit ("  %%t%u = call " FAT_PTR_TYPE " @__ada_integer_image(%s %%t%u)\n",
             t, Integer_Arith_Type (), arg_w2);
        }

      // Default: treat as integer
      } else {
        uint32_t arg_w3 = Emit_Widen_For_Intrinsic (arg_val, Expression_Llvm_Type (first_arg));
        Emit ("  %%t%u = call " FAT_PTR_TYPE " @__ada_integer_image(%s %%t%u)\n",
           t, Integer_Arith_Type (), arg_w3);
      }
      return t;
    }
    t = Emit_Fat_Pointer_Null (String_Bound_Type ());  // 'IMAGE no arg
    return t;
  }

  // T'VALUE (s) - parse string to type (RM 3.5.5)
  if (Slice_Equal_Ignore_Case (attr, S("VALUE"))) {
    if (first_arg) {
      uint32_t str_val = Generate_Expression (first_arg);
      if (Type_Is_Integer_Like (classify_type) or
        Type_Is_Universal_Integer (classify_type)) {

        // Integer'VALUE - parse string as integer
        const char *val_iat = Integer_Arith_Type ();
        Emit ("  %%t%u = call %s @__ada_integer_value(" FAT_PTR_TYPE " %%t%u)\n",
           t, val_iat, str_val);
        Temp_Set_Type (t, val_iat);

        // Convert to prefix type width if different
        const char *val_result_t = Type_To_Llvm (prefix_type);
        t = Emit_Convert (t, val_iat, val_result_t);

      // Float'VALUE - parse string as float
      } else if (Type_Is_Float_Representation (classify_type)) {
        Emit ("  %%t%u = call double @__ada_float_value(" FAT_PTR_TYPE " %%t%u)\n",
           t, str_val);

      // Character'VALUE - parse "'x'" format, strip leading/trailing spaces (RM 3.5.5)
      } else if (Type_Is_Character (classify_type)) {
        const char *val_bt = String_Bound_Type ();
        const char *val_iat = Integer_Arith_Type ();
        uint32_t str_ptr_raw = Emit_Fat_Pointer_Data (str_val, val_bt);
        uint32_t str_len_bt = Emit_Fat_Pointer_Length (str_val, val_bt);
        uint32_t str_len_raw = Emit_Convert (str_len_bt, val_bt, val_iat);

        // Trim leading/trailing spaces
        uint32_t lead_off = Emit_Temp ();
        Emit ("  %%t%u = call %s @__ada_count_leading_spaces(ptr %%t%u, %s %%t%u)\n",
           lead_off, val_iat, str_ptr_raw, val_iat, str_len_raw);
        uint32_t trail_off = Emit_Temp ();
        Emit ("  %%t%u = call %s @__ada_count_trailing_spaces(ptr %%t%u, %s %%t%u)\n",
           trail_off, val_iat, str_ptr_raw, val_iat, str_len_raw);
        uint32_t str_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
           str_ptr, str_ptr_raw, val_iat, lead_off);
        uint32_t trim_total = Emit_Temp ();
        Emit ("  %%t%u = add %s %%t%u, %%t%u\n", trim_total, val_iat, lead_off, trail_off);
        uint32_t str_len = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", str_len, val_iat, str_len_raw, trim_total);
        cg->needs_trim_helpers = true;

        // Must be exactly 3 chars: 'x'
        uint32_t len3 = Emit_Temp ();
        Emit ("  %%t%u = icmp eq %s %%t%u, 3\n", len3, val_iat, str_len);
        uint32_t ok_label = cg->label_id++;
        uint32_t fail_label = cg->label_id++;
        uint32_t end_label = cg->label_id++;
        Emit ("  br i1 %%t%u, label %%Lcval_ok%u, label %%Lcval_fail%u\n", len3, ok_label, fail_label);
        Emit ("Lcval_ok%u:\n", ok_label);

        // Check first and last chars are single quotes
        uint32_t c0_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s 0\n", c0_ptr, str_ptr, val_iat);
        uint32_t c0 = Emit_Temp ();
        Emit ("  %%t%u = load i8, ptr %%t%u\n", c0, c0_ptr);
        uint32_t c2_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s 2\n", c2_ptr, str_ptr, val_iat);
        uint32_t c2 = Emit_Temp ();
        Emit ("  %%t%u = load i8, ptr %%t%u\n", c2, c2_ptr);
        uint32_t q0 = Emit_Temp ();
        Emit ("  %%t%u = icmp eq i8 %%t%u, 39  ; check first quote\n", q0, c0);
        uint32_t q2 = Emit_Temp ();
        Emit ("  %%t%u = icmp eq i8 %%t%u, 39  ; check last quote\n", q2, c2);
        uint32_t both_q = Emit_Temp ();
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", both_q, q0, q2);
        uint32_t extract_label = cg->label_id++;
        Emit ("  br i1 %%t%u, label %%Lcval_ext%u, label %%Lcval_fail%u\n", both_q, extract_label, fail_label);
        Emit ("Lcval_ext%u:\n", extract_label);

        // Extract the character at position 1
        uint32_t c1_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s 1\n", c1_ptr, str_ptr, val_iat);
        uint32_t c1 = Emit_Temp ();
        Emit ("  %%t%u = load i8, ptr %%t%u\n", c1, c1_ptr);
        t = c1;
        Emit ("  br label %%Lcval_end%u\n", end_label);

        // Fail - raise CONSTRAINT_ERROR
        Emit ("Lcval_fail%u:\n", fail_label);
        Emit_Raise_Constraint_Error ("VALUE no match");
        Emit ("Lcval_end%u:\n", end_label);
        Temp_Set_Type (t, "i8");

      // Boolean'VALUE - match "TRUE"/"FALSE" case-insensitively, strip spaces (RM 3.5.5)
      } else if (Type_Is_Boolean (classify_type)) {
        const char *val_bt = String_Bound_Type ();
        const char *val_iat = Integer_Arith_Type ();
        uint32_t str_ptr_raw = Emit_Fat_Pointer_Data (str_val, val_bt);
        uint32_t str_len_bt = Emit_Fat_Pointer_Length (str_val, val_bt);
        uint32_t str_len_raw = Emit_Convert (str_len_bt, val_bt, val_iat);

        // Trim leading/trailing spaces
        uint32_t lead_off = Emit_Temp ();
        Emit ("  %%t%u = call %s @__ada_count_leading_spaces(ptr %%t%u, %s %%t%u)\n",
           lead_off, val_iat, str_ptr_raw, val_iat, str_len_raw);
        uint32_t trail_off = Emit_Temp ();
        Emit ("  %%t%u = call %s @__ada_count_trailing_spaces(ptr %%t%u, %s %%t%u)\n",
           trail_off, val_iat, str_ptr_raw, val_iat, str_len_raw);
        uint32_t str_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
           str_ptr, str_ptr_raw, val_iat, lead_off);
        uint32_t trim_total = Emit_Temp ();
        Emit ("  %%t%u = add %s %%t%u, %%t%u\n", trim_total, val_iat, lead_off, trail_off);
        uint32_t str_len = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", str_len, val_iat, str_len_raw, trim_total);
        cg->needs_trim_helpers = true;

        // Check for "TRUE" (length 4) and "FALSE" (length 5)
        uint32_t true_str_id = cg->string_id++;
        Emit_String_Const ("@.val_str%u = private unnamed_addr constant [5 x i8] c\"TRUE\\00\"\n", true_str_id);
        uint32_t false_str_id = cg->string_id++;
        Emit_String_Const ("@.val_str%u = private unnamed_addr constant [6 x i8] c\"FALSE\\00\"\n", false_str_id);
        uint32_t result_alloc = Emit_Temp ();
        Emit ("  %%t%u = alloca %s\n", result_alloc, val_iat);
        Emit ("  store %s 0, ptr %%t%u\n", val_iat, result_alloc);
        uint32_t check_true = cg->label_id++;
        uint32_t check_false_len = cg->label_id++;
        uint32_t check_false = cg->label_id++;
        uint32_t match_true = cg->label_id++;
        uint32_t match_false = cg->label_id++;
        uint32_t no_match = cg->label_id++;
        uint32_t end_label = cg->label_id++;

        // Check length == 4 (TRUE)
        uint32_t len4 = Emit_Temp ();
        Emit ("  %%t%u = icmp eq %s %%t%u, 4\n", len4, val_iat, str_len);
        Emit ("  br i1 %%t%u, label %%Lbval_ct%u, label %%Lbval_cfl%u\n", len4, check_true, check_false_len);
        Emit ("Lbval_ct%u:\n", check_true);
        uint32_t cmp_true = Emit_Temp ();
        Emit ("  %%t%u = call i32 @strncasecmp (ptr %%t%u, ptr @.val_str%u, i64 4)\n",
           cmp_true, str_ptr, true_str_id);
        uint32_t is_true = Emit_Temp ();
        Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", is_true, cmp_true);
        Emit ("  br i1 %%t%u, label %%Lbval_mt%u, label %%Lbval_cfl%u\n", is_true, match_true, check_false_len);
        Emit ("Lbval_mt%u:\n", match_true);
        Emit ("  store %s 1, ptr %%t%u\n", val_iat, result_alloc);
        Emit ("  br label %%Lbval_end%u\n", end_label);

        // Check length == 5 (FALSE)
        Emit ("Lbval_cfl%u:\n", check_false_len);
        uint32_t len5 = Emit_Temp ();
        Emit ("  %%t%u = icmp eq %s %%t%u, 5\n", len5, val_iat, str_len);
        Emit ("  br i1 %%t%u, label %%Lbval_cf%u, label %%Lbval_nm%u\n", len5, check_false, no_match);
        Emit ("Lbval_cf%u:\n", check_false);
        uint32_t cmp_false = Emit_Temp ();
        Emit ("  %%t%u = call i32 @strncasecmp (ptr %%t%u, ptr @.val_str%u, i64 5)\n",
           cmp_false, str_ptr, false_str_id);
        uint32_t is_false = Emit_Temp ();
        Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", is_false, cmp_false);
        Emit ("  br i1 %%t%u, label %%Lbval_mf%u, label %%Lbval_nm%u\n", is_false, match_false, no_match);
        Emit ("Lbval_mf%u:\n", match_false);
        Emit ("  store %s 0, ptr %%t%u\n", val_iat, result_alloc);
        Emit ("  br label %%Lbval_end%u\n", end_label);

        // No match - raise CONSTRAINT_ERROR
        Emit ("Lbval_nm%u:\n", no_match);
        Emit_Raise_Constraint_Error ("VALUE no match");
        Emit ("Lbval_end%u:\n", end_label);
        Emit ("  %%t%u = load %s, ptr %%t%u\n", t, val_iat, result_alloc);
        const char *bool_result_t = Type_To_Llvm (classify_type);
        if (bool_result_t and bool_result_t[0] == 'i')
          t = Emit_Convert (t, val_iat, bool_result_t);

      // Enumeration'VALUE - find literal by name and return position
      } else if (Type_Is_Enumeration (classify_type)) {

        // Find root enumeration type with literals
        Type_Info *enum_type = classify_type;
        while (enum_type and not enum_type->enumeration.literals) {
          if (enum_type->parent_type) enum_type = enum_type->parent_type;
          else if (enum_type->base_type) enum_type = enum_type->base_type;
          else break;
        }
        if (enum_type and enum_type->enumeration.literals and
          enum_type->enumeration.literal_count > 0) {

          // Enum'VALUE: compare input against each literal (case-insensitive).
          // Ada RM 3.5.5: leading/trailing blanks are stripped first.
          const char *val_bt = String_Bound_Type ();
          uint32_t str_ptr_raw = Emit_Fat_Pointer_Data (str_val, val_bt);
          uint32_t str_len_bt = Emit_Fat_Pointer_Length (str_val, val_bt);
          const char *val_iat = Integer_Arith_Type ();
          uint32_t str_len_raw = Emit_Convert (str_len_bt, val_bt, val_iat);

          // Trim leading/trailing spaces via runtime helpers (Ada RM 3.5)
          uint32_t lead_off = Emit_Temp ();
          Emit ("  %%t%u = call %s @__ada_count_leading_spaces(ptr %%t%u, %s %%t%u)\n",
             lead_off, val_iat, str_ptr_raw, val_iat, str_len_raw);

          // Backward scan for trailing spaces
          uint32_t trail_off = Emit_Temp ();
          Emit ("  %%t%u = call %s @__ada_count_trailing_spaces(ptr %%t%u, %s %%t%u)\n",
             trail_off, val_iat, str_ptr_raw, val_iat, str_len_raw);

          // Trimmed pointer and length
          uint32_t str_ptr = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
             str_ptr, str_ptr_raw, val_iat, lead_off);
          uint32_t trim_total = Emit_Temp ();
          Emit ("  %%t%u = add %s %%t%u, %%t%u\n", trim_total, val_iat, lead_off, trail_off);
          uint32_t str_len = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", str_len, val_iat, str_len_raw, trim_total);
          cg->needs_trim_helpers = true;
          uint32_t result_alloc = Emit_Temp ();
          Emit ("  %%t%u = alloca %s\n", result_alloc, val_iat);
          Emit ("  store %s 0, ptr %%t%u\n", val_iat, result_alloc);
          uint32_t end_label = cg->label_id++;
          for (uint32_t i = 0; i < enum_type->enumeration.literal_count; i++) {
            String_Slice lit = enum_type->enumeration.literals[i];
            uint32_t check_label = cg->label_id++;
            uint32_t next_label = cg->label_id++;

            // Character literals are case-sensitive; identifiers are uppercased
            // and compared case-insensitively (RM 3.5.5)
            bool is_char_lit = (lit.length >= 3 and lit.data[0] == '\'');
            uint32_t str_id = cg->string_id++;
            Emit_String_Const ("@.val_str%u = private unnamed_addr constant [%u x i8] c\"",
                   str_id, (unsigned)lit.length + 1);
            for (uint32_t j = 0; j < lit.length; j++) {
              Emit_String_Const ("%c",
                is_char_lit ? lit.data[j] : (char)toupper ((unsigned char)lit.data[j]));
            }
            Emit_String_Const ("\\00\"\n");

            // Check trimmed length matches literal
            uint32_t len_cmp = Emit_Temp ();
            Emit ("  %%t%u = icmp eq %s %%t%u, %u\n", len_cmp, val_iat, str_len, (unsigned)lit.length);
            Emit ("  br i1 %%t%u, label %%Lval%u, label %%Lval_next%u\n", len_cmp, check_label, next_label);
            Emit ("Lval%u:\n", check_label);
            uint32_t cmp_result = Emit_Temp ();

            // Case-sensitive comparison for character literals (RM 3.5.5)
            if (is_char_lit) {
              Emit ("  %%t%u = call i32 @memcmp (ptr %%t%u, ptr @.val_str%u, i64 %u)\n",
                 cmp_result, str_ptr, str_id, (unsigned)lit.length);

            // Case-insensitive comparison for identifiers
            } else {
              Emit ("  %%t%u = call i32 @strncasecmp (ptr %%t%u, ptr @.val_str%u, i64 %u)\n",
                 cmp_result, str_ptr, str_id, (unsigned)lit.length);
            }
            uint32_t cmp_eq = Emit_Temp ();
            Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", cmp_eq, cmp_result);
            uint32_t match_label = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%Lval_match%u, label %%Lval_next%u\n", cmp_eq, match_label, next_label);
            Emit ("Lval_match%u:\n", match_label);
            Emit ("  store %s %u, ptr %%t%u\n", val_iat, i, result_alloc);
            Emit ("  br label %%Lval_end%u\n", end_label);
            Emit ("Lval_next%u:\n", next_label);
          }

          // No match - raise CONSTRAINT_ERROR (RM 3.5.5)
          Emit_Raise_Constraint_Error ("VALUE no match");
          Emit ("Lval_end%u:\n", end_label);
          Emit ("  %%t%u = load %s, ptr %%t%u\n", t, val_iat, result_alloc);

          // Convert from i32 result to prefix type width so codegen
          // matches Expression_Llvm_Type prediction
          const char *result_t = Type_To_Llvm (prefix_type);
          if (result_t and result_t[0] == 'i')
            t = Emit_Convert (t, val_iat, result_t);

        // No literals found, fallback to integer value
        } else {
          const char *fb_iat = Integer_Arith_Type ();
          Emit ("  %%t%u = call %s @__ada_integer_value(" FAT_PTR_TYPE " %%t%u)\n",
             t, fb_iat, str_val);
          Temp_Set_Type (t, fb_iat);
          const char *fb_result_t = Type_To_Llvm (prefix_type);
          return Emit_Convert (t, fb_iat, fb_result_t);
        }

      // Default: treat as integer
      } else {
        const char *def_iat = Integer_Arith_Type ();
        Emit ("  %%t%u = call %s @__ada_integer_value(" FAT_PTR_TYPE " %%t%u)\n",
           t, def_iat, str_val);
        Temp_Set_Type (t, def_iat);
        const char *def_result_t = Type_To_Llvm (prefix_type);
        return Emit_Convert (t, def_iat, def_result_t);
      }
      return t;
    }
    return 0;
  }

  // T'WIDTH - maximum image width for type (RM 3.5.5)                                              
  // Per GNAT exp_imgv.adb Expand_Width_Attribute:                                                  
  // - For null range (FIRST > LAST), WIDTH is 0                                                    
  // - For enumeration: max length of literal names in range                                        
  // - For integer: max width of first/last images                                                  
  // - For boolean: max("FALSE", "TRUE") = 5                                                        
  // - For character: 3 ('X')                                                                       
  //                                                                                                
  // When bounds are not compile-time known, generate runtime code.                                 
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("WIDTH"))) {
    if (prefix_type) {
      bool lo_known = Type_Bound_Is_Compile_Time_Known (prefix_type->low_bound);
      bool hi_known = Type_Bound_Is_Compile_Time_Known (prefix_type->high_bound);

      // Find root enumeration type (traversing base_type and parent_type chains)
      Type_Info *root_enum = NULL;
      for (Type_Info *ti = prefix_type; ti; ti = ti->base_type ? ti->base_type : ti->parent_type) {
        if (Type_Is_Enumeration (ti) and ti->enumeration.literals) {
          root_enum = ti;
          break;
        }
        if (not ti->base_type and not ti->parent_type) break;
      }

      // Both bounds are compile-time known - compute WIDTH statically
      if (lo_known and hi_known) {
        int128_t width = 0;
        int128_t lo = Type_Bound_Value (prefix_type->low_bound);
        int128_t hi = Type_Bound_Value (prefix_type->high_bound);

        // Empty range - width is 0
        if (hi < lo) {
          width = 0;

        // Enumeration: max length of literal names in range
        } else if (root_enum) {
          for (int128_t i = lo; i <= hi and i < (int128_t)root_enum->enumeration.literal_count; i++) {
            if (i >= 0) {
              uint32_t len = root_enum->enumeration.literals[(int64_t)i].length;
              if (len > (uint32_t)width) width = (int128_t)len;
            }
          }

        // Boolean: "FALSE" is 5, "TRUE" is 4
        } else if (Type_Is_Boolean (prefix_type) or Type_Is_Boolean (prefix_type->base_type)) {
          width = (lo <= 0 and hi >= 0) ? 5 : (lo <= 1 and hi >= 1) ? 4 : 0;

        // Character: 'X' is 3 chars
        } else if (Type_Is_Character (prefix_type) or Type_Is_Character (prefix_type->base_type)) {
          width = 3;

        // Integer types: max width of first/last images
        } else {

          // Width includes leading space for non-negative
          int128_t abs_lo = lo < 0 ? -lo : lo;
          int128_t abs_hi = hi < 0 ? -hi : hi;
          int128_t max_abs = abs_lo > abs_hi ? abs_lo : abs_hi;
          int digits = 1;
          while (max_abs >= 10) { digits++; max_abs /= 10; }
          width = digits + 1;  // +1 for leading space or minus sign
        }
        Emit ("  %%t%u = add %s 0, %s  ; 'WIDTH\n", t, Integer_Arith_Type (), I128_Decimal (width));

      // Bounds not compile-time known - generate runtime code.                                     
      // Per GNAT exp_imgv.adb: generate if FIRST > LAST then 0 else <width>                        
      // For enumeration types with runtime bounds, we compute the full-range                       
      // width at compile time (since literals are known) and use 0 for null range.                 
      //                                                                                            
      } else {
        uint32_t lo_val = Generate_Bound_Value (prefix_type->low_bound, Integer_Arith_Type ());
        uint32_t hi_val = Generate_Bound_Value (prefix_type->high_bound, Integer_Arith_Type ());

        // Compare: is_null = (lo > hi)
        const char *width_type = Integer_Arith_Type ();
        uint32_t cmp = cg->temp_id++;
        Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u  ; FIRST > LAST?\n", cmp, width_type, lo_val, hi_val);

        // Compute the width for non-null range at compile time
        int64_t full_width = 0;

        // For enumeration: compute max width over full base type range
        if (root_enum) {
          for (uint32_t i = 0; i < root_enum->enumeration.literal_count; i++) {
            uint32_t len = root_enum->enumeration.literals[i].length;
            if (len > (uint32_t)full_width) full_width = (int64_t)len;
          }

        // Boolean: WIDTH depends on which values are in range.                                     
        // FALSE=0 has width 5, TRUE=1 has width 4.                                                 
        // If lo <= 0 (FALSE is in range): width = 5                                                
        // Otherwise (only TRUE in range): width = 4                                                
        // Generate: select (lo <= 0), 5, 4                                                         
        //                                                                                          
        } else if (Type_Is_Boolean (prefix_type) or Type_Is_Boolean (prefix_type->base_type)) {
          uint32_t has_false = cg->temp_id++;
          Emit ("  %%t%u = icmp sle %s %%t%u, 0  ; has FALSE?\n", has_false, width_type, lo_val);
          uint32_t bool_width = cg->temp_id++;
          Emit ("  %%t%u = select i1 %%t%u, %s 5, %s 4  ; FALSE=5, TRUE=4\n", bool_width, has_false, width_type, width_type);

          // Select: if is_null then 0 else bool_width
          Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u  ; 'WIDTH (runtime bool)\n",
             t, cmp, width_type, width_type, bool_width);
          return t;
        } else if (Type_Is_Character (prefix_type) or Type_Is_Character (prefix_type->base_type)) {
          full_width = 3;  // 'X'

        // Integer: compute WIDTH based on actual runtime bounds.                                   
        // WIDTH = max(digits(abs(lo)), digits(abs(hi))) + 1                                        
        // Generate runtime code to compute this.                                                   
        //                                                                                          
        } else {

          // Compute abs(lo): if lo < 0 then -lo else lo
          uint32_t neg_lo = cg->temp_id++;
          Emit ("  %%t%u = sub %s 0, %%t%u\n", neg_lo, width_type, lo_val);
          uint32_t is_neg_lo = cg->temp_id++;
          Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", is_neg_lo, width_type, lo_val);
          uint32_t abs_lo = cg->temp_id++;
          Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n", abs_lo, is_neg_lo, width_type, neg_lo, width_type, lo_val);

          // Compute abs(hi): if hi < 0 then -hi else hi
          uint32_t neg_hi = cg->temp_id++;
          Emit ("  %%t%u = sub %s 0, %%t%u\n", neg_hi, width_type, hi_val);
          uint32_t is_neg_hi = cg->temp_id++;
          Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", is_neg_hi, width_type, hi_val);
          uint32_t abs_hi = cg->temp_id++;
          Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n", abs_hi, is_neg_hi, width_type, neg_hi, width_type, hi_val);

          // max_abs = max(abs_lo, abs_hi)
          uint32_t cmp_abs = cg->temp_id++;
          Emit ("  %%t%u = icmp ugt %s %%t%u, %%t%u\n", cmp_abs, width_type, abs_lo, abs_hi);
          uint32_t max_abs = cg->temp_id++;
          Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n", max_abs, cmp_abs, width_type, abs_lo, width_type, abs_hi);

          // Count digits using comparison chain.                                                   
          // Derive max digits from type width (works for i8..i128):                                
          //   i8: 2, i16: 4, i32: 9, i64: 18, i128: 38                                             
          //                                                                                        
          int width_bits = Type_Bits (width_type);
          int max_digits = (width_bits <= 8) ? 2 : (width_bits <= 16) ? 4 :
                   (width_bits <= 32) ? 9 : (width_bits <= 64) ? 18 : 38;
          uint32_t digits_val = cg->temp_id++;
          Emit ("  %%t%u = add %s 0, 1  ; initial digits\n", digits_val, width_type);
          int64_t thresholds[] = {10, 100, 1000, 10000, 100000, 1000000, 10000000,
                       100000000, 1000000000, 10000000000LL, 100000000000LL,
                       1000000000000LL, 10000000000000LL, 100000000000000LL,
                       1000000000000000LL, 10000000000000000LL,
                       100000000000000000LL, 1000000000000000000LL};
          for (int d = 0; d < max_digits; d++) {
            uint32_t cmp_d = cg->temp_id++;
            Emit ("  %%t%u = icmp uge %s %%t%u, %lld\n", cmp_d, width_type, max_abs, (long long)thresholds[d]);
            uint32_t next_digits = cg->temp_id++;
            Emit ("  %%t%u = select i1 %%t%u, %s %d, %s %%t%u\n",
               next_digits, cmp_d, width_type, d + 2, width_type, digits_val);
            digits_val = next_digits;
          }

          // width = digits + 1 (for leading space or minus sign)
          uint32_t width_val = cg->temp_id++;
          Emit ("  %%t%u = add %s %%t%u, 1  ; +1 for sign/space\n", width_val, width_type, digits_val);
          full_width = 0;  // Not used - we have runtime value

          // Select: if is_null then 0 else width_val
          Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u  ; 'WIDTH (runtime)\n",
             t, cmp, width_type, width_type, width_val);
          return t;
        }

        // Select: if is_null then 0 else full_width
        Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %lld  ; 'WIDTH (runtime)\n",
           t, cmp, width_type, width_type, (long long)full_width);
      }
      return t;
    }
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Access Type Attributes                                                                         
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // X'ACCESS / X'UNCHECKED_ACCESS - identical codegen (RM 3.10.2)
  if (Slice_Equal_Ignore_Case (attr, S("ACCESS")) or
    Slice_Equal_Ignore_Case (attr, S("UNCHECKED_ACCESS"))) {
    Symbol *sym = node->attribute.prefix->symbol;
    if (sym) {
      Emit ("  %%t%u = getelementptr i8, ptr ", t);
      Emit_Symbol_Ref (sym);
      Emit (", i64 0  ; '%.*s\n", (int)attr.length, attr.data);
    } else {
      fprintf (stderr, "warning: 'ACCESS attribute applied to expression with no symbol\n");
      Emit ("  %%t%u = add %s 0, 0\n", t, Integer_Arith_Type ());
    }
    return t;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Floating-Point Type Attributes (RM 3.5.8)                                                      
  // These attributes return compile-time values for floating-point types.                          
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // T'DIGITS - number of significant decimal digits (RM 3.5.7)
  if (Slice_Equal_Ignore_Case (attr, S("DIGITS"))) {
    int64_t digits = Type_Is_Float (prefix_type)
      ? Float_Effective_Digits (prefix_type)
      : IEEE_DOUBLE_DIGITS;
    Emit ("  %%t%u = add %s 0, %lld  ; 'DIGITS\n", t, Integer_Arith_Type (), (long long)digits);
    return t;
  }

  // T'MANTISSA - number of binary digits (RM 3.5.8, 3.5.10)                                        
  // For floating-point: ceiling(D * log(10)/log(2)) + 1                                            
  // For fixed-point: ceiling(log2 (bound / small))                                                 
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("MANTISSA"))) {
    int64_t mantissa = IEEE_DOUBLE_MANTISSA - 1;  // Default for double
    if (Type_Is_Float (classify_type)) {
      Float_Model_Parameters (classify_type, &mantissa, NULL);
    } else if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = Type_Is_Fixed_Point (prefix_type) ? prefix_type : classify_type;
      double small = ft->fixed.small;
      double low_val = Type_Bound_Float_Value (ft->low_bound);
      double high_val = Type_Bound_Float_Value (ft->high_bound);
      if (small <= 0) small = ft->fixed.delta > 0 ? ft->fixed.delta : 1.0;
      double bound = fmax (fabs (low_val), fabs (high_val));
      if (bound > 0 and small > 0) {
        mantissa = (int64_t)ceil (log2 (bound / small));
        if (mantissa < 1) mantissa = 1;
      }
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'MANTISSA\n", t, Integer_Arith_Type (), (long long)mantissa);
    return t;
  }

  // T'EMAX - maximum binary exponent (RM 3.5.8)
  // For model numbers: EMAX = 4 * MANTISSA
  if (Slice_Equal_Ignore_Case (attr, S("EMAX"))) {
    int64_t emax = IEEE_DOUBLE_EMAX - 1;  // Default for double
    if (Type_Is_Float (prefix_type)) {
      Float_Model_Parameters (prefix_type, NULL, &emax);
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'EMAX\n", t, Integer_Arith_Type (), (long long)emax);
    return t;
  }

  // T'SAFE_EMAX - safe maximum exponent (RM 3.5.8)
  // For IEEE: MACHINE_EMAX - 1
  if (Slice_Equal_Ignore_Case (attr, S("SAFE_EMAX"))) {
    bool single = Type_Is_Float (prefix_type) and Float_Is_Single (prefix_type);
    int64_t safe_emax = single ? (IEEE_FLOAT_EMAX - 1) : (IEEE_DOUBLE_EMAX - 1);
    Emit ("  %%t%u = add %s 0, %lld  ; 'SAFE_EMAX\n", t, Integer_Arith_Type (), (long long)safe_emax);
    return t;
  }

  // T'EPSILON - model epsilon (RM 3.5.8(9))
  // EPSILON = 2^(1 - MANTISSA)
  if (Slice_Equal_Ignore_Case (attr, S("EPSILON"))) {
    int64_t mantissa = IEEE_DOUBLE_MANTISSA - 1;
    if (Type_Is_Float (prefix_type))
      Float_Model_Parameters (prefix_type, &mantissa, NULL);
    double epsilon = pow (2.0, 1 - mantissa);

    // generate at double (UNIVERSAL_REAL) - callers convert.
    // This matches Expression_Llvm_Type (UNIVERSAL_REAL) = "double".
    Emit_Float_Constant (t, "double", epsilon, "'EPSILON");
    return t;
  }

  // T'SMALL - fixed-point: power of 2 <= delta; float: 2^(-EMAX - 1)
  if (Slice_Equal_Ignore_Case (attr, S("SMALL"))) {
    double small_val;

    // Use classify_type which is resolved to actual type in generics
    if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = classify_type;
      small_val = ft->fixed.small;
      if (small_val <= 0) small_val = ft->fixed.delta;
      if (small_val <= 0) small_val = 1.0;
    } else if (Type_Is_Float (prefix_type)) {
      int64_t mantissa, emax;
      Float_Model_Parameters (prefix_type, &mantissa, &emax);
      small_val = pow (2.0, -(emax + 1));
    } else {
      small_val = pow (2.0, -(IEEE_DOUBLE_EMIN));  // 2^-1022
    }

    // generate at double (UNIVERSAL_REAL) - callers convert.
    Emit_Float_Constant (t, "double", small_val, "'SMALL");
    return t;
  }

  // T'LARGE - fixed-point: (2^MANTISSA - 1) * SMALL (RM 3.5.10)
  // float: 2^EMAX * (1 - 2^(-MANTISSA)) (RM 3.5.8(10))
  if (Slice_Equal_Ignore_Case (attr, S("LARGE"))) {
    double large_val;

    // Use classify_type which is resolved to actual type in generics
    if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = classify_type;
      double small = ft->fixed.small;
      if (small <= 0) small = ft->fixed.delta > 0 ? ft->fixed.delta : 1.0;
      double bound = fmax (fabs (Type_Bound_Float_Value (ft->low_bound)),
                 fabs (Type_Bound_Float_Value (ft->high_bound)));
      int64_t mantissa = (bound > 0 and small > 0) ?
                (int64_t)ceil (log2 (bound / small)) : 1;
      if (mantissa < 1) mantissa = 1;
      large_val = ((double)((1LL << mantissa) - 1)) * small;
    } else if (Type_Is_Float (prefix_type)) {
      int64_t mantissa, emax;
      Float_Model_Parameters (prefix_type, &mantissa, &emax);
      large_val = pow (2.0, emax) * (1.0 - pow (2.0, -mantissa));
    } else {
      large_val = __DBL_MAX__;
    }

    // generate at double (UNIVERSAL_REAL) - callers convert.
    Emit_Float_Constant (t, "double", large_val, "'LARGE");
    return t;
  }

  // T'SAFE_SMALL - smallest positive safe value                                                    
  // Fixed-point (RM 3.5.10): SAFE_SMALL = BASE'SMALL                                               
  // Float (RM 3.5.8): 2^(-(SAFE_EMAX+1))                                                           
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("SAFE_SMALL"))) {
    double safe_small;

    // Use classify_type which is resolved to actual type in generics
    if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = classify_type;
      Type_Info *base = ft->base_type ? ft->base_type : ft;
      safe_small = base->fixed.small;
      if (safe_small <= 0) safe_small = base->fixed.delta > 0 ? base->fixed.delta : 1.0;
    } else if (Type_Is_Float (prefix_type) and Float_Is_Single (prefix_type)) {
      safe_small = IEEE_FLOAT_MIN_NORMAL;
    } else {
      safe_small = IEEE_DOUBLE_MIN_NORMAL;
    }

    // generate at double (UNIVERSAL_REAL) - callers convert.
    Emit_Float_Constant (t, "double", safe_small, "'SAFE_SMALL");
    return t;
  }

  // T'SAFE_LARGE - largest safe value                                                              
  // Fixed-point (RM 3.5.10): SAFE_LARGE = BASE'LARGE = (2^B_MANT - 1) * BASE'SMALL                 
  // Float (RM 3.5.8): 2^(SAFE_EMAX) * (1 - 2^(-MANTISSA))                                          
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("SAFE_LARGE"))) {
    double safe_large;

    // Use classify_type which is resolved to actual type in generics
    if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = classify_type;
      Type_Info *base = ft->base_type ? ft->base_type : ft;
      double small = base->fixed.small;
      if (small <= 0) small = base->fixed.delta > 0 ? base->fixed.delta : 1.0;
      double low_val = Type_Bound_Float_Value (base->low_bound);
      double high_val = Type_Bound_Float_Value (base->high_bound);
      double bound = fmax (fabs (low_val), fabs (high_val));
      int64_t mant = (bound > 0 and small > 0) ?
              (int64_t)ceil (log2 (bound / small)) : 1;
      if (mant < 1) mant = 1;
      safe_large = ((double)((1LL << mant) - 1)) * small;
    } else {
      int emax = IEEE_DOUBLE_EMAX;
      int mantissa = IEEE_DOUBLE_MANTISSA;
      if (Type_Is_Float (prefix_type) and Float_Is_Single (prefix_type)) {
        emax = IEEE_FLOAT_EMAX;
        mantissa = IEEE_FLOAT_MANTISSA;
      }
      safe_large = pow (2.0, emax - 1) * (1.0 - pow (2.0, -mantissa));
    }

    // generate at double (UNIVERSAL_REAL) - callers convert.
    Emit_Float_Constant (t, "double", safe_large, "'SAFE_LARGE");
    return t;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Fixed-Point Type Attributes (RM 3.5.9)                                                         
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // T'DELTA - delta for fixed-point type (universal real)
  if (Slice_Equal_Ignore_Case (attr, S("DELTA"))) {
    double delta = 1.0;
    if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = Type_Is_Fixed_Point (prefix_type) ? prefix_type : classify_type;
      delta = ft->fixed.delta > 0 ? ft->fixed.delta : 1.0;
    }
    Emit ("  %%t%u = fadd double 0.0, %e  ; 'DELTA\n", t, delta);
    return t;
  }

  // T'FORE - minimum field width for integer part (RM 3.5.10(5))                                   
  // Includes a one-character prefix (minus sign or space).                                         
  // FORE = 2 when integer part is 0, otherwise 1 + 1 + floor (log10 (int_part))                    
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("FORE"))) {
    int64_t fore = 2;  // minimum: sign + at least one digit
    if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = Type_Is_Fixed_Point (prefix_type) ? prefix_type : classify_type;
      double bound = fmax (fabs (Type_Bound_Float_Value (ft->low_bound)),
                 fabs (Type_Bound_Float_Value (ft->high_bound)));
      if (bound >= 1.0) {
        fore = 2 + (int64_t)floor (log10 (bound));
      }
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'FORE\n", t, Integer_Arith_Type (), (long long)fore);
    return t;
  }

  // T'AFT - decimal digits after point in default output (RM 3.5.10)
  // AFT = smallest N such that 10^(-N) <= T'DELTA
  if (Slice_Equal_Ignore_Case (attr, S("AFT"))) {
    int64_t aft = 1;
    if (Type_Is_Fixed_Point (classify_type)) {
      Type_Info *ft = Type_Is_Fixed_Point (prefix_type) ? prefix_type : classify_type;
      double delta = ft->fixed.delta;
      if (delta > 0 and delta < 1.0) {
        aft = (int64_t)ceil (-log10 (delta));
      }
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'AFT\n", t, Integer_Arith_Type (), (long long)aft);
    return t;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Floating-Point Boolean Attributes (RM 3.5.8)                                                   
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // T'MACHINE_ROUNDS - does the hardware round? (RM 3.5.8)                                         
  // IEEE 754 hardware rounds, so return TRUE.                                                      
  // Boolean type in Standard is i8.                                                                
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("MACHINE_ROUNDS"))) {
    Emit ("  %%t%u = add i8 0, 1  ; 'MACHINE_ROUNDS (IEEE rounds)\n", t);
    Temp_Set_Type (t, "i8");
    return t;
  }

  // T'MACHINE_OVERFLOWS - does the hardware raise on overflow? (RM 3.5.8)                          
  // IEEE 754 generates infinity on overflow (doesn't trap), return FALSE.                          
  // Boolean type in Standard is i8.                                                                
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("MACHINE_OVERFLOWS"))) {
    Emit ("  %%t%u = add i8 0, 0  ; 'MACHINE_OVERFLOWS (IEEE no trap)\n", t);
    Temp_Set_Type (t, "i8");
    return t;
  }

  // T'MACHINE_RADIX - hardware floating-point radix (RM 3.5.8)
  // IEEE 754 uses radix 2
  if (Slice_Equal_Ignore_Case (attr, S("MACHINE_RADIX"))) {
    Emit ("  %%t%u = add %s 0, %d  ; 'MACHINE_RADIX (IEEE binary)\n", t, Integer_Arith_Type (), IEEE_MACHINE_RADIX);
    return t;
  }

  // T'MACHINE_MANTISSA - hardware mantissa bits (RM 3.5.8)
  // IEEE 754 double: 53 bits, float: 24 bits
  if (Slice_Equal_Ignore_Case (attr, S("MACHINE_MANTISSA"))) {
    int64_t machine_mantissa = IEEE_DOUBLE_MANTISSA;
    if (Type_Is_Float (prefix_type) and Float_Is_Single (prefix_type)) {
      machine_mantissa = IEEE_FLOAT_MANTISSA;
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'MACHINE_MANTISSA\n", t, Integer_Arith_Type (), (long long)machine_mantissa);
    return t;
  }

  // T'MACHINE_EMAX - hardware max exponent (RM 3.5.8)
  // IEEE 754 double: 1024, float: 128
  if (Slice_Equal_Ignore_Case (attr, S("MACHINE_EMAX"))) {
    int64_t machine_emax = IEEE_DOUBLE_EMAX;
    if (Type_Is_Float (prefix_type) and Float_Is_Single (prefix_type)) {
      machine_emax = IEEE_FLOAT_EMAX;
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'MACHINE_EMAX\n", t, Integer_Arith_Type (), (long long)machine_emax);
    return t;
  }

  // T'MACHINE_EMIN - hardware min exponent (RM 3.5.8)
  // IEEE 754 double: -1021, float: -125
  if (Slice_Equal_Ignore_Case (attr, S("MACHINE_EMIN"))) {
    int64_t machine_emin = IEEE_DOUBLE_EMIN;
    if (Type_Is_Float (prefix_type) and Float_Is_Single (prefix_type)) {
      machine_emin = IEEE_FLOAT_EMIN;
    }
    Emit ("  %%t%u = add %s 0, %lld  ; 'MACHINE_EMIN\n", t, Integer_Arith_Type (), (long long)machine_emin);
    return t;
  }

  // ───────────────────────────────────────────────────────────────────────────────────────────────
  // Object Attributes (RM 3.7.1, 9.9)                                                              
  // ───────────────────────────────────────────────────────────────────────────────────────────────

  // X'CONSTRAINED - is the object constrained? (RM 3.7.1)                                          
  // Returns TRUE if:                                                                               
  //   - Object type has no discriminants (always constrained)                                      
  //   - Object was declared with explicit discriminant constraint                                  
  //   - Object type has no default discriminant values (immutable)                                 
  // Returns FALSE if:                                                                              
  //   - Object type has discriminants with defaults and no explicit constraint                     
  // Use i64 for consistency with Boolean storage/comparison.                                       
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("CONSTRAINED"))) {
    Symbol *obj_sym = node->attribute.prefix ? node->attribute.prefix->symbol : NULL;
    Type_Info *obj_type = node->attribute.prefix ? node->attribute.prefix->type : NULL;
    bool is_constrained = true;  // Default: constrained
    if (obj_type and Type_Is_Record (obj_type) and obj_type->record.has_discriminants) {
      if (obj_type->record.is_constrained) {
        is_constrained = true;  // Explicitly constrained subtype
      } else if (obj_sym and obj_sym->is_disc_constrained) {
        is_constrained = true;  // Object declared with constraint
      } else if (obj_type->record.all_defaults) {
        is_constrained = false;  // Mutable: defaults, no constraint
      }
    }

    // Boolean-valued attributes produce i8 (Boolean storage type)
    Emit ("  %%t%u = add i8 0, %d  ; 'CONSTRAINED\n", t, is_constrained ? 1 : 0);
    return t;
  }

  // T'CALLABLE (RM 9.9): load TCB handle, check completed flag
  if (Slice_Equal_Ignore_Case (attr, S("CALLABLE"))) {
    uint32_t handle = Generate_Expression (node->attribute.prefix);
    Emit ("  %%t%u = call i8 @__ada_task_callable(ptr %%t%u)\n", t, handle);
    return t;
  }

  // T'TERMINATED (RM 9.9): load TCB handle, check completed flag
  if (Slice_Equal_Ignore_Case (attr, S("TERMINATED"))) {
    uint32_t handle = Generate_Expression (node->attribute.prefix);
    uint32_t tcb_ptr = Emit_Temp ();
    Emit ("  %%t%u = icmp eq ptr %%t%u, null\n", tcb_ptr, handle);
    uint32_t ok_l = cg->label_id++, nil_l = cg->label_id++, done_l = cg->label_id++;
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", tcb_ptr, nil_l, ok_l);
    cg->block_terminated = true;
    Emit_Label_Here (nil_l);
    Emit ("  br label %%L%u\n", done_l);
    Emit_Label_Here (ok_l);
    uint32_t gep = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 24\n", gep, handle);
    uint32_t val = Emit_Temp ();
    Emit ("  %%t%u = load i8, ptr %%t%u\n", val, gep);
    Emit ("  br label %%L%u\n", done_l);
    cg->block_terminated = true;
    Emit_Label_Here (done_l);
    Emit ("  %%t%u = phi i8 [ 0, %%L%u ], [ %%t%u, %%L%u ]\n", t, nil_l, val, ok_l);
    return t;
  }

  // T'STORAGE_SIZE (RM 13.7.1) - return user-specified value if set,                               
  // otherwise return a reasonable implementation-defined default.                                  
  // For derived access types, walk parent chain to find the clause.                                
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("STORAGE_SIZE"))) {
    int64_t ss = 0;
    Type_Info *st = prefix_type;
    while (st and st->storage_size == 0 and st->parent_type)
      st = st->parent_type;
    if (st and st->storage_size != 0)
      ss = st->storage_size;
    else if (prefix_type)
      ss = (int64_t)prefix_type->size * 8;
    Emit ("  %%t%u = add %s 0, %lld  ; 'STORAGE_SIZE\n", t, Integer_Arith_Type (),
       (long long)ss);
    return t;
  }

  // Record component representation attributes (RM 13.7.2):                                        
  // X.C'FIRST_BIT, X.C'LAST_BIT, X.C'POSITION                                                      
  // Prefix must be a selected component of a record.                                               
  //                                                                                                
  if (Slice_Equal_Ignore_Case (attr, S("FIRST_BIT")) or
    Slice_Equal_Ignore_Case (attr, S("LAST_BIT")) or
    Slice_Equal_Ignore_Case (attr, S("POSITION"))) {
    Syntax_Node *prefix = node->attribute.prefix;

    // Determine the record type from the prefix of the selected component
    if (prefix and prefix->kind == NK_SELECTED) {
      Type_Info *rec_type = prefix->selected.prefix ? prefix->selected.prefix->type : NULL;
      if (Type_Is_Access (rec_type) and rec_type->access.designated_type)
        rec_type = rec_type->access.designated_type;
      while (rec_type and not Type_Is_Record (rec_type) and rec_type->parent_type)
        rec_type = rec_type->parent_type;
      String_Slice comp_name = prefix->selected.selector;
      int64_t result = 0;
      if (rec_type and Type_Is_Record (rec_type)) {
        for (uint32_t i = 0; i < rec_type->record.component_count; i++) {
          if (Slice_Equal_Ignore_Case (rec_type->record.components[i].name, comp_name)) {
            Component_Info *ci = &rec_type->record.components[i];
            if (Slice_Equal_Ignore_Case (attr, S("FIRST_BIT")))
              result = ci->bit_offset;
            else if (Slice_Equal_Ignore_Case (attr, S("LAST_BIT")))
              result = ci->bit_offset + (ci->bit_size > 0 ? ci->bit_size : ci->component_type ? ci->component_type->size * 8 : 0) - 1;
            else  // POSITION
              result = ci->byte_offset;
            break;
          }
        }
      }
      Emit ("  %%t%u = add %s 0, %lld  ; '%.*s\n", t, Integer_Arith_Type (),
         (long long)result, (int)attr.length, attr.data);
      return t;
    }
  }

  // Unhandled attribute
  fprintf (stderr, "warning: unhandled attribute '%.*s'\n",
      (int)attr.length, attr.data);
  Emit ("  %%t%u = add %s 0, 0  ; unhandled '%.*s\n", t, Integer_Arith_Type (),
     (int)attr.length, attr.data);
  return t;
}

// Helper: Find component index by name in record type
int32_t Find_Record_Component (Type_Info *record_type, String_Slice name) {
  if (not Type_Is_Record (record_type)) return -1;
  for (uint32_t i = 0; i < record_type->record.component_count; i++) {
    if (Slice_Equal_Ignore_Case (record_type->record.components[i].name, name)) {
      return (int32_t)i;
    }
  }
  return -1;
}

// Check if a choice is "others"
bool Is_Others_Choice (Syntax_Node *choice) {
  return choice and (choice->kind == NK_OTHERS or
       (choice->kind == NK_IDENTIFIER and
      Slice_Equal_Ignore_Case (choice->string_val.text, S("others"))));
}

// ── Is_Static_Int_Node / Static_Int_Value ────────────────────────────────────────────────────────
// Recognise compile-time integer nodes so the aggregate codegen can                                
// take the fast static path.  Covers:                                                              
//   NK_INTEGER              - positive literal  (e.g.  3)                                          
//   NK_UNARY_OP (-, int)     - negated literal   (e.g. -1)                                         
//   NK_UNARY_OP (+, int)     - explicit positive (e.g. +1)                                         
//                                                                                                  
bool Is_Static_Int_Node (Syntax_Node *n) {
  if (not n) return false;
  if (n->kind == NK_INTEGER) return true;
  if (n->kind == NK_UNARY_OP and
    (n->unary.op == TK_MINUS or n->unary.op == TK_PLUS) and
    n->unary.operand and n->unary.operand->kind == NK_INTEGER)
    return true;
  return false;
}
int128_t Static_Int_Value (Syntax_Node *n) {
  if (n->kind == NK_INTEGER)
    return (int128_t)n->integer_lit.value;

  // NK_UNARY_OP
  int128_t v = (int128_t)n->unary.operand->integer_lit.value;
  return (n->unary.op == TK_MINUS) ? -v : v;
}

// ── § 13a: Array Aggregate Helpers (RM 4.3.2) ────────────────────────────────────────────────────
//                                                                                                  
// These helpers factor out the repeated patterns in Generate_Aggregate:                            
//   Agg_Classify       - count positional/named/others items                                       
//   Agg_Resolve_Elem   - generate element value, extract from fat ptr                              
//   Agg_Store_At       - store element at array index (scalar or composite)                        
//   Agg_Emit_Fill_Loop - emit a loop that fills a range with a value                               
//   Agg_Wrap_Fat_Ptr   - wrap data+bounds into { ptr, ptr }                                        
//                                                                                                  
// Design: each helper is a pure function of its arguments - no hidden                              
// state, no implicit coupling.  Haskell-flavoured C99 with `and`/`or`.                             
//                                                                                                  

// ── Agg_Classify: classify aggregate items into positional / named / others ──────────────────────

Agg_Class Agg_Classify (Syntax_Node *node) {
  Agg_Class r = {0, false, false, NULL};
  for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
    Syntax_Node *item = node->aggregate.items.items[i];
    if (item->kind == NK_ASSOCIATION) {
      r.has_named = true;
      if (item->association.choices.count > 0 and
        Is_Others_Choice (item->association.choices.items[0])) {
        r.has_others   = true;
        r.others_expr  = item->association.expression;
      }
    } else {
      r.n_positional++;
    }
  }
  return r;
}

// Check if a pair of static integer bounds spans an unreasonably large range                       
// (e.g. full index subtype: INTEGER -2^31..2^31-1) meaning the type checker                        
// couldn't determine narrower constraints.  Such "placeholder" bounds overflow                     
// uint32 size calculations and must be replaced with actual choice bounds.                         
//                                                                                                  
bool Bound_Pair_Overflows (Type_Bound low, Type_Bound high) {
  if (low.kind == BOUND_INTEGER and high.kind == BOUND_INTEGER) {
    int128_t span = (int128_t)high.int_value - (int128_t)low.int_value;
    return span >= (int128_t)UINT32_MAX;
  }
  return false;
}

// Check whether an aggregate will produce a fat pointer (dynamic alloca path).                     
// True when the type has dynamic bounds OR the aggregate's own named choices                       
// contain non-static range expressions (e.g. desugared T'RANGE).                                   
// ── Agg_Resolve_Elem: generate element value and unwrap fat ptrs. ────────────────────────────────
//                                                                                                  
// Given an expression node, generates the value and, depending on the                              
// target context, extracts the data pointer from fat pointers or                                   
// converts the scalar type.  Returns the SSA temp ready for storage.                               
//                                                                                                  
//   multidim          - are we generating a multi-dimensional array?                               
//   elem_is_composite - is the element type composite (record/array/row)?                          
//   agg_type          - the enclosing array's Type_Info                                            
//   elem_type         - LLVM type string for scalar elements (e.g. "i32")                          
//   elem_ti           - Type_Info* for the element subtype (for checks)                            
//                                                                                                  
uint32_t Agg_Resolve_Elem (Syntax_Node *expr,
  bool multidim, bool elem_is_composite, Type_Info *agg_type,
  const char *elem_type, Type_Info *elem_ti)
{
  cg->in_agg_component++;
  uint32_t val = Generate_Expression (expr);
  cg->in_agg_component--;

  // Fat pointer sources: extract data pointer for composite elements.                              
  // Three cases: (a) normal fat ptr, (b) dynamic inner aggregate fat ptr,                          
  // (c) non-fat-ptr inner aggregate.                                                               
  //                                                                                                
  if (elem_is_composite) {
    if (Expression_Produces_Fat_Pointer (expr, expr->type)) {
      val = Emit_Fat_Pointer_Data (val,
            Array_Bound_Llvm_Type (agg_type));
    } else if (multidim and expr->kind == NK_AGGREGATE and
           expr->type and
           (Type_Has_Dynamic_Bounds (expr->type) or

          // Inner sub-aggregate with full-range placeholder bounds:                                
          // its Generate_Aggregate will detect the overflow, override                              
          // bounds via early scan, and produce a fat pointer.                                      
          //                                                                                        
          (expr->type->kind == TYPE_ARRAY and
           expr->type->array.index_count > 0 and
           Bound_Pair_Overflows (
             expr->type->array.indices[0].low_bound,
             expr->type->array.indices[0].high_bound)))) {
      uint32_t loaded = Emit_Temp ();
      Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n",
         loaded, val);
      val = Emit_Temp ();
      Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE
         " %%t%u, 0\n", val, loaded);
    }

  // Fat-pointer element (dynamic-bound array/string): rebuild the                                  
  // fat pointer with the correct bounds from the element type's                                    
  // constraint, since the source (e.g. string literal) may have                                    
  // default bounds (1..N) instead of the target bounds (3..5).                                     
  //                                                                                                
  } else if (Type_Needs_Fat_Pointer_Load (elem_ti)) {
    const char *bt = Array_Bound_Llvm_Type (elem_ti);
    uint32_t data = Emit_Fat_Pointer_Data (val, bt);
    uint32_t lo = Emit_Bound_Value (&elem_ti->array.indices[0].low_bound);
    uint32_t hi = Emit_Bound_Value (&elem_ti->array.indices[0].high_bound);
    val = Emit_Fat_Pointer_Dynamic (data, lo, hi, bt);

  // Scalar: type-convert and apply fixed-point SMALL if needed.
  } else {
    const char *src_type = Expression_Llvm_Type (expr);
    if (elem_ti and elem_ti->kind == TYPE_FIXED and Is_Float_Type (src_type)) {
      double small = elem_ti->fixed.small;
      if (small <= 0) small = elem_ti->fixed.delta > 0
                  ? elem_ti->fixed.delta : 1.0;
      uint64_t sb; memcpy (&sb, &small, sizeof (sb));
      uint32_t st = Emit_Temp ();
      Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small\n",
         st, (unsigned long long)sb);
      uint32_t dv = Emit_Temp ();
      Emit ("  %%t%u = fdiv %s %%t%u, %%t%u  ; /small\n",
         dv, src_type, val, st);
      val = dv;
    }
    val = Emit_Convert (val, src_type, elem_type);
  }
  return val;
}

// ── Agg_Store_At: store element at a given array index. ──────────────────────────────────────────
//                                                                                                  
//   base      - SSA temp for the array's alloca                                                    
//   val       - SSA temp for the element value (scalar) or data ptr (composite)                    
//   idx       - flat zero-based index (compile-time constant)                                      
//   elem_type - LLVM type for scalar elements                                                      
//   elem_size - byte size of one element (for composite memcpy)                                    
//   is_composite - memcpy vs store?                                                                
//                                                                                                  
void Agg_Store_At_Static (uint32_t base, uint32_t val,
  int128_t idx, const char *elem_type, uint32_t elem_size, bool is_composite)
{
  uint32_t ptr = Emit_Temp ();
  if (is_composite) {
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %s\n",
       ptr, base, I128_Decimal (idx * (int128_t)elem_size));
    Emit ("  call void @llvm.memcpy.p0.p0.i64("
       "ptr %%t%u, ptr %%t%u, i64 %u, i1 false)\n",
       ptr, val, elem_size);
  } else {
    Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i64 %s\n",
       ptr, elem_type, base, I128_Decimal (idx));
    Emit ("  store %s %%t%u, ptr %%t%u\n", elem_type, val, ptr);
  }
}

// ── Agg_Store_At_Dynamic: store element at a runtime index. ──────────────────────────────────────
//                                                                                                  
//   arr_idx     - SSA temp: zero-based index (cur_idx - low)                                       
//   rt_row_size - SSA temp for runtime row size (0 → use elem_size)                                
//   idx_type    - LLVM type for index arithmetic (e.g. "i32")                                      
//                                                                                                  
void Agg_Store_At_Dynamic (uint32_t base,
  uint32_t val, uint32_t arr_idx, const char *idx_type,
  const char *elem_type, uint32_t elem_size, uint32_t rt_row_size,
  bool is_composite)
{
  if (is_composite) {
    uint32_t byte_off = Emit_Temp ();
    if (rt_row_size)
      Emit ("  %%t%u = mul %s %%t%u, %%t%u\n",
         byte_off, idx_type, arr_idx, rt_row_size);
    else
      Emit ("  %%t%u = mul %s %%t%u, %u\n",
         byte_off, idx_type, arr_idx, elem_size);
    uint32_t ptr = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
       ptr, base, idx_type, byte_off);
    if (rt_row_size) {
      uint32_t sz64 = Emit_Temp ();
      Emit ("  %%t%u = sext i32 %%t%u to i64\n", sz64, rt_row_size);
      Emit ("  call void @llvm.memcpy.p0.p0.i64("
         "ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
         ptr, val, sz64);
    } else {
      Emit ("  call void @llvm.memcpy.p0.p0.i64("
         "ptr %%t%u, ptr %%t%u, i64 %u, i1 false)\n",
         ptr, val, elem_size);
    }
  } else {
    uint32_t ptr = Emit_Temp ();
    Emit ("  %%t%u = getelementptr %s, ptr %%t%u, %s %%t%u\n",
       ptr, elem_type, base, idx_type, arr_idx);
    Emit ("  store %s %%t%u, ptr %%t%u\n", elem_type, val, ptr);
  }
}

// ── Agg_Elem_Is_Composite: check if array element needs memcpy vs store. ─────────────────────────
// Multi-dimensional arrays are always composite at the outer level.
bool Agg_Elem_Is_Composite (Type_Info *elem_ti, bool multidim) {
  return multidim or (elem_ti and (Type_Is_Record (elem_ti) or
                   Type_Is_Constrained_Array (elem_ti)));
}

// RM 3.7.1: After memcpy-ing a composite value into a record component,
// verify that the stored discriminant(s) match the component type's
// discriminant constraint(s).  E.g.  (H => INIT (1)) where H : PRIV (0).
//
void Emit_Comp_Disc_Check (uint32_t ptr,
                  Type_Info *comp_ti) {
  if (not comp_ti or not Type_Is_Record (comp_ti) or
    not comp_ti->record.has_disc_constraints or
    not comp_ti->record.disc_constraint_values) return;
  for (uint32_t di = 0; di < comp_ti->record.discriminant_count; di++) {
    Component_Info *dc = &comp_ti->record.components[di];
    const char *dt = Type_To_Llvm (dc->component_type);
    uint32_t dp = Emit_Temp ();
    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
       dp, ptr, dc->byte_offset);
    uint32_t dv = Emit_Temp ();
    Emit ("  %%t%u = load %s, ptr %%t%u\n", dv, dt, dp);
    uint32_t exp_v = Emit_Disc_Constraint_Value (comp_ti, di, dt);
    if (exp_v == 0)
      exp_v = Emit_Static_Int (0, dt);
    uint32_t ne = Emit_Temp ();
    Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", ne, dt, dv, exp_v);
    uint32_t ok_l = cg->label_id++;
    uint32_t fl_l = cg->label_id++;
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", ne, fl_l, ok_l);
    cg->block_terminated = true;
    Emit_Label_Here (fl_l);
    Emit_Raise_Constraint_Error ("component disc value vs constraint");
    Emit_Label_Here (ok_l);
  }
}

// RM 4.3.2(6) helper: emit inner sub-aggregate bounds consistency tracking.                        
// Called after processing each element of a multidim aggregate.  Reads the                         
// child sub-aggregate's reported bounds (cg->inner_agg_bnd_lo/hi[0..n-1])                          
// and compares them against first-seen expected values.  Uses a single                             
// first-seen flag and single mismatch flag shared across all dimensions.                           
//                                                                                                  
//  inner_trk_lo/hi: arrays of alloca SSA ids, one per tracked dimension                            
//  inner_trk_first: alloca i1 for first-seen flag                                                  
//  inner_trk_mm:    alloca i1 for mismatch accumulator                                             
//  n_inner_dims:    max number of dimension levels to track                                        
//                                                                                                  
void Emit_Inner_Consistency_Track (
  uint32_t *inner_trk_lo, uint32_t *inner_trk_hi,
  uint32_t inner_trk_first, uint32_t inner_trk_mm,
  int n_inner_dims, const char *bt)
{
  int n_child = cg->inner_agg_bnd_n;
  if (n_child <= 0) return;
  if (n_child > n_inner_dims) n_child = n_inner_dims;

  // Capture child bounds into local C arrays (SSA temp ids)
  uint32_t child_lo[MAX_AGG_DIMS], child_hi[MAX_AGG_DIMS];
  for (int d = 0; d < n_child; d++) {
    child_lo[d] = cg->inner_agg_bnd_lo[d];
    child_hi[d] = cg->inner_agg_bnd_hi[d];
  }
  cg->inner_agg_bnd_n = 0;  // consumed

  // Emit runtime branching: first-seen → store; else → compare
  uint32_t fs = Emit_Temp ();
  Emit ("  %%t%u = load i1, ptr %%t%u\n", fs, inner_trk_first);
  uint32_t st_lbl = cg->label_id++;
  uint32_t ck_lbl = cg->label_id++;
  uint32_t dn_lbl = cg->label_id++;
  Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
     fs, ck_lbl, st_lbl);
  cg->block_terminated = true;

  // First time: store expected bounds for all child dims
  Emit_Label_Here (st_lbl);
  for (int d = 0; d < n_child; d++) {
    Emit ("  store %s %%t%u, ptr %%t%u\n",
       bt, child_lo[d], inner_trk_lo[d]);
    Emit ("  store %s %%t%u, ptr %%t%u\n",
       bt, child_hi[d], inner_trk_hi[d]);
  }
  Emit ("  store i1 1, ptr %%t%u\n", inner_trk_first);
  Emit ("  br label %%L%u\n", dn_lbl);
  cg->block_terminated = true;

  // Subsequent: compare all dims against expected, accumulate mismatch
  Emit_Label_Here (ck_lbl);
  uint32_t accum = 0;  // tracks accumulated mismatch SSA
  for (int d = 0; d < n_child; d++) {
    uint32_t elo = Emit_Temp ();
    Emit ("  %%t%u = load %s, ptr %%t%u\n",
       elo, bt, inner_trk_lo[d]);
    uint32_t ehi = Emit_Temp ();
    Emit ("  %%t%u = load %s, ptr %%t%u\n",
       ehi, bt, inner_trk_hi[d]);
    uint32_t nl = Emit_Temp ();
    Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
       nl, bt, child_lo[d], elo);
    uint32_t nh = Emit_Temp ();
    Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
       nh, bt, child_hi[d], ehi);
    uint32_t dim_mm = Emit_Temp ();
    Emit ("  %%t%u = or i1 %%t%u, %%t%u\n", dim_mm, nl, nh);
    if (accum == 0) {
      accum = dim_mm;
    } else {
      uint32_t merged = Emit_Temp ();
      Emit ("  %%t%u = or i1 %%t%u, %%t%u\n",
         merged, accum, dim_mm);
      accum = merged;
    }
  }

  // Merge into the global mismatch flag
  uint32_t old_mm = Emit_Temp ();
  Emit ("  %%t%u = load i1, ptr %%t%u\n", old_mm, inner_trk_mm);
  uint32_t new_mm = Emit_Temp ();
  Emit ("  %%t%u = or i1 %%t%u, %%t%u\n",
     new_mm, old_mm, accum ? accum : old_mm);
  Emit ("  store i1 %%t%u, ptr %%t%u\n", new_mm, inner_trk_mm);
  Emit ("  br label %%L%u\n", dn_lbl);
  cg->block_terminated = true;
  Emit_Label_Here (dn_lbl);
}

// RM 4.3.2: T'RANGE in a choice position denotes the discrete range                                
// T'FIRST..T'LAST.  Normalize these into NK_RANGE nodes so downstream                              
// choice-scanning code has a single representation to handle.                                      
// Applied recursively to inner sub-aggregates for multidim arrays.                                 
//                                                                                                  
void Desugar_Aggregate_Range_Choices (Syntax_Node *agg) {
  for (uint32_t ci = 0; ci < agg->aggregate.items.count; ci++) {
    Syntax_Node *cit = agg->aggregate.items.items[ci];
    if (cit->kind == NK_ASSOCIATION) {
      for (uint32_t cc = 0; cc < cit->association.choices.count; cc++) {
        Syntax_Node *ch = cit->association.choices.items[cc];
        if (ch->kind == NK_ATTRIBUTE and
          Slice_Equal_Ignore_Case (ch->attribute.name, S("RANGE"))) {
          Syntax_Node *lo = Node_New (NK_ATTRIBUTE, ch->location);
          lo->attribute.prefix = ch->attribute.prefix;
          lo->attribute.name   = S("FIRST");
          lo->type = ch->type;
          Syntax_Node *hi = Node_New (NK_ATTRIBUTE, ch->location);
          hi->attribute.prefix = ch->attribute.prefix;
          hi->attribute.name   = S("LAST");
          hi->type = ch->type;
          for (uint32_t ai = 0; ai < ch->attribute.arguments.count; ai++) {
            Node_List_Push (&lo->attribute.arguments,
                     ch->attribute.arguments.items[ai]);
            Node_List_Push (&hi->attribute.arguments,
                     ch->attribute.arguments.items[ai]);
          }
          Syntax_Node *range = Node_New (NK_RANGE, ch->location);
          range->range.low  = lo;
          range->range.high = hi;
          cit->association.choices.items[cc] = range;
        }
      }

      // Recurse into inner sub-aggregate (multidim)
      if (cit->association.expression and
        cit->association.expression->kind == NK_AGGREGATE)
        Desugar_Aggregate_Range_Choices (cit->association.expression);
    } else if (cit->kind == NK_AGGREGATE) {
      Desugar_Aggregate_Range_Choices (cit);
    }
  }
}

// ── § 13b.0: Record Aggregate Component Store ────────────────────────────────────────────────────
//                                                                                                  
// Literate summary:  a record aggregate (A => 1, B => "hello", C => rec)                           
// must store each component value to the correct byte offset.  Three                               
// representations arise - fat pointers, composite pointers, and scalars                            
// - each requiring distinct LLVM IR.  This helper unifies the logic that                           
// was previously duplicated across named, positional, and OTHERS paths.                            
//                                                                                                  
//   data Src = Fat { ptr, ptr }   -- unconstrained array / dynamic bounds                          
//            | Ptr ptr             -- composite record / constrained array                         
//            | Val T               -- scalar / enumeration                                         
//                                                                                                  
//   store :: Src → CompPtr → IO ()                                                                 
//   store (Fat fp) dst = Emit_Fat_To_Array_Memcpy fp dst                                           
//   store (Ptr p)  dst = memcpy dst p (sizeof comp)                                                
//   store (Val v)  dst = store v dst                                                               
//                                                                                                  
// After storage, discriminant values are mirrored to disc_agg_temps and                            
// constraint-checked against the enclosing type (RM 3.7.1, 4.3.1).                                 
// Disc_Alloc_Entry and Disc_Alloc_Info are defined earlier in §13 declarations                                            
//                                                                                                  

// Compute the byte size for a composite component memcpy.  For disc-
// dependent arrays (size==0), derive from the source expression.
uint32_t Agg_Comp_Byte_Size (Type_Info *ti, Syntax_Node *src_expr) {
  uint32_t sz = ti->size;
  if (sz == 0 and Type_Is_Array_Like (ti) and ti->array.element_type) {
    uint32_t esz = ti->array.element_type->size;
    if (esz == 0) esz = 1;
    if (src_expr and src_expr->kind == NK_AGGREGATE)
      sz = src_expr->aggregate.items.count * esz;
    else if (src_expr and src_expr->type and src_expr->type->size > 0)
      sz = src_expr->type->size;
  }
  return sz > 0 ? sz : 8;
}

// Store a generated value into a record component slot.
// Handles all three cases: fat pointer, composite ptr, scalar.
void Agg_Rec_Store (uint32_t val, uint32_t dest_ptr,
  Component_Info *comp, Syntax_Node *src_expr)
{
  Type_Info *ti = comp->component_type;
  const char *src_type = Expression_Llvm_Type (src_expr);
  const char *comp_type = Type_To_Llvm (ti);
  bool is_fat = src_type and strstr (src_type, "{ ptr, ptr }") != NULL;
  bool is_ptr = src_type and strcmp (src_type, "ptr") == 0;

  // Fat → constrained: extract data+bounds, compute length, memcpy
  if (ti and is_fat and (Type_Is_String (ti) or Type_Is_Array_Like (ti))) {
    Emit_Fat_To_Array_Memcpy (val, dest_ptr, ti);

  // Composite → memcpy with size from type or source expression
  } else if (ti and is_ptr and (Type_Is_Record (ti) or Type_Is_Constrained_Array (ti))) {
    Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)\n",
       dest_ptr, val, Agg_Comp_Byte_Size (ti, src_expr));
    Emit_Comp_Disc_Check (dest_ptr, ti);

  // Scalar: convert and store, with constraint check (RM 4.3.1)
  } else {
    val = Emit_Convert (val, src_type, comp_type);
    if (ti and Type_Is_Scalar (ti) and not comp->is_discriminant)
      val = Emit_Constraint_Check_With_Type (val, ti,
        src_expr ? src_expr->type : NULL, comp_type);
    Emit ("  store %s %%t%u, ptr %%t%u\n", comp_type, val, dest_ptr);
  }
}

// After storing a discriminant, mirror it to disc_agg_temps and verify
// against the enclosing type's discriminant constraint (RM 3.7.1).
void Agg_Rec_Disc_Post (uint32_t val, Component_Info *comp, uint32_t disc_ordinal,
  Type_Info *agg_type, Disc_Alloc_Info *da_info)
{
  const char *dt = Type_To_Llvm (comp->component_type);

  // Coerce val to the component's storage type (e.g. i32→i8 for narrow
  // discrete types) before mirroring and constraint-checking.
  val = Emit_Coerce_Default_Int (val, dt);

  // Mirror to disc_agg_temp alloca
  for (uint32_t i = 0; i < da_info->count; i++) {
    if (Slice_Equal_Ignore_Case (da_info->entries[i].sym->name, comp->name)) {
      Emit ("  store %s %%t%u, ptr %%t%u\n",
         dt, val, da_info->entries[i].temp);
      break;
    }
  }

  // RM 3.7.1: if the aggregate type is constrained, check that the
  // supplied discriminant value matches the expected constraint.
  if (not agg_type->record.has_disc_constraints) return;
  if (disc_ordinal >= agg_type->record.discriminant_count) return;
  uint32_t exp_v;
  if (cg->disc_cache_count > 0 and cg->disc_cache_type == agg_type and
    disc_ordinal < cg->disc_cache_count and cg->disc_cache[disc_ordinal]) {
    exp_v = Emit_Convert (cg->disc_cache[disc_ordinal],
      Integer_Arith_Type (), dt);
  } else {
    exp_v = Emit_Disc_Constraint_Value (agg_type, disc_ordinal, dt);
    if (exp_v == 0) exp_v = Emit_Static_Int (0, dt);
  }
  uint32_t ne = Emit_Temp ();
  Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", ne, dt, val, exp_v);
  uint32_t ok_l = cg->label_id++, fl_l = cg->label_id++;
  Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", ne, fl_l, ok_l);
  cg->block_terminated = true;
  Emit_Label_Here (fl_l);
  Emit_Raise_Constraint_Error ("discriminant value vs constraint");
  Emit_Label_Here (ok_l);
}

// Count how many discriminants precede component index `ci`.
uint32_t Disc_Ordinal_Before (Type_Info *type_info, uint32_t comp_index) {
  uint32_t count = 0;
  for (uint32_t k = 0; k < comp_index; k++)
    if (type_info->record.components[k].is_discriminant) count++;
  return count;
}

// ── § 13b: Generate_Aggregate - array & record aggregate codegen. ────────────────────────────────
//                                                                                                  
// Structure:                                                                                       
//   1. Array aggregate (RM 4.3.2):                                                                 
//      a. Classify items, compute dimension bounds                                                 
//      b. Dynamic-bounds path (any BOUND_EXPR): alloca, loop, fat ptr                              
//      c. Static-bounds path: alloca[N], unrolled init, optional fat ptr                           
//   2. Record aggregate (RM 4.3.1):                                                                
//      a. Alloca, positional/named stores, others fill                                             
//                                                                                                  
uint32_t Generate_Aggregate (Syntax_Node *node) {
  Type_Info *agg_type = node->type;

  // Normalize T'RANGE choices → T'FIRST..T'LAST (recursively)
  if (node->kind == NK_AGGREGATE)
    Desugar_Aggregate_Range_Choices (node);
  if (not agg_type) {
    Report_Error (node->location, "untyped aggregate in codegen");
    return 0;
  }

  // Array aggregate - allocate on stack and initialize
  if (Type_Is_Array_Like (agg_type) and agg_type->array.index_count > 0) {
    bool elem_is_fat = Type_Needs_Fat_Pointer_Load (agg_type->array.element_type);
    const char *elem_type = elem_is_fat ? FAT_PTR_TYPE
                : Type_To_Llvm (agg_type->array.element_type);
    uint32_t elem_size = agg_type->array.element_type ?
               agg_type->array.element_type->size : 8;
    if (elem_size == 0 and elem_is_fat) elem_size = FAT_PTR_ALLOC_SIZE;
    else if (elem_size == 0) elem_size = 8;

    // RM 4.3.3(6): For positional aggregates of unconstrained array types,                         
    // the lower bound of each dimension is the index subtype's 'FIRST.                             
    // Derive bounds for ALL dimensions from index_type when BOUND_NONE.                            
    //                                                                                              
    uint32_t agg_ndims = agg_type->array.index_count;
    if (agg_ndims > 8) agg_ndims = 8;
    Type_Bound dim_lo[8], dim_hi[8];
    for (uint32_t d = 0; d < agg_ndims; d++) {
      dim_lo[d] = agg_type->array.indices[d].low_bound;
      dim_hi[d] = agg_type->array.indices[d].high_bound;
      if (dim_lo[d].kind == BOUND_NONE and agg_type->array.indices[d].index_type)
        dim_lo[d] = agg_type->array.indices[d].index_type->low_bound;
      if (dim_hi[d].kind == BOUND_NONE and agg_type->array.indices[d].index_type)
        dim_hi[d] = agg_type->array.indices[d].index_type->high_bound;
    }

    // Classify items: positional count, named, others (RM 4.3.2)
    Agg_Class agg_cls = Agg_Classify (node);
    uint32_t n_positional = agg_cls.n_positional;
    bool has_named = agg_cls.has_named;
    {

      // For positional aggregates, override upper bound from element count:                        
      // high = low + N - 1.  For multidimensional aggregates, also adjust                          
      // inner dimension bounds from the first inner aggregate (RM 4.3.2).                          
      //                                                                                            
      if (n_positional > 0 and not has_named and
        dim_lo[0].kind == BOUND_INTEGER) {
        dim_hi[0] = (Type_Bound){
          .kind = BOUND_INTEGER,
          .int_value = dim_lo[0].int_value + (int128_t)n_positional - 1
        };
      }

      // Walk into nested inner aggregates to fix inner dimension bounds.                           
      // E.g. ((5,4,3),(2,1,0)) for ARRAY (STC1 RANGE <>, STC2 RANGE <>)                            
      // - the inner aggregate has 3 elements, so dim_hi[1] = STC2'FIRST + 3 - 1.                   
      //                                                                                            
      if (n_positional > 0 and not has_named and agg_ndims > 1) {
        Syntax_Node *inner = node->aggregate.items.items[0];
        for (uint32_t d = 1; d < agg_ndims and inner; d++) {
          if (inner->kind != NK_AGGREGATE) break;
          uint32_t inner_n = 0;
          bool inner_named = false;
          for (uint32_t j = 0; j < inner->aggregate.items.count; j++) {
            if (inner->aggregate.items.items[j]->kind == NK_ASSOCIATION)
              inner_named = true;
            else inner_n++;
          }
          if (inner_n > 0 and not inner_named and
            dim_lo[d].kind == BOUND_INTEGER) {
            dim_hi[d] = (Type_Bound){
              .kind = BOUND_INTEGER,
              .int_value = dim_lo[d].int_value + (int128_t)inner_n - 1
            };
          }

          // Descend into the first element of this inner aggregate
          // for the next dimension (3-D, 4-D, ...)
          inner = (inner->aggregate.items.count > 0) ?
              inner->aggregate.items.items[0] : NULL;
        }
      }

      // Named aggregates: compute outer bounds from explicit choice indices.                       
      // E.g. (1 => "WHEN", 2 => "WHAT") → dim_lo[0]=1, dim_hi[0]=2.                                
      // Also compute inner dimension from first inner element's size.                              
      //                                                                                            
      if (has_named and not agg_type->array.is_constrained) {
        int128_t named_lo = INT64_MAX, named_hi = INT64_MIN;
        bool found_named = false;
        Syntax_Node *first_inner_expr = NULL;
        for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
          Syntax_Node *item = node->aggregate.items.items[i];
          if (item->kind != NK_ASSOCIATION) continue;
          if (not first_inner_expr)
            first_inner_expr = item->association.expression;
          for (uint32_t c = 0; c < item->association.choices.count; c++) {
            Syntax_Node *ch = item->association.choices.items[c];
            if (Is_Others_Choice (ch)) continue;
            if (Is_Static_Int_Node (ch)) {
              int128_t v = Static_Int_Value (ch);
              if (v < named_lo) named_lo = v;
              if (v > named_hi) named_hi = v;
              found_named = true;
            } else if (ch->kind == NK_INTEGER) {
              int128_t v = (int128_t)ch->integer_lit.value;
              if (v < named_lo) named_lo = v;
              if (v > named_hi) named_hi = v;
              found_named = true;
            } else if (ch->kind == NK_RANGE) {
              if (Is_Static_Int_Node (ch->range.low)) {
                int128_t v = Static_Int_Value (ch->range.low);
                if (v < named_lo) named_lo = v;
                found_named = true;
              } else if (ch->range.low and ch->range.low->kind == NK_INTEGER) {
                int128_t v = (int128_t)ch->range.low->integer_lit.value;
                if (v < named_lo) named_lo = v;
                found_named = true;
              }
              if (Is_Static_Int_Node (ch->range.high)) {
                int128_t v = Static_Int_Value (ch->range.high);
                if (v > named_hi) named_hi = v;
                found_named = true;
              } else if (ch->range.high and ch->range.high->kind == NK_INTEGER) {
                int128_t v = (int128_t)ch->range.high->integer_lit.value;
                if (v > named_hi) named_hi = v;
                found_named = true;
              }
            }
          }
        }
        if (found_named) {
          dim_lo[0] = (Type_Bound){.kind = BOUND_INTEGER, .int_value = named_lo};
          dim_hi[0] = (Type_Bound){.kind = BOUND_INTEGER, .int_value = named_hi};
        }

        // Inner dimension: if inner elements are string literals,
        // use string length for the second dimension extent.
        if (agg_ndims > 1 and first_inner_expr and dim_lo[1].kind == BOUND_INTEGER) {
          if (first_inner_expr->kind == NK_STRING and
            first_inner_expr->string_val.text.length > 0) {
            uint32_t slen = (uint32_t)first_inner_expr->string_val.text.length;
            dim_hi[1] = (Type_Bound){
              .kind = BOUND_INTEGER,
              .int_value = dim_lo[1].int_value + (int128_t)slen - 1
            };

          // Inner aggregate: count its elements
          } else if (first_inner_expr->kind == NK_AGGREGATE) {
            uint32_t inner_cnt = 0;
            for (uint32_t j = 0; j < first_inner_expr->aggregate.items.count; j++) {
              if (first_inner_expr->aggregate.items.items[j]->kind != NK_ASSOCIATION)
                inner_cnt++;
            }
            if (inner_cnt > 0) {
              dim_hi[1] = (Type_Bound){
                .kind = BOUND_INTEGER,
                .int_value = dim_lo[1].int_value + (int128_t)inner_cnt - 1
              };
            }
          }
        }
      }
    }
    Type_Bound low_bound = dim_lo[0], high_bound = dim_hi[0];

    // For multidim aggregates with unconstrained types (or constrained                             
    // types whose inner bounds span the full index subtype range), walk                            
    // into inner sub-aggregates and propagate their choice bounds to                               
    // dim_lo[1..N]/dim_hi[1..N].  Without this, inner dims default to                              
    // the full index subtype range (e.g. INTEGER: -2^31..2^31-1),                                  
    // causing 4 GB allocations.                                                                    
    // RANGE choices have already been desugared to NK_RANGE by                                     
    // Desugar_Aggregate_Range_Choices, so only NK_RANGE is checked.                                
    // Only propagate when inner bounds are effectively unconstrained:                              
    // unconstrained type, or constrained type where inner dim bounds                               
    // are full-range placeholders that would overflow size calcs.                                  
    //                                                                                              
    if (agg_ndims > 1) {
      {
      bool need_propagation = not agg_type->array.is_constrained;
      if (not need_propagation) {
        for (uint32_t d = 1; d < agg_ndims and not need_propagation; d++)
          need_propagation = Bound_Pair_Overflows (dim_lo[d], dim_hi[d]);
      }
      if (need_propagation) {
        Syntax_Node *inner_agg = NULL;
        for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
          Syntax_Node *item = node->aggregate.items.items[i];
          if (item->kind == NK_ASSOCIATION) {
            inner_agg = item->association.expression; break;
          } else if (item->kind == NK_AGGREGATE) {
            inner_agg = item; break;
          }
        }
        for (uint32_t d = 1; d < agg_ndims and inner_agg; d++) {
          if (inner_agg->kind != NK_AGGREGATE) break;

          // Extract choice bounds from inner sub-aggregate
          bool found = false;
          for (uint32_t ci = 0; ci < inner_agg->aggregate.items.count and not found; ci++) {
            Syntax_Node *cit = inner_agg->aggregate.items.items[ci];
            if (cit->kind != NK_ASSOCIATION) continue;
            for (uint32_t cc = 0; cc < cit->association.choices.count and not found; cc++) {
              Syntax_Node *ch = cit->association.choices.items[cc];
              if (ch->kind == NK_RANGE) {
                dim_lo[d] = Is_Static_Int_Node (ch->range.low)
                  ? (Type_Bound){.kind=BOUND_INTEGER,
                     .int_value=Static_Int_Value (ch->range.low)}
                  : (Type_Bound){.kind=BOUND_EXPR, .expr=ch->range.low};
                dim_hi[d] = Is_Static_Int_Node (ch->range.high)
                  ? (Type_Bound){.kind=BOUND_INTEGER,
                     .int_value=Static_Int_Value (ch->range.high)}
                  : (Type_Bound){.kind=BOUND_EXPR, .expr=ch->range.high};
                found = true;
              }
            }
          }

          // Descend into first inner association's value for next dim
          Syntax_Node *next = NULL;
          for (uint32_t i = 0; i < inner_agg->aggregate.items.count; i++) {
            if (inner_agg->aggregate.items.items[i]->kind == NK_ASSOCIATION) {
              next = inner_agg->aggregate.items.items[i]->association.expression;
              break;
            }
          }
          inner_agg = next;
        }
      }
      }  // end need_propagation scope
    }

    // For multi-dimensional arrays, the effective "element" of the outer                           
    // aggregate is a row (slice along the first dimension), not the scalar                         
    // element_type.  Compute the row size so memcpy uses the right length.                         
    //                                                                                              
    bool multidim = (agg_ndims > 1);
    uint32_t row_size = elem_size;
    bool inner_dynamic = false;
    if (multidim) {
      for (uint32_t d = 1; d < agg_ndims; d++) {
        if (dim_lo[d].kind == BOUND_EXPR or dim_hi[d].kind == BOUND_EXPR) {
          inner_dynamic = true; break;
        }
      }
      if (not inner_dynamic) {
        uint32_t row_elems = 1;
        for (uint32_t d = 1; d < agg_ndims; d++) {
          int128_t lo = Type_Bound_Value (dim_lo[d]);
          int128_t hi = Type_Bound_Value (dim_hi[d]);
          int128_t cnt = hi - lo + 1;
          if (cnt > 0) row_elems *= (uint32_t)cnt;
        }
        row_size = row_elems * elem_size;
        elem_size = row_size;  // outer dim "element" is a row
      }
    }

    // Early scan: detect if any named choice has non-static bounds                                 
    // (e.g. T'RANGE desugared to T'FIRST..T'LAST, or function calls).                              
    // Negated integer literals like -1 are static and must NOT force                               
    // the dynamic path - only genuine runtime expressions do.                                      
    // For CONSTRAINED types the type already supplies static bounds;                               
    // we only override dim_lo/dim_hi for UNCONSTRAINED types where                                 
    // the choices determine the aggregate's bounds (RM 4.3.2(4)).                                  
    //                                                                                              
    bool has_dynamic_choice_early = false;
    for (uint32_t ci = 0; ci < node->aggregate.items.count; ci++) {
      Syntax_Node *cit = node->aggregate.items.items[ci];
      if (cit->kind != NK_ASSOCIATION) continue;
      for (uint32_t cc = 0; cc < cit->association.choices.count; cc++) {
        Syntax_Node *ch = cit->association.choices.items[cc];
        if (ch->kind == NK_RANGE and
          (not Is_Static_Int_Node (ch->range.low) or
           not Is_Static_Int_Node (ch->range.high))) {
          has_dynamic_choice_early = true;

          // RM 4.3.2(4): For unconstrained types, named aggregate                                  
          // bounds come from the choices.  Also override for                                       
          // constrained types whose dim-0 bounds are full-range                                    
          // placeholders (would overflow size calculations).                                       
          //                                                                                        
          if (not agg_type->array.is_constrained or
            Bound_Pair_Overflows (dim_lo[0], dim_hi[0])) {
            if (not Is_Static_Int_Node (ch->range.low)) {
              dim_lo[0] = (Type_Bound){.kind = BOUND_EXPR, .expr = ch->range.low};
              low_bound = dim_lo[0];
            }
            if (not Is_Static_Int_Node (ch->range.high)) {
              dim_hi[0] = (Type_Bound){.kind = BOUND_EXPR, .expr = ch->range.high};
              high_bound = dim_hi[0];
            }
          }
          break;
        }
      }
      if (has_dynamic_choice_early) break;
    }

    // Any dimension with dynamic bounds requires runtime path (RM 3.6.1).                          
    // Type_Bound_Value returns 0 for BOUND_EXPR so compile-time size                               
    // calculation would be wrong; the dynamic path evaluates bounds at                             
    // runtime via Emit_Single_Bound.                                                               
    // Note: has_dynamic_choice_early only forces dynamic_bounds when it                            
    // actually changed dim_lo/dim_hi to BOUND_EXPR (unconstrained types).                          
    // Constrained types keep their static dim bounds and use the static                            
    // path - dynamic choice expressions are evaluated for side effects                             
    // via the must_eval_low/must_eval_high logic in the static path.                               
    //                                                                                              
    bool dynamic_bounds = false;
    for (uint32_t d = 0; d < agg_ndims; d++) {
      if (dim_lo[d].kind == BOUND_EXPR or dim_hi[d].kind == BOUND_EXPR) {
        dynamic_bounds = true; break;
      }
    }

    // Dynamic bounds: generate runtime allocation and loop-based init
    if (dynamic_bounds) {
      const char *iat_bnd = Integer_Arith_Type ();

      // RM 3.2.1: When BOUND_EXPR has no cached_temp, the expression                               
      // would be re-evaluated with stale side effects (e.g., F(I) long                             
      // after the declaration that created this constrained subtype).                              
      // For positional aggregates, derive bounds from the count instead                            
      // of re-evaluating the type's constraint expression.                                         
      //                                                                                            
      bool bounds_stale = false;
      if (high_bound.kind == BOUND_EXPR and not high_bound.cached_temp
        and high_bound.expr)
        bounds_stale = true;
      Agg_Class ac_early = Agg_Classify (node);
      uint32_t low_val, high_val;

      // Positional aggregate with stale constraint: use count as bound.                            
      // low = index type FIRST (usually 1 for NATURAL/POSITIVE),                                   
      // high = low + n_positional - 1.                                                             
      //                                                                                            
      if (bounds_stale and ac_early.n_positional > 0 and not ac_early.has_others) {
        int128_t lo_static = 1;
        if (low_bound.kind == BOUND_INTEGER)
          lo_static = low_bound.int_value;
        else if (agg_type->base_type and agg_type->base_type->array.index_count > 0
             and agg_type->base_type->array.indices[0].index_type)
          lo_static = Type_Bound_Value (
            agg_type->base_type->array.indices[0].index_type->low_bound);
        low_val = Emit_Static_Int (lo_static, iat_bnd);
        high_val = Emit_Static_Int (lo_static + (int128_t)ac_early.n_positional - 1, iat_bnd);
      } else {
        low_val = Emit_Single_Bound (&low_bound, iat_bnd);
        high_val = Emit_Single_Bound (&high_bound, iat_bnd);
      }

      // Report bounds to outer multidim aggregate for consistency check
      if (cg->in_agg_component > 0) {
        cg->inner_agg_bnd_lo[0] = low_val;
        cg->inner_agg_bnd_hi[0] = high_val;
        cg->inner_agg_bnd_n = 1;
      }

      // Track which expression nodes produced low_val / high_val so
      // the range-choice loop below can reuse them (avoid double eval).
      Syntax_Node *bound_low_expr  = (low_bound.kind == BOUND_EXPR)
                     ? low_bound.expr : NULL;
      Syntax_Node *bound_high_expr = (high_bound.kind == BOUND_EXPR)
                     ? high_bound.expr : NULL;

      // For multidim with dynamic inner bounds, compute row_size and
      // total_flat_count at runtime so allocation is correct.
      uint32_t rt_row_size = 0;  // SSA temp for runtime row size
      uint32_t rt_inner_lo[8] = {0}, rt_inner_hi[8] = {0};

      // Evaluate inner dimension bounds at runtime
      if (multidim and inner_dynamic) {
        uint32_t rt_row_elems = 0;
        for (uint32_t d = 1; d < agg_ndims; d++) {
          rt_inner_lo[d] = Emit_Single_Bound (&dim_lo[d], iat_bnd);
          rt_inner_hi[d] = Emit_Single_Bound (&dim_hi[d], iat_bnd);
          uint32_t dim_len = Emit_Length_From_Bounds (rt_inner_lo[d],
                                 rt_inner_hi[d], iat_bnd);
          if (d == 1)
            rt_row_elems = dim_len;
          else {
            uint32_t prod = Emit_Temp ();
            Emit ("  %%t%u = mul %s %%t%u, %%t%u\n",
               prod, iat_bnd, rt_row_elems, dim_len);
            rt_row_elems = prod;
          }
        }

        // RM 4.3.2(3): inner dimension choice bounds must belong to                                
        // the corresponding index subtype, even for null outer ranges.                             
        // Check at runtime before computing row_size.                                              
        //                                                                                          
        for (uint32_t d = 1; d < agg_ndims; d++) {
          Type_Info *idx_t = NULL;
          if (agg_type->array.indices and d < agg_type->array.index_count)
            idx_t = agg_type->array.indices[d].index_type;
          if (not idx_t and agg_type->base_type and
            Type_Is_Array_Like (agg_type->base_type) and
            d < agg_type->base_type->array.index_count and
            agg_type->base_type->array.indices)
            idx_t = agg_type->base_type->array.indices[d].index_type;
          if (idx_t and idx_t->low_bound.kind == BOUND_INTEGER and
            idx_t->high_bound.kind == BOUND_INTEGER) {
            int64_t is_lo = (int64_t)Type_Bound_Value (idx_t->low_bound);
            int64_t is_hi = (int64_t)Type_Bound_Value (idx_t->high_bound);

            // Skip if full INTEGER range (no real subtype constraint)
            if (is_lo != (int64_t)(-2147483648LL) or
              is_hi != (int64_t)2147483647LL) {
              uint32_t nc = Emit_Temp ();
              Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u"
                 "  ; inner dim %u null?\n",
                 nc, iat_bnd, rt_inner_lo[d], rt_inner_hi[d], d);
              uint32_t sk = cg->label_id++;
              uint32_t ck = cg->label_id++;
              Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                 nc, sk, ck);
              cg->block_terminated = true;
              Emit_Label_Here (ck);
              Emit_Range_Check_With_Raise (rt_inner_lo[d],
                is_lo, is_hi, iat_bnd,
                "dynamic aggregate inner index subtype check");
              Emit_Range_Check_With_Raise (rt_inner_hi[d],
                is_lo, is_hi, iat_bnd,
                "dynamic aggregate inner index subtype check");
              Emit ("  br label %%L%u\n", sk);
              cg->block_terminated = true;
              Emit_Label_Here (sk);
            }
          }
        }
        uint32_t raw_row_size = Emit_Temp ();
        uint32_t scalar_sz = agg_type->array.element_type ?
                   agg_type->array.element_type->size : 8;
        if (scalar_sz == 0) scalar_sz = 8;
        Emit ("  %%t%u = mul %s %%t%u, %u  ; row byte size\n",
           raw_row_size, iat_bnd, rt_row_elems, scalar_sz);

        // Clamp to >= 0: null inner ranges produce negative sizes
        uint32_t neg = Emit_Temp ();
        Emit ("  %%t%u = icmp slt %s %%t%u, 0\n",
           neg, iat_bnd, raw_row_size);
        rt_row_size = Emit_Temp ();
        Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
           rt_row_size, neg, iat_bnd, iat_bnd, raw_row_size);
      }

      // Calculate count and byte size
      uint32_t count_plus1 = Emit_Length_From_Bounds (low_val, high_val, iat_bnd);
      uint32_t byte_size;

      // Runtime elem_size: total = outer_count * row_byte_size
      if (rt_row_size) {
        byte_size = Emit_Temp ();
        Emit ("  %%t%u = mul %s %%t%u, %%t%u\n",
           byte_size, iat_bnd, count_plus1, rt_row_size);
      } else {
        byte_size = Emit_Temp ();
        Emit ("  %%t%u = mul %s %%t%u, %u\n",
           byte_size, iat_bnd, count_plus1, elem_size);
      }

      // Clamp byte_size to 0 for null ranges (RM 3.6.1)
      uint32_t neg_chk = Emit_Temp ();
      Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", neg_chk, iat_bnd, byte_size);
      uint32_t clamped = Emit_Temp ();
      Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
         clamped, neg_chk, iat_bnd, iat_bnd, byte_size);

      // Dynamic stack allocation
      uint32_t base = Emit_Temp ();
      Emit ("  %%t%u = alloca i8, %s %%t%u  ; dynamic array aggregate\n", base, iat_bnd, clamped);

      // Classify items: find OTHERS clause (value generated in loop below)
      Agg_Class ac = ac_early;  // reuse early classification
      bool has_others = ac.has_others;
      uint32_t others_val = 0;  // unused; OTHERS re-evaluated per component

      // RM 4.3.2(6): Dynamic aggregate bounds vs constraint check.                                 
      // When the aggregate type is a constrained subtype of an unconstrained                       
      // base and the constraint bounds are runtime-determined, verify at                           
      // runtime that the aggregate's "natural" bounds match the constraint.                        
      // Positional: per RM 4.3.2(4), lower bound = applicable index                                
      //   constraint's first, so bounds automatically match if count = constraint                  
      //   size.  Check n_positional vs constraint element count.                                   
      // Named (static choices): choice bounds must equal constraint bounds.                        
      // Skip when bounds reference discriminants whose disc_agg_temp                               
      // hasn't been set up yet (not safely evaluable; RM 3.7.1).                                   
      //                                                                                            
      {
        bool bounds_ref_unset_disc = false;
        for (uint32_t d = 0; d < agg_ndims and not bounds_ref_unset_disc; d++) {
          Type_Bound *b[2] = {&agg_type->array.indices[d].low_bound,
                    &agg_type->array.indices[d].high_bound};
          for (int bi = 0; bi < 2; bi++) {
            if (b[bi]->kind == BOUND_EXPR and b[bi]->expr and
              b[bi]->expr->symbol and
              b[bi]->expr->symbol->kind == SYMBOL_DISCRIMINANT and
              b[bi]->expr->symbol->disc_agg_temp == 0)
              bounds_ref_unset_disc = true;

            // RM 3.2.1: Also skip when BOUND_EXPR has no cached                                    
            // temp - the expression would be re-evaluated with                                     
            // stale side effects (e.g., F(I) after elaboration).                                   
            //                                                                                      
            if (b[bi]->kind == BOUND_EXPR and b[bi]->expr and
              not b[bi]->cached_temp and
              (not b[bi]->expr->symbol or
               b[bi]->expr->symbol->kind != SYMBOL_DISCRIMINANT))
              bounds_ref_unset_disc = true;
          }
        }
      if (agg_type->array.is_constrained and not bounds_ref_unset_disc) {
        bool has_unc_base = agg_type->base_type and
          Type_Is_Array_Like (agg_type->base_type) and
          not agg_type->base_type->array.is_constrained;
        if (has_unc_base) {
          const char *ait = Integer_Arith_Type ();
          if (ac.n_positional > 0 and not ac.has_named) {
            uint32_t clo = Emit_Coerce (low_val, ait);
            uint32_t chi = Emit_Coerce (high_val, ait);
            {

              // RM 4.3.2(5): For a positional aggregate of a constrained                           
              // subtype, only the count must match - the lower bound is                            
              // determined by the constraint.                                                      
              //                                                                                    
              uint32_t n_pos = Emit_Static_Int ((int128_t)ac.n_positional, ait);
              uint32_t con_len = Emit_Temp ();
              Emit ("  %%t%u = sub %s %%t%u, %%t%u\n",
                 con_len, ait, chi, clo);
              uint32_t con_cnt = Emit_Temp ();
              Emit ("  %%t%u = add %s %%t%u, 1\n",
                 con_cnt, ait, con_len);
              uint32_t mismatch = Emit_Temp ();
              Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                 mismatch, ait, n_pos, con_cnt);
              uint32_t ok_lbl = cg->label_id++;
              uint32_t fail_lbl = cg->label_id++;
              Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                 mismatch, fail_lbl, ok_lbl);
              cg->block_terminated = true;
              Emit_Label_Here (fail_lbl);
              Emit_Raise_Constraint_Error ("positional aggregate count vs dynamic constraint");
              Emit_Label_Here (ok_lbl);
            }

            // RM 4.3.2: Parenthesized aggregate - INDEX_SUBTYPE'FIRST                              
            // may differ from the runtime constraint lower bound.                                  
            // Compare against the type's actual constraint bound, not                              
            // clo (which may be a stale-bounds fallback).                                          
            //                                                                                      
            if (node->aggregate.is_parenthesized and agg_type->base_type and
              agg_type->base_type->array.index_count > 0 and
              agg_type->base_type->array.indices[0].index_type) {
              Type_Info *idx_ty = agg_type->base_type->array.indices[0].index_type;
              uint32_t isf = (idx_ty->low_bound.kind == BOUND_INTEGER)
                ? Emit_Static_Int (idx_ty->low_bound.int_value, ait)
                : Emit_Coerce (Emit_Bound_Value (&idx_ty->low_bound), ait);
              uint32_t con_lo = Emit_Coerce (
                Emit_Single_Bound (&agg_type->array.indices[0].low_bound, ait), ait);
              uint32_t ne = Emit_Temp ();
              Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                 ne, ait, isf, con_lo);
              uint32_t ok2 = cg->label_id++;
              uint32_t fail2 = cg->label_id++;
              Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                 ne, fail2, ok2);
              cg->block_terminated = true;
              Emit_Label_Here (fail2);
              Emit_Raise_Constraint_Error ("parenthesized aggregate bounds vs dynamic constraint");
              Emit_Label_Here (ok2);
            }
          }
          if (ac.has_named and not ac.has_others and agg_ndims == 1) {
            int128_t ch_lo = INT64_MAX, ch_hi = INT64_MIN;
            bool found_lo = false, found_hi = false;
            for (uint32_t ci = 0; ci < node->aggregate.items.count; ci++) {
              Syntax_Node *cit = node->aggregate.items.items[ci];
              if (cit->kind != NK_ASSOCIATION) continue;
              for (uint32_t cc = 0; cc < cit->association.choices.count; cc++) {
                Syntax_Node *ch = cit->association.choices.items[cc];
                if (Is_Others_Choice (ch)) continue;
                if (Is_Static_Int_Node (ch)) {
                  int128_t v = Static_Int_Value (ch);
                  if (not found_lo or v < ch_lo) ch_lo = v;
                  if (not found_hi or v > ch_hi) ch_hi = v;
                  found_lo = true; found_hi = true;
                } else if (ch->kind == NK_RANGE) {
                  if (ch->range.low and Is_Static_Int_Node (ch->range.low)) {
                    int128_t v = Static_Int_Value (ch->range.low);
                    if (not found_lo or v < ch_lo) ch_lo = v;
                    found_lo = true;
                  }
                  if (ch->range.high and Is_Static_Int_Node (ch->range.high)) {
                    int128_t v = Static_Int_Value (ch->range.high);
                    if (not found_hi or v > ch_hi) ch_hi = v;
                    found_hi = true;
                  }
                }
              }
            }
            if (found_lo and found_hi) {
              uint32_t nclo = Emit_Static_Int (ch_lo, ait);
              uint32_t nchi = Emit_Static_Int (ch_hi, ait);
              uint32_t clo = Emit_Coerce (low_val, ait);
              uint32_t chi = Emit_Coerce (high_val, ait);
              uint32_t ne_lo = Emit_Temp ();
              Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", ne_lo, ait, nclo, clo);
              uint32_t ne_hi = Emit_Temp ();
              Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", ne_hi, ait, nchi, chi);
              uint32_t mismatch = Emit_Temp ();
              Emit ("  %%t%u = or i1 %%t%u, %%t%u\n", mismatch, ne_lo, ne_hi);
              uint32_t ok_lbl = cg->label_id++;
              uint32_t fail_lbl = cg->label_id++;
              Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                 mismatch, fail_lbl, ok_lbl);
              cg->block_terminated = true;
              Emit_Label_Here (fail_lbl);
              Emit_Raise_Constraint_Error ("named aggregate bounds vs dynamic constraint");
              Emit_Label_Here (ok_lbl);
            }
          }
        }
      }
      // RM 4.3.2: When bounds are stale (bounds_ref_unset_disc), the                               
      // constraint checks above are skipped.  But parenthesized                                    
      // aggregates still need a lower-bound check: INDEX_SUBTYPE'FIRST                             
      // may differ from the constraint's lower bound.  Evaluate the                                
      // constraint bound directly - safe for variable references.                                  
      //                                                                                            
      if (bounds_ref_unset_disc and agg_type->array.is_constrained and
        ac.n_positional > 0 and not ac.has_named and
        node->aggregate.is_parenthesized) {
        bool has_unc_base_p = agg_type->base_type and
          Type_Is_Array_Like (agg_type->base_type) and
          not agg_type->base_type->array.is_constrained;
        if (has_unc_base_p and
          agg_type->base_type->array.index_count > 0 and
          agg_type->base_type->array.indices[0].index_type) {
          const char *ait = Integer_Arith_Type ();
          Type_Info *idx_ty = agg_type->base_type->array.indices[0].index_type;
          uint32_t isf = (idx_ty->low_bound.kind == BOUND_INTEGER)
            ? Emit_Static_Int (idx_ty->low_bound.int_value, ait)
            : Emit_Coerce (Emit_Bound_Value (&idx_ty->low_bound), ait);
          uint32_t con_lo = Emit_Coerce (
            Emit_Single_Bound (&agg_type->array.indices[0].low_bound, ait), ait);
          uint32_t ne = Emit_Temp ();
          Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", ne, ait, isf, con_lo);
          uint32_t ok_p = cg->label_id++;
          uint32_t fail_p = cg->label_id++;
          Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
             ne, fail_p, ok_p);
          cg->block_terminated = true;
          Emit_Label_Here (fail_p);
          Emit_Raise_Constraint_Error (
            "parenthesized aggregate bounds vs stale dynamic constraint");
          Emit_Label_Here (ok_p);
        }
      }
      }  // end bounds_ref_disc scope

      // RM 4.3.2(6): for multidim aggregates, all inner sub-aggregates                             
      // must have the same bounds.  Track expected inner bounds and                                
      // compare each subsequent sub-aggregate's bounds.                                            
      //                                                                                            
      uint32_t dyn_inner_trk_lo[MAX_AGG_DIMS] = {0};
      uint32_t dyn_inner_trk_hi[MAX_AGG_DIMS] = {0};
      uint32_t dyn_inner_trk_first = 0, dyn_inner_trk_mm = 0;
      int dyn_n_inner_dims = 0;
      bool check_inner_consistency = multidim and agg_ndims > 1;
      if (check_inner_consistency) {
        dyn_n_inner_dims = agg_ndims - 1;
        if (dyn_n_inner_dims > MAX_AGG_DIMS)
          dyn_n_inner_dims = MAX_AGG_DIMS;
        const char *ait2 = Integer_Arith_Type ();
        for (int d = 0; d < dyn_n_inner_dims; d++) {
          dyn_inner_trk_lo[d] = Emit_Temp ();
          Emit ("  %%t%u = alloca %s  ; dyn expected inner lo [dim %d]\n",
             dyn_inner_trk_lo[d], ait2, d);
          dyn_inner_trk_hi[d] = Emit_Temp ();
          Emit ("  %%t%u = alloca %s  ; dyn expected inner hi [dim %d]\n",
             dyn_inner_trk_hi[d], ait2, d);
        }
        dyn_inner_trk_first = Emit_Temp ();
        Emit ("  %%t%u = alloca i1  ; first inner seen\n",
           dyn_inner_trk_first);
        Emit ("  store i1 0, ptr %%t%u\n", dyn_inner_trk_first);
        dyn_inner_trk_mm = Emit_Temp ();
        Emit ("  %%t%u = alloca i1  ; inner mismatch flag\n",
           dyn_inner_trk_mm);
        Emit ("  store i1 0, ptr %%t%u\n", dyn_inner_trk_mm);
      }

      // Positional elements: store each at its index offset.
      cg->inner_agg_bnd_n = 0;  // clear before associations
      {
        Type_Info *elem_ti = agg_type->array.element_type;
        bool ecomp = Agg_Elem_Is_Composite (elem_ti, multidim);
        const char *ait = Integer_Arith_Type ();
        uint32_t positional_idx = 0;
        for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
          Syntax_Node *item = node->aggregate.items.items[i];
          if (item->kind == NK_ASSOCIATION) continue;
          uint32_t val = Agg_Resolve_Elem (item, multidim,
            ecomp, agg_type, elem_type, elem_ti);

          // RM 4.3.2(6): inner consistency tracking for positional
          if (check_inner_consistency and cg->inner_agg_bnd_n > 0)
            Emit_Inner_Consistency_Track (dyn_inner_trk_lo, dyn_inner_trk_hi,
              dyn_inner_trk_first, dyn_inner_trk_mm,
              dyn_n_inner_dims,
              Array_Bound_Llvm_Type (agg_type));

          // RM 4.3.2: check scalar element against subtype
          if (not ecomp and elem_ti and Type_Is_Scalar (elem_ti))
            val = Emit_Constraint_Check_With_Type (val, elem_ti,
              item->type, elem_type);
          uint32_t pidx = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %u\n", pidx, ait, positional_idx);
          Agg_Store_At_Dynamic (base, val, pidx, ait,
            elem_type, elem_size, rt_row_size, ecomp);
          positional_idx++;
        }
      }

      // RM 4.3.2: for constrained arrays WITHOUT OTHERS, the aggregate                             
      // bounds (min-low, max-high across all named choices) must match                             
      // the constraint bounds - even for null ranges.  Track at runtime                            
      // so we handle dynamic (function-call) choice bounds.                                        
      //                                                                                            
      uint32_t agg_bnd_lo_var = 0, agg_bnd_hi_var = 0;
      bool track_named_bounds = agg_type->array.is_constrained and
        ac.has_named and not ac.has_others;
      if (track_named_bounds) {
        const char *ait = Integer_Arith_Type ();
        agg_bnd_lo_var = Emit_Temp ();
        Emit ("  %%t%u = alloca %s  ; track agg min-low\n",
           agg_bnd_lo_var, ait);
        uint32_t il = Emit_Static_Int (2147483647LL, ait);
        Emit ("  store %s %%t%u, ptr %%t%u\n", ait, il, agg_bnd_lo_var);
        agg_bnd_hi_var = Emit_Temp ();
        Emit ("  %%t%u = alloca %s  ; track agg max-high\n",
           agg_bnd_hi_var, ait);
        uint32_t ih = Emit_Static_Int ((int128_t)-2147483648LL, ait);
        Emit ("  store %s %%t%u, ptr %%t%u\n", ait, ih, agg_bnd_hi_var);
      }

      // For dynamic aggregates with named range association (1..H1 => val),
      // generate a loop to initialize all elements
      for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
        Syntax_Node *item = node->aggregate.items.items[i];
        if (item->kind == NK_ASSOCIATION and item->association.choices.count > 0) {
          Syntax_Node *choice = item->association.choices.items[0];
          if (Is_Others_Choice (choice)) continue;

          // Generate loop bounds, reusing already-evaluated                                        
          // SSA values when the expression node was used for                                       
          // the aggregate's overall bounds (avoids double                                          
          // evaluation of side-effecting expressions).                                             
          //                                                                                        
          if (choice->kind == NK_RANGE) {
            uint32_t rng_low_val, rng_high_val;
            if (Is_Static_Int_Node (choice->range.low)) {
              rng_low_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld\n", rng_low_val, Integer_Arith_Type (),
                 (long long)Static_Int_Value (choice->range.low));
            } else if (bound_low_expr and choice->range.low == bound_low_expr) {
              rng_low_val = low_val;  // reuse
            } else {
              rng_low_val = Generate_Expression (choice->range.low);
            }
            if (Is_Static_Int_Node (choice->range.high)) {
              rng_high_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld\n", rng_high_val, Integer_Arith_Type (),
                 (long long)Static_Int_Value (choice->range.high));
            } else if (bound_high_expr and choice->range.high == bound_high_expr) {
              rng_high_val = high_val;  // reuse
            } else {
              rng_high_val = Generate_Expression (choice->range.high);
            }

            // Coerce range bounds to loop index type.                                              
            // RM 4.3.2: expression is evaluated ONCE PER                                           
            // COMPONENT - Generate_Expression goes inside                                          
            // the loop body.                                                                       
            //                                                                                      
            const char *agg_idx_type = Integer_Arith_Type ();
            rng_low_val = Emit_Coerce (rng_low_val, agg_idx_type);
            rng_high_val = Emit_Coerce (rng_high_val, agg_idx_type);

            // Update overall aggregate bounds tracking
            if (track_named_bounds) {
              uint32_t cl = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 cl, agg_idx_type, agg_bnd_lo_var);
              uint32_t lt = Emit_Temp ();
              Emit ("  %%t%u = icmp slt %s %%t%u, %%t%u\n",
                 lt, agg_idx_type, rng_low_val, cl);
              uint32_t nl = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nl, lt, agg_idx_type, rng_low_val, agg_idx_type, cl);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 agg_idx_type, nl, agg_bnd_lo_var);
              uint32_t ch = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 ch, agg_idx_type, agg_bnd_hi_var);
              uint32_t gt = Emit_Temp ();
              Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n",
                 gt, agg_idx_type, rng_high_val, ch);
              uint32_t nh = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nh, gt, agg_idx_type, rng_high_val, agg_idx_type, ch);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 agg_idx_type, nh, agg_bnd_hi_var);
            }

            // RM 4.3.2(3): non-null range choice bounds must                                       
            // belong to the index subtype.  Check at runtime.                                      
            // Only check when index_type has meaningful bounds                                     
            // (named subtype like STA, not anonymous ranges).                                      
            //                                                                                      
            if (agg_type->array.indices and
              agg_type->array.indices[0].index_type and
              agg_type->array.indices[0].index_type->low_bound.kind == BOUND_INTEGER and
              agg_type->array.indices[0].index_type->high_bound.kind == BOUND_INTEGER) {
              Type_Info *idx_t = agg_type->array.indices[0].index_type;
              int64_t is_lo = (int64_t)Type_Bound_Value (idx_t->low_bound);
              int64_t is_hi = (int64_t)Type_Bound_Value (idx_t->high_bound);

              // Skip check if bounds are full INTEGER range (no real subtype)
              if (is_lo != (int64_t)(-2147483648LL) or
                is_hi != (int64_t)2147483647LL) {

                // Only check for non-null ranges (lo <= hi)
                uint32_t null_cmp = Emit_Temp ();
                Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u"
                     "  ; null range?\n",
                   null_cmp, agg_idx_type, rng_low_val, rng_high_val);
                uint32_t skip_lbl = cg->label_id++;
                uint32_t chk_lbl  = cg->label_id++;
                Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                   null_cmp, skip_lbl, chk_lbl);
                cg->block_terminated = true;
                Emit_Label_Here (chk_lbl);
                Emit_Range_Check_With_Raise (rng_low_val,
                  is_lo, is_hi, agg_idx_type,
                  "dynamic aggregate index subtype check");
                Emit_Range_Check_With_Raise (rng_high_val,
                  is_lo, is_hi, agg_idx_type,
                  "dynamic aggregate index subtype check");
                Emit ("  br label %%L%u\n", skip_lbl);
                cg->block_terminated = true;
                Emit_Label_Here (skip_lbl);
              }
            }
            Type_Info *elem_ti = agg_type->array.element_type;
            bool ecomp = Agg_Elem_Is_Composite (elem_ti, multidim);
            uint32_t loop_var = Emit_Temp ();
            Emit ("  %%t%u = alloca %s\n", loop_var, agg_idx_type);
            Emit ("  store %s %%t%u, ptr %%t%u\n", agg_idx_type, rng_low_val, loop_var);
            uint32_t loop_start = cg->label_id++;
            uint32_t loop_body = cg->label_id++;
            uint32_t loop_end = cg->label_id++;
            Emit ("  br label %%L%u\n", loop_start);
            cg->block_terminated = true;
            Emit_Label_Here (loop_start);
            uint32_t cur_idx = Emit_Temp ();
            Emit ("  %%t%u = load %s, ptr %%t%u\n", cur_idx, agg_idx_type, loop_var);
            uint32_t cmp = Emit_Temp ();
            Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp, agg_idx_type, cur_idx, rng_high_val);
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp, loop_body, loop_end);
            cg->block_terminated = true;
            Emit_Label_Here (loop_body);

            // RM 4.3.2(6): expression evaluated once per component
            uint32_t val = Agg_Resolve_Elem (item->association.expression, multidim,
              ecomp, agg_type, elem_type, elem_ti);

            // RM 4.3.2(6): inner consistency tracking for named range
            if (check_inner_consistency and cg->inner_agg_bnd_n > 0)
              Emit_Inner_Consistency_Track (dyn_inner_trk_lo, dyn_inner_trk_hi,
                dyn_inner_trk_first, dyn_inner_trk_mm,
                dyn_n_inner_dims,
                Array_Bound_Llvm_Type (agg_type));
            uint32_t arr_idx = Emit_Temp ();
            Emit ("  %%t%u = sub %s %%t%u, %%t%u\n",
               arr_idx, agg_idx_type, cur_idx, low_val);
            Agg_Store_At_Dynamic (base, val, arr_idx,
              agg_idx_type, elem_type, elem_size, rt_row_size, ecomp);

            // Increment and loop
            uint32_t next_idx = Emit_Temp ();
            Emit ("  %%t%u = add %s %%t%u, 1\n", next_idx, agg_idx_type, cur_idx);
            Emit ("  store %s %%t%u, ptr %%t%u\n", agg_idx_type, next_idx, loop_var);
            Emit ("  br label %%L%u\n", loop_start);
            cg->block_terminated = true;
            Emit_Label_Here (loop_end);
            cg->block_terminated = false;

          // Single-index named association: INDEX => expr.
          // Evaluate the index, compute offset, store once.
          } else {
            uint32_t idx_val = Generate_Expression (choice);
            const char *agg_idx_type = Integer_Arith_Type ();
            idx_val = Emit_Coerce (idx_val, agg_idx_type);

            // Update overall aggregate bounds for single-index
            if (track_named_bounds) {
              uint32_t cl = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 cl, agg_idx_type, agg_bnd_lo_var);
              uint32_t lt = Emit_Temp ();
              Emit ("  %%t%u = icmp slt %s %%t%u, %%t%u\n",
                 lt, agg_idx_type, idx_val, cl);
              uint32_t nl = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nl, lt, agg_idx_type, idx_val, agg_idx_type, cl);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 agg_idx_type, nl, agg_bnd_lo_var);
              uint32_t ch = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 ch, agg_idx_type, agg_bnd_hi_var);
              uint32_t gt = Emit_Temp ();
              Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n",
                 gt, agg_idx_type, idx_val, ch);
              uint32_t nh = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nh, gt, agg_idx_type, idx_val, agg_idx_type, ch);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 agg_idx_type, nh, agg_bnd_hi_var);
            }
            Type_Info *elem_ti = agg_type->array.element_type;
            bool ecomp = Agg_Elem_Is_Composite (elem_ti, multidim);
            uint32_t val = Agg_Resolve_Elem (item->association.expression, multidim,
              ecomp, agg_type, elem_type, elem_ti);

            // RM 4.3.2(6): inner sub-aggregate bounds consistency
            if (check_inner_consistency and cg->inner_agg_bnd_n > 0)
              Emit_Inner_Consistency_Track (dyn_inner_trk_lo, dyn_inner_trk_hi,
                dyn_inner_trk_first, dyn_inner_trk_mm,
                dyn_n_inner_dims,
                Array_Bound_Llvm_Type (agg_type));
            uint32_t arr_idx = Emit_Temp ();
            Emit ("  %%t%u = sub %s %%t%u, %%t%u\n",
               arr_idx, agg_idx_type, idx_val, low_val);
            Agg_Store_At_Dynamic (base, val, arr_idx,
              agg_idx_type, elem_type, elem_size, rt_row_size, ecomp);
          }
        }
      }

      // RM 4.3.2: post-loop check - overall aggregate choice bounds                                
      // must match the constraint bounds for constrained arrays                                    
      // without OTHERS.  Applies even for null arrays.                                             
      //                                                                                            
      if (track_named_bounds) {
        const char *ait = Integer_Arith_Type ();
        uint32_t fl = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n",
           fl, ait, agg_bnd_lo_var);
        uint32_t fh = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n",
           fh, ait, agg_bnd_hi_var);
        uint32_t ne_lo = Emit_Temp ();
        Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
           ne_lo, ait, fl, low_val);
        uint32_t ne_hi = Emit_Temp ();
        Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
           ne_hi, ait, fh, high_val);
        uint32_t mismatch = Emit_Temp ();
        Emit ("  %%t%u = or i1 %%t%u, %%t%u\n",
           mismatch, ne_lo, ne_hi);
        uint32_t ok_lbl = cg->label_id++;
        uint32_t fail_lbl = cg->label_id++;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           mismatch, fail_lbl, ok_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (fail_lbl);
        Emit_Raise_Constraint_Error ("aggregate named range vs constraint");
        Emit_Label_Here (ok_lbl);
      }

      // For multidim constrained arrays, also check inner dimension                                
      // bounds from the first inner sub-aggregate's named range                                    
      // against the type's inner constraint.  Runs OUTSIDE the outer                               
      // loop so null outer ranges still get checked (RM 4.3.2).                                    
      //                                                                                            
      if (agg_type->array.is_constrained and multidim and
        agg_ndims > 1 and ac.has_named and not ac.has_others) {
        for (uint32_t ai = 0; ai < node->aggregate.items.count; ai++) {
          Syntax_Node *aitem = node->aggregate.items.items[ai];
          if (aitem->kind != NK_ASSOCIATION) continue;
          if (aitem->association.choices.count > 0 and
            Is_Others_Choice (aitem->association.choices.items[0]))
            continue;
          Syntax_Node *inner = aitem->association.expression;
          if (not inner or inner->kind != NK_AGGREGATE) continue;

          // Find first named range in inner aggregate
          for (uint32_t ii = 0; ii < inner->aggregate.items.count; ii++) {
            Syntax_Node *iit = inner->aggregate.items.items[ii];
            if (iit->kind != NK_ASSOCIATION or
              iit->association.choices.count == 0) continue;
            Syntax_Node *ich = iit->association.choices.items[0];
            if (Is_Others_Choice (ich)) continue;
            if (ich->kind != NK_RANGE) continue;
            const char *ait = Integer_Arith_Type ();
            uint32_t ir_lo = Generate_Expression (ich->range.low);
            ir_lo = Emit_Coerce (ir_lo, ait);
            uint32_t ir_hi = Generate_Expression (ich->range.high);
            ir_hi = Emit_Coerce (ir_hi, ait);
            uint32_t exp_lo = Emit_Single_Bound (&dim_lo[1], ait);
            uint32_t exp_hi = Emit_Single_Bound (&dim_hi[1], ait);
            uint32_t ilo_ne = Emit_Temp ();
            Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
               ilo_ne, ait, ir_lo, exp_lo);
            uint32_t ihi_ne = Emit_Temp ();
            Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
               ihi_ne, ait, ir_hi, exp_hi);
            uint32_t imm = Emit_Temp ();
            Emit ("  %%t%u = or i1 %%t%u, %%t%u\n",
               imm, ilo_ne, ihi_ne);
            uint32_t iok = cg->label_id++;
            uint32_t ifail = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               imm, ifail, iok);
            cg->block_terminated = true;
            Emit_Label_Here (ifail);
            Emit_Raise_Constraint_Error ("inner aggregate named range vs constraint");
            Emit_Label_Here (iok);
            goto inner_dim_check_done;
          }
          break;  // only check first non-others association
        }
        inner_dim_check_done: ;
      }

      // RM 4.3.3(5): OTHERS fills positions not covered by positional
      // or named associations.  Skip the first n_positional slots.
      if (has_others) {
        Type_Info *elem_ti = agg_type->array.element_type;
        bool ecomp = Agg_Elem_Is_Composite (elem_ti, multidim);

        // Start index = low + n_positional (skip already-filled slots)
        const char *oth_idx_type = Integer_Arith_Type ();
        uint32_t start_val;
        if (n_positional > 0) {
          start_val = Emit_Temp ();
          Emit ("  %%t%u = add %s %%t%u, %u  ; skip %u positional\n",
             start_val, oth_idx_type, low_val, n_positional, n_positional);
        } else {
          start_val = low_val;
        }

        // Find the OTHERS expression for re-evaluation (RM 4.3.3(5))
        Syntax_Node *oth_expr = NULL;
        for (uint32_t oi = 0; oi < node->aggregate.items.count; oi++) {
          Syntax_Node *oitem = node->aggregate.items.items[oi];
          if (oitem->kind == NK_ASSOCIATION and oitem->association.choices.count > 0 and
            Is_Others_Choice (oitem->association.choices.items[0])) {
            oth_expr = oitem->association.expression;
            break;
          }
        }
        uint32_t loop_var = Emit_Temp ();
        Emit ("  %%t%u = alloca %s\n", loop_var, oth_idx_type);
        Emit ("  store %s %%t%u, ptr %%t%u\n", oth_idx_type, start_val, loop_var);
        uint32_t loop_start = cg->label_id++;
        uint32_t loop_body = cg->label_id++;
        uint32_t loop_end = cg->label_id++;
        Emit ("  br label %%L%u\n", loop_start);
        cg->block_terminated = true;
        Emit_Label_Here (loop_start);
        uint32_t cur_idx = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n", cur_idx, oth_idx_type, loop_var);
        uint32_t cmp = Emit_Temp ();
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp, oth_idx_type, cur_idx, high_val);
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp, loop_body, loop_end);
        cg->block_terminated = true;
        Emit_Label_Here (loop_body);

        // RM 4.3.3(5): re-evaluate per component
        uint32_t loop_others_val = oth_expr
          ? Agg_Resolve_Elem (oth_expr, multidim,
              ecomp, agg_type, elem_type, elem_ti)
          : others_val;
        uint32_t arr_idx = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n",
           arr_idx, oth_idx_type, cur_idx, low_val);
        Agg_Store_At_Dynamic (base, loop_others_val, arr_idx,
          oth_idx_type, elem_type, elem_size, rt_row_size, ecomp);
        uint32_t next_idx = Emit_Temp ();
        Emit ("  %%t%u = add %s %%t%u, 1\n", next_idx, oth_idx_type, cur_idx);
        Emit ("  store %s %%t%u, ptr %%t%u\n", oth_idx_type, next_idx, loop_var);
        Emit ("  br label %%L%u\n", loop_start);
        cg->block_terminated = true;
        Emit_Label_Here (loop_end);
        cg->block_terminated = false;
      }

      // RM 4.3.2(6): post-loop inner consistency mismatch check
      if (check_inner_consistency) {
        uint32_t mm_val = Emit_Temp ();
        Emit ("  %%t%u = load i1, ptr %%t%u\n",
           mm_val, dyn_inner_trk_mm);
        uint32_t ok_lbl = cg->label_id++;
        uint32_t fail_lbl = cg->label_id++;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           mm_val, fail_lbl, ok_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (fail_lbl);
        Emit_Raise_Constraint_Error ("inner sub-aggregate bounds mismatch (dynamic path)");
        Emit_Label_Here (ok_lbl);
      }

      // Propagate inner dimension bounds to parent for deep nesting
      if (cg->in_agg_component > 0 and check_inner_consistency) {
        const char *bt = Array_Bound_Llvm_Type (agg_type);
        int dim_count = 1;  // bnd[0] already set from early reporting
        cg->inner_agg_bnd_lo[0] = low_val;
        cg->inner_agg_bnd_hi[0] = high_val;
        for (int d = 0; d < dyn_n_inner_dims and dim_count < MAX_AGG_DIMS; d++) {
          cg->inner_agg_bnd_lo[dim_count] = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u  ; dyn deep inner lo [dim %d]\n",
             cg->inner_agg_bnd_lo[dim_count], bt, dyn_inner_trk_lo[d], d);
          cg->inner_agg_bnd_hi[dim_count] = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u  ; dyn deep inner hi [dim %d]\n",
             cg->inner_agg_bnd_hi[dim_count], bt, dyn_inner_trk_hi[d], d);
          dim_count++;
        }
        cg->inner_agg_bnd_n = dim_count;
      }

      // For dynamic bounds arrays, return a fat pointer { ptr, ptr }                               
      // where the bounds pointer contains ALL dimension bounds in flat                             
      // layout [lo0, hi0, lo1, hi1, ...] so multi-dim indexing works.                              
      //                                                                                            
      uint32_t fat_ptr = Emit_Temp ();
      {
        const char *agg_bt = Array_Bound_Llvm_Type (agg_type);
        Emit ("  %%t%u = alloca " FAT_PTR_TYPE "  ; dynamic array fat ptr\n", fat_ptr);
        Temp_Set_Type (fat_ptr, "ptr");
        Temp_Mark_Fat_Alloca (fat_ptr);
        if (multidim and agg_ndims > 1) {
          uint32_t md_lo[8], md_hi[8];
          md_lo[0] = low_val;
          md_hi[0] = high_val;
          for (uint32_t d = 1; d < agg_ndims; d++) {
            md_lo[d] = rt_inner_lo[d] ? rt_inner_lo[d]
                 : Emit_Static_Int (Type_Bound_Value (dim_lo[d]), iat_bnd);
            md_hi[d] = rt_inner_hi[d] ? rt_inner_hi[d]
                 : Emit_Static_Int (Type_Bound_Value (dim_hi[d]), iat_bnd);
          }
          uint32_t bounds_alloca = Emit_Alloc_Bounds_MultiDim (md_lo, md_hi, agg_ndims, agg_bt);
          uint32_t ds = Emit_Temp ();
          Emit ("  %%t%u = getelementptr " FAT_PTR_TYPE ", ptr %%t%u, i32 0, i32 0\n", ds, fat_ptr);
          Emit ("  store ptr %%t%u, ptr %%t%u\n", base, ds);
          uint32_t bs = Emit_Temp ();
          Emit ("  %%t%u = getelementptr " FAT_PTR_TYPE ", ptr %%t%u, i32 0, i32 1\n", bs, fat_ptr);
          Emit ("  store ptr %%t%u, ptr %%t%u\n", bounds_alloca, bs);
        } else {
          Emit_Store_Fat_Pointer_Fields_To_Temp (base, low_val, high_val, fat_ptr, agg_bt);
        }
      }
      return fat_ptr;
    }

    // Static bounds: use compile-time allocation and unrolled initialization
    uint32_t base = Emit_Temp ();
    int128_t low = Type_Bound_Value (low_bound);
    int128_t high = Type_Bound_Value (high_bound);
    int128_t count = high - low + 1;
    if (count < 1) count = 1;  // Ensure at least 1 element for safety

    // Report bounds to outer multidim aggregate for consistency check
    if (cg->in_agg_component > 0) {
      const char *ait = Integer_Arith_Type ();
      cg->inner_agg_bnd_lo[0] = Emit_Static_Int (low, ait);
      cg->inner_agg_bnd_hi[0] = Emit_Static_Int (high, ait);
      cg->inner_agg_bnd_n = 1;
    }

    // RM 4.3.2(3): index subtype bounds for aggregate constraint checking.                         
    // Each choice bound of a non-null range must belong to the index                               
    // subtype.  This check applies when:                                                           
    //   (a) the type is unconstrained (choices define aggregate bounds),                           
    //   (b) it's a constrained subtype of an unconstrained base                                    
    //       (T IS BASE (5..7) where BASE IS ARRAY (ST RANGE <>)),                                  
    //   (c) directly constrained with named index type                                             
    //       (ARRAY (STA RANGE 5..6, ...) where STA IS INTEGER RANGE 4..7);                         
    //       choice bounds must belong to the index type (STA = 4..7).                              
    //                                                                                              
    bool has_unconstrained_base = agg_type->base_type and
      Type_Is_Array_Like (agg_type->base_type) and
      not agg_type->base_type->array.is_constrained;
    bool need_idx_subtype_check = not agg_type->array.is_constrained or
                    has_unconstrained_base;
    int128_t idx_sub_lo = low, idx_sub_hi = high;
    if (has_unconstrained_base and
      agg_type->base_type->array.index_count > 0 and
      agg_type->base_type->array.indices and
      agg_type->base_type->array.indices[0].index_type) {
      idx_sub_lo = Type_Bound_Value (
        agg_type->base_type->array.indices[0].index_type->low_bound);
      idx_sub_hi = Type_Bound_Value (
        agg_type->base_type->array.indices[0].index_type->high_bound);
    } else if (not agg_type->array.is_constrained and
           agg_type->array.index_count > 0 and
           agg_type->array.indices and
           agg_type->array.indices[0].index_type) {

      // Directly unconstrained type: get index subtype from index_type
      idx_sub_lo = Type_Bound_Value (
        agg_type->array.indices[0].index_type->low_bound);
      idx_sub_hi = Type_Bound_Value (
        agg_type->array.indices[0].index_type->high_bound);
    }

    // RM 4.3.2(3): for named aggregates of unconstrained types, the                                
    // bounds are determined by the choices.  The lower bound is the                                
    // minimum of all range-low values; the upper bound is the maximum                              
    // of all range-high values.  For a null range (L..H where L>H),                                
    // the bounds stay L..H because we track lows and highs separately.                             
    //                                                                                              
    bool has_choice_lo = false, has_choice_hi = false;
    bool early_has_others = false;
    bool has_dynamic_choice = false;  // non-integer range bounds
    int128_t choice_lo = 0, choice_hi = 0;
    for (uint32_t ci = 0; ci < node->aggregate.items.count; ci++) {
      Syntax_Node *cit = node->aggregate.items.items[ci];
      if (cit->kind != NK_ASSOCIATION) continue;
      for (uint32_t cc = 0; cc < cit->association.choices.count; cc++) {
        Syntax_Node *ch = cit->association.choices.items[cc];
        if (Is_Others_Choice (ch)) { early_has_others = true; continue; }
        if (ch->kind == NK_RANGE) {
          if (Is_Static_Int_Node (ch->range.low)) {
            int128_t v = Static_Int_Value (ch->range.low);
            if (not has_choice_lo or v < choice_lo) choice_lo = v;
            has_choice_lo = true;
          } else {
            has_dynamic_choice = true;
          }
          if (Is_Static_Int_Node (ch->range.high)) {
            int128_t v = Static_Int_Value (ch->range.high);
            if (not has_choice_hi or v > choice_hi) choice_hi = v;
            has_choice_hi = true;
          } else {
            has_dynamic_choice = true;
          }
        } else if (Is_Static_Int_Node (ch)) {
          int128_t v = Static_Int_Value (ch);
          if (not has_choice_lo or v < choice_lo) choice_lo = v;
          has_choice_lo = true;
          if (not has_choice_hi or v > choice_hi) choice_hi = v;
          has_choice_hi = true;
        }
      }
    }

    // Dynamic choice bounds (e.g. T'RANGE): find the first NK_RANGE                                
    // choice with non-integer bounds and use its expressions as                                    
    // the aggregate bounds for the dynamic path.                                                   
    //                                                                                              
    if (has_dynamic_choice and not early_has_others) {
      for (uint32_t ci = 0; ci < node->aggregate.items.count and dynamic_bounds; ci++) {
        Syntax_Node *cit = node->aggregate.items.items[ci];
        if (cit->kind != NK_ASSOCIATION) continue;
        for (uint32_t cc = 0; cc < cit->association.choices.count; cc++) {
          Syntax_Node *ch = cit->association.choices.items[cc];
          if (ch->kind == NK_RANGE) {
            if (ch->range.low->kind != NK_INTEGER) {
              dim_lo[0] = (Type_Bound){.kind = BOUND_EXPR, .expr = ch->range.low};
              low_bound = dim_lo[0];
            }
            if (ch->range.high->kind != NK_INTEGER) {
              dim_hi[0] = (Type_Bound){.kind = BOUND_EXPR, .expr = ch->range.high};
              high_bound = dim_hi[0];
            }
            break;
          }
        }
        break;
      }

    // RM 4.3.2(5): Named aggregate bounds are determined by the                                    
    // lowest and highest choices.  For aggregates without OTHERS,                                  
    // the aggregate storage uses choice bounds (sliding occurs at                                  
    // assignment for constrained targets).  With OTHERS, the                                       
    // aggregate must cover the full type range.                                                    
    //                                                                                              
    } else if (has_choice_lo and has_choice_hi and not early_has_others) {
      low = choice_lo;
      high = choice_hi;
      count = high - low + 1;
      if (count < 1) count = 1;
    }

    // RM 4.3.2(6): For constrained array subtypes, the aggregate bounds                            
    // must match the constraint bounds.  For positional aggregates, the                            
    // lower bound is INDEX_SUBTYPE'FIRST (from the base unconstrained                              
    // type), which may differ from the constraint.  For named aggregates                           
    // without OTHERS, the bounds come from the choices.                                            
    //                                                                                              
    if (agg_type->array.is_constrained and
      agg_type->array.indices[0].low_bound.kind == BOUND_INTEGER and
      agg_type->array.indices[0].high_bound.kind == BOUND_INTEGER) {
      int128_t con_lo = Type_Bound_Value (agg_type->array.indices[0].low_bound);
      int128_t con_hi = Type_Bound_Value (agg_type->array.indices[0].high_bound);

      // RM 4.3.2(5): Count must match constraint size.
      if (n_positional > 0 and not has_named and has_unconstrained_base) {
        int128_t expected = con_hi - con_lo + 1;
        if ((int128_t)n_positional != expected) {
          Emit_Raise_Constraint_Error ("positional aggregate count vs constraint");
          uint32_t cont = cg->label_id++;
          Emit_Label_Here (cont);
        }

        // RM 4.3.2: A parenthesized aggregate ((a,b,c)) uses                                       
        // INDEX_SUBTYPE'FIRST as lower bound, which may differ from                                
        // the constraint.  Detect and raise CONSTRAINT_ERROR.                                      
        //                                                                                          
        if (node->aggregate.is_parenthesized and agg_type->base_type) {
          Type_Info *base = agg_type->base_type;
          if (base->array.index_count > 0 and
            base->array.indices[0].index_type) {
            Type_Info *idx_ty = base->array.indices[0].index_type;
            if (idx_ty->low_bound.kind == BOUND_INTEGER) {
              if (idx_ty->low_bound.int_value != con_lo) {
                Emit_Raise_Constraint_Error ("parenthesized aggregate bounds vs constraint");
                uint32_t cont = cg->label_id++;
                Emit_Label_Here (cont);
              }

            // Dynamic INDEX_SUBTYPE'FIRST: runtime comparison
            } else {
              const char *ait = Integer_Arith_Type ();
              uint32_t isf = Emit_Bound_Value (&idx_ty->low_bound);
              isf = Emit_Coerce (isf, ait);
              uint32_t clo_v = Emit_Static_Int (con_lo, ait);
              uint32_t ne = Emit_Temp ();
              Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                 ne, ait, isf, clo_v);
              uint32_t ok = cg->label_id++;
              uint32_t fail = cg->label_id++;
              Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                 ne, fail, ok);
              cg->block_terminated = true;
              Emit_Label_Here (fail);
              Emit_Raise_Constraint_Error ("parenthesized aggregate bounds vs constraint (dyn idx)");
              Emit_Label_Here (ok);
            }
          }
        }
      }
      if (has_choice_lo and has_choice_hi and not early_has_others
        and has_unconstrained_base and agg_ndims == 1) {

        // Named aggregate of constrained 1-D subtype of unconstrained                              
        // base: choice bounds must match the constraint (array-of-array                            
        // component).  For multidim arrays (agg_ndims > 1), sliding                                
        // applies at assignment per RM 5.2.1.                                                      
        //                                                                                          
        if (low != con_lo or high != con_hi) {
          Emit_Raise_Constraint_Error ("named aggregate bounds vs constraint");
          uint32_t cont = cg->label_id++;
          Emit_Label_Here (cont);
        }
      }
    }

    // RM 4.3.2(6): Dynamic constraint check in the static path.                                    
    // Positional overrides may convert BOUND_EXPR → BOUND_INTEGER, routing                         
    // through the static path even though the TYPE's constraint is dynamic.                        
    // Detect this and emit a runtime check against the original bounds.                            
    // Skip when bounds reference discriminants whose disc_agg_temp                                 
    // hasn't been set up yet (not safely evaluable; RM 3.7.1).                                     
    //                                                                                              
    {
      bool bounds_ref_disc_s = false;
      for (uint32_t d = 0; d < agg_ndims and not bounds_ref_disc_s; d++) {
        Type_Bound *b[2] = {&agg_type->array.indices[d].low_bound,
                  &agg_type->array.indices[d].high_bound};
        for (int bi = 0; bi < 2; bi++) {
          if (b[bi]->kind == BOUND_EXPR and b[bi]->expr and
            b[bi]->expr->symbol and
            b[bi]->expr->symbol->kind == SYMBOL_DISCRIMINANT and
            b[bi]->expr->symbol->disc_agg_temp == 0)
            bounds_ref_disc_s = true;

          // RM 3.2.1: Also skip when BOUND_EXPR has no cached                                      
          // temp - the expression would be re-evaluated with                                       
          // stale side effects (e.g., F(I) after elaboration).                                     
          //                                                                                        
          if (b[bi]->kind == BOUND_EXPR and b[bi]->expr and
            not b[bi]->cached_temp and
            (not b[bi]->expr->symbol or
             b[bi]->expr->symbol->kind != SYMBOL_DISCRIMINANT))
            bounds_ref_disc_s = true;
        }
      }
    if (agg_type->array.is_constrained and not bounds_ref_disc_s and
      (agg_type->array.indices[0].low_bound.kind == BOUND_EXPR or
       agg_type->array.indices[0].high_bound.kind == BOUND_EXPR)) {
      bool has_unc_base_d = agg_type->base_type and
        Type_Is_Array_Like (agg_type->base_type) and
        not agg_type->base_type->array.is_constrained;
      if (has_unc_base_d) {
        const char *ait = Integer_Arith_Type ();
        if (n_positional > 0 and not has_named) {
          uint32_t clo = Emit_Bound_Value (&agg_type->array.indices[0].low_bound);
          clo = Emit_Coerce (clo, ait);
          uint32_t chi = Emit_Bound_Value (&agg_type->array.indices[0].high_bound);
          chi = Emit_Coerce (chi, ait);
          {

            // RM 4.3.2(5): For a positional aggregate of a constrained                             
            // subtype (whether sub-aggregate or direct), only the count                            
            // must match the constraint - the lower bound is determined                            
            // by the constraint, not by INDEX_SUBTYPE'FIRST.                                       
            //                                                                                      
            uint32_t n_pos = Emit_Static_Int ((int128_t)n_positional, ait);
            uint32_t cdiff = Emit_Temp ();
            Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", cdiff, ait, chi, clo);
            uint32_t ccnt = Emit_Temp ();
            Emit ("  %%t%u = add %s %%t%u, 1\n", ccnt, ait, cdiff);
            uint32_t mismatch = Emit_Temp ();
            Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", mismatch, ait, n_pos, ccnt);
            uint32_t ok_lbl = cg->label_id++;
            uint32_t fail_lbl = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               mismatch, fail_lbl, ok_lbl);
            cg->block_terminated = true;
            Emit_Label_Here (fail_lbl);
            Emit_Raise_Constraint_Error ("positional aggregate count vs dynamic constraint");
            Emit_Label_Here (ok_lbl);
          }

          // Parenthesized: INDEX_SUBTYPE'FIRST vs runtime constraint
          if (node->aggregate.is_parenthesized and agg_type->base_type and
            agg_type->base_type->array.index_count > 0 and
            agg_type->base_type->array.indices[0].index_type) {
            Type_Info *idx_ty = agg_type->base_type->array.indices[0].index_type;
            uint32_t isf = (idx_ty->low_bound.kind == BOUND_INTEGER)
              ? Emit_Static_Int (idx_ty->low_bound.int_value, ait)
              : Emit_Coerce (Emit_Bound_Value (&idx_ty->low_bound), ait);
            uint32_t ne = Emit_Temp ();
            Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
               ne, ait, isf, clo);
            uint32_t ok2 = cg->label_id++;
            uint32_t fail2 = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               ne, fail2, ok2);
            cg->block_terminated = true;
            Emit_Label_Here (fail2);
            Emit_Raise_Constraint_Error ("parenthesized aggregate bounds vs dynamic constraint (static path)");
            Emit_Label_Here (ok2);
          }
        }
        if (has_choice_lo and has_choice_hi and not early_has_others
          and agg_ndims == 1) {
          uint32_t nclo = Emit_Static_Int (choice_lo, ait);
          uint32_t nchi = Emit_Static_Int (choice_hi, ait);
          uint32_t clo = Emit_Bound_Value (&agg_type->array.indices[0].low_bound);
          clo = Emit_Coerce (clo, ait);
          uint32_t chi = Emit_Bound_Value (&agg_type->array.indices[0].high_bound);
          chi = Emit_Coerce (chi, ait);
          uint32_t ne_lo = Emit_Temp ();
          Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", ne_lo, ait, nclo, clo);
          uint32_t ne_hi = Emit_Temp ();
          Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", ne_hi, ait, nchi, chi);
          uint32_t mismatch = Emit_Temp ();
          Emit ("  %%t%u = or i1 %%t%u, %%t%u\n", mismatch, ne_lo, ne_hi);
          uint32_t ok_lbl = cg->label_id++;
          uint32_t fail_lbl = cg->label_id++;
          Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
             mismatch, fail_lbl, ok_lbl);
          cg->block_terminated = true;
          Emit_Label_Here (fail_lbl);
          Emit_Raise_Constraint_Error ("named aggregate bounds vs dynamic constraint");
          Emit_Label_Here (ok_lbl);
        }
      }
    }
    }  // end bounds_ref_disc_s scope

    // For unconstrained types, track LLVM SSA values for the aggregate's                           
    // actual bounds (from choice expressions) to populate the fat pointer.                         
    // These are set during range-choice processing below.                                          
    //                                                                                              
    uint32_t agg_lo_ssa = 0, agg_hi_ssa = 0;
    uint32_t agg_d1_lo_ssa = 0, agg_d1_hi_ssa = 0;  // dim 1 inner choice SSA

    // Check if element type is composite (record or constrained array).                            
    // For composite elements, Generate_Expression returns a ptr to an alloca,                      
    // so we must use memcpy to copy element data instead of store.                                 
    // Multi-dimensional arrays are always composite at the outer level                             
    // because each "element" is a row (inner array).                                               
    // Exception: fat-pointer elements ({ ptr, ptr }) are stored via                                
    // `store` like scalars, not via memcpy.                                                        
    //                                                                                              
    Type_Info *elem_ti = agg_type->array.element_type;
    bool elem_is_composite = multidim or (elem_ti and (Type_Is_Record (elem_ti) or
      Type_Is_Constrained_Array (elem_ti)));

    // Fat-pointer elements are value-typed { ptr, ptr }; store, don't memcpy.
    if (elem_is_fat) elem_is_composite = false;

    // Allocate as byte array so the size matches the actual data layout
    if (elem_is_composite) {
      int128_t total_bytes = count * (int128_t)elem_size;
      Emit ("  %%t%u = alloca [%s x i8]  ; array aggregate (composite elems)\n",
         base, I128_Decimal (total_bytes));
    } else {
      Emit ("  %%t%u = alloca [%s x %s]  ; array aggregate\n",
         base, I128_Decimal (count), elem_type);
    }

    // Track which elements are initialized (for others clause)
    bool *initialized = Arena_Allocate ((size_t)count * sizeof (bool));
    for (int128_t i = 0; i < count; i++) initialized[i] = false;

    // Default value for "others" clause (if any)
    uint32_t others_val = 0;
    bool has_others = false;
    Syntax_Node *others_item_expr = NULL;  // RM 4.3.3(5): re-evaluate per component

    // First pass: find "others" clause and save expression node
    for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
      Syntax_Node *item = node->aggregate.items.items[i];
      if (item->kind == NK_ASSOCIATION and item->association.choices.count > 0) {
        if (Is_Others_Choice (item->association.choices.items[0])) {
          others_item_expr = item->association.expression;
          has_others = true;
          break;
        }
      }
    }

    // RM 4.3.2(6): For constrained arrays with dynamic named choices                               
    // and no OTHERS, track actual evaluated bounds at runtime to compare                           
    // against the constraint.  Static choices are handled above.                                   
    //                                                                                              
    uint32_t s_bnd_lo_var = 0, s_bnd_hi_var = 0;
    bool s_track_dyn = agg_type->array.is_constrained and
      has_dynamic_choice and not has_others and has_named;
    if (s_track_dyn) {
      const char *bt = Array_Bound_Llvm_Type (agg_type);
      s_bnd_lo_var = Emit_Temp ();
      Emit ("  %%t%u = alloca %s  ; track named min-low\n",
         s_bnd_lo_var, bt);
      uint32_t il = Emit_Static_Int (2147483647LL, bt);
      Emit ("  store %s %%t%u, ptr %%t%u\n", bt, il, s_bnd_lo_var);
      s_bnd_hi_var = Emit_Temp ();
      Emit ("  %%t%u = alloca %s  ; track named max-high\n",
         s_bnd_hi_var, bt);
      uint32_t ih = Emit_Static_Int ((int128_t)-2147483648LL, bt);
      Emit ("  store %s %%t%u, ptr %%t%u\n", bt, ih, s_bnd_hi_var);
    }

    // RM 4.3.2(6): For multidim aggregates, track inner sub-aggregate                              
    // bounds for consistency checking across rows.  The check is                                   
    // deferred so all sub-aggregates are evaluated (for side effects)                              
    // before raising CONSTRAINT_ERROR.                                                             
    //                                                                                              
    uint32_t inner_trk_lo[MAX_AGG_DIMS] = {0};
    uint32_t inner_trk_hi[MAX_AGG_DIMS] = {0};
    uint32_t inner_trk_first = 0, inner_trk_mm = 0;
    int n_inner_dims = 0;
    bool check_inner_consistency = multidim and agg_ndims > 1
      and not has_others;
    if (check_inner_consistency) {
      n_inner_dims = agg_ndims - 1;
      if (n_inner_dims > MAX_AGG_DIMS) n_inner_dims = MAX_AGG_DIMS;
      const char *bt = Array_Bound_Llvm_Type (agg_type);
      for (int d = 0; d < n_inner_dims; d++) {
        inner_trk_lo[d] = Emit_Temp ();
        Emit ("  %%t%u = alloca %s  ; expected inner lo [dim %d]\n",
           inner_trk_lo[d], bt, d);
        inner_trk_hi[d] = Emit_Temp ();
        Emit ("  %%t%u = alloca %s  ; expected inner hi [dim %d]\n",
           inner_trk_hi[d], bt, d);
      }
      inner_trk_first = Emit_Temp ();
      Emit ("  %%t%u = alloca i1  ; first inner seen?\n",
         inner_trk_first);
      Emit ("  store i1 0, ptr %%t%u\n", inner_trk_first);
      inner_trk_mm = Emit_Temp ();
      Emit ("  %%t%u = alloca i1  ; inner mismatch flag\n",
         inner_trk_mm);
      Emit ("  store i1 0, ptr %%t%u\n", inner_trk_mm);
    }

    // Clear inner bounds before the association loop so that stale                                 
    // values from early bounds reporting don't cause false inner-                                  
    // consistency mismatches when an element is a non-aggregate                                    
    // (e.g. string literal).                                                                       
    //                                                                                              
    cg->inner_agg_bnd_n = 0;

    // Second pass: initialize elements
    uint32_t positional_idx = 0;
    for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
      Syntax_Node *item = node->aggregate.items.items[i];

      // Named association: handle each choice
      if (item->kind == NK_ASSOCIATION) {
        for (uint32_t c = 0; c < item->association.choices.count; c++) {
          Syntax_Node *choice = item->association.choices.items[c];
          if (Is_Others_Choice (choice)) {
            continue;  // Handle in third pass
          }

          // Range choice: 1..5 => value                                                            
          // RM 4.3.2: the expression is evaluated ONCE PER                                         
          // COMPONENT, so Generate_Expression must be called                                       
          // inside the per-element loop.                                                           
          //                                                                                        
          if (choice->kind == NK_RANGE) {
            int128_t rng_low, rng_high;
            uint32_t rng_lo_ssa = 0, rng_hi_ssa = 0;
            bool must_eval_low  = not Is_Static_Int_Node (choice->range.low);
            bool must_eval_high = not Is_Static_Int_Node (choice->range.high);
            if (not must_eval_low) {
              rng_low = Static_Int_Value (choice->range.low);
            } else {
              uint32_t ev = Generate_Expression (choice->range.low);
              rng_low = low;
              const char *bt = Array_Bound_Llvm_Type (agg_type);
              const char *st = Expression_Llvm_Type (choice->range.low);
              rng_lo_ssa = (strcmp (st, bt) != 0)
                ? Emit_Convert (ev, st, bt) : ev;
              if (not agg_type->array.is_constrained and agg_lo_ssa == 0)
                agg_lo_ssa = rng_lo_ssa;
            }
            if (not must_eval_high) {
              rng_high = Static_Int_Value (choice->range.high);
            } else {
              uint32_t ev = Generate_Expression (choice->range.high);
              rng_high = high;
              const char *bt = Array_Bound_Llvm_Type (agg_type);
              const char *st = Expression_Llvm_Type (choice->range.high);
              rng_hi_ssa = (strcmp (st, bt) != 0)
                ? Emit_Convert (ev, st, bt) : ev;
              if (not agg_type->array.is_constrained and agg_hi_ssa == 0)
                agg_hi_ssa = rng_hi_ssa;
            }

            // Track dynamic named range bounds for constraint check
            if (s_track_dyn) {
              const char *bt = Array_Bound_Llvm_Type (agg_type);
              uint32_t lo_v = rng_lo_ssa ? rng_lo_ssa
                : Emit_Static_Int (rng_low, bt);
              uint32_t hi_v = rng_hi_ssa ? rng_hi_ssa
                : Emit_Static_Int (rng_high, bt);

              // min(cur_lo, lo_v)
              uint32_t clo = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 clo, bt, s_bnd_lo_var);
              uint32_t lt = Emit_Temp ();
              Emit ("  %%t%u = icmp slt %s %%t%u, %%t%u\n",
                 lt, bt, lo_v, clo);
              uint32_t nlo = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nlo, lt, bt, lo_v, bt, clo);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 bt, nlo, s_bnd_lo_var);

              // max(cur_hi, hi_v)
              uint32_t chi = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 chi, bt, s_bnd_hi_var);
              uint32_t gt = Emit_Temp ();
              Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n",
                 gt, bt, hi_v, chi);
              uint32_t nhi = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nhi, gt, bt, hi_v, bt, chi);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 bt, nhi, s_bnd_hi_var);
            }

            // RM 4.3.2(3): for a non-null range, the bounds must                                   
            // belong to the index subtype.  Raise CONSTRAINT_ERROR                                 
            // if not.  Null ranges (lo > hi) are exempt.                                           
            //                                                                                      
            if (need_idx_subtype_check) {
              const char *bt = Array_Bound_Llvm_Type (agg_type);

              // Both bounds static: compile-time check
              if (not must_eval_low and not must_eval_high) {
                if (rng_low <= rng_high and
                  (rng_low < idx_sub_lo or rng_low > idx_sub_hi or
                   rng_high < idx_sub_lo or rng_high > idx_sub_hi)) {
                  Emit_Raise_Constraint_Error ("aggregate index check");
                  uint32_t cont = cg->label_id++;
                  Emit_Label_Here (cont);
                }

              // At least one expression bound: runtime check
              } else {
                uint32_t lo_s = rng_lo_ssa ? rng_lo_ssa
                        : Emit_Static_Int (rng_low, bt);
                uint32_t hi_s = rng_hi_ssa ? rng_hi_ssa
                        : Emit_Static_Int (rng_high, bt);

                // Is range non-null? (lo <= hi)
                uint32_t null_cmp = Emit_Temp ();
                Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u"
                     "  ; null range?\n",
                   null_cmp, bt, lo_s, hi_s);
                uint32_t skip_lbl = cg->label_id++;
                uint32_t chk_lbl  = cg->label_id++;
                Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                   null_cmp, skip_lbl, chk_lbl);
                cg->block_terminated = true;
                Emit_Label_Here (chk_lbl);
                Emit_Range_Check_With_Raise (lo_s,
                  (int64_t)idx_sub_lo, (int64_t)idx_sub_hi,
                  bt, "aggregate index subtype check");
                Emit_Range_Check_With_Raise (hi_s,
                  (int64_t)idx_sub_lo, (int64_t)idx_sub_hi,
                  bt, "aggregate index subtype check");
                Emit ("  br label %%L%u\n", skip_lbl);
                cg->block_terminated = true;
                Emit_Label_Here (skip_lbl);
              }
            }

            // Detect multidim inner aggregates with expression
            // bounds that must be evaluated exactly once.
            bool inline_multidim = false;
            if (multidim and item->association.expression->kind == NK_AGGREGATE) {
              Syntax_Node *ia = item->association.expression;
              for (uint32_t qi = 0; qi < ia->aggregate.items.count; qi++) {
                Syntax_Node *qit = ia->aggregate.items.items[qi];
                if (qit->kind != NK_ASSOCIATION) continue;
                for (uint32_t qc = 0; qc < qit->association.choices.count; qc++) {
                  Syntax_Node *qch = qit->association.choices.items[qc];
                  if (qch->kind == NK_RANGE) {
                    if (not Is_Static_Int_Node (qch->range.low))
                      inline_multidim = true;
                    if (not Is_Static_Int_Node (qch->range.high))
                      inline_multidim = true;
                  }
                }
              }
            }

            // Multidimensional aggregate with expression bounds:                                   
            // inline the inner aggregate.  RM 4.3.2(6): for                                        
            // (F..G => (H..I => J)), the inner bounds H,I are                                      
            // evaluated once; the value J is evaluated once per                                    
            // component (outer × inner, zero if null).                                             
            //                                                                                      
            if (inline_multidim) {
              Syntax_Node *inner_agg = item->association.expression;
              int128_t inner_low  = Type_Bound_Value (dim_lo[1]);
              int128_t inner_high = Type_Bound_Value (dim_hi[1]);
              int128_t inner_count = (inner_high >= inner_low)
                         ? (inner_high - inner_low + 1) : 0;

              // Evaluate inner choice bounds once for side effects,                                
              // even when the outer range is null (RM 4.3.2(6)).                                   
              // Also check inner bounds against dim 1 index subtype                                
              // and capture SSA values for fat pointer.                                            
              //                                                                                    
              int128_t inner_idx_sub_lo = inner_low, inner_idx_sub_hi = inner_high;
              if (agg_type->base_type and
                Type_Is_Array_Like (agg_type->base_type) and
                not agg_type->base_type->array.is_constrained and
                agg_type->base_type->array.index_count > 1 and
                agg_type->base_type->array.indices and
                agg_type->base_type->array.indices[1].index_type) {
                inner_idx_sub_lo = Type_Bound_Value (
                  agg_type->base_type->array.indices[1].index_type->low_bound);
                inner_idx_sub_hi = Type_Bound_Value (
                  agg_type->base_type->array.indices[1].index_type->high_bound);
              } else if (not agg_type->array.is_constrained and
                     agg_type->array.index_count > 1 and
                     agg_type->array.indices and
                     agg_type->array.indices[1].index_type) {
                inner_idx_sub_lo = Type_Bound_Value (
                  agg_type->array.indices[1].index_type->low_bound);
                inner_idx_sub_hi = Type_Bound_Value (
                  agg_type->array.indices[1].index_type->high_bound);
              }
              Syntax_Node *inner_val_expr = NULL;
              uint32_t inner_lo_ssa = 0, inner_hi_ssa = 0;
              for (uint32_t qi = 0; qi < inner_agg->aggregate.items.count; qi++) {
                Syntax_Node *qi_item = inner_agg->aggregate.items.items[qi];
                if (qi_item->kind != NK_ASSOCIATION) continue;
                if (not inner_val_expr)
                  inner_val_expr = qi_item->association.expression;
                for (uint32_t qc = 0; qc < qi_item->association.choices.count; qc++) {
                  Syntax_Node *qch = qi_item->association.choices.items[qc];
                  if (qch->kind == NK_RANGE) {
                    uint32_t ilo_s = 0, ihi_s = 0;
                    bool ilo_expr = not Is_Static_Int_Node (qch->range.low);
                    bool ihi_expr = not Is_Static_Int_Node (qch->range.high);
                    if (ilo_expr) {
                      uint32_t ev = Generate_Expression (qch->range.low);
                      const char *bt = Array_Bound_Llvm_Type (agg_type);
                      const char *st = Expression_Llvm_Type (qch->range.low);
                      ilo_s = (strcmp (st, bt) != 0)
                        ? Emit_Convert (ev, st, bt) : ev;
                      if (not inner_lo_ssa) inner_lo_ssa = ilo_s;
                      if (not agg_d1_lo_ssa) agg_d1_lo_ssa = ilo_s;
                    }
                    if (ihi_expr) {
                      uint32_t ev = Generate_Expression (qch->range.high);
                      const char *bt = Array_Bound_Llvm_Type (agg_type);
                      const char *st = Expression_Llvm_Type (qch->range.high);
                      ihi_s = (strcmp (st, bt) != 0)
                        ? Emit_Convert (ev, st, bt) : ev;
                      if (not inner_hi_ssa) inner_hi_ssa = ihi_s;
                      if (not agg_d1_hi_ssa) agg_d1_hi_ssa = ihi_s;
                    }

                    // RM 4.3.2(3): inner dim non-null range bounds
                    // must belong to dim 1 index subtype.
                    {
                      const char *bt = Array_Bound_Llvm_Type (agg_type);
                      if (not ilo_expr and not ihi_expr) {
                        int128_t rlo = Static_Int_Value (qch->range.low);
                        int128_t rhi = Static_Int_Value (qch->range.high);
                        if (rlo <= rhi and
                          (rlo < inner_idx_sub_lo or rlo > inner_idx_sub_hi or
                           rhi < inner_idx_sub_lo or rhi > inner_idx_sub_hi)) {
                          Emit_Raise_Constraint_Error ("aggregate inner index check");
                          uint32_t cont = cg->label_id++;
                          Emit_Label_Here (cont);
                        }
                      } else {
                        uint32_t lo_v = ilo_s ? ilo_s
                          : Emit_Static_Int (Static_Int_Value (qch->range.low), bt);
                        uint32_t hi_v = ihi_s ? ihi_s
                          : Emit_Static_Int (Static_Int_Value (qch->range.high), bt);
                        uint32_t nc = Emit_Temp ();
                        Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u"
                             "  ; inner null?\n", nc, bt, lo_v, hi_v);
                        uint32_t sk = cg->label_id++;
                        uint32_t ck = cg->label_id++;
                        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                           nc, sk, ck);
                        cg->block_terminated = true;
                        Emit_Label_Here (ck);
                        Emit_Range_Check_With_Raise (lo_v,
                          (int64_t)inner_idx_sub_lo, (int64_t)inner_idx_sub_hi,
                          bt, "aggregate inner index subtype check");
                        Emit_Range_Check_With_Raise (hi_v,
                          (int64_t)inner_idx_sub_lo, (int64_t)inner_idx_sub_hi,
                          bt, "aggregate inner index subtype check");
                        Emit ("  br label %%L%u\n", sk);
                        cg->block_terminated = true;
                        Emit_Label_Here (sk);
                      }
                    }
                  }
                }
              }

              // Nested loop: for each (row, col) cell, evaluate J                                  
              // and store at the flat offset in row-major order.                                   
              // When outer choice bounds are dynamic, fall back to                                 
              // type-count iteration (positions 0..count-1) since                                  
              // we can't iterate a C loop over runtime values.                                     
              //                                                                                    
              if (inner_val_expr) {
                int128_t eff_lo = rng_low, eff_hi = rng_high;
                if (must_eval_low or must_eval_high) {
                  eff_lo = low;
                  eff_hi = high;  // preserves null range (high<low → 0 iter)
                }
                for (int128_t oi = eff_lo; oi <= eff_hi; oi++) {
                  int128_t oai = oi - low;
                  if (oai < 0 or oai >= count) continue;
                  for (int128_t ci = inner_low; ci <= inner_high; ci++) {
                    int128_t flat = oai * inner_count + (ci - inner_low);
                    cg->in_agg_component++;
                    uint32_t val = Generate_Expression (inner_val_expr);
                    cg->in_agg_component--;
                    const char *st = Expression_Llvm_Type (inner_val_expr);
                    val = Emit_Convert (val, st, elem_type);
                    Agg_Store_At_Static (base, val, flat,
                      elem_type, elem_size, false);
                  }
                  initialized[oai] = true;
                }
              }

            // For multidim with static inner bounds, check inner
            // aggregate choices against dim 1 index subtype.
            } else {
              if (multidim and item->association.expression->kind == NK_AGGREGATE) {
                int128_t isl = Type_Bound_Value (dim_lo[1]);
                int128_t ish = Type_Bound_Value (dim_hi[1]);
                if (agg_type->base_type and
                  Type_Is_Array_Like (agg_type->base_type) and
                  not agg_type->base_type->array.is_constrained and
                  agg_type->base_type->array.index_count > 1 and
                  agg_type->base_type->array.indices and
                  agg_type->base_type->array.indices[1].index_type) {
                  isl = Type_Bound_Value (agg_type->base_type->array.indices[1].index_type->low_bound);
                  ish = Type_Bound_Value (agg_type->base_type->array.indices[1].index_type->high_bound);
                } else if (not agg_type->array.is_constrained and
                       agg_type->array.index_count > 1 and
                       agg_type->array.indices and
                       agg_type->array.indices[1].index_type) {
                  isl = Type_Bound_Value (agg_type->array.indices[1].index_type->low_bound);
                  ish = Type_Bound_Value (agg_type->array.indices[1].index_type->high_bound);
                }
                Syntax_Node *ia = item->association.expression;
                for (uint32_t qi = 0; qi < ia->aggregate.items.count; qi++) {
                  Syntax_Node *qit = ia->aggregate.items.items[qi];
                  if (qit->kind != NK_ASSOCIATION) continue;
                  for (uint32_t qc = 0; qc < qit->association.choices.count; qc++) {
                    Syntax_Node *qch = qit->association.choices.items[qc];
                    if (Is_Others_Choice (qch)) continue;
                    if (qch->kind == NK_RANGE and
                      Is_Static_Int_Node (qch->range.low) and
                      Is_Static_Int_Node (qch->range.high)) {
                      int128_t rlo = Static_Int_Value (qch->range.low);
                      int128_t rhi = Static_Int_Value (qch->range.high);
                      if (rlo <= rhi and
                        (rlo < isl or rlo > ish or rhi < isl or rhi > ish)) {
                        Emit_Raise_Constraint_Error ("aggregate inner index check");
                        uint32_t cont = cg->label_id++;
                        Emit_Label_Here (cont);
                      }
                    } else if (Is_Static_Int_Node (qch)) {
                      int128_t v = Static_Int_Value (qch);
                      if (v < isl or v > ish) {
                        Emit_Raise_Constraint_Error ("aggregate inner index check");
                        uint32_t cont = cg->label_id++;
                        Emit_Label_Here (cont);
                      }
                    }
                  }
                }
              }

              // Non-multidim (or non-aggregate inner expression):
              // evaluate expression per component (RM 4.3.2(6)).
              for (int128_t idx = rng_low; idx <= rng_high; idx++) {
                int128_t arr_idx = idx - low;
                if (arr_idx >= 0 and arr_idx < count) {
                  uint32_t val = Agg_Resolve_Elem (item->association.expression, multidim,
                    elem_is_composite, agg_type, elem_type, elem_ti);
                  Agg_Store_At_Static (base, val, arr_idx,
                    elem_type, elem_size, elem_is_composite);
                  initialized[arr_idx] = true;
                }
              }

              // RM 4.3.2(6): track inner bounds for consistency
              if (check_inner_consistency and cg->inner_agg_bnd_n > 0)
                Emit_Inner_Consistency_Track (inner_trk_lo, inner_trk_hi,
                  inner_trk_first, inner_trk_mm,
                  n_inner_dims, Array_Bound_Llvm_Type (agg_type));
            }
          } else if (choice->kind == NK_IDENTIFIER and choice->symbol and
                 choice->symbol->kind == SYMBOL_TYPE and choice->symbol->type) {

            // Type name as choice: T => val means T'FIRST..T'LAST => val
            // (RM 4.3.2(4)). Re-evaluate expression per component.
            Type_Info *ct = choice->symbol->type;
            int128_t rng_low = Type_Bound_Value (ct->low_bound);
            int128_t rng_high = Type_Bound_Value (ct->high_bound);
            for (int128_t idx = rng_low; idx <= rng_high; idx++) {
              int128_t arr_idx = idx - low;
              if (arr_idx >= 0 and arr_idx < count) {
                uint32_t val = Agg_Resolve_Elem (item->association.expression, multidim,
                  elem_is_composite, agg_type, elem_type, elem_ti);
                Agg_Store_At_Static (base, val, arr_idx,
                  elem_type, elem_size, elem_is_composite);
                initialized[arr_idx] = true;
              }
            }

          // Single index: 3 => value (or -1 => value)
          } else if (Is_Static_Int_Node (choice)) {

            // RM 4.3.2(3): single index always non-null; must be
            // in the index subtype.
            int128_t cv = Static_Int_Value (choice);
            if (need_idx_subtype_check) {
              if (cv < idx_sub_lo or cv > idx_sub_hi) {
                Emit_Raise_Constraint_Error ("aggregate index check");
                uint32_t cont = cg->label_id++;
                Emit_Label_Here (cont);
              }
            }

            // Track single index for dynamic bounds constraint check
            if (s_track_dyn) {
              const char *bt = Array_Bound_Llvm_Type (agg_type);
              uint32_t iv = Emit_Static_Int (cv, bt);
              uint32_t clo = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 clo, bt, s_bnd_lo_var);
              uint32_t lt = Emit_Temp ();
              Emit ("  %%t%u = icmp slt %s %%t%u, %%t%u\n",
                 lt, bt, iv, clo);
              uint32_t nlo = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nlo, lt, bt, iv, bt, clo);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 bt, nlo, s_bnd_lo_var);
              uint32_t chi = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u\n",
                 chi, bt, s_bnd_hi_var);
              uint32_t gt = Emit_Temp ();
              Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n",
                 gt, bt, iv, chi);
              uint32_t nhi = Emit_Temp ();
              Emit ("  %%t%u = select i1 %%t%u, %s %%t%u, %s %%t%u\n",
                 nhi, gt, bt, iv, bt, chi);
              Emit ("  store %s %%t%u, ptr %%t%u\n",
                 bt, nhi, s_bnd_hi_var);
            }
            int128_t idx = cv - low;
            if (idx >= 0 and idx < count) {
              uint32_t val = Agg_Resolve_Elem (item->association.expression, multidim,
                elem_is_composite, agg_type, elem_type, elem_ti);
              Agg_Store_At_Static (base, val, idx,
                elem_type, elem_size, elem_is_composite);
              initialized[idx] = true;
            }

            // RM 4.3.2(6): track inner bounds for consistency
            if (check_inner_consistency and cg->inner_agg_bnd_n > 0)
              Emit_Inner_Consistency_Track (inner_trk_lo, inner_trk_hi,
                inner_trk_first, inner_trk_mm,
                n_inner_dims, Array_Bound_Llvm_Type (agg_type));
          }
        }

      // Positional association
      } else {
        if (positional_idx < (uint32_t)count) {
          uint32_t val = Agg_Resolve_Elem (item, multidim,
            elem_is_composite, agg_type, elem_type, elem_ti);

          // RM 4.3.2: check scalar element against subtype constraint
          if (not elem_is_composite and elem_ti and Type_Is_Scalar (elem_ti))
            val = Emit_Constraint_Check_With_Type (val, elem_ti,
              item->type, elem_type);
          Agg_Store_At_Static (base, val, (int128_t)positional_idx,
            elem_type, elem_size, elem_is_composite);
          initialized[positional_idx] = true;
          positional_idx++;

          // RM 4.3.2(6): track inner bounds for consistency
          if (check_inner_consistency and cg->inner_agg_bnd_n > 0)
            Emit_Inner_Consistency_Track (inner_trk_lo, inner_trk_hi,
              inner_trk_first, inner_trk_mm,
              n_inner_dims, Array_Bound_Llvm_Type (agg_type));
        }
      }
    }

    // Report actual aggregate bounds to outer multidim aggregate.                                  
    // bnd[0] = this aggregate's own first-dimension bounds.                                        
    // bnd[1..] = deeper inner dimensions' bounds (from inner tracking).                            
    // This overrides the early reporting (pre-choice processing).                                  
    //                                                                                              
    if (cg->in_agg_component > 0) {
      const char *bt = Array_Bound_Llvm_Type (agg_type);
      int dim_count = 0;
      if (s_track_dyn) {
        cg->inner_agg_bnd_lo[0] = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u  ; inner actual lo\n",
           cg->inner_agg_bnd_lo[0], bt, s_bnd_lo_var);
        cg->inner_agg_bnd_hi[0] = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u  ; inner actual hi\n",
           cg->inner_agg_bnd_hi[0], bt, s_bnd_hi_var);
      } else {
        cg->inner_agg_bnd_lo[0] = Emit_Static_Int (low, bt);
        cg->inner_agg_bnd_hi[0] = Emit_Static_Int (high, bt);
      }
      dim_count = 1;

      // Propagate tracked inner dimensions' bounds upward
      if (check_inner_consistency) {
        for (int d = 0; d < n_inner_dims and dim_count < MAX_AGG_DIMS; d++) {
          cg->inner_agg_bnd_lo[dim_count] = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u  ; deep inner lo [dim %d]\n",
             cg->inner_agg_bnd_lo[dim_count], bt, inner_trk_lo[d], d);
          cg->inner_agg_bnd_hi[dim_count] = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u  ; deep inner hi [dim %d]\n",
             cg->inner_agg_bnd_hi[dim_count], bt, inner_trk_hi[d], d);
          dim_count++;
        }
      }
      cg->inner_agg_bnd_n = dim_count;
    }

    // Note: For the static path, dynamic named bounds vs constraint                                
    // is NOT checked here because sliding occurs at assignment                                     
    // (RM 5.2.1(3)).  Array-of-array components are checked at                                     
    // line ~27238 (has_unconstrained_base condition).                                              
    //                                                                                              

    // RM 4.3.2(6): Check inner sub-aggregate bounds consistency.                                   
    // After all rows are processed, check if any had mismatched bounds.                            
    // Also check first-seen bounds against the second dimension constraint.                        
    // Only check if at least one inner sub-aggregate was tracked.                                  
    //                                                                                              
    if (check_inner_consistency) {
      uint32_t was_seen = Emit_Temp ();
      Emit ("  %%t%u = load i1, ptr %%t%u  ; any inner seen?\n",
         was_seen, inner_trk_first);
      uint32_t skip_lbl = cg->label_id++;
      uint32_t do_check_lbl = cg->label_id++;
      Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
         was_seen, do_check_lbl, skip_lbl);
      cg->block_terminated = true;
      Emit_Label_Here (do_check_lbl);

      // RM 4.3.2(6): Only check consistency across rows.                                           
      // For multidim, sliding handles bounds adjustment to constraint.                             
      // So do NOT compare inner bounds against constraint here.                                    
      //                                                                                            
      uint32_t mm = Emit_Temp ();
      Emit ("  %%t%u = load i1, ptr %%t%u  ; inner mismatch?\n",
         mm, inner_trk_mm);
      uint32_t ok_lbl = cg->label_id++;
      uint32_t fail_lbl = cg->label_id++;
      Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
         mm, fail_lbl, ok_lbl);
      cg->block_terminated = true;
      Emit_Label_Here (fail_lbl);
      Emit_Raise_Constraint_Error ("sub-aggregate bounds mismatch (RM 4.3.2(6))");
      Emit_Label_Here (ok_lbl);
      Emit ("  br label %%L%u\n", skip_lbl);
      cg->block_terminated = true;
      Emit_Label_Here (skip_lbl);
    }

    // Third pass: fill uninitialized with "others" value.                                          
    // RM 4.3.3(5): the expression is evaluated once per component,                                 
    // so allocators produce distinct objects for each element.                                     
    //                                                                                              
    if (has_others and others_item_expr) {
      for (int128_t idx = 0; idx < count; idx++) {
        if (not initialized[idx]) {
          others_val = Agg_Resolve_Elem (others_item_expr,
            multidim, elem_is_composite, agg_type,
            elem_type, elem_ti);

          // RM 4.3.2: check scalar element against subtype
          if (not elem_is_composite and elem_ti and Type_Is_Scalar (elem_ti))
            others_val = Emit_Constraint_Check_With_Type (others_val,
              elem_ti, others_item_expr->type, elem_type);
          Agg_Store_At_Static (base, others_val, idx,
            elem_type, elem_size, elem_is_composite);
        }
      }
    }

    // Wrap in a fat pointer when the caller expects { ptr, ptr }.                                  
    // This is required for unconstrained types AND constrained types                               
    // with dynamic bounds (BOUND_EXPR), since Expression_Llvm_Type                                 
    // reports FAT_PTR_TYPE for both - and positional aggregates may                                
    // override BOUND_EXPR → BOUND_INTEGER in dim_hi, routing through                               
    // the static path even though the TYPE still has dynamic bounds.                               
    // Multi-dimensional arrays store bounds for ALL dimensions.                                    
    //                                                                                              
    if (not agg_type->array.is_constrained or
      Type_Has_Dynamic_Bounds (agg_type)) {
      const char *agg_bt = Array_Bound_Llvm_Type (agg_type);
      if (multidim) {
        uint32_t mlo[8], mhi[8];

        // Dimension 0 uses the (possibly overridden) choice-based
        // low/high; other dimensions use type bounds.
        for (uint32_t d = 0; d < agg_ndims; d++) {
          int128_t dlo = (d == 0) ? low  : Type_Bound_Value (dim_lo[d]);
          int128_t dhi = (d == 0) ? high : Type_Bound_Value (dim_hi[d]);
          if (d == 0 and agg_lo_ssa != 0) {
            mlo[d] = agg_lo_ssa;
          } else if (d == 1 and agg_d1_lo_ssa != 0) {
            mlo[d] = agg_d1_lo_ssa;
          } else {
            mlo[d] = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %s  ; dim%u lo\n",
               mlo[d], agg_bt, I128_Decimal (dlo), d);
          }
          Temp_Set_Type (mlo[d], agg_bt);
          if (d == 0 and agg_hi_ssa != 0) {
            mhi[d] = agg_hi_ssa;
          } else if (d == 1 and agg_d1_hi_ssa != 0) {
            mhi[d] = agg_d1_hi_ssa;
          } else {
            mhi[d] = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %s  ; dim%u hi\n",
               mhi[d], agg_bt, I128_Decimal (dhi), d);
          }
          Temp_Set_Type (mhi[d], agg_bt);
        }
        uint32_t fat_val = Emit_Fat_Pointer_MultiDim (base, mlo, mhi, agg_ndims, agg_bt);
        uint32_t fat_ptr = Emit_Temp ();
        Emit ("  %%t%u = alloca " FAT_PTR_TYPE "  ; multidim array fat ptr\n", fat_ptr);
        Temp_Set_Type (fat_ptr, "ptr");
        Temp_Mark_Fat_Alloca (fat_ptr);
        Emit ("  store " FAT_PTR_TYPE " %%t%u, ptr %%t%u\n", fat_val, fat_ptr);
        return fat_ptr;
      }

      // Use SSA-evaluated choice bounds if available (expression                                   
      // bounds like IDENT_INT (6)), otherwise use the compile-time                                 
      // low/high already overridden from static choice values.                                     
      //                                                                                            
      uint32_t low_temp;
      if (agg_lo_ssa != 0) {
        low_temp = agg_lo_ssa;
      } else {
        low_temp = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %s  ; static agg low\n",
           low_temp, agg_bt, I128_Decimal (low));
      }
      Temp_Set_Type (low_temp, agg_bt);
      uint32_t high_temp;
      if (agg_hi_ssa != 0) {
        high_temp = agg_hi_ssa;
      } else {
        high_temp = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %s  ; static agg high\n",
           high_temp, agg_bt, I128_Decimal (high));
      }
      Temp_Set_Type (high_temp, agg_bt);
      uint32_t fat_ptr = Emit_Temp ();
      Emit ("  %%t%u = alloca " FAT_PTR_TYPE "  ; static array fat ptr\n", fat_ptr);
      Temp_Set_Type (fat_ptr, "ptr");  // alloca yields ptr
      Temp_Mark_Fat_Alloca (fat_ptr);
      Emit_Store_Fat_Pointer_Fields_To_Temp (base, low_temp, high_temp, fat_ptr, agg_bt);
      return fat_ptr;
    }
    return base;
  }

  // Record aggregate - allocate [N x i8] and fill fields by offset
  if (Type_Is_Record (agg_type)) {
    uint32_t base = Emit_Temp ();
    uint32_t record_size = agg_type->size > 0 ? agg_type->size : 8;

    // Adjust record_size for discriminant-dependent array/string components
    // whose sizes are not included in the type's static size (RM 3.7.1).
    for (uint32_t ci = 0; ci < agg_type->record.component_count; ci++) {
      Component_Info *comp_ci = &agg_type->record.components[ci];
      Type_Info *cti = comp_ci->component_type;
      if (not cti or not Type_Is_Array_Like (cti)) continue;
      for (uint32_t xi = 0; xi < cti->array.index_count; xi++) {
        Type_Bound *lo = &cti->array.indices[xi].low_bound;
        Type_Bound *hi = &cti->array.indices[xi].high_bound;
        if (hi->kind == BOUND_EXPR and hi->expr and hi->expr->symbol) {
          Symbol *disc = hi->expr->symbol;
          Type_Info *disc_ty = disc->type;
          if (disc_ty and disc_ty->high_bound.kind == BOUND_INTEGER) {
            int64_t max_hi = disc_ty->high_bound.int_value;
            int64_t lo_val = (lo->kind == BOUND_INTEGER) ? lo->int_value : 0;
            int64_t max_extent = max_hi - lo_val + 1;
            if (max_extent < 0) max_extent = 0;
            uint32_t elem_sz = (cti->array.element_type and
              cti->array.element_type->size > 0) ?
              cti->array.element_type->size : 1;
            uint32_t needed = comp_ci->byte_offset +
              (uint32_t)(max_extent * elem_sz);
            if (needed > record_size) record_size = needed;
          }
        }
      }
    }
    Emit ("  %%t%u = alloca [%u x i8]  ; record aggregate\n", base, record_size);

    // Pre-allocate temp-based discriminant storage for dependent array                             
    // bounds (RM 3.7.1).  Uses temp IDs instead of symbol names so each                            
    // aggregate gets its own allocas (avoiding dominance violations when                           
    // the same record type appears in multiple blocks).                                            
    //                                                                                              
    Disc_Alloc_Entry disc_alloc[16];
    uint32_t disc_alloc_count = 0;
    for (uint32_t ci = 0; ci < agg_type->record.component_count; ci++) {
      Component_Info *comp_ci = &agg_type->record.components[ci];
      Type_Info *cti = comp_ci->component_type;

      // Array component: alloca for disc symbols in index bounds
      if (cti and Type_Is_Array_Like (cti)) {
        for (uint32_t xi = 0; xi < cti->array.index_count; xi++) {
          Type_Bound *bounds[2] = { &cti->array.indices[xi].low_bound,
                        &cti->array.indices[xi].high_bound };
          for (int bi = 0; bi < 2; bi++) {
            if (bounds[bi]->kind == BOUND_EXPR and bounds[bi]->expr and
              bounds[bi]->expr->symbol) {
              Symbol *disc_sym = bounds[bi]->expr->symbol;
              bool already = false;
              for (uint32_t da = 0; da < disc_alloc_count; da++) {
                if (disc_alloc[da].sym == disc_sym) { already = true; break; }
              }
              if (already) continue;
              const char *disc_type = Type_To_Llvm (disc_sym->type);
              if (not disc_type or disc_type[0] == '\0') disc_type = Integer_Arith_Type ();
              uint32_t dt = Emit_Temp ();
              Emit ("  %%t%u = alloca %s  ; disc for aggregate bounds\n", dt, disc_type);
              disc_sym->disc_agg_temp = dt;
              if (disc_alloc_count < 16)
                disc_alloc[disc_alloc_count++] = (typeof(disc_alloc[0])){disc_sym, dt};
            }
          }
        }
      }

      // Record component: alloca for disc symbols in disc constraints
      if (cti and Type_Is_Record (cti) and cti->record.has_disc_constraints and
        cti->record.disc_constraint_exprs) {
        for (uint32_t di = 0; di < cti->record.discriminant_count; di++) {
          Syntax_Node *ce = cti->record.disc_constraint_exprs[di];
          if (ce and ce->symbol) {
            Symbol *disc_sym = ce->symbol;
            bool already = false;
            for (uint32_t da = 0; da < disc_alloc_count; da++) {
              if (disc_alloc[da].sym == disc_sym) { already = true; break; }
            }
            if (already) continue;
            const char *disc_type = Type_To_Llvm (disc_sym->type);
            if (not disc_type or disc_type[0] == '\0') disc_type = Integer_Arith_Type ();
            uint32_t dt = Emit_Temp ();
            Emit ("  %%t%u = alloca %s  ; disc for rec constraint\n", dt, disc_type);
            disc_sym->disc_agg_temp = dt;
            if (disc_alloc_count < 16)
              disc_alloc[disc_alloc_count++] = (typeof(disc_alloc[0])){disc_sym, dt};
          }
        }
      }
    }

    // Track initialized components for others clause
    uint32_t comp_count = agg_type->record.component_count;
    bool *initialized = Arena_Allocate (comp_count * sizeof (bool));
    for (uint32_t i = 0; i < comp_count; i++) initialized[i] = false;

    // Default value for "others" clause
    Syntax_Node *others_expr = NULL;
    bool has_others = false;

    // First pass: find "others" clause (defer generation to third pass)
    for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
      Syntax_Node *item = node->aggregate.items.items[i];
      if (item->kind == NK_ASSOCIATION and item->association.choices.count > 0) {
        if (Is_Others_Choice (item->association.choices.items[0])) {
          others_expr = item->association.expression;
          has_others = true;
          break;
        }
      }
    }

    // Wrap disc_alloc array for the helper API
    Disc_Alloc_Info da_info = { .entries = disc_alloc, .count = disc_alloc_count };

    // ── Second pass: initialize fields ───────────────────────────────────────────────────────────
    // Both named (field_name => expr) and positional (expr, expr, ...)                             
    // forms use the unified Agg_Rec_Store / Agg_Rec_Disc_Post helpers.                             
    //                                                                                              
    uint32_t positional_idx = 0;
    for (uint32_t i = 0; i < node->aggregate.items.count; i++) {
      Syntax_Node *item = node->aggregate.items.items[i];

      // Named: field_name => value
      if (item->kind == NK_ASSOCIATION) {
        for (uint32_t c = 0; c < item->association.choices.count; c++) {
          Syntax_Node *choice = item->association.choices.items[c];
          if (Is_Others_Choice (choice)) continue;
          if (choice->kind == NK_IDENTIFIER) {
            int32_t ci = Find_Record_Component (agg_type, choice->string_val.text);
            if (ci < 0) continue;
            Component_Info *comp = &agg_type->record.components[ci];
            cg->in_agg_component++;
            uint32_t val = Generate_Expression (item->association.expression);
            cg->in_agg_component--;
            uint32_t ptr = Emit_Temp ();
            Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
               ptr, base, comp->byte_offset);
            Agg_Rec_Store (val, ptr, comp, item->association.expression);
            if (comp->is_discriminant)
              Agg_Rec_Disc_Post (val, comp,
                Disc_Ordinal_Before (agg_type, (uint32_t)ci),
                agg_type, &da_info);
            initialized[ci] = true;
          }
        }

      // Positional: initialize component by position
      } else {
        if (positional_idx < comp_count) {
          Component_Info *comp = &agg_type->record.components[positional_idx];
          cg->in_agg_component++;
          uint32_t val = Generate_Expression (item);
          cg->in_agg_component--;
          uint32_t ptr = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
             ptr, base, comp->byte_offset);
          Agg_Rec_Store (val, ptr, comp, item);
          if (comp->is_discriminant)
            Agg_Rec_Disc_Post (val, comp,
              Disc_Ordinal_Before (agg_type, positional_idx),
              agg_type, &da_info);
          initialized[positional_idx] = true;
          positional_idx++;
        }
      }
    }

    // Determine selected variant for OTHERS filtering (RM 3.7.3).                                  
    // In a variant record aggregate, OTHERS only applies to components                             
    // in the fixed part and the selected variant - not all variants.                               
    //                                                                                              
    int32_t selected_variant = -1;
    if (agg_type->record.has_discriminants and
      agg_type->record.variant_count > 0) {

      // Method 1: infer from explicitly named variant components.                                  
      // If C => 3 was named and C is in the WHEN TRUE variant, the                                 
      // selected variant must be WHEN TRUE.                                                        
      //                                                                                            
      if (has_others) {
        for (uint32_t ci = 0; ci < comp_count; ci++) {
          if (initialized[ci] and
            agg_type->record.components[ci].variant_index >= 0) {
            selected_variant =
              agg_type->record.components[ci].variant_index;
            break;
          }
        }
      }

      // Method 2: determine from discriminant value in OTHERS expr.
      if (selected_variant < 0 and has_others) {
        int64_t disc_val = 0;
        bool disc_known = false;

        // Try the OTHERS expression as a discriminant value
        if (others_expr and others_expr->symbol and
          others_expr->symbol->kind == SYMBOL_LITERAL) {
          disc_val = (int64_t)others_expr->symbol->frame_offset;
          disc_known = true;
        } else if (others_expr and others_expr->kind == NK_INTEGER) {
          disc_val = others_expr->integer_lit.value;
          disc_known = true;
        }
        if (disc_known) {
          for (uint32_t vi = 0; vi < agg_type->record.variant_count; vi++) {
            if (disc_val >= agg_type->record.variants[vi].disc_value_low and
              disc_val <= agg_type->record.variants[vi].disc_value_high) {
              selected_variant = (int32_t)vi;
              break;
            }
            if (agg_type->record.variants[vi].is_others)
              selected_variant = (int32_t)vi;
          }
        }
      }

      // Method 3: determine from disc_constraint_values on the type
      if (selected_variant < 0 and agg_type->record.disc_constraint_values and
        (not agg_type->record.disc_constraint_exprs or
         not agg_type->record.disc_constraint_exprs[0])) {
        int64_t dv = agg_type->record.disc_constraint_values[0];
        for (uint32_t vi = 0; vi < agg_type->record.variant_count; vi++) {
          if (dv >= agg_type->record.variants[vi].disc_value_low and
            dv <= agg_type->record.variants[vi].disc_value_high) {
            selected_variant = (int32_t)vi;
            break;
          }
          if (agg_type->record.variants[vi].is_others)
            selected_variant = (int32_t)vi;
        }
      }
    }

    // ── Third pass: fill uninitialized with OTHERS value (RM 4.3.1) ──────────────────────────────

    if (has_others and others_expr) {
      for (uint32_t idx = 0; idx < comp_count; idx++) {
        if (initialized[idx]) continue;
        Component_Info *comp = &agg_type->record.components[idx];
        if (comp->variant_index >= 0 and selected_variant >= 0 and
          comp->variant_index != selected_variant) continue;

        // Set type context so nested aggregates resolve correctly
        Type_Info *saved = others_expr->type;
        if (not others_expr->type) others_expr->type = comp->component_type;
        cg->in_agg_component++;
        uint32_t val = Generate_Expression (others_expr);
        cg->in_agg_component--;
        uint32_t ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
           ptr, base, comp->byte_offset);
        Agg_Rec_Store (val, ptr, comp, others_expr);
        others_expr->type = saved;
      }
    }

    // RM 3.6.1(7): Check disc-dependent array bounds against index subtype.                        
    // This catches cases like SM_ARR (1..D1) when D1 exceeds the SM range.                         
    // RM 3.7.3: Skip checks for components in unselected variants.                                 
    //                                                                                              
    if (disc_alloc_count > 0) {

      // Skip variant components not in the selected variant
      for (uint32_t ci = agg_type->record.discriminant_count;
         ci < agg_type->record.component_count; ci++) {
        if (selected_variant >= 0 and
          agg_type->record.components[ci].variant_index >= 0 and
          agg_type->record.components[ci].variant_index != selected_variant)
          continue;
        Type_Info *ct = agg_type->record.components[ci].component_type;
        if (not ct or not Type_Is_Array_Like (ct)) continue;
        for (uint32_t xi = 0; xi < ct->array.index_count; xi++) {
          Type_Info *idx_ty = ct->array.indices[xi].index_type;
          if (not idx_ty or not Type_Is_Scalar (idx_ty)) continue;
          Type_Bound *lo = &ct->array.indices[xi].low_bound;
          Type_Bound *hi = &ct->array.indices[xi].high_bound;

          // Only check bounds that reference discriminants
          bool has_disc_ref = false;
          if (lo->kind == BOUND_EXPR and lo->expr and lo->expr->symbol
            and lo->expr->symbol->kind == SYMBOL_DISCRIMINANT)
            has_disc_ref = true;
          if (hi->kind == BOUND_EXPR and hi->expr and hi->expr->symbol
            and hi->expr->symbol->kind == SYMBOL_DISCRIMINANT)
            has_disc_ref = true;
          if (not has_disc_ref) continue;
          const char *bt = Type_To_Llvm (idx_ty);
          if (not bt or bt[0] == '\0') bt = "i32";
          uint32_t lo_val = 0, hi_val = 0;
          for (int bx = 0; bx < 2; bx++) {
            Type_Bound *bound = (bx == 0) ? lo : hi;
            uint32_t bval = 0;
            if (bound->kind == BOUND_INTEGER) {
              bval = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld\n", bval, bt,
                 (long long)bound->int_value);
            } else if (bound->kind == BOUND_EXPR and bound->expr) {
              bval = Generate_Expression (bound->expr);
              bval = Emit_Coerce_Default_Int (bval, bt);
            }
            if (bx == 0) lo_val = bval; else hi_val = bval;
          }
          if (lo_val > 0 and hi_val > 0) {
            uint32_t cmp = Emit_Temp ();
            uint32_t lbl_chk = cg->label_id++;
            uint32_t lbl_end = cg->label_id++;
            Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n",
               cmp, bt, lo_val, hi_val);
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               cmp, lbl_chk, lbl_end);
            Emit ("L%u:\n", lbl_chk);
            Emit_Constraint_Check (lo_val, idx_ty, NULL);
            Emit_Constraint_Check (hi_val, idx_ty, NULL);
            Emit ("  br label %%L%u\n", lbl_end);
            Emit ("L%u:\n", lbl_end);
          } else if (lo_val > 0) {
            Emit_Constraint_Check (lo_val, idx_ty, NULL);
          } else if (hi_val > 0) {
            Emit_Constraint_Check (hi_val, idx_ty, NULL);
          }
        }
      }
    }

    // Clear disc_agg_temp so it doesn't leak into other contexts
    // Type'(expression) - generate expression and convert to target type if needed
    for (uint32_t da = 0; da < disc_alloc_count; da++)
      disc_alloc[da].sym->disc_agg_temp = 0;
    return base;
  }
  fprintf (stderr, "warning: Generate_Aggregate: unhandled aggregate type kind=%d at %s:%u\n",
      agg_type->kind,
      node->location.filename ? node->location.filename : "<unknown>",
      node->location.line);
  return 0;
}
uint32_t Generate_Qualified (Syntax_Node *node) {
  uint32_t result = Generate_Expression (node->qualified.expression);

  // Get source and destination types
  Type_Info *src_type = node->qualified.expression ? node->qualified.expression->type : NULL;
  Type_Info *dst_type = node->qualified.subtype_mark ? node->qualified.subtype_mark->type : NULL;
  if (not dst_type) return result;
  const char *src_llvm = Expression_Llvm_Type (node->qualified.expression);

  // RM 4.7: Qualified expression checks value against subtype constraint
  if (Type_Is_Scalar (dst_type)) {
    Emit_Constraint_Check_With_Type (result, dst_type, src_type, src_llvm);
  }
  if (src_type and src_type != dst_type) {
    const char *dst_llvm = Type_To_Llvm (dst_type);
    if (strcmp (src_llvm, dst_llvm) != 0) {
      result = Emit_Convert (result, src_llvm, dst_llvm);
    }
  }

  // no widening - value stays at native type width.
  // Callers use Emit_Convert at use sites.
  return result;
}
uint32_t Generate_Allocator (Syntax_Node *node) {

  // new T or new T'(value)
  Type_Info *access_type = node->type;  // The access type being created
  if (not access_type) {
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = call ptr @malloc (i64 8)\n", t);
    Temp_Set_Type (t, "ptr");
    return t;
  }

  // Fat pointer decision must match Type_Needs_Fat_Pointer / Type_To_Llvm.                         
  // Access to unconstrained arrays always uses fat pointer, even when                              
  // the access subtype adds a constraint (RM 3.10).                                                
  //                                                                                                
  Type_Info *designated = Type_Is_Access (access_type) ?
              access_type->access.designated_type : NULL;
  bool is_fat_ptr = Type_Needs_Fat_Pointer (access_type);

  // Access to unconstrained array with initializer
  if (is_fat_ptr and node->allocator.expression) {
    Type_Info *init_type = node->allocator.expression->type;

    // Check what LLVM type the expression actually returns.                                        
    // Constrained array aggregates return ptr, unconstrained return fat pointer.                   
    // For qualified expressions, look at the inner expression.                                     
    //                                                                                              
    Syntax_Node *inner_expr = node->allocator.expression;
    if (inner_expr->kind == NK_QUALIFIED and inner_expr->qualified.expression) {
      inner_expr = inner_expr->qualified.expression;
    }
    const char *expr_llvm_type = Expression_Llvm_Type (inner_expr);
    bool init_returns_ptr = Llvm_Type_Is_Pointer (expr_llvm_type);

    // Also check if the aggregate is constrained
    Type_Info *agg_type = inner_expr->type;
    if (not agg_type and inner_expr->kind == NK_AGGREGATE) {
      agg_type = node->allocator.expression->type;  // Use outer type
    }
    bool init_is_constrained = Type_Is_Constrained_Array (agg_type);
    uint32_t init_val = Generate_Expression (node->allocator.expression);

    // After generation, check if the result is actually a fat pointer                              
    // alloca (dynamic-bounds aggregate).  Expression_Llvm_Type returns                             
    // "ptr" for allocas, but if it's marked as a fat alloca, we must                               
    // use the fat pointer extraction path.                                                         
    //                                                                                              
    if (init_returns_ptr and Temp_Is_Fat_Alloca (init_val)) {
      init_returns_ptr = false;
      init_is_constrained = false;
    }
    uint32_t src_data, low_t, high_t, len_t;
    bool len_is_bytes = false;  // true if len_t is in bytes, false if elements

    // Constrained array initializer: returns ptr, not fat pointer.
    // Extract bounds from the type and use the ptr directly.
    if (init_returns_ptr or init_is_constrained) {
      src_data = init_val;  // Already a pointer to array data

      // Get bounds from the constrained type.                                                      
      // Bounds must be in the designated type's bt for Emit_Fat_Pointer_Dynamic.                   
      // len_t stays as i64 since it's used for malloc/memcpy.                                      
      //                                                                                            
      const char *con_bt = Array_Bound_Llvm_Type (designated);
      if (init_type->array.index_count > 0 and
        init_type->array.indices[0].low_bound.kind == BOUND_INTEGER and
        init_type->array.indices[0].high_bound.kind == BOUND_INTEGER) {
        int64_t lo = init_type->array.indices[0].low_bound.int_value;
        int64_t hi = init_type->array.indices[0].high_bound.int_value;
        low_t = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", low_t, con_bt, (long long)lo);
        high_t = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", high_t, con_bt, (long long)hi);
        len_t = Emit_Temp ();
        int64_t length = hi - lo + 1;
        uint32_t elem_size = init_type->array.element_type ?
                   init_type->array.element_type->size : 1;
        if (elem_size == 0) elem_size = 1;
        Emit ("  %%t%u = add %s 0, %lld\n", len_t, Integer_Arith_Type (), (long long)(length * elem_size));
        len_is_bytes = true;

      // Dynamic bounds - use 1-based defaults
      } else {
        low_t = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, 1\n", low_t, con_bt);
        high_t = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, 1\n", high_t, con_bt);
        len_t = Emit_Temp ();
        uint32_t elem_size = init_type->array.element_type ?
                   init_type->array.element_type->size : 1;
        if (elem_size == 0) elem_size = 8;
        Emit ("  %%t%u = add %s 0, %u\n", len_t, Integer_Arith_Type (), elem_size);
        len_is_bytes = true;
      }

    // Unconstrained array or string: returns fat pointer VALUE.
    // Fat_Ptr_As_Value handles alloca-style fat pointers transparently.
    } else {
      const char *alloc_bt = Array_Bound_Llvm_Type (designated);
      src_data = Emit_Fat_Pointer_Data (init_val, alloc_bt);
      low_t = Emit_Fat_Pointer_Low (init_val, alloc_bt);
      high_t = Emit_Fat_Pointer_High (init_val, alloc_bt);
      len_t = Emit_Fat_Pointer_Length (init_val, alloc_bt);
      len_is_bytes = false;
    }

    // Widen len to i64 for system calls (malloc/memcpy)
    const char *new_bt = Array_Bound_Llvm_Type (designated);
    uint32_t len_t_64 = len_t;

    // len_t is in Integer_Arith_Type (i32) from constrained path, already in bytes
    if (len_is_bytes) {
      len_t_64 = Emit_Extend_To_I64 (len_t, Integer_Arith_Type ());

    // len_t is in new_bt (element count) from Emit_Fat_Pointer_Length.
    // Multiply by element size to get byte count for malloc/memcpy.
    } else {
      uint32_t elem_size = 1;
      if (init_type and init_type->array.element_type)
        elem_size = init_type->array.element_type->size;
      if (elem_size == 0) elem_size = 1;
      if (elem_size > 1) {
        uint32_t byte_len = Emit_Temp ();
        Emit ("  %%t%u = mul %s %%t%u, %u\n",
           byte_len, new_bt, len_t, elem_size);
        len_t_64 = Emit_Extend_To_I64 (byte_len, new_bt);
      } else {
        len_t_64 = Emit_Extend_To_I64 (len_t, new_bt);
      }
    }

    // Allocate heap space for array data
    uint32_t heap_ptr = Emit_Temp ();
    Emit ("  %%t%u = call ptr @malloc (i64 %%t%u)\n", heap_ptr, len_t_64);

    // Copy data: memcpy (heap_ptr, src_data, length)
    Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
       heap_ptr, src_data, len_t_64);

    // Build result fat pointer with heap-allocated bounds.                                         
    // Allocator results must survive across function returns,                                      
    // so bounds cannot be on the stack (alloca).                                                   
    //                                                                                              
    return Emit_Fat_Pointer_Heap (heap_ptr, low_t, high_t, new_bt);
  }

  // Handle NEW T(bounds) without initializer - allocate unconstrained array
  // Get bounds from the subtype mark's type
  if (is_fat_ptr and not node->allocator.expression and node->allocator.subtype_mark) {
    Type_Info *subtype = node->allocator.subtype_mark->type;

    // Generate bound values in the designated type's bt
    if (subtype and subtype->kind == TYPE_ARRAY and subtype->array.index_count > 0) {
      const char *new_bt = Array_Bound_Llvm_Type (designated);
      uint32_t low_t = Emit_Single_Bound (&subtype->array.indices[0].low_bound, new_bt);
      uint32_t high_t = Emit_Single_Bound (&subtype->array.indices[0].high_bound, new_bt);

      // Calculate size: (high - low + 1) * elem_size
      const char *alloc_iat = Integer_Arith_Type ();
      uint32_t elem_size = subtype->array.element_type ?
                 subtype->array.element_type->size : 8;
      if (elem_size == 0) elem_size = 8;
      uint32_t high_conv = Emit_Convert (high_t, new_bt, alloc_iat);
      uint32_t low_conv = Emit_Convert (low_t, new_bt, alloc_iat);
      uint32_t len_plus1 = Emit_Length_From_Bounds (low_conv, high_conv, alloc_iat);
      uint32_t byte_size = Emit_Temp ();
      Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_size, alloc_iat, len_plus1, elem_size);

      // Allocate heap space - widen to i64 for malloc ABI
      uint32_t byte_size_64 = Emit_Extend_To_I64 (byte_size, alloc_iat);
      uint32_t heap_ptr = Emit_Temp ();
      Emit ("  %%t%u = call ptr @malloc (i64 %%t%u)\n", heap_ptr, byte_size_64);

      // Return fat pointer with heap-allocated bounds
      return Emit_Fat_Pointer_Heap (heap_ptr, low_t, high_t, new_bt);
    }
  }

  // Simple allocation (constrained types, scalar access, or no initializer).                       
  // For composite designated types (arrays, records), allocate the designated                      
  // type's storage and memcpy the initializer.  For scalar types, use store.                       
  //                                                                                                
  uint64_t alloc_size = 8;  // Default: pointer-sized
  bool designated_is_composite = false;
  if (designated) {

    // Constrained array: compute actual byte size from element count
    if (Type_Is_Constrained_Array (designated)) {
      int128_t count = Array_Element_Count (designated);
      uint32_t elem_sz = designated->array.element_type ?
                 designated->array.element_type->size : 1;
      if (elem_sz == 0) elem_sz = 8;
      alloc_size = (uint64_t)(count > 0 ? count : 1) * elem_sz;
      designated_is_composite = true;
    } else if (Type_Is_Record (designated)) {
      alloc_size = designated->size > 0 ? designated->size : 8;
      designated_is_composite = true;
    } else {
      alloc_size = designated->size > 0 ? designated->size : 8;
    }
  }
  uint32_t t = Emit_Temp ();
  Emit ("  %%t%u = call ptr @malloc (i64 %llu)\n", t, (unsigned long long)alloc_size);

  // If there's an initializer, copy it into the allocated memory
  if (node->allocator.expression) {
    uint32_t val = Generate_Expression (node->allocator.expression);

    // Composite type: memcpy from initializer to heap
    if (designated_is_composite) {
      Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %llu, i1 false)\n",
         t, val, (unsigned long long)alloc_size);

    // Scalar type: store value - convert to designated type if needed
    } else {
      const char *desg_llvm = designated ? Type_To_Llvm (designated) : Integer_Arith_Type ();
      const char *val_llvm = Expression_Llvm_Type (node->allocator.expression);
      if (strcmp (val_llvm, desg_llvm) != 0) {
        val = Emit_Convert (val, val_llvm, desg_llvm);
      }
      Emit ("  store %s %%t%u, ptr %%t%u\n", desg_llvm, val, t);
    }

  // NEW T without initializer for record types (RM 4.8):                                           
  // Initialize discriminant constraints and component defaults.                                    
  // The designated type (or its subtype constraint) provides                                       
  // discriminant values; component default_expr nodes provide defaults.                            
  //                                                                                                
  } else if (designated and Type_Is_Record (designated)) {
    Type_Info *ty = designated;

    // Zero-initialize first to handle unset fields cleanly
    uint32_t sz64 = Emit_Temp ();
    Emit ("  %%t%u = add i64 0, %llu\n", sz64, (unsigned long long)alloc_size);
    Emit ("  call void @llvm.memset.p0.i64(ptr %%t%u, i8 0, i64 %%t%u, i1 false)"
       "  ; zero-init record\n", t, sz64);

    // Initialize discriminant constraint values if constrained subtype
    if (ty->record.has_disc_constraints and ty->record.disc_constraint_values) {
      for (uint32_t di = 0; di < ty->record.discriminant_count; di++) {
        Component_Info *dc = &ty->record.components[di];
        uint32_t dp = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u  ; disc %.*s\n",
           dp, t, dc->byte_offset, (int)dc->name.length, dc->name.data);
        const char *dt = Type_To_Llvm (dc->component_type);
        {
          uint32_t val = Emit_Disc_Constraint_Value (ty, di, dt);
          if (val == 0) val = Emit_Static_Int (0, dt);

          // RM 3.7.2: Check disc value against disc subtype
          if (dc->component_type and Type_Is_Scalar (dc->component_type))
            Emit_Constraint_Check (val, dc->component_type, NULL);
          Emit ("  store %s %%t%u, ptr %%t%u\n", dt, val, dp);
        }
      }

      // RM 3.7.2(3): Check nested component disc constraints
      Emit_Nested_Disc_Checks (ty);
    }

    // Apply component defaults (RM 3.7)
    for (uint32_t ci = 0; ci < ty->record.component_count; ci++) {
      Component_Info *comp = &ty->record.components[ci];
      if (not comp->default_expr) continue;
      if (comp->is_discriminant and ty->record.has_disc_constraints) continue;
      uint32_t val = Generate_Expression (comp->default_expr);
      if (val == 0) continue;
      uint32_t comp_ptr = Emit_Temp ();
      if (ty->rt_global_id > 0) {
        uint32_t rt_off = Emit_Temp ();
        Emit ("  %%t%u = load i64, ptr @__rt_rec_%u_off%u\n",
           rt_off, ty->rt_global_id, ci);
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %%t%u  ; %.*s rt default\n",
           comp_ptr, t, rt_off,
           (int)comp->name.length, comp->name.data);
      } else {
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u  ; %.*s default\n",
           comp_ptr, t, comp->byte_offset,
           (int)comp->name.length, comp->name.data);
      }
      Type_Info *comp_type = comp->component_type;
      bool comp_is_composite = Type_Is_Composite (comp_type);
      bool has_rt_sz = comp_type and comp_type->rt_global_id > 0;
      if (comp_is_composite and (comp_type->size > 0 or has_rt_sz)) {
        uint32_t data_ptr = val;
        bool is_fat_agg = comp->default_expr->kind == NK_AGGREGATE and
          comp_type and Type_Is_Array_Like (comp_type) and
          (Type_Is_Unconstrained_Array (comp_type) or
           Aggregate_Produces_Fat_Pointer (comp_type));
        if (is_fat_agg) {
          uint32_t loaded = Emit_Temp ();
          Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n",
             loaded, val);
          data_ptr = Emit_Temp ();
          Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE
             " %%t%u, 0\n", data_ptr, loaded);
        }
        if (has_rt_sz) {
          uint32_t rtsz = Emit_Temp ();
          Emit ("  %%t%u = load i64, ptr @__rt_type_%u_size\n",
             rtsz, comp_type->rt_global_id);
          Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
             "  ; %.*s rt default memcpy\n",
             comp_ptr, data_ptr, rtsz,
             (int)comp->name.length, comp->name.data);
        } else {
          Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)"
             "  ; %.*s default memcpy\n",
             comp_ptr, data_ptr, comp_type->size,
             (int)comp->name.length, comp->name.data);
        }

      // Dynamic-size composite (disc-dependent array in NEW):
      // extract data from fat-pointer aggregate and memcpy.
      } else if (comp_is_composite) {
        bool is_fat_agg = comp->default_expr->kind == NK_AGGREGATE and
          comp_type and Type_Is_Array_Like (comp_type) and
          (Type_Is_Unconstrained_Array (comp_type) or
           Aggregate_Produces_Fat_Pointer (comp_type));
        if (is_fat_agg) {
          uint32_t loaded = Emit_Temp ();
          Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n", loaded, val);
          uint32_t data_ptr = Emit_Temp ();
          Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n",
             data_ptr, loaded);
          uint32_t bnds = Emit_Temp ();
          Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n",
             bnds, loaded);
          uint32_t ndim = comp_type->array.index_count;
          uint32_t esz = (comp_type->array.element_type and
            comp_type->array.element_type->size > 0)
            ? comp_type->array.element_type->size : 4;
          const char *bt = Array_Bound_Llvm_Type (comp_type);
          uint32_t tsz = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %u\n", tsz, bt, esz);
          for (uint32_t d = 0; d < ndim; d++) {
            uint32_t lp = Emit_Temp ();
            Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
               lp, bt, bnds, d * 2);
            uint32_t lo = Emit_Temp ();
            Emit ("  %%t%u = load %s, ptr %%t%u\n", lo, bt, lp);
            uint32_t hp = Emit_Temp ();
            Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
               hp, bt, bnds, d * 2 + 1);
            uint32_t hi = Emit_Temp ();
            Emit ("  %%t%u = load %s, ptr %%t%u\n", hi, bt, hp);
            uint32_t cnt = Emit_Temp ();
            Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", cnt, bt, hi, lo);
            uint32_t c1 = Emit_Temp ();
            Emit ("  %%t%u = add %s %%t%u, 1\n", c1, bt, cnt);
            uint32_t neg = Emit_Temp ();
            Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", neg, bt, c1);
            uint32_t cl = Emit_Temp ();
            Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
               cl, neg, bt, bt, c1);
            uint32_t nt = Emit_Temp ();
            Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", nt, bt, tsz, cl);
            tsz = nt;
          }
          uint32_t sz64 = tsz;
          if (strcmp (bt, "i64") != 0) {
            sz64 = Emit_Temp ();
            Emit ("  %%t%u = sext %s %%t%u to i64\n", sz64, bt, tsz);
          }
          Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
             "  ; %.*s dynamic default memcpy\n",
             comp_ptr, data_ptr, sz64,
             (int)comp->name.length, comp->name.data);
        }
      } else {
        const char *store_type = Type_To_Llvm (comp_type);
        const char *val_type = Temp_Get_Type (val);
        if (not val_type or strlen (val_type) == 0) {
          Type_Info *expr_type = comp->default_expr->type;
          if (expr_type) val_type = Type_To_Llvm (expr_type);
        }
        if (not val_type or strlen (val_type) == 0)
          val_type = Expression_Llvm_Type (comp->default_expr);
        if (not store_type or strlen (store_type) == 0)
          store_type = val_type;
        if (Type_Is_Float (comp_type)) {
          const char *flt_ty = Float_Llvm_Type_Of (comp_type);
          val = Emit_Convert (val, val_type, flt_ty);
          Emit ("  store %s %%t%u, ptr %%t%u\n", flt_ty, val, comp_ptr);
        } else {
          if (comp_type and comp_type->kind == TYPE_FIXED and
            val_type and Is_Float_Type (val_type)) {
            double small = comp_type->fixed.small;
            if (small <= 0) small = comp_type->fixed.delta > 0 ? comp_type->fixed.delta : 1.0;
            uint64_t sb; memcpy (&sb, &small, sizeof (sb));
            uint32_t st = Emit_Temp ();
            Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small\n", st, (unsigned long long)sb);
            uint32_t dv = Emit_Temp ();
            Emit ("  %%t%u = fdiv %s %%t%u, %%t%u  ; rec default/small\n", dv, val_type, val, st);
            val = dv;
          }
          val = Emit_Convert (val, val_type, store_type);
          Emit ("  store %s %%t%u, ptr %%t%u\n", store_type, val, comp_ptr);
        }
      }
    }

    // RM 3.7.2: For unconstrained record types with disc defaults,                                 
    // check that default disc values are compatible with nested                                    
    // disc constraints and array bounds.  Disc symbols resolve                                     
    // via disc_agg_temp pointing to the disc's memory in the record.                               
    //                                                                                              
    if (ty->record.has_discriminants and not ty->record.has_disc_constraints) {
      Symbol *alloc_disc_syms[16] = {NULL};
      uint32_t alloc_disc_count = 0;
      for (uint32_t ci = ty->record.discriminant_count;
         ci < ty->record.component_count; ci++) {
        Type_Info *ct = ty->record.components[ci].component_type;
        if (ct and Type_Is_Record (ct) and ct->record.has_disc_constraints
          and ct->record.disc_constraint_exprs) {
          for (uint32_t di = 0; di < ct->record.discriminant_count; di++) {
            Syntax_Node *ce = ct->record.disc_constraint_exprs[di];
            if (not ce or not ce->symbol or
              ce->symbol->kind != SYMBOL_DISCRIMINANT or
              ce->symbol->disc_agg_temp != 0) continue;
            for (uint32_t pdi = 0;
               pdi < ty->record.discriminant_count; pdi++) {
              if (Slice_Equal_Ignore_Case (ce->symbol->name,
                  ty->record.components[pdi].name)) {
                uint32_t dp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
                   dp, t, ty->record.components[pdi].byte_offset);
                ce->symbol->disc_agg_temp = dp;
                if (alloc_disc_count < 16)
                  alloc_disc_syms[alloc_disc_count++] = ce->symbol;
                break;
              }
            }
          }
        }
        if (ct and Type_Is_Array_Like (ct)) {
          for (uint32_t xi = 0; xi < ct->array.index_count; xi++) {
            Type_Bound *bounds[2] = {&ct->array.indices[xi].low_bound,
                         &ct->array.indices[xi].high_bound};
            for (int bi = 0; bi < 2; bi++) {
              if (bounds[bi]->kind == BOUND_EXPR and bounds[bi]->expr
                and bounds[bi]->expr->symbol
                and bounds[bi]->expr->symbol->kind == SYMBOL_DISCRIMINANT
                and bounds[bi]->expr->symbol->disc_agg_temp == 0) {
                for (uint32_t pdi = 0;
                   pdi < ty->record.discriminant_count; pdi++) {
                  if (Slice_Equal_Ignore_Case (
                      bounds[bi]->expr->symbol->name,
                      ty->record.components[pdi].name)) {
                    uint32_t dp = Emit_Temp ();
                    Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
                       dp, t, ty->record.components[pdi].byte_offset);
                    bounds[bi]->expr->symbol->disc_agg_temp = dp;
                    if (alloc_disc_count < 16)
                      alloc_disc_syms[alloc_disc_count++] =
                        bounds[bi]->expr->symbol;
                    break;
                  }
                }
              }
            }
          }
        }
      }
      if (alloc_disc_count > 0) {
        Emit_Nested_Disc_Checks (ty);
        for (uint32_t i = 0; i < alloc_disc_count; i++)
          alloc_disc_syms[i]->disc_agg_temp = 0;
      }
    }
  }

  // RM 9.2: Allocating a task type via NEW activates the task immediately.
  // The allocated memory stores the thread handle (ptr-sized).
  if (designated and Type_Is_Task (designated)) {
    uint32_t handle_tmp = Emit_Temp ();
    Emit ("  %%t%u = call ptr @__ada_task_start(ptr @", handle_tmp);
    Emit_Task_Function_Name (designated->defining_symbol, designated->name);
    Emit (", ");
    if (cg->current_nesting_level > 0) {
      Emit ("ptr %%__frame_base)\n");
    } else {
      Emit ("ptr null)\n");
    }
    Emit ("  store ptr %%t%u, ptr %%t%u  ; store task handle\n", handle_tmp, t);
  }
  return t;
}
uint32_t Generate_Expression (Syntax_Node *node) {
  if (not node) return 0;
  switch (node->kind) {
    case NK_INTEGER:    return Generate_Integer_Literal (node);
    case NK_REAL:       return Generate_Real_Literal (node);
    case NK_STRING:     return Generate_String_Literal (node);
    case NK_CHARACTER:  {  // Character literal - extract char from text "'X'"
               uint32_t t = Emit_Temp ();
               int64_t ch = 0;

               // Check if resolved as enumeration literal
               if (node->symbol and node->symbol->kind == SYMBOL_LITERAL) {
                 ch = node->symbol->frame_offset;
               } else if (node->string_val.text.length >= 2) {
                 ch = (unsigned char)node->string_val.text.data[1];

                 // If the node's type is a user-defined enumeration containing                     
                 // character literals, use the position within the enumeration                     
                 // rather than the ASCII code (RM 3.5.1).                                          
                 //                                                                                 
                 Type_Info *etype = node->type;
                 while (etype and (etype->parent_type or etype->base_type))
                   etype = etype->parent_type ? etype->parent_type : etype->base_type;
                 if (etype and etype->kind == TYPE_ENUMERATION and
                   etype->enumeration.literals and etype->enumeration.literal_count > 0) {
                   char target = node->string_val.text.data[1];
                   for (uint32_t j = 0; j < etype->enumeration.literal_count; j++) {
                     String_Slice lit = etype->enumeration.literals[j];
                     if (lit.length == 3 and lit.data[0] == '\'' and
                       lit.data[1] == target and lit.data[2] == '\'') {
                       ch = (int64_t)j;
                       break;
                     }
                   }
                 }
               }

               // character literals use native type width (i8),
               // not Integer_Arith_Type. Widening at use sites.
               const char *ch_type = node->type ? Type_To_Llvm (node->type) : "i8";
               Emit ("  %%t%u = add %s 0, %lld\n", t, ch_type, (long long)ch);
               Temp_Set_Type (t, ch_type);
               return t; }
    case NK_NULL:       { uint32_t t = Emit_Temp ();
               Emit ("  %%t%u = inttoptr i64 0 to ptr\n", t);
               Temp_Set_Type (t, "ptr");
               return t; }
    case NK_IDENTIFIER: return Generate_Identifier (node);
    case NK_SELECTED:   return Generate_Selected (node);
    case NK_ATTRIBUTE:  return Generate_Attribute (node);
    case NK_BINARY_OP:  return Generate_Binary_Op (node);
    case NK_UNARY_OP:   return Generate_Unary_Op (node);
    case NK_APPLY:      return Generate_Apply (node);
    case NK_AGGREGATE:  return Generate_Aggregate (node);
    case NK_QUALIFIED:  return Generate_Qualified (node);
    case NK_ALLOCATOR:  return Generate_Allocator (node);
    default:
      Report_Error (node->location, "unsupported expression kind %d in codegen",
             (int)node->kind);
      return 0;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.4 Statement Code Generation                                                                  
//                                                                                                  
// Statements modify state while expressions compute values, a distinction Ada enforces.            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Generate_Statement_List (Node_List *list) {
  for (uint32_t i = 0; i < list->count; i++) {
    Syntax_Node *stmt = list->items[i];
    if (not stmt) continue;

    // After a terminator (ret/br), we need a new basic block.                                      
    // Labeled statements (NK_LABEL, NK_BLOCK with label, NK_LOOP with label)                       
    // emit their own labels. For unlabeled statements, emit a fresh label.                         
    //                                                                                              
    if (cg->block_terminated) {
      bool will_emit_label = stmt->kind == NK_LABEL or
        (stmt->kind == NK_BLOCK and stmt->block_stmt.label_symbol) or
        (stmt->kind == NK_LOOP and stmt->loop_stmt.label_symbol);

      // Emit a fresh basic block for unreachable code. The subsequent
      // statement will generate instructions that fill this block.
      if (not will_emit_label) {
        uint32_t dead_label = cg->label_id++;
        Emit_Label_Here (dead_label);
        cg->block_terminated = false;

      // The next statement emits its own label - reset block_terminated
      // so it adds an instruction to its label block properly
      } else {
        cg->block_terminated = false;
      }
    }
    Generate_Statement (stmt);
  }
}
void Generate_Assignment (Syntax_Node *node) {
  Syntax_Node *target = node->assignment.target;

  // Emit source location and assignment target info
  Emit_Location (node->location);
  if (target->symbol) {
    Emit ("  ; ASSIGN %.*s :=\n",
       (int)target->symbol->name.length, target->symbol->name.data);
  } else if (target->kind == NK_SELECTED and target->selected.selector.length > 0) {
    Emit ("  ; ASSIGN .%.*s :=\n",
       (int)target->selected.selector.length, target->selected.selector.data);
  } else if (target->kind == NK_APPLY) {
    Emit ("  ; ASSIGN indexed/slice :=\n");
  }

  // For RENAMES: redirect to the renamed object
  if (target->kind == NK_IDENTIFIER and target->symbol and target->symbol->renamed_object) {
    target = target->symbol->renamed_object;
  }

  // Handle indexed component target (array element or slice assignment)
  if (target->kind == NK_APPLY) {
    Type_Info *prefix_type = target->apply.prefix->type;
    bool is_array_target = prefix_type and
      (prefix_type->kind == TYPE_ARRAY or prefix_type->kind == TYPE_STRING);
    if (is_array_target) {
      Symbol *array_sym = target->apply.prefix->symbol;
      if (not array_sym) return;

      // For unconstrained (STRING / unconstrained array) the variable                              
      // holds a fat pointer - we must load it and extract the data ptr.                            
      // For constrained arrays the variable IS the data pointer.                                   
      //                                                                                            
      bool target_is_uncon = (not Type_Is_Constrained_Array (prefix_type) and
                  Type_Is_String (prefix_type)) or
                   Type_Is_Unconstrained_Array (prefix_type);
      Syntax_Node *arg = target->apply.arguments.items[0];

      // Check for slice assignment: ARR (low .. high) := source
      // Array slice assignment using memcpy
      if (arg->kind == NK_RANGE) {
        Type_Info *elem_type_info = prefix_type->array.element_type;
        uint32_t elem_sz = elem_type_info ? elem_type_info->size : 1;
        if (elem_sz == 0) elem_sz = 1;

        // Get destination base address
        uint32_t dest_base;
        int128_t low_bound;

        // Load fat pointer, extract data ptr and low bound
        if (target_is_uncon) {
          const char *sa_bt = Array_Bound_Llvm_Type (prefix_type);
          uint32_t fat = Emit_Load_Fat_Pointer (array_sym, sa_bt);
          dest_base = Emit_Fat_Pointer_Data (fat, sa_bt);

          // Low bound comes from the fat pointer at runtime
          low_bound = 0;  // We'll use dynamic low below
          const char *usa_t = Integer_Arith_Type ();
          uint32_t fat_low = Emit_Fat_Pointer_Low (fat, sa_bt);
          uint32_t fat_low_conv = Emit_Convert (fat_low, sa_bt, usa_t);

          // Calculate destination start offset from slice low bound
          uint32_t dest_low_expr = Generate_Expression (arg->range.low);

          // Subtract dynamic low bound from fat pointer
          uint32_t adj = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %%t%u"
             "  ; adjust for dynamic low bound\n",
             adj, usa_t, dest_low_expr, fat_low_conv);
          uint32_t dest_ptr = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
             dest_ptr, dest_base, usa_t, adj);

          // Generate source and copy
          Syntax_Node *src = node->assignment.value;
          uint32_t dest_high_expr = Generate_Expression (arg->range.high);
          uint32_t length = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %%t%u\n",
             length, usa_t, dest_high_expr, dest_low_expr);
          uint32_t len_plus_one = Emit_Temp ();
          Emit ("  %%t%u = add %s %%t%u, 1\n", len_plus_one, usa_t, length);
          uint32_t byte_len_nat = Emit_Temp ();
          Emit ("  %%t%u = mul %s %%t%u, %u\n",
             byte_len_nat, usa_t, len_plus_one, elem_sz);
          uint32_t byte_len = Emit_Extend_To_I64 (byte_len_nat, usa_t);

          // Get source data
          uint32_t src_val = Generate_Expression (src);
          const char *src_llvm = Expression_Llvm_Type (src);
          uint32_t src_data;
          if (Llvm_Type_Is_Fat_Pointer (src_llvm)) {
            src_data = Emit_Fat_Pointer_Data (src_val, Array_Bound_Llvm_Type (prefix_type));
          } else {
            src_data = src_val;
          }
          Emit ("  call void @llvm.memcpy.p0.p0.i64("
             "ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
             "  ; uncon slice assignment\n",
             dest_ptr, src_data, byte_len);
          return;
        }

        // Constrained array slice assignment (original code)
        const char *csa_t = Integer_Arith_Type ();
        low_bound = Array_Low_Bound (prefix_type);

        // Get destination base address
        dest_base = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr ", dest_base);
        Emit_Symbol_Storage (array_sym);
        Emit (", %s 0\n", csa_t);

        // Evaluate slice bounds and compute copy length from original range.
        // Length must use raw bounds (not index-adjusted) per Ada RM 5.2.1.
        uint32_t dest_lo_raw = Generate_Expression (arg->range.low);
        uint32_t dest_hi_raw = Generate_Expression (arg->range.high);
        uint32_t length = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", length, csa_t, dest_hi_raw, dest_lo_raw);

        // Destination pointer: adjust low bound to array-relative index
        uint32_t dest_idx = dest_lo_raw;
        if (low_bound != 0) {
          dest_idx = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %s\n", dest_idx, csa_t, dest_lo_raw, I128_Decimal (low_bound));
        }
        uint32_t dest_ptr = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
           dest_ptr, dest_base, csa_t, dest_idx);

        // Ada RM 5.2.1: sliding semantics - source slides to destination.
        Syntax_Node *src = node->assignment.value;
        uint32_t len_plus_one = Emit_Temp ();
        Emit ("  %%t%u = add %s %%t%u, 1\n", len_plus_one, csa_t, length);
        uint32_t byte_len = Emit_Temp ();
        Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_len, csa_t, len_plus_one, elem_sz);
        uint32_t byte_len_64 = Emit_Extend_To_I64 (byte_len, csa_t);

        // Determine source data pointer
        uint32_t src_ptr;
        if (src->kind == NK_APPLY and src->apply.arguments.count > 0 and
          src->apply.arguments.items[0]->kind == NK_RANGE) {

          // Source is a slice: SRC_ARR (lo..hi)
          Symbol *src_sym = src->apply.prefix->symbol;
          Type_Info *src_type = src->apply.prefix->type;
          Syntax_Node *src_range = src->apply.arguments.items[0];
          if (src_sym and src_type and
            (src_type->kind == TYPE_ARRAY or src_type->kind == TYPE_STRING)) {
            int128_t src_low_bound = Array_Low_Bound (src_type);
            uint32_t src_base;
            uint32_t src_fat_low = 0;
            const char *ssb = NULL;
            bool src_is_uncon = (not Type_Is_Constrained_Array (src_type) and
                        Type_Is_String (src_type)) or
                       Type_Is_Unconstrained_Array (src_type);
            if (src_is_uncon) {
              ssb = Array_Bound_Llvm_Type (src_type);
              uint32_t sfat = Emit_Load_Fat_Pointer (src_sym, ssb);
              src_base = Emit_Fat_Pointer_Data (sfat, ssb);
              src_fat_low = Emit_Fat_Pointer_Low (sfat, ssb);
            } else {
              src_base = Emit_Temp ();
              Emit ("  %%t%u = getelementptr i8, ptr ", src_base);
              Emit_Symbol_Storage (src_sym);
              Emit (", %s 0\n", csa_t);
            }
            uint32_t src_start = Generate_Expression (src_range->range.low);
            if (src_is_uncon) {
              uint32_t src_fat_cvt = Emit_Convert (src_fat_low, ssb, csa_t);
              uint32_t adj = Emit_Temp ();
              Emit ("  %%t%u = sub %s %%t%u, %%t%u\n",
                 adj, csa_t, src_start, src_fat_cvt);
              src_start = adj;
            } else if (src_low_bound != 0) {
              uint32_t adj = Emit_Temp ();
              Emit ("  %%t%u = sub %s %%t%u, %s\n",
                 adj, csa_t, src_start, I128_Decimal (src_low_bound));
              src_start = adj;
            }
            src_ptr = Emit_Temp ();
            Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %%t%u\n",
               src_ptr, src_base, csa_t, src_start);

          // Fallback: evaluate as general expression
          } else {
            goto general_slice_source;
          }
        } else {
        general_slice_source:;

          // General source: identifier, string literal, function call, etc.                        
          // Ada RM 5.2.1: source slides to destination index range.                                
          // We evaluate the source, extract its data pointer, and memcpy.                          
          //                                                                                        
          uint32_t src_val = Generate_Expression (src);
          const char *src_llvm = Expression_Llvm_Type (src);
          if (Llvm_Type_Is_Fat_Pointer (src_llvm)) {
            src_ptr = Emit_Fat_Pointer_Data (src_val,
              Array_Bound_Llvm_Type (src->type ? src->type : prefix_type));

          // Whole constrained array: get address of storage
          } else if (src->kind == NK_IDENTIFIER and src->symbol) {
            src_ptr = Emit_Temp ();
            Emit ("  %%t%u = getelementptr i8, ptr ", src_ptr);
            Emit_Symbol_Storage (src->symbol);
            Emit (", %s 0\n", csa_t);

          // Expression producing a value (aggregate, call, etc.) -
          // spill to alloca, then use the alloca as source ptr
          } else {
            uint32_t spill = Emit_Temp ();
            Emit ("  %%t%u = alloca %s\n", spill, src_llvm);
            Emit ("  store %s %%t%u, ptr %%t%u\n", src_llvm, src_val, spill);
            src_ptr = spill;
          }
        }
        Emit ("  call void @llvm.memcpy.p0.p0.i64("
           "ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
           "  ; slice assignment\n", dest_ptr, src_ptr, byte_len_64);
        return;
      }

      // Array element assignment: DATA (I) := value                                                
      // Generate_Lvalue handles both unconstrained (fat pointer) and                               
      // constrained arrays - computes element address in one call.                                 
      //                                                                                            
      const char *elem_type_str = Type_To_Llvm (prefix_type->array.element_type);
      uint32_t elem_ptr = Generate_Lvalue (target);
      uint32_t value = Generate_Expression (node->assignment.value);
      const char *value_type = Expression_Llvm_Type (node->assignment.value);
      value = Emit_Convert (value, value_type, elem_type_str);
      Emit ("  store %s %%t%u, ptr %%t%u  ; array element assign\n",
         elem_type_str, value, elem_ptr);
      return;
    }

    // Access-to-array implicit dereference + indexing: ACC_ARR (I) := val                          
    // where ACC_ARR is an access type whose designated type is an array.                           
    // Generate_Lvalue already handles this case properly.                                          
    //                                                                                              
    if (prefix_type and Type_Is_Access (prefix_type) and
      prefix_type->access.designated_type) {
      Type_Info *desig = prefix_type->access.designated_type;
      if (desig->kind == TYPE_ARRAY or desig->kind == TYPE_STRING) {
        const char *elem_type_str = Type_To_Llvm (desig->array.element_type);
        uint32_t elem_ptr = Generate_Lvalue (target);
        uint32_t value = Generate_Expression (node->assignment.value);
        const char *value_type = Expression_Llvm_Type (node->assignment.value);
        value = Emit_Convert (value, value_type, elem_type_str);
        Emit ("  store %s %%t%u, ptr %%t%u  ; access-array element assign\n",
           elem_type_str, value, elem_ptr);
        return;
      }
    }
  }

  // Handle .ALL dereference assignment (NK_UNARY_OP with TK_ALL)
  if (target->kind == NK_UNARY_OP and target->unary.op == TK_ALL) {
    Syntax_Node *operand = target->unary.operand;
    Type_Info *operand_type = operand->type;
    if (Type_Is_Access (operand_type)) {
      Type_Info *designated = operand_type->access.designated_type;

      // Generate_Lvalue loads the pointer value (the storage address)
      uint32_t ptr = Generate_Lvalue (target);

      // Composite types: copy contents via memcpy (RM 5.2)
      if (designated and (Type_Is_Record (designated) or
        (designated->kind == TYPE_ARRAY and designated->size > 0))) {
        uint32_t value = Generate_Expression (node->assignment.value);
        uint32_t sz = designated->size > 0 ? designated->size : 8;
        Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)  ; .ALL composite assign\n",
           ptr, value, sz);
        return;
      }

      // Scalar / access types: store value directly
      const char *dest_type = Type_To_Llvm (designated);
      uint32_t value = Generate_Expression (node->assignment.value);
      const char *value_type = Expression_Llvm_Type (node->assignment.value);
      value = Emit_Convert (value, value_type, dest_type);
      Emit ("  store %s %%t%u, ptr %%t%u  ; .ALL assignment\n",
         dest_type, value, ptr);
      return;
    }
  }

  // Handle selected component target (record field assignment or .ALL)                             
  // Generate_Lvalue computes the storage address - we just determine                               
  // the store type and let Generate_Lvalue handle address computation.                             
  //                                                                                                
  if (target->kind == NK_SELECTED) {
    Syntax_Node *prefix = target->selected.prefix;
    Type_Info *prefix_type = prefix->type;

    // Determine the target type being assigned to
    Type_Info *assign_type = NULL;
    if (Type_Is_Access (prefix_type) and
      Slice_Equal_Ignore_Case (target->selected.selector, S("ALL"))) {

      // .ALL dereference: target is designated type
      assign_type = prefix_type->access.designated_type;

    // Record field: find component type
    } else {
      Type_Info *record_type = prefix_type;
      if (Type_Is_Access (prefix_type) and
        Type_Is_Record (prefix_type->access.designated_type)) {
        record_type = prefix_type->access.designated_type;
      }
      if (Type_Is_Record (record_type)) {
        for (uint32_t i = 0; i < record_type->record.component_count; i++) {
          if (Slice_Equal_Ignore_Case (record_type->record.components[i].name,
                        target->selected.selector)) {
            assign_type = record_type->record.components[i].component_type;
            break;
          }
        }
      }
    }

    // Generate_Lvalue handles .ALL dereference, implicit dereference,
    // and direct field offset - all address computation is unified.
    uint32_t addr = Generate_Lvalue (target);
    uint32_t value = Generate_Expression (node->assignment.value);

    // Composite types: copy contents via memcpy (RM 5.2)
    if (assign_type and (Type_Is_Record (assign_type) or
      (assign_type->kind == TYPE_ARRAY and assign_type->size > 0))) {
      uint32_t sz = assign_type->size > 0 ? assign_type->size : 8;
      Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)  ; composite selected assign\n",
         addr, value, sz);
      return;
    }

    // Discriminant-dependent array stored inline (size=0 at compile time).                         
    // RHS is a fat pointer {ptr,ptr} - extract data ptr and copy bytes                             
    // using the bounds from the fat pointer. (RM 5.2, 3.7.1)                                       
    //                                                                                              
    if (assign_type and Type_Is_Array_Like (assign_type) and
      assign_type->size == 0 and Type_Has_Dynamic_Bounds (assign_type)) {
      const char *value_type = Expression_Llvm_Type (node->assignment.value);
      uint32_t data_ptr;

      // Extract data pointer and bounds from fat pointer
      if (value_type and Llvm_Type_Is_Fat_Pointer (value_type)) {
        data_ptr = Emit_Temp ();
        Emit ("  %%t%u = extractvalue %s %%t%u, 0\n", data_ptr, value_type, value);

        // Get length from bounds
        const char *bt = Array_Bound_Llvm_Type (assign_type);
        uint32_t bnd_ptr = Emit_Temp ();
        Emit ("  %%t%u = extractvalue %s %%t%u, 1\n", bnd_ptr, value_type, value);
        uint32_t lo = Emit_Temp (), hi = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n", lo, bt, bnd_ptr);
        uint32_t hi_gep = Emit_Temp ();
        int bt_sz = (strcmp (bt, "i32") == 0) ? 4 : (strcmp (bt, "i16") == 0) ? 2 : 4;
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %d\n", hi_gep, bnd_ptr, bt_sz);
        Emit ("  %%t%u = load %s, ptr %%t%u\n", hi, bt, hi_gep);
        uint32_t len = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", len, bt, hi, lo);
        uint32_t len1 = Emit_Temp ();
        Emit ("  %%t%u = add %s %%t%u, 1\n", len1, bt, len);
        uint32_t elem_sz = assign_type->array.element_type
          ? assign_type->array.element_type->size : 1;
        if (elem_sz == 0) elem_sz = 1;
        uint32_t byte_cnt = len1;
        if (elem_sz > 1) {
          byte_cnt = Emit_Temp ();
          Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_cnt, bt, len1, elem_sz);
        }

        // Clamp to 0 for null arrays
        uint32_t is_neg = Emit_Temp (), clamped = Emit_Temp (), sz64 = Emit_Temp ();
        Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", is_neg, bt, byte_cnt);
        Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n", clamped, is_neg, bt, bt, byte_cnt);
        Emit ("  %%t%u = sext %s %%t%u to i64\n", sz64, bt, clamped);
        Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)  ; disc-dep array assign\n",
           addr, data_ptr, sz64);

      // Value is already a pointer to data - use memcpy with a safe size
      } else {
        data_ptr = value;
        Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 8, i1 false)  ; disc-dep array assign (ptr)\n",
           addr, data_ptr);
      }
      return;
    }

    // Scalar / access types: store value directly
    const char *store_type = Type_To_Llvm (assign_type);
    const char *value_type = Expression_Llvm_Type (node->assignment.value);
    value = Emit_Convert (value, value_type, store_type);
    Emit ("  store %s %%t%u, ptr %%t%u  ; selected assign\n",
       store_type, value, addr);
    return;
  }

  // Simple variable target
  Symbol *target_sym = target->symbol;
  if (not target_sym) {
    fprintf (stderr, "warning: assignment target has no symbol at %s:%u\n",
        target->location.filename ? target->location.filename : "<unknown>",
        target->location.line);
    return;
  }
  Type_Info *ty = target_sym->type;
  if (cg->current_instance)
    ty = Resolve_Generic_Actual_Type (ty);

  // Handle record assignment (use memcpy) (RM 5.2, 3.7.2)
  if (Type_Is_Record (ty)) {
    uint32_t src_ptr = Generate_Expression (node->assignment.value);
    uint32_t record_size = ty->size > 0 ? ty->size : 8;

    // For constrained discriminated records, verify source discriminants match                     
    // target constraints before assignment (Constraint_Error if mismatch).                         
    // Mutable records (all_defaults, no constraint) allow discriminant change.                     
    // Load each discriminant from source and compare with target                                   
    //                                                                                              
    if (ty->record.has_discriminants and target_sym->is_disc_constrained) {
      for (uint32_t di = 0; di < ty->record.discriminant_count; di++) {
        Component_Info *dc = &ty->record.components[di];
        const char *dt = Type_To_Llvm (dc->component_type);

        // Load source discriminant
        const char *iat_dc = Integer_Arith_Type ();
        uint32_t src_dp = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, %s %u\n",
           src_dp, src_ptr, iat_dc, dc->byte_offset);
        uint32_t src_dv = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u  ; src disc %.*s\n",
           src_dv, dt, src_dp, (int)dc->name.length, dc->name.data);
        if (strcmp (dt, iat_dc) != 0) {
          src_dv = Emit_Convert (src_dv, dt, iat_dc);
        }

        // Load target discriminant
        uint32_t tgt_dp = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr ", tgt_dp);
        Emit_Symbol_Ref (target_sym);
        Emit (", %s %u\n", iat_dc, dc->byte_offset);
        uint32_t tgt_dv = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u  ; tgt disc %.*s\n",
           tgt_dv, dt, tgt_dp, (int)dc->name.length, dc->name.data);
        if (strcmp (dt, iat_dc) != 0) {
          tgt_dv = Emit_Convert (tgt_dv, dt, iat_dc);
        }

        // Compare and raise Constraint_Error on mismatch
        uint32_t cmp = Emit_Temp ();
        Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u  ; disc match?\n",
           cmp, iat_dc, src_dv, tgt_dv);
        uint32_t lbl_ok = cg->label_id++;
        uint32_t lbl_fail = cg->label_id++;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", cmp, lbl_ok, lbl_fail);
        cg->block_terminated = true;
        Emit_Label_Here (lbl_fail);
        Emit_Raise_Constraint_Error ("discriminant mismatch in assignment");
        Emit_Label_Here (lbl_ok);
      }
    }

    // Copy record data (whole record for mutable, or non-discriminant part for constrained)
    Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr ");
    Emit_Symbol_Ref (target_sym);
    Emit (", ptr %%t%u, i64 %u, i1 false)  ; record assignment\n",
       src_ptr, record_size);
    return;
  }

  // Handle constrained array assignment (use memcpy, not store)                                    
  // Check if source is unconstrained (fat pointer) or constrained (ptr).                           
  // Fat pointer sources: STRING type, unconstrained arrays, string literals,                       
  // concatenation results, and slice expressions.                                                  
  //                                                                                                
  if (Type_Is_Constrained_Array (ty)) {
    Type_Info *src_type = node->assignment.value->type;
    if (cg->current_instance)
      src_type = Resolve_Generic_Actual_Type (src_type);
    bool src_is_fat_ptr = Expression_Produces_Fat_Pointer (
      node->assignment.value, src_type);

    // Constrained arrays with dynamic bounds are stored as fat pointers.                           
    // Aggregates produce fat pointer allocas but Expression_Produces_Fat_Pointer                   
    // returns false for NK_AGGREGATE.  Detect this case.                                           
    //                                                                                              
    bool target_is_fat = Type_Has_Dynamic_Bounds (ty);
    bool src_is_agg_fat = (not src_is_fat_ptr and target_is_fat and
      node->assignment.value->kind == NK_AGGREGATE);
    uint32_t src_ptr = Generate_Expression (node->assignment.value);

    // Aggregate produced a fat pointer alloca - load and store
    if (src_is_agg_fat) {
      uint32_t fp = Emit_Temp ();
      Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u  ; load agg fat ptr\n",
         fp, src_ptr);
      Emit ("  store " FAT_PTR_TYPE " %%t%u, ptr ", fp);
      Emit_Symbol_Ref (target_sym);
      Emit ("  ; array assignment (dynamic constrained)\n");

    // Source is unconstrained/string - extract data pointer from fat pointer
    } else if (src_is_fat_ptr) {

      // Length check: verify source length matches constrained target length
      const char *ca_bt = Array_Bound_Llvm_Type (ty);
      uint32_t src_len = Emit_Fat_Pointer_Length (src_ptr, ca_bt);
      if (ty->array.index_count > 0) {
        int128_t lo = Type_Bound_Value (ty->array.indices[0].low_bound);
        int128_t hi = Type_Bound_Value (ty->array.indices[0].high_bound);
        int128_t dst_length = hi - lo + 1;
        if (dst_length < 0) dst_length = 0;
        uint32_t dst_len = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %s  ; constrained target length\n",
           dst_len, ca_bt, I128_Decimal (dst_length));
        Emit_Length_Check (src_len, dst_len, ca_bt, ty);
      }
      Emit_Fat_Pointer_Copy_To_Name (src_ptr, target_sym, ca_bt);

    // Target is fat pointer but source isn't aggregate/fat - store as fat ptr
    } else if (target_is_fat) {
      uint32_t fp = Fat_Ptr_As_Value (src_ptr);
      Emit ("  store " FAT_PTR_TYPE " %%t%u, ptr ", fp);
      Emit_Symbol_Ref (target_sym);
      Emit ("  ; array assignment (dynamic constrained)\n");

    // Source is constrained - memcpy directly
    } else {
      uint32_t array_size = ty->size > 0 ? ty->size : 8;
      Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr ");
      Emit_Symbol_Ref (target_sym);
      Emit (", ptr %%t%u, i64 %u, i1 false)  ; array assignment\n",
         src_ptr, array_size);
    }
    return;
  }

  // Handle unconstrained array/STRING variable assignment.                                         
  // These variables store a fat pointer { ptr, { bound, bound } }.                                 
  // IMPORTANT: In Ada, unconstrained objects have fixed constraints                                
  // after elaboration.  Assignment copies data INTO the existing                                   
  // data storage - it does NOT replace the fat pointer.                                            
  // (Replacing the fat pointer would orphan the data alloca and                                    
  // create dangling pointers to source storage.)                                                   
  //                                                                                                
  // Algorithm:                                                                                     
  //   1. Load existing fat pointer to get data pointer and length                                  
  //   2. Generate source expression                                                                
  //   3. Extract source data pointer (from fat ptr or constrained ptr)                             
  //   4. memcpy source data to existing data storage                                               
  //                                                                                                
  if ((not Type_Is_Constrained_Array (ty) and Type_Is_String (ty)) or
    Type_Is_Unconstrained_Array (ty)) {
    Syntax_Node *src = node->assignment.value;
    Type_Info *src_type = src->type;
    if (cg->current_instance)
      src_type = Resolve_Generic_Actual_Type (src_type);
    bool src_is_fat = Expression_Produces_Fat_Pointer (src, src_type);

    // Load existing fat pointer from the target variable
    const char *ua_bt = Array_Bound_Llvm_Type (ty);
    uint32_t existing_fat = Emit_Load_Fat_Pointer (target_sym, ua_bt);
    uint32_t dest_data = Emit_Fat_Pointer_Data (existing_fat, ua_bt);

    // Compute total flat element count: product of all dimension lengths.                          
    // For 1D arrays this is just (high - low + 1).  For multidim arrays                            
    // (e.g. 3D: dim1_count * dim2_count * dim3_count) we must multiply                             
    // all dimension lengths to get the correct byte size.                                          
    //                                                                                              
    uint32_t ndims = ty->array.index_count;
    uint32_t dest_total_elems;
    if (ndims <= 1) {
      dest_total_elems = Emit_Fat_Pointer_Length (existing_fat, ua_bt);
    } else {
      dest_total_elems = Emit_Fat_Pointer_Length_Dim (existing_fat, ua_bt, 0);
      for (uint32_t d = 1; d < ndims; d++) {
        uint32_t dim_len = Emit_Fat_Pointer_Length_Dim (existing_fat, ua_bt, d);
        uint32_t prod = Emit_Temp ();
        Emit ("  %%t%u = mul %s %%t%u, %%t%u\n",
           prod, ua_bt, dest_total_elems, dim_len);
        dest_total_elems = prod;
      }
    }
    uint32_t dest_len_64 = Emit_Extend_To_I64 (dest_total_elems, ua_bt);

    // Convert element count to byte count.  For STRING/CHARACTER                                   
    // arrays the element size is 1, so this is a no-op.  For arrays                                
    // of larger types (INTEGER, records, etc.) we must scale.                                      
    //                                                                                              
    uint32_t elem_sz = (ty->array.element_type and
              ty->array.element_type->size > 0)
             ? ty->array.element_type->size : 1;
    uint32_t dest_bytes_64 = dest_len_64;
    if (elem_sz > 1) {
      dest_bytes_64 = Emit_Temp ();
      Emit ("  %%t%u = mul i64 %%t%u, %u"
           "  ; elem_count * elem_size\n",
         dest_bytes_64, dest_len_64, elem_sz);
    }

    // Generate source and copy data to existing storage
    uint32_t src_val = Generate_Expression (src);

    // Aggregates with unconstrained type return a fat pointer ALLOCA
    // (ptr to { ptr, ptr }), not a loaded value.  Promote to fat.
    if (not src_is_fat and src->kind == NK_AGGREGATE and src_type and
      Type_Is_Array_Like (src_type) and not src_type->array.is_constrained) {
      uint32_t loaded_fat = Emit_Temp ();
      Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u"
           "  ; load agg fat ptr alloca\n",
         loaded_fat, src_val);
      src_val = loaded_fat;
      src_is_fat = true;
    }

    // Source is fat pointer - extract data pointer, check length, copy.                            
    // For multidim arrays, only check dim-1 length here; the aggregate's                           
    // own constraint checking handles inner dimension consistency.                                 
    //                                                                                              
    if (src_is_fat) {
      uint32_t dest_dim1_len = Emit_Fat_Pointer_Length (existing_fat, ua_bt);
      uint32_t src_len = Emit_Fat_Pointer_Length (src_val, ua_bt);
      Emit_Length_Check (src_len, dest_dim1_len, ua_bt, ty);
      uint32_t src_data = Emit_Fat_Pointer_Data (src_val, ua_bt);
      Emit ("  call void @llvm.memcpy.p0.p0.i64("
         "ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
         "  ; uncon array assign\n",
         dest_data, src_data, dest_bytes_64);

    // Source is constrained (ptr) - memcpy directly
    } else {
      Emit ("  call void @llvm.memcpy.p0.p0.i64("
         "ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
         "  ; uncon array assign from constrained\n",
         dest_data, src_val, dest_bytes_64);
    }
    return;
  }
  uint32_t value = Generate_Expression (node->assignment.value);
  const char *type_str = Type_To_Llvm (ty);

  // Determine source type from the value expression.                                               
  // Resolve through generic actuals so that a formal TYPE_PRIVATE is                               
  // treated as its actual representation (e.g., FLOAT > double).                                   
  //                                                                                                
  Type_Info *value_type = node->assignment.value->type;
  if (cg->current_instance)
    value_type = Resolve_Generic_Actual_Type (value_type);
  bool is_src_float = Type_Is_Float_Representation (value_type);
  bool is_dst_float = Type_Is_Float_Representation (ty);

  // Convert between float and integer if needed                                                    
  // Float to integer: use fptosi. For fixed-point targets, divide by                               
  // SMALL first to get the scaled integer representation (RM 4.5.5)                                
  //                                                                                                
  if (is_src_float and not is_dst_float) {
    if (Type_Is_Fixed_Point (ty)) {
      double small = ty->fixed.small;
      if (small <= 0) small = ty->fixed.delta > 0 ? ty->fixed.delta : 1.0;
      uint64_t bits; memcpy (&bits, &small, sizeof (bits));
      uint32_t small_t = Emit_Temp ();
      Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small=%g\n",
         small_t, (unsigned long long)bits, small);
      uint32_t div_t = Emit_Temp ();
      Emit ("  %%t%u = fdiv double %%t%u, %%t%u\n", div_t, value, small_t);
      value = div_t;
    }
    const char *src_ftype = Float_Llvm_Type_Of (value_type);

    // Ada RM 4.6: round to nearest before truncation
    uint32_t rounded = Emit_Temp ();
    Emit ("  %%t%u = call %s @llvm.round.%s(%s %%t%u)\n",
       rounded, src_ftype, src_ftype, src_ftype, value);
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = fptosi %s %%t%u to %s\n", t, src_ftype, rounded, type_str);
    value = t;

  // Integer to float: use sitofp - use native src/dst types
  } else if (not is_src_float and is_dst_float) {
    const char *src_type = Expression_Llvm_Type (node->assignment.value);
    const char *dst_ftype = Float_Llvm_Type_Of (ty);
    uint32_t t = Emit_Temp ();
    Emit ("  %%t%u = sitofp %s %%t%u to %s\n", t, src_type, value, dst_ftype);
    value = t;

  // Float to float: may need conversion if sizes differ.
  // Determine actual source float type from the expression's type info.
  } else if (is_src_float and is_dst_float) {
    const char *src_ftype = Float_Llvm_Type_Of (value_type);
    const char *dst_ftype = type_str;  // actual storage type
    if (strcmp (src_ftype, dst_ftype) != 0) {
      value = Emit_Convert (value, src_ftype, dst_ftype);
    }

  // Integer/boolean to target type: use actual expression type.                                    
  // constraint check BEFORE conversion, so the check                                               
  // sees the value at its actual type (not yet converted).                                         
  //                                                                                                
  } else {
    const char *src_type_str = Expression_Llvm_Type (node->assignment.value);
    if (ty and Type_Is_Scalar (ty)) {
      Type_Info *src_type_info = node->assignment.value->type;
      Emit_Constraint_Check_With_Type (value, ty, src_type_info, src_type_str);
    }
    value = Emit_Convert (value, src_type_str, type_str);
  }

  // Float scalar constraint check - done after float conversion since                              
  // Emit_Constraint_Check's float path handles its own type conversion.                            
  // Pass actual_val_type so the check knows the current value width                                
  // (value may have been fptrunc'd/fpext'd above).                                                 
  //                                                                                                
  if (ty and Type_Is_Scalar (ty) and (is_src_float or is_dst_float)) {
    Type_Info *src_type_info = node->assignment.value->type;
    Emit_Constraint_Check_With_Type (value, ty, src_type_info, type_str);
  }
  Emit ("  store %s %%t%u, ptr ", type_str, value);
  Emit_Symbol_Storage (target_sym);
  Emit ("\n");
}
void Generate_If_Statement (Syntax_Node *node) {
  Emit_Location (node->location);
  Emit ("  ; IF statement\n");
  uint32_t end_label = Emit_Label ();
  Emit ("  ; -- evaluate condition\n");
  uint32_t cond = Generate_Expression (node->if_stmt.condition);
  uint32_t then_label = Emit_Label ();
  uint32_t next_label = Emit_Label ();
  const char *cond_type = Expression_Llvm_Type (node->if_stmt.condition);
  cond = Emit_Convert (cond, cond_type, "i1");
  Emit ("  br i1 %%t%u, label %%L%u, label %%L%u  ; IF cond -> THEN / ELSE\n",
     cond, then_label, next_label);
  cg->block_terminated = true;
  Emit ("\n  ; -- THEN branch\n");
  Emit_Label_Here (then_label);
  Generate_Statement_List (&node->if_stmt.then_stmts);
  Emit_Branch_If_Needed (end_label);

  // ELSIF parts: each is an NK_IF node with condition + then_stmts
  for (uint32_t i = 0; i < node->if_stmt.elsif_parts.count; i++) {
    Syntax_Node *elsif = node->if_stmt.elsif_parts.items[i];
    Emit ("\n  ; -- ELSIF #%u\n", i + 1);
    Emit_Location (elsif->location);
    Emit_Label_Here (next_label);
    uint32_t ec = Generate_Expression (elsif->if_stmt.condition);
    uint32_t elsif_then = Emit_Label ();
    next_label = Emit_Label ();
    const char *ec_type = Expression_Llvm_Type (elsif->if_stmt.condition);
    ec = Emit_Convert (ec, ec_type, "i1");
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u  ; ELSIF cond\n",
       ec, elsif_then, next_label);
    cg->block_terminated = true;
    Emit_Label_Here (elsif_then);
    Generate_Statement_List (&elsif->if_stmt.then_stmts);
    Emit_Branch_If_Needed (end_label);
  }
  Emit_Label_Here (next_label);
  if (node->if_stmt.else_stmts.count > 0) {
    Emit ("  ; -- ELSE branch\n");
    Generate_Statement_List (&node->if_stmt.else_stmts);
  }
  Emit_Branch_If_Needed (end_label);
  Emit ("  ; -- END IF\n");
  Emit_Label_Here (end_label);
}
void Generate_Loop_Statement (Syntax_Node *node) {
  Emit_Location (node->location);

  // Emit LLVM label for Ada label (enables GOTO targeting this loop)
  Symbol *label_sym = node->loop_stmt.label_symbol;
  if (label_sym) {
    Emit ("  ; LOOP %.*s:\n",
       (int)label_sym->name.length, label_sym->name.data);
    if (label_sym->llvm_label_id == 0)
      label_sym->llvm_label_id = cg->label_id++;
    if (not cg->block_terminated) {
      Emit ("  br label %%L%u\n", label_sym->llvm_label_id);
      cg->block_terminated = true;
    }
    Emit_Label_Here (label_sym->llvm_label_id);  // loop label
    cg->block_terminated = false;  // New block started
  } else {
    Emit ("  ; LOOP (anonymous)\n");
  }
  uint32_t loop_start = Emit_Label ();
  uint32_t loop_body = Emit_Label ();
  uint32_t loop_end = Emit_Label ();
  uint32_t saved_exit = cg->loop_exit_label;
  uint32_t saved_cont = cg->loop_continue_label;
  cg->loop_exit_label = loop_end;
  cg->loop_continue_label = loop_start;

  // Store exit label on loop symbol for named EXIT statements
  if (label_sym) label_sym->loop_exit_label_id = loop_end;
  Emit_Branch_If_Needed (loop_start);
  Emit ("  ; -- loop header (L%u)\n", loop_start);
  Emit_Label_Here (loop_start);

  // Condition check for WHILE loops (FOR loops dispatched to Generate_For_Loop)
  if (node->loop_stmt.iteration_scheme) {
    Emit ("  ; -- WHILE condition\n");
    Syntax_Node *scheme = node->loop_stmt.iteration_scheme;
    uint32_t cond = Generate_Expression (scheme);
    const char *cond_type = Expression_Llvm_Type (scheme);
    cond = Emit_Convert (cond, cond_type, "i1");
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u  ; WHILE cond -> body/exit\n",
       cond, loop_body, loop_end);
    cg->block_terminated = true;
  } else {
    Emit_Branch_If_Needed (loop_body);
  }
  Emit ("  ; -- loop body (L%u)\n", loop_body);
  Emit_Label_Here (loop_body);
  Generate_Statement_List (&node->loop_stmt.statements);
  Emit ("  ; -- back to loop header\n");
  Emit_Branch_If_Needed (loop_start);
  Emit ("  ; -- END LOOP (L%u)\n", loop_end);
  Emit_Label_Here (loop_end);
  cg->loop_exit_label = saved_exit;
  cg->loop_continue_label = saved_cont;
}
void Generate_Return_Statement (Syntax_Node *node) {
  Emit_Location (node->location);
  cg->has_return = true;

  // Check if we're in a BIP function - result built into __BIPaccess
  bool is_bip = BIP_In_BIP_Function ();
  if (cg->current_function) {
    Emit ("  ; RETURN from %.*s%s\n",
       (int)cg->current_function->name.length, cg->current_function->name.data,
       is_bip ? " (BIP)" : "");
  } else {
    Emit ("  ; RETURN\n");
  }
  if (node->return_stmt.expression) {
    Syntax_Node *expr = node->return_stmt.expression;

    // For BIP functions, build result directly into destination
    if (is_bip and cg->current_function and cg->current_function->return_type) {
      Type_Info *ret_type = cg->current_function->return_type;

      // Generate expression and copy to __BIPaccess destination
      // Composite: generate expression (gives ptr), memcpy to dest
      if (Type_Is_Record (ret_type) or Type_Is_Array_Like (ret_type)) {
        uint32_t value = Generate_Expression (expr);
        uint32_t size = ret_type->size > 0 ? ret_type->size : 8;
        Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%__BIPaccess, ptr %%t%u, i64 %u, i1 false)  ; BIP return\n",
           value, size);

      // Scalar: generate, convert, store to destination
      } else {
        uint32_t value = Generate_Expression (expr);
        const char *type_str = Type_To_Llvm_Sig (ret_type);
        const char *expr_type = Expression_Llvm_Type (expr);
        value = Emit_Convert (value, expr_type, type_str);
        Emit ("  store %s %%t%u, ptr %%__BIPaccess  ; BIP scalar return\n",
           type_str, value);
      }
      Emit ("  ret void\n");
      cg->block_terminated = true;
      BIP_End_Function ();
      return;
    }
    uint32_t value = Generate_Expression (expr);
    const char *type_str = cg->current_function and cg->current_function->return_type
      ? Type_To_Llvm_Sig (cg->current_function->return_type) : Integer_Arith_Type ();

    // Convert from expression type to return type
    Type_Info *ret_type = cg->current_function ? cg->current_function->return_type : NULL;
    const char *actual_ty = Temp_Get_Type (value);
    bool val_is_float = (actual_ty and Is_Float_Type (actual_ty)) or
              Type_Is_Float_Representation (expr->type) or
              (expr->type and expr->type->kind == TYPE_UNIVERSAL_REAL);

    // Float expression > fixed-point return: divide by SMALL, fptosi
    if (ret_type and Type_Is_Fixed_Point (ret_type) and val_is_float) {
      double small = ret_type->fixed.small;
      if (small <= 0) small = ret_type->fixed.delta > 0 ? ret_type->fixed.delta : 1.0;
      uint64_t bits; memcpy (&bits, &small, sizeof (bits));
      const char *src_fty = actual_ty ? actual_ty : "double";
      uint32_t small_t = Emit_Temp ();
      Emit ("  %%t%u = fadd %s 0.0, 0x%016llX  ; small=%g\n",
         small_t, src_fty, (unsigned long long)bits, small);
      uint32_t div_t = Emit_Temp ();
      Emit ("  %%t%u = fdiv %s %%t%u, %%t%u\n", div_t, src_fty, value, small_t);
      uint32_t conv_t = Emit_Temp ();
      Emit ("  %%t%u = fptosi %s %%t%u to %s\n", conv_t, src_fty, div_t, type_str);
      value = conv_t;
    } else {
      const char *expr_type = Expression_Llvm_Type (expr);
      value = Emit_Convert (value, expr_type, type_str);
    }

    // RM 6.5: Check return value against function return subtype constraint
    if (ret_type and Type_Is_Scalar (ret_type)) {
      Emit_Constraint_Check_With_Type (value, ret_type, expr->type, type_str);
    }

    // RM 6.5(3): For constrained access subtypes, check that the                                   
    // designated object's discriminant/bounds match the constraint.                                
    // Apply_Type_Conversion with access target calls                                               
    // Apply_Discriminant_Check on the designated object.                                           
    //                                                                                              
    if (ret_type and Type_Is_Access (ret_type) and ret_type->access.designated_type) {
      Type_Info *des = ret_type->access.designated_type;
      if (des->kind == TYPE_RECORD and des->record.has_disc_constraints and
        des->record.discriminant_count > 0 and des->record.disc_constraint_values) {

        // RM 4.8: Non-null, then discriminant check.  Extract data
        // pointer from fat-pointer access values first.
        uint32_t ret_ptr = value;
        { const char *vty = Temp_Get_Type (value);
          if (vty and Llvm_Type_Is_Fat_Pointer (vty)) {
            ret_ptr = Emit_Temp ();
            Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n",
               ret_ptr, value);
          }
        }
        uint32_t is_null = Emit_Temp ();
        Emit ("  %%t%u = icmp eq ptr %%t%u, null\n", is_null, ret_ptr);
        uint32_t skip_lbl = Emit_Label ();
        uint32_t chk_lbl  = Emit_Label ();
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           is_null, skip_lbl, chk_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (chk_lbl);

        Component_Info *dc = &des->record.components[0];
        const char *disc_ty = dc->component_type ? Type_To_Llvm (dc->component_type)
                             : Integer_Arith_Type ();
        uint32_t dp = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
           dp, ret_ptr, dc->byte_offset);
        uint32_t dv = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n", dv, disc_ty, dp);
        const char *iat = Integer_Arith_Type ();
        dv = Emit_Convert (dv, disc_ty, iat);
        uint32_t ev = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", ev, iat,
           (long long)des->record.disc_constraint_values[0]);
        Emit_Discriminant_Check (dv, ev, iat, des);
        Emit ("  br label %%L%u\n", skip_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (skip_lbl);
        cg->block_terminated = false;
      }

      // Array-constrained access: check bounds of designated array.
      // Extract data pointer from fat-pointer access values.
      if (Type_Is_Array_Like (des) and des->array.is_constrained and
        des->array.index_count > 0) {
        uint32_t data_ptr = value;
        { const char *vty = Temp_Get_Type (value);
          if (vty and Llvm_Type_Is_Fat_Pointer (vty)) {
            data_ptr = Emit_Temp ();
            Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n",
               data_ptr, value);
          }
        }
        uint32_t is_null = Emit_Temp ();
        Emit ("  %%t%u = icmp eq ptr %%t%u, null\n", is_null, data_ptr);
        uint32_t skip_lbl = Emit_Label ();
        uint32_t chk_lbl  = Emit_Label ();
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           is_null, skip_lbl, chk_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (chk_lbl);

        // Heap arrays store bounds at [-2] and [-1] before data.
        const char *bt = Array_Bound_Llvm_Type (des);
        int128_t exp_lo = Array_Low_Bound (des);
        int128_t exp_hi = (des->array.index_count > 0 and des->array.indices)
          ? Type_Bound_Value (des->array.indices[0].high_bound) : 0;

        uint32_t blo_p = Emit_Temp ();
        Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i64 -2\n", blo_p, bt, data_ptr);
        uint32_t blo = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n", blo, bt, blo_p);
        uint32_t bhi_p = Emit_Temp ();
        Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i64 -1\n", bhi_p, bt, data_ptr);
        uint32_t bhi = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n", bhi, bt, bhi_p);

        // Compare lo and hi
        const char *iat = Integer_Arith_Type ();
        blo = Emit_Convert (blo, bt, iat);
        bhi = Emit_Convert (bhi, bt, iat);
        uint32_t elo = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %s\n", elo, iat, I128_Decimal (exp_lo));
        uint32_t ehi = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %s\n", ehi, iat, I128_Decimal (exp_hi));
        uint32_t clo = Emit_Temp ();
        Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", clo, iat, blo, elo);
        uint32_t chi = Emit_Temp ();
        Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n", chi, iat, bhi, ehi);
        uint32_t bad = Emit_Temp ();
        Emit ("  %%t%u = or i1 %%t%u, %%t%u\n", bad, clo, chi);
        uint32_t raise_lbl = Emit_Label ();
        uint32_t ok_lbl = Emit_Label ();
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n", bad, raise_lbl, ok_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (raise_lbl);
        Emit_Raise_Constraint_Error ("access array bounds check");
        Emit_Label_Here (ok_lbl);
        Emit ("  br label %%L%u\n", skip_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (skip_lbl);
        cg->block_terminated = false;
      }
    }

    // RM 6.5: For unconstrained array returns, the data pointer in the fat                         
    // pointer points to a local alloca that becomes invalid after return.                          
    // Copy the data to the secondary stack so it survives (Standard:                               
    // Build_Allocate_For_Return with secondary stack allocation).                                  
    //                                                                                              
    if (ret_type and Type_Is_Array_Like (ret_type) and not ret_type->array.is_constrained and
      type_str and strstr (type_str, "{ ptr, ptr }") != NULL) {
      const char *rbt = Array_Bound_Llvm_Type (ret_type);
      uint32_t old_data = Emit_Fat_Pointer_Data (value, rbt);

      // Compute total byte size from bounds
      uint32_t ndims = ret_type->array.index_count;
      if (ndims < 1) ndims = 1;
      uint32_t scalar_sz = ret_type->array.element_type ?
                 ret_type->array.element_type->size : 1;
      uint32_t total = 0;
      for (uint32_t d = 0; d < ndims; d++) {
        uint32_t dl = Emit_Fat_Pointer_Low_Dim (value, rbt, d);
        uint32_t dh = Emit_Fat_Pointer_High_Dim (value, rbt, d);
        uint32_t dim_len = Emit_Length_From_Bounds (dl, dh, rbt);
        uint32_t conv = Emit_Convert (dim_len, rbt, Integer_Arith_Type ());
        if (d == 0) { total = conv; }
        else {
          uint32_t product = Emit_Temp ();
          Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", product, Integer_Arith_Type (), total, conv);
          total = product;
        }
      }
      uint32_t bsz = Emit_Temp ();
      Emit ("  %%t%u = mul %s %%t%u, %u\n", bsz, Integer_Arith_Type (), total, scalar_sz);
      uint32_t bsz64 = Emit_Extend_To_I64 (bsz, Integer_Arith_Type ());
      uint32_t sec_data = Emit_Temp ();
      Emit ("  %%t%u = call ptr @__ada_sec_stack_alloc(i64 %%t%u)\n", sec_data, bsz64);
      Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
         sec_data, old_data, bsz64);

      // Copy bounds to secondary stack too (also on callee stack)
      uint32_t old_bnd = Emit_Temp ();
      Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n", old_bnd, value);
      uint32_t bnd_bytes = ndims * 2 * (uint32_t)(strcmp (rbt, "i64") == 0 ? 8 : 4);
      uint32_t sec_bnd = Emit_Temp ();
      Emit ("  %%t%u = call ptr @__ada_sec_stack_alloc(i64 %u)\n", sec_bnd, bnd_bytes);
      Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)\n",
         sec_bnd, old_bnd, bnd_bytes);

      // Rebuild fat pointer with secondary stack data and bounds
      uint32_t new_fp = Emit_Temp ();
      Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " undef, ptr %%t%u, 0\n", new_fp, sec_data);
      uint32_t new_fp2 = Emit_Temp ();
      Emit ("  %%t%u = insertvalue " FAT_PTR_TYPE " %%t%u, ptr %%t%u, 1\n", new_fp2, new_fp, sec_bnd);
      value = new_fp2;
    }
    Emit ("  ret %s %%t%u\n", type_str, value);
    cg->block_terminated = true;

  // Task entry points return ptr for pthread compatibility
  } else if (cg->in_task_body) {
    Emit ("  ret ptr null\n");
    cg->block_terminated = true;
  } else {
    Emit ("  ret void\n");
    cg->block_terminated = true;
  }
}
void Generate_Case_Statement (Syntax_Node *node) {

  // CASE expr IS WHEN choice => stmts; ... END CASE;
  Emit_Location (node->location);
  Emit ("  ; CASE statement\n");
  Emit ("  ; -- evaluate selector expression\n");
  uint32_t selector = Generate_Expression (node->case_stmt.expression);
  const char *case_type = Expression_Llvm_Type (node->case_stmt.expression);

  // Coerce selector to its declared type (arithmetic may widen to i32)
  selector = Emit_Coerce (selector, case_type);
  uint32_t end_label = Emit_Label ();

  // Generate switch-like structure using branches
  uint32_t num_alts = node->case_stmt.alternatives.count;
  uint32_t *alt_labels = Arena_Allocate (num_alts * sizeof (uint32_t));

  // Allocate labels for each alternative
  for (uint32_t i = 0; i < num_alts; i++) {
    alt_labels[i] = Emit_Label ();
  }

  // Generate branching logic
  Emit ("  ; -- test %u alternative(s)\n", num_alts);
  for (uint32_t i = 0; i < num_alts; i++) {
    Syntax_Node *alt = node->case_stmt.alternatives.items[i];
    uint32_t next_check = (i + 1 < num_alts) ? Emit_Label () : end_label;
    Emit ("  ; -- WHEN alternative #%u (%u choice(s))\n",
       i + 1, alt->association.choices.count);

    // Check each choice in this alternative
    for (uint32_t j = 0; j < alt->association.choices.count; j++) {
      Syntax_Node *choice = alt->association.choices.items[j];

      // OTHERS matches everything - jump to alternative
      if (choice->kind == NK_OTHERS) {
        Emit ("  br label %%L%u\n", alt_labels[i]);

      // Range check: low <= selector <= high
      } else if (choice->kind == NK_RANGE) {
        uint32_t low = Generate_Expression (choice->range.low);
        uint32_t high = Generate_Expression (choice->range.high);

        // Normalize range bounds to selector type
        const char *low_t = Expression_Llvm_Type (choice->range.low);
        const char *high_t = Expression_Llvm_Type (choice->range.high);
        if (strcmp (low_t, case_type) != 0)
          low = Emit_Convert (low, low_t, case_type);
        if (strcmp (high_t, case_type) != 0)
          high = Emit_Convert (high, high_t, case_type);
        uint32_t cmp1 = Emit_Temp ();
        uint32_t cmp2 = Emit_Temp ();
        uint32_t both = Emit_Temp ();
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp1, case_type, low, selector);
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp2, case_type, selector, high);
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", both, cmp1, cmp2);
        uint32_t next_choice = (j + 1 < alt->association.choices.count) ?
                     Emit_Label () : next_check;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           both, alt_labels[i], next_choice);
        if (j + 1 < alt->association.choices.count) {
          Emit_Label_Here (next_choice);
        }

      // Subtype range: WHEN T RANGE low..high => (RM 5.4)
      } else if (choice->kind == NK_SUBTYPE_INDICATION) {
        uint32_t low, high;
        const char *bound_type = case_type;
        Syntax_Node *constraint = choice->subtype_ind.constraint;
        if (constraint and constraint->kind == NK_RANGE_CONSTRAINT and
          constraint->range_constraint.range and
          constraint->range_constraint.range->kind == NK_RANGE) {
          low = Generate_Expression (constraint->range_constraint.range->range.low);
          high = Generate_Expression (constraint->range_constraint.range->range.high);
          bound_type = Expression_Llvm_Type (constraint->range_constraint.range->range.low);

        // Bare subtype name: WHEN SUBTYPE => use declared bounds
        } else {
          Bound_Temps bounds = Emit_Bounds (choice->type, 0);
          low = bounds.low_temp;
          high = bounds.high_temp;
          bound_type = bounds.bound_type;
        }

        // Coerce bounds to selector type for icmp
        if (strcmp (bound_type, case_type) != 0) {
          low = Emit_Convert (low, bound_type, case_type);
          high = Emit_Convert (high, bound_type, case_type);
        }
        uint32_t cmp1 = Emit_Temp ();
        uint32_t cmp2 = Emit_Temp ();
        uint32_t both = Emit_Temp ();
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp1, case_type, low, selector);
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp2, case_type, selector, high);
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", both, cmp1, cmp2);
        uint32_t next_choice = (j + 1 < alt->association.choices.count) ?
                     Emit_Label () : next_check;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           both, alt_labels[i], next_choice);
        if (j + 1 < alt->association.choices.count) {
          Emit_Label_Here (next_choice);
        }
      } else if (choice->symbol and
             (choice->symbol->kind == SYMBOL_TYPE or
            choice->symbol->kind == SYMBOL_SUBTYPE) and
             choice->symbol->type) {

        // Bare subtype name as case choice (RM 5.4): range check
        Bound_Temps bounds = Emit_Bounds (choice->symbol->type, 0);
        uint32_t low = bounds.low_temp, high = bounds.high_temp;

        // Coerce to selector type if needed
        if (strcmp (bounds.bound_type, case_type) != 0) {
          low = Emit_Convert (low, bounds.bound_type, case_type);
          high = Emit_Convert (high, bounds.bound_type, case_type);
        }
        uint32_t cmp1 = Emit_Temp (), cmp2 = Emit_Temp (), both = Emit_Temp ();
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp1, case_type, low, selector);
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n", cmp2, case_type, selector, high);
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n", both, cmp1, cmp2);
        uint32_t next_choice = (j + 1 < alt->association.choices.count) ?
                     Emit_Label () : next_check;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           both, alt_labels[i], next_choice);
        if (j + 1 < alt->association.choices.count)
          Emit_Label_Here (next_choice);

      // Single value check
      } else {
        uint32_t val = Generate_Expression (choice);

        // Normalize choice value to selector type (e.g. boolean
        // expressions produce i1 but BOOLEAN selector is i8).
        const char *val_type = Expression_Llvm_Type (choice);
        if (strcmp (val_type, case_type) != 0) {
          val = Emit_Convert (val, val_type, case_type);
        }
        uint32_t cmp = Emit_Temp ();
        Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n", cmp, case_type, selector, val);
        uint32_t next_choice = (j + 1 < alt->association.choices.count) ?
                     Emit_Label () : next_check;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           cmp, alt_labels[i], next_choice);
        if (j + 1 < alt->association.choices.count) {
          Emit_Label_Here (next_choice);
        }
      }
    }
    if (i + 1 < num_alts) {
      Emit_Label_Here (next_check);
    }
  }

  // Generate alternative bodies - expression is a block with statements
  Emit ("  ; -- CASE alternative bodies\n");
  for (uint32_t i = 0; i < num_alts; i++) {
    Syntax_Node *alt = node->case_stmt.alternatives.items[i];
    Emit ("  ; -- alternative #%u body (L%u)\n", i + 1, alt_labels[i]);
    Emit_Label_Here (alt_labels[i]);
    if (alt->association.expression and
      alt->association.expression->kind == NK_BLOCK) {
      Generate_Statement_List (&alt->association.expression->block_stmt.statements);
    }
    Emit_Branch_If_Needed (end_label);
  }
  Emit ("  ; -- END CASE (L%u)\n", end_label);
  Emit_Label_Here (end_label);
}
void Generate_For_Loop (Syntax_Node *node) {

  // FOR loop with iteration variable - iteration_scheme is NK_BINARY_OP TK_IN
  Syntax_Node *iter = node->loop_stmt.iteration_scheme;

  // Not a FOR loop - fall back to simple loop
  if (not iter or iter->kind != NK_BINARY_OP or iter->binary.op != TK_IN) {
    return;
  }
  Syntax_Node *loop_id = iter->binary.left;
  Syntax_Node *range = iter->binary.right;
  Symbol *loop_var = loop_id->symbol;
  bool is_reverse = node->loop_stmt.is_reverse;

  // Emit source location and FOR loop header comment
  Emit_Location (node->location);
  Symbol *label_sym = node->loop_stmt.label_symbol;
  if (label_sym) {
    Emit ("  ; FOR %.*s IN %s", (int)label_sym->name.length, label_sym->name.data,
       is_reverse ? "REVERSE" : "");
  } else if (loop_var) {
    Emit ("  ; FOR %.*s IN %s", (int)loop_var->name.length, loop_var->name.data,
       is_reverse ? "REVERSE" : "");
  } else {
    Emit ("  ; FOR <anon> IN %s", is_reverse ? "REVERSE" : "");
  }
  Emit (" ... LOOP\n");
  uint32_t loop_start = Emit_Label ();
  uint32_t loop_body = Emit_Label ();
  uint32_t loop_end = Emit_Label ();

  // Store exit label on loop symbol for named EXIT statements
  if (label_sym) label_sym->loop_exit_label_id = loop_end;
  uint32_t saved_exit = cg->loop_exit_label;
  cg->loop_exit_label = loop_end;

  // Allocate loop variable - use native Integer type, not i64
  const char *loop_type = Integer_Arith_Type ();
  if (loop_var) {
    Emit ("  ; -- alloca loop variable %.*s\n",
       (int)loop_var->name.length, loop_var->name.data);
    Emit ("  %%");
    Emit_Symbol_Name (loop_var);
    Emit (" = alloca %s\n", loop_type);
  }

  // Get range bounds
  Emit ("  ; -- compute range bounds\n");
  uint32_t low_val, high_val;
  if (range and range->kind == NK_RANGE) {
    low_val = Generate_Expression (range->range.low);
    high_val = Generate_Expression (range->range.high);

    // Convert bounds to loop_type - Generate_Expression may produce at
    // a narrower type (e.g. i8 for CHARACTER ranges).
    low_val = Emit_Coerce (low_val, loop_type);
    high_val = Emit_Coerce (high_val, loop_type);
  } else if (range and range->kind == NK_ATTRIBUTE and
         Slice_Equal_Ignore_Case (range->attribute.name, S("RANGE"))) {

    // X'RANGE attribute - need to generate both 'FIRST and 'LAST
    Type_Info *prefix_type = range->attribute.prefix->type;
    Symbol *prefix_sym = range->attribute.prefix->symbol;

    // Check if this is an unconstrained array needing runtime bounds
    Syntax_Node *range_arg_f = range->attribute.arguments.count > 0
                 ? range->attribute.arguments.items[0] : NULL;
    uint32_t for_dim = Get_Dimension_Index (range_arg_f);
    if (prefix_type and Type_Is_Unconstrained_Array (prefix_type) and
      prefix_sym and (prefix_sym->kind == SYMBOL_PARAMETER or
               prefix_sym->kind == SYMBOL_VARIABLE or
               prefix_sym->kind == SYMBOL_DISCRIMINANT)) {
      const char *loop_bt = Array_Bound_Llvm_Type (prefix_type);
      uint32_t fat = Emit_Load_Fat_Pointer (prefix_sym, loop_bt);
      Bound_Temps bounds = Emit_Bounds_From_Fat_Dim (fat, loop_bt, for_dim);

      // Convert bounds from native bt to loop type
      low_val = Emit_Convert (bounds.low_temp, loop_bt, loop_type);
      high_val = Emit_Convert (bounds.high_temp, loop_bt, loop_type);

    // Constrained array - use compile-time bounds
    } else if (Type_Is_Array_Like (prefix_type)) {
      Syntax_Node *range_arg = range->attribute.arguments.count > 0
                   ? range->attribute.arguments.items[0] : NULL;
      uint32_t dim = Get_Dimension_Index (range_arg);
      if (dim < prefix_type->array.index_count) {
        Bound_Temps bounds = Emit_Bounds (prefix_type, dim);
        low_val = bounds.low_temp;
        high_val = bounds.high_temp;
      } else {
        fprintf (stderr, "warning: FOR loop RANGE attribute dimension out of bounds, defaulting to 0\n");
        low_val = high_val = 0;
      }
    } else {

      // Single expression used as range - evaluate as both low and high
      low_val = Generate_Expression (range);
      high_val = low_val;
    }

  // Subtype indication with constraint: SUBTYPE_NAME RANGE low..high                               
  // Per Ada RM 3.6.1(4)/5.5(9): the bounds of the discrete_range                                   
  // must belong to the subtype - CONSTRAINT_ERROR is raised otherwise.                             
  // Apply_Range_Check on both low and high bounds.                                                 
  //                                                                                                
  } else if (range and range->kind == NK_SUBTYPE_INDICATION) {
    Syntax_Node *constraint = range->subtype_ind.constraint;
    if (constraint and constraint->kind == NK_RANGE_CONSTRAINT and
      constraint->range_constraint.range) {
      Syntax_Node *actual_range = constraint->range_constraint.range;
      if (actual_range->kind == NK_RANGE) {
        low_val = Generate_Expression (actual_range->range.low);
        high_val = Generate_Expression (actual_range->range.high);
        low_val = Emit_Coerce (low_val, loop_type);
        high_val = Emit_Coerce (high_val, loop_type);

        // RM 3.2.2(11): check range bounds against subtype ST.                                     
        // Null ranges (low > high) are always compatible.                                          
        // Apply_Range_Check on both bounds.                                                        
        //                                                                                          
        Type_Info *st = range->subtype_ind.subtype_mark
                ? range->subtype_ind.subtype_mark->type : NULL;
        if (st) {
          uint32_t is_null = Emit_Temp ();
          Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n",
             is_null, loop_type, low_val, high_val);
          uint32_t chk_lbl = Emit_Label ();
          uint32_t skip_lbl = Emit_Label ();
          Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
             is_null, skip_lbl, chk_lbl);
          cg->block_terminated = true;
          Emit_Label_Here (chk_lbl);
          Emit_Constraint_Check (low_val, st, NULL);
          Emit_Constraint_Check (high_val, st, NULL);
          Emit ("  br label %%L%u\n", skip_lbl);
          cg->block_terminated = true;
          Emit_Label_Here (skip_lbl);
        }

      // Range might be a name like T'RANGE
      } else {
        low_val = Generate_Expression (actual_range);
        high_val = low_val;
      }

    // No range constraint - use the subtype's type bounds
    } else {
      Bound_Temps b = Emit_Bounds (range->type, 0);
      low_val = b.low_temp;
      high_val = b.high_temp;
    }

  // Just a type name: FOR I IN TYPE_NAME LOOP - iterate over type's range
  } else if (range and range->kind == NK_IDENTIFIER) {
    Bound_Temps b = Emit_Bounds (range->type, 0);
    low_val = b.low_temp;
    high_val = b.high_temp;

  // Other expression - evaluate as low bound, assume scalar with same high
  } else {
    low_val = Generate_Expression (range);
    high_val = low_val;
  }

  // Initialize loop variable
  if (loop_var) {
    Emit ("  ; -- init %.*s := %s bound\n",
       (int)loop_var->name.length, loop_var->name.data,
       is_reverse ? "high" : "low");
    Emit ("  store %s %%t%u, ptr %%", loop_type, is_reverse ? high_val : low_val);
    Emit_Symbol_Name (loop_var);
    Emit ("\n");
  }

  // Loop start - check condition
  Emit ("  ; -- loop header (L%u)\n", loop_start);
  Emit ("  br label %%L%u\n", loop_start);
  cg->block_terminated = true;
  Emit_Label_Here (loop_start);
  uint32_t cur = Emit_Temp ();
  if (loop_var) {
    Emit ("  ; -- load current %.*s\n",
       (int)loop_var->name.length, loop_var->name.data);
    Emit ("  %%t%u = load %s, ptr %%", cur, loop_type);
    Emit_Symbol_Name (loop_var);
    Emit ("\n");
  }
  uint32_t cond = Emit_Temp ();
  if (is_reverse) {
    Emit ("  %%t%u = icmp sge %s %%t%u, %%t%u  ; %.*s >= low?\n",
       cond, loop_type, cur, low_val,
       loop_var ? (int)loop_var->name.length : 1,
       loop_var ? loop_var->name.data : "?");
  } else {
    Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u  ; %.*s <= high?\n",
       cond, loop_type, cur, high_val,
       loop_var ? (int)loop_var->name.length : 1,
       loop_var ? loop_var->name.data : "?");
  }
  Emit ("  br i1 %%t%u, label %%L%u, label %%L%u  ; -> body / end\n", cond, loop_body, loop_end);
  cg->block_terminated = true;

  // Loop body
  Emit ("  ; -- loop body (L%u)\n", loop_body);
  Emit_Label_Here (loop_body);
  Generate_Statement_List (&node->loop_stmt.statements);

  // Ada RM 5.5: loop parameter takes each value exactly once.                                      
  // Must check if cur == final_value BEFORE incrementing to avoid                                  
  // overflow when iterating up to TYPE'LAST or down to TYPE'FIRST.                                 
  //                                                                                                
  Emit ("  ; -- check if at final value before %s\n", is_reverse ? "decrement" : "increment");
  if (loop_var) {
    uint32_t at_end = Emit_Temp ();
    if (is_reverse) {
      Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u  ; %.*s = low? (exit before underflow)\n",
         at_end, loop_type, cur, low_val,
         (int)loop_var->name.length, loop_var->name.data);
    } else {
      Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u  ; %.*s = high? (exit before overflow)\n",
         at_end, loop_type, cur, high_val,
         (int)loop_var->name.length, loop_var->name.data);
    }
    uint32_t loop_inc = Emit_Label ();
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u  ; -> end / inc\n",
       at_end, loop_end, loop_inc);
    cg->block_terminated = true;
    Emit ("  ; -- loop increment (L%u)\n", loop_inc);
    Emit_Label_Here (loop_inc);
    uint32_t next = Emit_Temp ();
    if (is_reverse) {
      Emit ("  %%t%u = sub %s %%t%u, 1  ; %.*s - 1\n",
         next, loop_type, cur, (int)loop_var->name.length, loop_var->name.data);
    } else {
      Emit ("  %%t%u = add %s %%t%u, 1  ; %.*s + 1\n",
         next, loop_type, cur, (int)loop_var->name.length, loop_var->name.data);
    }
    Emit ("  store %s %%t%u, ptr %%", loop_type, next);
    Emit_Symbol_Name (loop_var);
    Emit ("\n");
  }
  Emit ("  ; -- back to loop header\n");
  Emit ("  br label %%L%u\n", loop_start);
  cg->block_terminated = true;
  Emit ("  ; -- END FOR LOOP (L%u)\n", loop_end);
  Emit_Label_Here (loop_end);
  cg->loop_exit_label = saved_exit;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.4.8 Exception Handling                                                                       
//                                                                                                  
// The stack unwinder's memory is what makes exceptions possible.                                   
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Generate_Raise_Statement (Syntax_Node *node) {

  // RAISE E; or RAISE; (reraise)
  Emit_Location (node->location);
  if (node->raise_stmt.exception_name) {
    Symbol *exc = node->raise_stmt.exception_name->symbol;
    if (exc) {
      Emit ("  ; RAISE ");
      Emit_Symbol_Name (exc);
      Emit ("\n");

      // Store exception identity and call __ada_raise
      uint32_t exc_addr = Emit_Temp ();
      Emit ("  %%t%u = ptrtoint ptr ", exc_addr);
      Emit_Exception_Ref (exc);
      Emit (" to i64\n");
      Emit ("  call void @__ada_raise(i64 %%t%u)\n", exc_addr);
    }

  // Reraise current exception
  } else {
    Emit ("  ; RAISE (reraise)\n");
    Emit ("  call void @__ada_reraise()\n");
  }
  Emit ("  unreachable\n");
}
void Generate_Block_Statement (Syntax_Node *node) {

  // Emit location and block info comment
  Emit_Location (node->location);
  Symbol *label_sym = node->block_stmt.label_symbol;
  if (label_sym) {
    Emit ("  ; BLOCK %.*s:\n", (int)label_sym->name.length, label_sym->name.data);
  } else {
    Emit ("  ; BLOCK (anonymous)\n");
  }

  // Emit LLVM label for Ada label (enables GOTO targeting this block)
  if (label_sym) {
    if (label_sym->llvm_label_id == 0)
      label_sym->llvm_label_id = cg->label_id++;
    if (not cg->block_terminated) {
      Emit ("  br label %%L%u\n", label_sym->llvm_label_id);
      cg->block_terminated = true;
    }
    Emit ("  ; -- block entry (L%u)\n", label_sym->llvm_label_id);
    Emit_Label_Here (label_sym->llvm_label_id);  // block label
    cg->block_terminated = false;  // New block started
  }

  // Block with optional declarations and exception handlers
  bool has_handlers = node->block_stmt.handlers.count > 0;

  // Per Ada RM: Exception handlers only cover the statement part,                                  
  // NOT the declarative part. Exceptions in declarations propagate                                 
  // to the enclosing block's handler.                                                              
  //                                                                                                
  if (has_handlers) {
    Emit ("  ; -- has %u exception handler(s)\n", node->block_stmt.handlers.count);

    // Allocate handler frame first (needed for stack allocation order)
    Emit ("  ; -- alloca exception handler frame\n");
    uint32_t handler_frame = Emit_Temp ();
    Emit ("  %%t%u = alloca { ptr, [200 x i8] }, align 16  ; handler frame\n", handler_frame);

    // Generate declarations BEFORE setting up the exception handler.
    // This ensures exceptions in declarative part propagate outward.
    Emit ("  ; -- block declarations (not covered by handler)\n");
    Generate_Declaration_List (&node->block_stmt.declarations);

    // Now setup exception handling for the statement part only
    Emit ("  ; -- setup exception handler for statement part\n");
    uint32_t handler_label = Emit_Label ();
    uint32_t normal_label = Emit_Label ();
    uint32_t end_label = Emit_Label ();

    // Push exception handler
    Emit ("  ; -- push handler and call setjmp\n");
    Emit ("  call void @__ada_push_handler(ptr %%t%u)\n", handler_frame);

    // Call setjmp on the jmp_buf field (field 1)
    uint32_t jmp_buf = Emit_Temp ();
    Emit ("  %%t%u = getelementptr { ptr, [200 x i8] }, ptr %%t%u, i32 0, i32 1\n",
       jmp_buf, handler_frame);
    uint32_t setjmp_result = Emit_Temp ();
    Emit ("  %%t%u = call i32 @setjmp(ptr %%t%u)\n", setjmp_result, jmp_buf);

    // Branch based on setjmp return
    uint32_t is_normal = Emit_Temp ();
    Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", is_normal, setjmp_result);
    Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
       is_normal, normal_label, handler_label);
    cg->block_terminated = true;  // conditional branch terminates block

    // Normal execution path
    Emit ("  ; -- normal execution (L%u)\n", normal_label);
    Emit_Label_Here (normal_label);

    // Save and set exception context
    uint32_t saved_handler = cg->exception_handler_label;
    uint32_t saved_jmp_buf = cg->exception_jmp_buf;
    bool saved_in_region = cg->in_exception_region;
    cg->exception_handler_label = handler_label;
    cg->exception_jmp_buf = jmp_buf;
    cg->in_exception_region = true;

    // Generate block statements (handler covers only this part)
    Emit ("  ; -- BEGIN block statements (covered by handler)\n");
    Generate_Statement_List (&node->block_stmt.statements);

    // Pop handler on normal exit
    Emit ("  ; -- normal exit: pop handler\n");
    Emit ("  call void @__ada_pop_handler()\n");
    Emit ("  br label %%L%u\n", end_label);
    cg->block_terminated = true;  // unconditional branch terminates block

    // Exception handler entry
    Emit ("  ; -- EXCEPTION handler entry (L%u)\n", handler_label);
    Emit_Label_Here (handler_label);
    cg->block_terminated = false;
    Emit ("  call void @__ada_pop_handler()\n");

    // Get exception identity and dispatch to handlers
    uint32_t exc_id = Emit_Current_Exception_Id ();
    Generate_Exception_Dispatch (&node->block_stmt.handlers, exc_id, end_label);

    // End of block
    Emit ("  ; -- END BLOCK (L%u)\n", end_label);
    Emit_Label_Here (end_label);

    // Restore exception context
    cg->exception_handler_label = saved_handler;
    cg->exception_jmp_buf = saved_jmp_buf;
    cg->in_exception_region = saved_in_region;

  // Simple block without exception handlers
  } else {
    if (node->block_stmt.declarations.count > 0)
      Emit ("  ; -- block declarations\n");
    Generate_Declaration_List (&node->block_stmt.declarations);
    Emit ("  ; -- block statements\n");
    Generate_Statement_List (&node->block_stmt.statements);
    Emit ("  ; -- END BLOCK\n");
  }
}
void Generate_Statement (Syntax_Node *node) {
  if (not node) return;
  switch (node->kind) {

    // Procedure call - might be NK_APPLY, NK_IDENTIFIER (no args), or
    // NK_SELECTED (for entry calls like Task.Entry without args)
    case NK_ASSIGNMENT:
      Generate_Assignment (node);
      break;
    case NK_CALL_STMT: {
      Emit_Location (node->location);
      Syntax_Node *target = node->assignment.target;

      // Emit procedure name comment if available
      if (target->symbol) {
        Emit ("  ; CALL %.*s\n",
           (int)target->symbol->name.length, target->symbol->name.data);
      } else if (target->kind == NK_APPLY and target->apply.prefix and
             target->apply.prefix->symbol) {
        Emit ("  ; CALL %.*s(...)\n",
           (int)target->apply.prefix->symbol->name.length,
           target->apply.prefix->symbol->name.data);
      } else if (target->kind == NK_SELECTED) {
        Emit ("  ; CALL (selected component)\n");
      } else {
        Emit ("  ; CALL (expression)\n");
      }
      if (target->kind == NK_APPLY) {
        Generate_Expression (target);

      // Selected component - might be a parameterless entry call like T.E1
      } else if (target->kind == NK_SELECTED) {
        Symbol *entry_sym = target->symbol;

        // Entry call without explicit arguments - if the entry has                                 
        // parameters with defaults, evaluate them and pass a proper                                
        // params block (RM 9.5: defaults evaluated at call site).                                  
        //                                                                                          
        if (entry_sym and entry_sym->kind == SYMBOL_ENTRY) {
          Emit ("  ; Entry call: %.*s\n",
             (int)entry_sym->name.length, entry_sym->name.data);
          uint32_t n_params = entry_sym->parameter_count;
          uint32_t param_block = Emit_Temp ();
          if (n_params > 0) {
            Emit ("  %%t%u = alloca [%u x i64]  ; entry default params\n",
               param_block, n_params);
            for (uint32_t p = 0; p < n_params; p++) {
              Syntax_Node *def = entry_sym->parameters[p].default_value;
              if (def) {
                uint32_t val = Generate_Expression (def);
                const char *vt = Expression_Llvm_Type (def);
                val = Emit_Convert (val, vt, "i64");
                uint32_t pp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr [%u x i64], ptr %%t%u, i64 0, i64 %u\n",
                   pp, n_params, param_block, p);
                Emit ("  store i64 %%t%u, ptr %%t%u\n", val, pp);
              }
            }
          } else {
            Emit ("  %%t%u = inttoptr i64 0 to ptr  ; no parameters\n", param_block);
          }

          // Get task object from prefix.                                                           
          // For access-to-task, load the pointer (implicit dereference).                           
          // For .ALL dereference (P.ALL.E1), unwrap to get the access var.                         
          //                                                                                        
          uint32_t task_ptr = Emit_Temp ();
          Syntax_Node *pfx = target->selected.prefix;
          Symbol *task_sym = pfx->symbol;

          // Handle explicit .ALL: prefix is NK_UNARY_OP (TK_ALL)
          if (not task_sym and pfx->kind == NK_UNARY_OP and
            pfx->unary.op == TK_ALL and pfx->unary.operand) {
            task_sym = pfx->unary.operand->symbol;
          }
          if (task_sym and task_sym->type and Type_Is_Access (task_sym->type)) {
            Emit ("  %%t%u = load ptr, ptr ", task_ptr);
            Emit_Symbol_Storage (task_sym);
            Emit ("  ; access-to-task deref\n");
          } else if (task_sym) {
            Emit ("  %%t%u = getelementptr i8, ptr ", task_ptr);
            Emit_Symbol_Storage (task_sym);
            Emit (", i64 0  ; task object\n");
          } else {
            Emit ("  %%t%u = inttoptr i64 0 to ptr  ; no task\n", task_ptr);
          }

          // Get entry index (simple entry, not family)
          const char *eidx_t2 = Integer_Arith_Type ();
          uint32_t entry_idx = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %u  ; entry index (simple entry)\n",
             entry_idx, eidx_t2, entry_sym->entry_index * 1000);

          // Call runtime entry call function - widen entry index for RTS ABI
          uint32_t entry_idx_64 = Emit_Extend_To_I64 (entry_idx, eidx_t2);
          Emit ("  call void @__ada_entry_call(ptr %%t%u, i64 %%t%u, ptr %%t%u)\n",
             task_ptr, entry_idx_64, param_block);
        } else if (entry_sym and (entry_sym->kind == SYMBOL_PROCEDURE or
                     entry_sym->kind == SYMBOL_FUNCTION)) {

          // Qualified procedure call like Pkg.Proc - generate actual call
          Symbol *proc = entry_sym;
          bool callee_is_nested = Subprogram_Needs_Static_Chain (proc);
          uint32_t frame_pre = callee_is_nested ?
            Precompute_Nested_Frame_Arg (proc) : 0;

          // Function call - capture result
          if (proc->kind == SYMBOL_FUNCTION) {
            const char *ret_type = proc->return_type ?
              Type_To_Llvm_Sig (proc->return_type) : "i32";
            uint32_t t = Emit_Temp ();
            Emit ("  %%t%u = call %s @", t, ret_type);
            Emit_Symbol_Name (proc);
            Emit ("(");
            if (callee_is_nested) {
              Emit_Nested_Frame_Arg (proc, frame_pre);
            }
            Emit (")\n");

          // Procedure call - void return
          } else {
            Emit ("  call void @");
            Emit_Symbol_Name (proc);
            Emit ("(");
            if (callee_is_nested) {
              Emit_Nested_Frame_Arg (proc, frame_pre);
            }
            Emit (")\n");
          }
        }

      // Parameterless procedure/function call
      } else if (target->kind == NK_IDENTIFIER) {
        Symbol *original_sym = target->symbol;  // Keep for defaults
        Symbol *proc = original_sym;

        // Follow rename chain to get actual target for function name
        while (proc and proc->renamed_object and
             (proc->kind == SYMBOL_FUNCTION or proc->kind == SYMBOL_PROCEDURE)) {
          Symbol *renamed_target = (Symbol *)proc->renamed_object;
          if (renamed_target->kind == SYMBOL_FUNCTION or
            renamed_target->kind == SYMBOL_PROCEDURE) {
            proc = renamed_target;
          } else {
            break;
          }
        }

        // Check if calling a nested function (transitively inside another subprogram)
        if (proc and (proc->kind == SYMBOL_PROCEDURE or proc->kind == SYMBOL_FUNCTION)) {
          bool callee_is_nested = Subprogram_Needs_Static_Chain (proc);

          // Pre-compute default arguments BEFORE building the call                                 
          // instruction, so complex expressions (record/array aggregates)                          
          // don't interleave their IR with the call text.                                          
          //                                                                                        
          #define MAX_DEFAULT_ARGS 32
          uint32_t default_vals[MAX_DEFAULT_ARGS];
          const char *default_types[MAX_DEFAULT_ARGS];
          uint32_t n_defaults = 0;
          if (original_sym->parameter_count > 0) {
            for (uint32_t i = 0; i < original_sym->parameter_count and
               i < MAX_DEFAULT_ARGS; i++) {
              if (original_sym->parameters[i].default_value) {
                uint32_t val = Generate_Expression (original_sym->parameters[i].default_value);
                Type_Info *pt = original_sym->parameters[i].param_type;
                bool is_composite = pt and (pt->kind == TYPE_RECORD or
                  pt->kind == TYPE_ARRAY or pt->kind == TYPE_STRING or
                  pt->kind == TYPE_TASK);
                bool is_access = pt and (pt->kind == TYPE_ACCESS);

                // Composite default: extract data ptr from fat ptr                                 
                // for statically-constrained array formals.                                        
                // Dynamic-bounded constrained arrays use fat ptrs                                  
                // in the ABI (Type_To_Llvm_Sig), so pass as-is.                                    
                //                                                                                  
                if (is_composite) {
                  bool sig_is_fat = pt and
                    (pt->kind == TYPE_ARRAY or pt->kind == TYPE_STRING) and
                    pt->array.is_constrained and Type_Has_Dynamic_Bounds (pt);
                  const char *vty = Temp_Get_Type (val);
                  if (not sig_is_fat and
                    (Temp_Is_Fat_Alloca (val) or
                     (vty and Llvm_Type_Is_Fat_Pointer (vty))) and
                    pt and Type_Is_Constrained_Array (pt)) {
                    val = Fat_Ptr_As_Value (val);
                    uint32_t dp = Emit_Temp ();
                    Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE
                       " %%t%u, 0\n", dp, val);
                    val = dp;
                  }
                  if (sig_is_fat) {
                    val = Fat_Ptr_As_Value (val);

                    // String literals carry default bounds 1..N but                                
                    // must match the formal's index constraint (RM 4.3.2).                         
                    // Rebuild fat pointer with parameter type's bounds.                            
                    //                                                                              
                    Syntax_Node *def = original_sym->parameters[i].default_value;
                    if (def and def->kind == NK_STRING and
                      pt->array.index_count > 0) {
                      const char *bt = Array_Bound_Llvm_Type (pt);
                      uint32_t dp = Emit_Temp ();
                      Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE
                         " %%t%u, 0\n", dp, val);
                      uint32_t lo = Emit_Single_Bound (&pt->array.indices[0].low_bound, bt);
                      uint32_t hi = Emit_Single_Bound (&pt->array.indices[0].high_bound, bt);
                      val = Emit_Fat_Pointer_Dynamic (dp, lo, hi, bt);
                    }
                    default_vals[i] = val;
                    default_types[i] = FAT_PTR_TYPE;
                  } else {
                    default_vals[i] = val;
                    default_types[i] = "ptr";
                  }

                  // Array constraint check: compare aggregate element                              
                  // count per dimension against the formal parameter                               
                  // type's runtime bounds (RM 6.4.1).                                              
                  //                                                                                
                  Syntax_Node *def_expr = original_sym->parameters[i].default_value;
                  if (pt and
                    (pt->kind == TYPE_ARRAY or pt->kind == TYPE_STRING) and
                    pt->array.index_count > 0 and pt->array.indices and
                    def_expr->kind == NK_AGGREGATE) {
                    const char *iat = Integer_Arith_Type ();

                    // Count elements per dimension from aggregate literal.                         
                    // For OTHERS-only aggregates, skip the length check                            
                    // since OTHERS inherently matches the type's range.                            
                    //                                                                              
                    uint32_t agg_dim_lengths[8] = {0};
                    bool agg_is_others_only[8] = {false};
                    uint32_t agg_ndims = 0;
                    Syntax_Node *cur_agg = def_expr;
                    for (uint32_t d = 0; d < pt->array.index_count and d < 8; d++) {
                      if (not cur_agg or cur_agg->kind != NK_AGGREGATE) break;

                      // Count actual elements, not just items.                                     
                      // Named range choices like 0..5 contribute                                   
                      // (hi-lo+1) elements, not just 1.                                            
                      //                                                                            
                      {
                        uint32_t elem_cnt = 0;
                        bool has_others_here = false;
                        for (uint32_t qi = 0; qi < cur_agg->aggregate.items.count; qi++) {
                          Syntax_Node *qi_n = cur_agg->aggregate.items.items[qi];
                          if (qi_n->kind == NK_ASSOCIATION) {
                            for (uint32_t qc = 0; qc < qi_n->association.choices.count; qc++) {
                              Syntax_Node *ch = qi_n->association.choices.items[qc];
                              if (Is_Others_Choice (ch)) {
                                has_others_here = true;
                              } else if (ch->kind == NK_RANGE and
                                     Is_Static_Int_Node (ch->range.low) and
                                     Is_Static_Int_Node (ch->range.high)) {
                                int128_t rlo = Static_Int_Value (ch->range.low);
                                int128_t rhi = Static_Int_Value (ch->range.high);
                                int128_t cnt = rhi - rlo + 1;
                                if (cnt > 0) elem_cnt += (uint32_t)cnt;
                              } else if (Is_Static_Int_Node (ch)) {
                                elem_cnt++;
                              } else {
                                elem_cnt++;  // dynamic: count as 1
                              }
                            }
                          } else {
                            elem_cnt++;  // positional
                          }
                        }
                        if (has_others_here) {
                          agg_is_others_only[d] = true;
                        }
                        agg_dim_lengths[d] = elem_cnt;
                      }

                      // (OTHERS detection handled above)
                      agg_ndims = d + 1;
                      if (cur_agg->aggregate.items.count > 0) {
                        Syntax_Node *first = cur_agg->aggregate.items.items[0];
                        if (first and first->kind == NK_ASSOCIATION)
                          first = first->association.expression;
                        cur_agg = first;
                      } else {
                        cur_agg = NULL;
                      }
                    }
                    for (uint32_t d = 0; d < agg_ndims and d < pt->array.index_count; d++) {
                      if (agg_is_others_only[d]) continue;  // OTHERS matches any range
                      Index_Info *fi = &pt->array.indices[d];
                      uint32_t f_lo = Emit_Bound_Value (&fi->low_bound);
                      uint32_t f_hi = Emit_Bound_Value (&fi->high_bound);
                      if (f_lo and f_hi) {
                        uint32_t fl = Emit_Temp ();
                        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", fl, iat,
                           Emit_Convert (f_hi, iat, iat),
                           Emit_Convert (f_lo, iat, iat));
                        uint32_t fl1 = Emit_Temp ();
                        Emit ("  %%t%u = add %s %%t%u, 1\n", fl1, iat, fl);

                        // Clamp to 0 for null ranges (RM 3.6.1)
                        uint32_t neg = Emit_Temp ();
                        Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", neg, iat, fl1);
                        uint32_t clamped = Emit_Temp ();
                        Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
                           clamped, neg, iat, iat, fl1);
                        uint32_t al = Emit_Temp ();
                        Emit ("  %%t%u = add %s 0, %u  ; agg dim %u count\n",
                           al, iat, agg_dim_lengths[d], d);
                        Emit_Length_Check (al, clamped, iat, pt);
                      }
                    }
                  }
                  Type_Info *def_type = original_sym->parameters[i].default_value->type;

                  // Record discriminant constraint check (RM 3.7.2)
                  if (pt and pt->kind == TYPE_RECORD and
                    pt->record.has_disc_constraints and
                    pt->record.disc_constraint_values and
                    def_type and def_type->kind == TYPE_RECORD) {
                    for (uint32_t d = 0; d < pt->record.discriminant_count; d++) {
                      Component_Info *dc = &pt->record.components[d];
                      if (not dc->component_type) continue;
                      const char *dt = Type_To_Llvm (dc->component_type);

                      // Load discriminant from aggregate
                      uint32_t dp = Emit_Temp ();
                      Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
                         dp, val, dc->byte_offset);
                      uint32_t dv = Emit_Temp ();
                      Emit ("  %%t%u = load %s, ptr %%t%u\n", dv, dt, dp);

                      // Get constraint value
                      uint32_t cv;
                      cv = Emit_Disc_Constraint_Value (pt, d, dt);
                      if (cv == 0) {
                        cv = Emit_Temp ();
                        Emit ("  %%t%u = add %s 0, 0\n", cv, dt);
                      }

                      // Compare
                      uint32_t cmp = Emit_Temp ();
                      Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                         cmp, dt, dv, cv);
                      uint32_t rl = cg->label_id++;
                      uint32_t cl = cg->label_id++;
                      Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                         cmp, rl, cl);
                      cg->block_terminated = true;
                      Emit_Label_Here (rl);
                      Emit_Raise_Constraint_Error ("discriminant check");
                      Emit_Label_Here (cl);
                      cg->block_terminated = false;
                    }
                  }

                  // Record component constraint check (RM 3.3.2)
                  if (pt and pt->kind == TYPE_RECORD and def_type and
                    def_type->kind == TYPE_RECORD) {
                    for (uint32_t ci = 0; ci < pt->record.component_count; ci++) {
                      Component_Info *comp = &pt->record.components[ci];
                      if (comp->is_discriminant or not comp->component_type) continue;
                      Type_Info *ct = comp->component_type;
                      if (not Type_Is_Scalar (ct)) continue;
                      bool has_bounds = (ct->low_bound.kind == BOUND_INTEGER or
                        ct->low_bound.kind == BOUND_FLOAT or
                        ct->low_bound.kind == BOUND_EXPR) and
                        (ct->high_bound.kind == BOUND_INTEGER or
                        ct->high_bound.kind == BOUND_FLOAT or
                        ct->high_bound.kind == BOUND_EXPR);
                      if (not has_bounds) continue;
                      const char *ct_llvm = Type_To_Llvm (ct);

                      // Load component value from aggregate
                      uint32_t cp = Emit_Temp ();
                      Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
                         cp, val, comp->byte_offset);
                      uint32_t cv2 = Emit_Temp ();
                      Emit ("  %%t%u = load %s, ptr %%t%u\n", cv2, ct_llvm, cp);
                      Emit_Constraint_Check_With_Type (cv2, ct, NULL, ct_llvm);
                    }
                  }

                // Access type default is already a ptr
                } else if (is_access) {
                  default_vals[i] = val;
                  default_types[i] = "ptr";
                } else {
                  const char *param_type = pt ?
                    Type_To_Llvm (pt) : Integer_Arith_Type ();
                  val = Emit_Convert (val, Integer_Arith_Type (), param_type);

                  // Scalar constraint check (RM 6.4.1)
                  val = Emit_Constraint_Check_With_Type (val, pt,
                    original_sym->parameters[i].default_value->type,
                    param_type);
                  default_vals[i] = val;
                  default_types[i] = param_type;
                }
              } else {
                default_vals[i] = 0;
                default_types[i] = Integer_Arith_Type ();
              }
            }
            n_defaults = original_sym->parameter_count < MAX_DEFAULT_ARGS
              ? original_sym->parameter_count : MAX_DEFAULT_ARGS;
          }

          // Precompute static chain pointer before call (RM 8.3)
          uint32_t frame_pre = callee_is_nested ?
            Precompute_Nested_Frame_Arg (proc) : 0;
          if (proc->return_type) {
            Emit ("  call %s @", Type_To_Llvm_Sig (proc->return_type));
          } else {
            Emit ("  call void @");
          }
          Emit_Symbol_Name (proc);
          Emit ("(");

          // Pass frame pointer to nested functions (RM 8.3 static chain)
          bool frame_emitted = false;
          if (callee_is_nested) {
            frame_emitted = Emit_Nested_Frame_Arg (proc, frame_pre);
          }

          // Emit pre-computed default arguments
          for (uint32_t i = 0; i < n_defaults; i++) {
            if (frame_emitted or i > 0) Emit (", ");
            if (default_vals[i]) {
              Emit ("%s %%t%u", default_types[i], default_vals[i]);
            } else {
              Emit ("%s 0", default_types[i]);
            }
          }
          Emit (")\n");
        }
      }
    } break;
    case NK_RETURN:
      Generate_Return_Statement (node);
      break;
    case NK_IF:
      Generate_If_Statement (node);
      break;
    case NK_LOOP:
      if (node->loop_stmt.iteration_scheme and
        node->loop_stmt.iteration_scheme->kind == NK_BINARY_OP and
        node->loop_stmt.iteration_scheme->binary.op == TK_IN) {
        Generate_For_Loop (node);
      } else {
        Generate_Loop_Statement (node);
      }
      break;

    // Named EXIT targets a specific loop; unnamed exits innermost
    case NK_CASE:
      Generate_Case_Statement (node);
      break;
    case NK_EXIT:
      {
        Emit_Location (node->location);
        Symbol *tgt = node->exit_stmt.target;
        if (tgt) {
          Emit ("  ; EXIT %.*s", (int)tgt->name.length, tgt->name.data);
        } else {
          Emit ("  ; EXIT (innermost loop)");
        }
        if (node->exit_stmt.condition)
          Emit (" WHEN ...\n");
        else
          Emit ("\n");
        uint32_t exit_label = cg->loop_exit_label;
        if (tgt and tgt->loop_exit_label_id)
          exit_label = tgt->loop_exit_label_id;
        if (node->exit_stmt.condition) {
          Syntax_Node *exit_cond = node->exit_stmt.condition;
          Emit ("  ; -- evaluate exit condition\n");
          uint32_t cond = Generate_Expression (exit_cond);
          const char *cond_type = Expression_Llvm_Type (exit_cond);
          cond = Emit_Convert (cond, cond_type, "i1");
          uint32_t cont = Emit_Label ();
          Emit ("  br i1 %%t%u, label %%L%u, label %%L%u  ; WHEN true -> exit / continue\n",
             cond, exit_label, cont);
          cg->block_terminated = true;
          Emit_Label_Here (cont);
        } else {
          Emit ("  br label %%L%u  ; unconditional exit\n", exit_label);
          cg->block_terminated = true;
        }
      }
      break;

    // DELAY expression - sleep for specified duration
    case NK_NULL_STMT:

      // No code needed
      break;
    case NK_BLOCK:
      Generate_Block_Statement (node);
      break;
    case NK_RAISE:
      Generate_Raise_Statement (node);
      break;
    case NK_DELAY:
      {

        // Expression should be in seconds, convert to microseconds
        uint32_t val = Generate_Expression (node->delay_stmt.expression);

        // If expression is fixed-point (e.g. DURATION), convert from
        // scaled integer to seconds: sitofp then multiply by SMALL
        Type_Info *delay_type = node->delay_stmt.expression->type;
        if (delay_type and Type_Is_Fixed_Point (delay_type)) {
          const char *fix_llvm = Type_To_Llvm (delay_type);
          uint32_t dbl_val = Emit_Temp ();
          Emit ("  %%t%u = sitofp %s %%t%u to double\n", dbl_val, fix_llvm, val);
          double small = delay_type->fixed.small;
          if (small <= 0) small = delay_type->fixed.delta > 0 ? delay_type->fixed.delta : 1.0;
          uint64_t sbits; memcpy (&sbits, &small, sizeof (sbits));
          uint32_t sec_val = Emit_Temp ();
          Emit ("  %%t%u = fmul double %%t%u, 0x%016llX  ; * SMALL\n",
             sec_val, dbl_val, (unsigned long long)sbits);
          val = sec_val;
        }

        // Convert to microseconds (assuming Duration in seconds)
        uint32_t us = Emit_Temp ();
        Emit ("  %%t%u = fmul double %%t%u, 1.0e6\n", us, val);
        uint32_t us_int = Emit_Temp ();
        Emit ("  %%t%u = fptoui double %%t%u to i64\n", us_int, us);
        Emit ("  call void @__ada_delay(i64 %%t%u)\n", us_int);
      }
      break;

    // ACCEPT statement - rendezvous with caller (Ada 83 9.5)
    // Runtime: wait for entry call, execute body, complete rendezvous
    case NK_ACCEPT:
      {
        Emit ("  ; ACCEPT %.*s\n",
           (int)node->accept_stmt.entry_name.length,
           node->accept_stmt.entry_name.data);

        // Get entry index - combine entry_sym's base index with family offset.                     
        // For entry families: entry_idx = base * 1000 + family_arg                                 
        // For simple entries: entry_idx = base * 1000                                              
        //                                                                                          
        const char *acc_eidx_t = Integer_Arith_Type ();
        uint32_t entry_idx = Emit_Temp ();
        uint32_t base_idx = node->accept_stmt.entry_sym ?
                  node->accept_stmt.entry_sym->entry_index : 0;
        if (node->accept_stmt.index) {
          uint32_t idx_val = Generate_Expression (node->accept_stmt.index);
          const char *idx_t = Expression_Llvm_Type (node->accept_stmt.index);
          idx_val = Emit_Convert (idx_val, idx_t, acc_eidx_t);
          Emit ("  %%t%u = add %s %u, %%t%u  ; entry index (base + family)\n",
             entry_idx, acc_eidx_t, base_idx * 1000, idx_val);
        } else {
          Emit ("  %%t%u = add %s 0, %u  ; entry index (simple entry)\n",
             entry_idx, acc_eidx_t, base_idx * 1000);
        }

        // Wait for entry call - widen entry index for RTS ABI
        uint32_t entry_idx_64 = Emit_Extend_To_I64 (entry_idx, acc_eidx_t);
        uint32_t caller_ptr = Emit_Temp ();
        Emit ("  %%t%u = call ptr @__ada_accept_wait(i64 %%t%u)\n",
           caller_ptr, entry_idx_64);

        // Extract params pointer from rendezvous record.                                           
        // RV layout: { ptr task, i64 entry_idx, ptr params, i8 complete, ptr next }                
        // Params is at offset 2 (the third pointer-sized slot).                                    
        //                                                                                          
        uint32_t params_slot = Emit_Temp ();
        Emit ("  %%t%u = getelementptr ptr, ptr %%t%u, i64 2\n",
           params_slot, caller_ptr);
        uint32_t params_ptr = Emit_Temp ();
        Emit ("  %%t%u = load ptr, ptr %%t%u  ; params from rv record\n",
           params_ptr, params_slot);

        // Generate parameters - allocate space and copy from caller's parameter block.             
        // For composite types (arrays/records), allocate the full type size                        
        // and memcpy from the pointer in the params block.                                         
        // For scalars, load the i64 value directly.                                                
        //                                                                                          
        uint32_t param_idx = 0;
        for (uint32_t i = 0; i < node->accept_stmt.parameters.count; i++) {
          Syntax_Node *param = node->accept_stmt.parameters.items[i];
          if (param and param->kind == NK_PARAM_SPEC) {
            for (uint32_t j = 0; j < param->param_spec.names.count; j++) {
              Syntax_Node *name = param->param_spec.names.items[j];
              if (name and name->symbol) {
                Type_Info *pt = name->symbol->type;
                bool is_array = pt and pt->kind == TYPE_ARRAY;
                uint32_t type_size = pt ? pt->size : 0;

                // Load source pointer from params block
                uint32_t pp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr i64, ptr %%t%u, i64 %u\n",
                   pp, params_ptr, param_idx);
                uint32_t pv = Emit_Temp ();
                Emit ("  %%t%u = load i64, ptr %%t%u\n", pv, pp);

                // Array parameter: alloca correct size, memcpy from source.                        
                // For static sizes, use compile-time constant.                                     
                // For dynamic sizes (bounds are BOUND_EXPR), compute at runtime.                   
                //                                                                                  
                if (is_array) {
                  uint32_t src = Emit_Temp ();
                  Emit ("  %%t%u = inttoptr i64 %%t%u to ptr\n", src, pv);

                  // Static size known at compile time
                  if (type_size > 0) {
                    Emit ("  %%");
                    Emit_Symbol_Name (name->symbol);
                    Emit (" = alloca [%u x i8], align 8\n", type_size);
                    Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
                    Emit_Symbol_Name (name->symbol);
                    Emit (", ptr %%t%u, i64 %u, i1 false)\n",
                       src, type_size);

                  // Dynamic bounds: src is caller's fat ptr { data_ptr, bounds_ptr }.
                  // Create a proper local fat pointer so 'LENGTH etc. work.
                  } else {
                    const char *iat = Integer_Arith_Type ();
                    uint32_t ndims = pt->array.index_count;
                    uint32_t elem_sz = pt->array.element_type ?
                               pt->array.element_type->size : 4;
                    if (elem_sz == 0) elem_sz = 4;

                    // 1. Dereference caller's fat ptr to get data_ptr
                    uint32_t dp_gep = Emit_Temp ();
                    Emit ("  %%t%u = getelementptr { ptr, ptr }, ptr %%t%u, i32 0, i32 0\n",
                       dp_gep, src);
                    uint32_t data_ptr = Emit_Temp ();
                    Emit ("  %%t%u = load ptr, ptr %%t%u\n", data_ptr, dp_gep);

                    // 2. Evaluate type bounds, compute data byte size
                    uint32_t lo_regs[16], hi_regs[16];
                    uint32_t total = 0;
                    for (uint32_t d = 0; d < ndims and d < 16; d++) {
                      lo_regs[d] = Emit_Bound_Value (&pt->array.indices[d].low_bound);
                      hi_regs[d] = Emit_Bound_Value (&pt->array.indices[d].high_bound);
                      uint32_t dlen = Emit_Length_From_Bounds (lo_regs[d], hi_regs[d], iat);
                      if (d == 0) { total = dlen; }
                      else {
                        uint32_t p2 = Emit_Temp ();
                        Emit ("  %%t%u = mul %s %%t%u, %%t%u\n",
                           p2, iat, total, dlen);
                        total = p2;
                      }
                    }
                    uint32_t bsz = Emit_Temp ();
                    Emit ("  %%t%u = mul %s %%t%u, %u\n",
                       bsz, iat, total, elem_sz);
                    uint32_t neg_chk = Emit_Temp ();
                    Emit ("  %%t%u = icmp slt %s %%t%u, 0\n",
                       neg_chk, iat, bsz);
                    uint32_t csz = Emit_Temp ();
                    Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
                       csz, neg_chk, iat, iat, bsz);

                    // 3. Alloca for local data copy, memcpy from data_ptr
                    uint32_t data_alloca = Emit_Temp ();
                    Emit ("  %%t%u = alloca i8, %s %%t%u, align 8\n",
                       data_alloca, iat, csz);
                    uint32_t csz64 = Emit_Extend_To_I64 (csz, iat);
                    Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
                       data_alloca, data_ptr, csz64);

                    // 4. Alloca for local bounds struct [2*ndims x i32]
                    uint32_t bounds_alloca = Emit_Temp ();
                    Emit ("  %%t%u = alloca [%u x i32]\n",
                       bounds_alloca, 2 * ndims);
                    for (uint32_t d = 0; d < ndims and d < 16; d++) {
                      uint32_t lo_slot = Emit_Temp ();
                      Emit ("  %%t%u = getelementptr i32, ptr %%t%u, i32 %u\n",
                         lo_slot, bounds_alloca, d * 2);
                      Emit ("  store %s %%t%u, ptr %%t%u\n",
                         iat, lo_regs[d], lo_slot);
                      uint32_t hi_slot = Emit_Temp ();
                      Emit ("  %%t%u = getelementptr i32, ptr %%t%u, i32 %u\n",
                         hi_slot, bounds_alloca, d * 2 + 1);
                      Emit ("  store %s %%t%u, ptr %%t%u\n",
                         iat, hi_regs[d], hi_slot);
                    }

                    // 5. Create fat pointer variable { data_ptr, bounds_ptr }
                    Emit ("  %%");
                    Emit_Symbol_Name (name->symbol);
                    Emit (" = alloca { ptr, ptr }, align 8\n");
                    uint32_t fp_d = Emit_Temp ();
                    Emit ("  %%t%u = getelementptr { ptr, ptr }, ptr %%",
                       fp_d);
                    Emit_Symbol_Name (name->symbol);
                    Emit (", i32 0, i32 0\n");
                    Emit ("  store ptr %%t%u, ptr %%t%u\n",
                       data_alloca, fp_d);
                    uint32_t fp_b = Emit_Temp ();
                    Emit ("  %%t%u = getelementptr { ptr, ptr }, ptr %%",
                       fp_b);
                    Emit_Symbol_Name (name->symbol);
                    Emit (", i32 0, i32 1\n");
                    Emit ("  store ptr %%t%u, ptr %%t%u\n",
                       bounds_alloca, fp_b);
                  }

                // Scalar: alloca i64, store value directly
                } else {
                  Emit ("  %%");
                  Emit_Symbol_Name (name->symbol);
                  Emit (" = alloca i64, align 8\n");
                  Emit ("  store i64 %%t%u, ptr %%", pv);
                  Emit_Symbol_Name (name->symbol);
                  Emit ("\n");
                }
                param_idx++;
              }
            }
          }
        }

        // Execute accept body
        Generate_Statement_List (&node->accept_stmt.statements);

        // Complete rendezvous - unblocks the caller
        Emit ("  call void @__ada_accept_complete(ptr %%t%u)\n", caller_ptr);
      }
      break;
    case NK_SELECT:

      // SELECT statement - selective wait (Ada 83 9.7)                                             
      // Forms: selective_wait, conditional_entry_call, timed_entry_call                            
      // Runtime: check open alternatives, wait or execute else                                     
      //                                                                                            
      {
        uint32_t done_label = cg->label_id++;
        bool has_else = (node->select_stmt.else_part != NULL);
        bool has_delay = false;
        uint32_t delay_label = 0;

        // Check for delay and terminate alternatives
        bool has_terminate = false;
        uint32_t retry_label = 0;
        for (uint32_t i = 0; i < node->select_stmt.alternatives.count; i++) {
          if (node->select_stmt.alternatives.items[i]->kind == NK_DELAY) {
            has_delay = true;
            delay_label = cg->label_id++;
          }
          if (node->select_stmt.alternatives.items[i]->kind == NK_NULL_STMT) {
            has_terminate = true;
          }
        }
        if (has_terminate) {
          retry_label = cg->label_id++;
          Emit ("  br label %%L%u\n", retry_label);
          cg->block_terminated = true;
          Emit_Label_Here (retry_label);  // selective wait retry
        }
        bool delay_label_emitted = false;
        bool skipped_delay = false;  // Track if current iteration was a skipped delay

        // Generate alternatives
        for (uint32_t i = 0; i < node->select_stmt.alternatives.count; i++) {
          Syntax_Node *alt = node->select_stmt.alternatives.items[i];
          uint32_t next_label = cg->label_id++;
          skipped_delay = false;
          switch (alt->kind) {
            case NK_ASSOCIATION:

              // Guarded alternative: WHEN cond => stmt
              {
                Syntax_Node *guard_expr = alt->association.choices.items[0];
                uint32_t guard = Generate_Expression (guard_expr);
                const char *guard_type = Expression_Llvm_Type (guard_expr);
                guard = Emit_Convert (guard, guard_type, "i1");
                Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                   guard, cg->label_id, next_label);
                cg->block_terminated = true;
                Emit_Label_Here (cg->label_id++);
                if (alt->association.expression)
                  Generate_Statement (alt->association.expression);
                Emit ("  br label %%L%u\n", done_label);
              }
              break;
            case NK_ACCEPT:

              // Accept alternative
              {
                Emit ("  ; accept alternative: %.*s\n",
                   (int)alt->accept_stmt.entry_name.length,
                   alt->accept_stmt.entry_name.data);

                // Get entry index - combine base index with family index.
                // Formula: entry_idx = base * 1000 + family_arg
                const char *sel_eidx_t = Integer_Arith_Type ();
                uint32_t entry_idx = Emit_Temp ();
                uint32_t sel_base_idx = alt->accept_stmt.entry_sym ?
                            alt->accept_stmt.entry_sym->entry_index : 0;
                if (alt->accept_stmt.index) {
                  uint32_t idx_val = Generate_Expression (alt->accept_stmt.index);
                  const char *idx_t = Expression_Llvm_Type (alt->accept_stmt.index);
                  idx_val = Emit_Convert (idx_val, idx_t, sel_eidx_t);
                  Emit ("  %%t%u = add %s %u, %%t%u  ; entry index (base + family)\n",
                     entry_idx, sel_eidx_t, sel_base_idx * 1000, idx_val);
                } else {
                  Emit ("  %%t%u = add %s 0, %u  ; entry index (simple entry)\n",
                     entry_idx, sel_eidx_t, sel_base_idx * 1000);
                }

                // Check if entry call is pending - widen for RTS ABI
                uint32_t entry_idx_64 = Emit_Extend_To_I64 (entry_idx, sel_eidx_t);
                uint32_t caller_ptr = Emit_Temp ();
                Emit ("  %%t%u = call ptr @__ada_accept_try(i64 %%t%u)\n",
                   caller_ptr, entry_idx_64);
                uint32_t has_caller = Emit_Temp ();
                Emit ("  %%t%u = icmp ne ptr %%t%u, null\n",
                   has_caller, caller_ptr);
                Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                   has_caller, cg->label_id, next_label);
                cg->block_terminated = true;
                Emit_Label_Here (cg->label_id++);

                // Load parameters from caller
                uint32_t sel_param_idx = 0;
                for (uint32_t pi = 0; pi < alt->accept_stmt.parameters.count; pi++) {
                  Syntax_Node *param = alt->accept_stmt.parameters.items[pi];
                  if (param and param->kind == NK_PARAM_SPEC) {
                    for (uint32_t pj = 0; pj < param->param_spec.names.count; pj++) {
                      Syntax_Node *pname = param->param_spec.names.items[pj];

                      // Allocate space for the parameter
                      if (pname and pname->symbol) {
                        Emit ("  %%");
                        Emit_Symbol_Name (pname->symbol);
                        Emit (" = alloca i64, align 8\n");

                        // Load value from caller's parameter block
                        uint32_t param_ptr = Emit_Temp ();
                        Emit ("  %%t%u = getelementptr i64, ptr %%t%u, i64 %u\n",
                           param_ptr, caller_ptr, sel_param_idx);
                        uint32_t param_val = Emit_Temp ();
                        Emit ("  %%t%u = load i64, ptr %%t%u\n", param_val, param_ptr);

                        // Store to allocated space
                        Emit ("  store i64 %%t%u, ptr %%", param_val);
                        Emit_Symbol_Name (pname->symbol);
                        Emit ("\n");
                        sel_param_idx++;
                      }
                    }
                  }
                }

                // Execute accept body
                Generate_Statement_List (&alt->accept_stmt.statements);

                // Complete rendezvous
                Emit ("  call void @__ada_accept_complete(ptr %%t%u)\n", caller_ptr);
                Emit ("  br label %%L%u\n", done_label);
              }
              break;
            case NK_DELAY:

              // Delay alternative - only emit code once for multiple delays.                       
              // In Ada, multiple delays would pick the shortest, but we simplify                   
              // by using the first delay's duration for all.                                       
              //                                                                                    
              if (not delay_label_emitted) {
                Emit_Label_Here (delay_label);  // delay alternative
                delay_label_emitted = true;
                {
                  uint32_t dur = Generate_Expression (alt->delay_stmt.expression);

                  // Convert fixed-point to double seconds
                  Type_Info *dur_type = alt->delay_stmt.expression->type;
                  if (dur_type and Type_Is_Fixed_Point (dur_type)) {
                    const char *fix_llvm = Type_To_Llvm (dur_type);
                    uint32_t dbl_val = Emit_Temp ();
                    Emit ("  %%t%u = sitofp %s %%t%u to double\n", dbl_val, fix_llvm, dur);
                    double sm = dur_type->fixed.small;
                    if (sm <= 0) sm = dur_type->fixed.delta > 0 ? dur_type->fixed.delta : 1.0;
                    uint64_t sb; memcpy (&sb, &sm, sizeof (sb));
                    uint32_t sec = Emit_Temp ();
                    Emit ("  %%t%u = fmul double %%t%u, 0x%016llX  ; * SMALL\n",
                       sec, dbl_val, (unsigned long long)sb);
                    dur = sec;
                  }
                  uint32_t us = Emit_Temp ();
                  Emit ("  %%t%u = fmul double %%t%u, 1.0e6\n", us, dur);
                  uint32_t us_int = Emit_Temp ();
                  Emit ("  %%t%u = fptoui double %%t%u to i64\n", us_int, us);
                  Emit ("  call void @__ada_delay(i64 %%t%u)\n", us_int);
                }
                Emit ("  br label %%L%u\n", done_label);

              // Subsequent delays: skip code generation entirely
              } else {
                skipped_delay = true;
              }
              break;

            // Emit the next_label for branches that skip this alternative
            case NK_NULL_STMT:

              // Terminate alternative (RM 9.7.1):                                                  
              // Instead of immediately terminating, loop back                                      
              // to re-check accept alternatives.  The task will                                    
              // be terminated when the master scope completes                                      
              // and calls exit (), ending the process.                                             
              //                                                                                    
              Emit ("  ; terminate alternative - sleep and retry\n");
              Emit ("  %%_usel%u = call i32 @usleep(i32 1000)\n",
                 cg->label_id);
              Emit ("  br label %%L%u\n", retry_label);
              break;
            default:
              break;
          }

          // For skipped delay alternatives, don't emit next_label since
          // we've already branched to delay_label and this would be unreachable
          if (not skipped_delay) {
            Emit_Label_Here (next_label);

            // If this isn't the last alternative, fall through to next;
            // otherwise go to delay or done
            bool is_last = (i == node->select_stmt.alternatives.count - 1);

            // Check if next alternative is delay - branch to delay_label instead
            if (not is_last) {
              Syntax_Node *next_alt = node->select_stmt.alternatives.items[i + 1];
              if (next_alt and next_alt->kind == NK_DELAY and has_delay) {
                Emit ("  br label %%L%u\n", delay_label);
              }

              // Otherwise fall through (no br needed, will hit next iteration's code)
            }
          }
        }

        // Else clause or fall through to delay
        if (has_else) {
          Generate_Statement (node->select_stmt.else_part);

        // Already branched to delay_label above
        } else if (has_delay) {
        }
        Emit ("  br label %%L%u\n", done_label);
        cg->block_terminated = true;
        Emit_Label_Here (done_label);
      }
      break;
    case NK_ABORT:

      // ABORT statement - abort named tasks (Ada 83 9.10)
      for (uint32_t i = 0; i < node->abort_stmt.task_names.count; i++) {
        Syntax_Node *task_name = node->abort_stmt.task_names.items[i];
        uint32_t task_ptr = Generate_Expression (task_name);
        Emit ("  call void @__ada_task_abort (ptr %%t%u)\n", task_ptr);
      }
      break;

    // Ada label - allocate LLVM label ID and emit label
    case NK_LABEL:
      {
        Symbol *label_sym = node->label_node.symbol;
        if (label_sym) {
          if (label_sym->llvm_label_id == 0)
            label_sym->llvm_label_id = cg->label_id++;

          // Need a branch to the label to terminate previous block (if not already)
          if (not cg->block_terminated) {
            Emit ("  br label %%L%u\n", label_sym->llvm_label_id);
            cg->block_terminated = true;
          }
          Emit_Label_Here (label_sym->llvm_label_id);  // user label
          cg->block_terminated = false;  // New block started
        }

        // Generate the labeled statement
        if (node->label_node.statement) {
          Generate_Statement (node->label_node.statement);
        }
      }
      break;

    // GOTO statement - use resolved target label and branch
    case NK_GOTO:
      {
        Symbol *label_sym = node->goto_stmt.target;
        if (label_sym) {
          if (label_sym->llvm_label_id == 0) {
            label_sym->llvm_label_id = cg->label_id++;
          }
          Emit ("  br label %%L%u  ; goto %.*s\n",
             label_sym->llvm_label_id,
             (int)node->goto_stmt.name.length,
             node->goto_stmt.name.data);
          cg->block_terminated = true;  // br is a terminator
        } else {
          Emit ("  ; ERROR: undefined label %.*s\n",
             (int)node->goto_stmt.name.length,
             node->goto_stmt.name.data);
        }
      }
      break;

    // Strip quotes if present
    default:
      fprintf (stderr, "warning: unhandled statement kind %d at %s:%u\n",
          node->kind,
          node->location.filename ? node->location.filename : "<unknown>",
          node->location.line);
      break;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.5 Declaration Code Generation                                                                
//                                                                                                  
// Names get bound to meanings, and those bindings are what we generate.                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Check if external name is a builtin/runtime function (already defined)
bool Is_Builtin_Function (String_Slice name) {
  if (name.length >= 2 and name.data[0] == '"' and name.data[name.length-1] == '"') {
    name.data++;
    name.length -= 2;
  }
  const char *builtins[] = {

    // Custom text_io helpers
    "__text_io_new_line", "__text_io_put_char", "__text_io_put",
    "__text_io_put_line", "__text_io_put_int", "__text_io_put_float",
    "__text_io_get_char", "__text_io_get_line",
    "__ada_stdin", "__ada_stdout", "__ada_stderr",

    // Standard C library functions (declared in runtime preamble)
    "memcmp", "setjmp", "longjmp", "exit", "malloc", "realloc", "free",
    "usleep", "printf", "putchar", "strtod", "snprintf", "strlen",
    "fputc", "fputs", "fgetc", "fgets", "fprintf",
    "fopen", "fclose", "fflush", "feof", "ftell", "fseek",
    "ungetc", "remove", "getchar",
    NULL
  };
  for (int i = 0; builtins[i]; i++) {
    if (name.length == strlen (builtins[i]) and
      memcmp (name.data, builtins[i], name.length) == 0) {
      return true;
    }
  }
  return false;
}

// Emit function signature for extern declaration
void Emit_Extern_Subprogram (Symbol *sym) {
  if (not sym) return;
  if (sym->kind != SYMBOL_FUNCTION and sym->kind != SYMBOL_PROCEDURE) return;

  // Skip if already emitted
  if (sym->extern_emitted) return;
  sym->extern_emitted = true;

  // Skip if this is a builtin function that we've already defined
  if (sym->is_imported and sym->external_name.length > 0) {
    if (Is_Builtin_Function (sym->external_name)) {
      return;
    }
  }

  // Get return type
  const char *ret_type = sym->return_type ? Type_To_Llvm_Sig (sym->return_type) : "void";
  Emit ("declare %s @", ret_type);
  Emit_Symbol_Name (sym);
  Emit ("(");

  // Emit parameter types
  for (uint32_t i = 0; i < sym->parameter_count; i++) {
    if (i > 0) Emit (", ");

    // Handle ALI-loaded symbols without full parameter info
    Type_Info *ty = (sym->parameters and i < sym->parameter_count)
            ? sym->parameters[i].param_type : NULL;
    if (ty) {
      Emit ("%s", Type_To_Llvm_Sig (ty));
    } else {
      Emit ("%s", Integer_Arith_Type ());  // Derive from INTEGER for missing param types
    }
  }
  Emit (")\n");
}
void Generate_Declaration_List (Node_List *list) {
  for (uint32_t i = 0; i < list->count; i++) {
    Generate_Declaration (list->items[i]);
  }
}

// Generate an LLVM SSA temp containing the constraint value for                                    
// discriminant disc_index of record type_info.  Prefers the pre-evaluated                          
// alloca (RM 3.7.1 evaluate-once semantics) over the expression                                    
// over the static compile-time value.  Returns 0 when unavailable.                                 
//                                                                                                  
uint32_t Emit_Disc_Constraint_Value (Type_Info *type_info, uint32_t disc_index,
                        const char *disc_type)
{
  if (type_info->record.disc_constraint_preeval and
    type_info->record.disc_constraint_preeval[disc_index]) {
    uint32_t val = Emit_Temp ();
    Emit ("  %%t%u = load %s, ptr %%t%u  ; preeval disc val\n",
       val, disc_type, type_info->record.disc_constraint_preeval[disc_index]);
    return val;
  }
  if (type_info->record.disc_constraint_exprs and
    type_info->record.disc_constraint_exprs[disc_index]) {
    uint32_t val = Generate_Expression (type_info->record.disc_constraint_exprs[disc_index]);
    return Emit_Coerce_Default_Int (val, disc_type);
  }
  if (type_info->record.disc_constraint_values) {
    uint32_t val = Emit_Temp ();
    Emit ("  %%t%u = add %s 0, %lld\n", val, disc_type,
       (long long)type_info->record.disc_constraint_values[disc_index]);
    return val;
  }
  return 0;
}

// RM 3.7.2(3): When a record type with disc-dependent component constraints                        
// is explicitly constrained, the deferred checks on pre-evaluated non-disc                         
// expressions must be performed.  For each component that has disc-dependent                       
// constraints, resolve discriminant values from the parent's now-known                             
// constraints and check pre-evaluated values against their disc subtypes.                          
//                                                                                                  
void Emit_Nested_Disc_Checks (Type_Info *parent_type)
{
  if (not parent_type or parent_type->kind != TYPE_RECORD) return;

  // RM 3.7.3: Determine selected variant to skip absent components.                                
  // Only check components in the fixed part or the selected variant.                               
  // For dynamic disc values, emit runtime range checks.                                            
  //                                                                                                
  int32_t selected_variant = -2;  // -2 = unknown, -1+ = variant index
  uint32_t runtime_disc_val = 0;  // LLVM temp for runtime disc value
  const char *runtime_disc_type = NULL;

  // Check if disc expression is dynamic (disc_constraint_exprs[0] set)
  if (parent_type->record.variant_count > 0) {
    bool is_dynamic = (parent_type->record.disc_constraint_exprs and
               parent_type->record.disc_constraint_exprs[0]);

    // Static: use compile-time range matching
    if (not is_dynamic and parent_type->record.disc_constraint_values) {
      int64_t dv = parent_type->record.disc_constraint_values[0];
      selected_variant = -1;  // default: no matching variant
      for (uint32_t vi = 0; vi < parent_type->record.variant_count; vi++) {
        if (dv >= parent_type->record.variants[vi].disc_value_low and
          dv <= parent_type->record.variants[vi].disc_value_high) {
          selected_variant = (int32_t)vi;
          break;
        }
        if (parent_type->record.variants[vi].is_others)
          selected_variant = (int32_t)vi;
      }

    // Dynamic: evaluate disc expression for runtime variant check.
    // selected_variant stays -2 to trigger runtime branching below.
    } else if (is_dynamic) {
      Syntax_Node *dexpr = parent_type->record.disc_constraint_exprs[0];

      // Try resolving via disc_agg_temp first (allocator/aggregate path)
      if (dexpr->symbol and dexpr->symbol->disc_agg_temp) {
        runtime_disc_type = Type_To_Llvm (
          parent_type->record.components[0].component_type);
        if (not runtime_disc_type) runtime_disc_type = "i32";
        runtime_disc_val = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u\n",
           runtime_disc_val, runtime_disc_type,
           dexpr->symbol->disc_agg_temp);
      } else {
        runtime_disc_val = Generate_Expression (dexpr);
        runtime_disc_type = Integer_Arith_Type ();
        runtime_disc_val = Emit_Coerce_Default_Int (runtime_disc_val, runtime_disc_type);
      }
    }
  }

  // Skip variant components not in the selected variant
  for (uint32_t ci = parent_type->record.discriminant_count;
     ci < parent_type->record.component_count; ci++) {
    int32_t comp_vi = parent_type->record.components[ci].variant_index;
    if (selected_variant != -2 and comp_vi >= 0 and
      comp_vi != selected_variant)
      continue;

    // Runtime variant guard: if disc value is dynamic and this component
    // is in a variant, emit a conditional branch to skip it at runtime.
    uint32_t rt_skip_lbl = 0;
    if (selected_variant == -2 and runtime_disc_val > 0 and comp_vi >= 0 and
      (uint32_t)comp_vi < parent_type->record.variant_count) {
      Variant_Info *vinfo = &parent_type->record.variants[comp_vi];
      if (not vinfo->is_others) {
        uint32_t lo = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", lo, runtime_disc_type,
           (long long)vinfo->disc_value_low);
        uint32_t hi = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", hi, runtime_disc_type,
           (long long)vinfo->disc_value_high);
        uint32_t cmp_lo = Emit_Temp ();
        Emit ("  %%t%u = icmp sge %s %%t%u, %%t%u\n",
           cmp_lo, runtime_disc_type, runtime_disc_val, lo);
        uint32_t cmp_hi = Emit_Temp ();
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n",
           cmp_hi, runtime_disc_type, runtime_disc_val, hi);
        uint32_t in_range = Emit_Temp ();
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n",
           in_range, cmp_lo, cmp_hi);
        uint32_t check_lbl = cg->label_id++;
        rt_skip_lbl = cg->label_id++;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           in_range, check_lbl, rt_skip_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (check_lbl);
      }
    }
    Type_Info *ct = parent_type->record.components[ci].component_type;
    if (not ct or ct->kind != TYPE_RECORD or
      not ct->record.has_disc_constraints) {
      if (rt_skip_lbl) {
        Emit ("  br label %%L%u\n", rt_skip_lbl);
        cg->block_terminated = true;
        Emit_Label_Here (rt_skip_lbl);
      }
      continue;
    }
    for (uint32_t di = 0; di < ct->record.discriminant_count; di++) {
      Component_Info *dc = &ct->record.components[di];
      if (not dc->component_type or not Type_Is_Scalar (dc->component_type))
        continue;
      const char *dt = Type_To_Llvm (dc->component_type);
      uint32_t val = 0;
      if (ct->record.disc_constraint_preeval and
        ct->record.disc_constraint_preeval[di]) {

        // Pre-evaluated non-disc expression: load and check
        val = Emit_Temp ();
        Emit ("  %%t%u = load %s, ptr %%t%u  ; nested disc check\n",
           val, dt, ct->record.disc_constraint_preeval[di]);
      } else if (ct->record.disc_constraint_exprs and
             ct->record.disc_constraint_exprs[di]) {

        // Disc-dependent: resolve from parent's constraints
        Syntax_Node *cexpr = ct->record.disc_constraint_exprs[di];
        if (cexpr->symbol) {
          for (uint32_t pdi = 0;
             pdi < parent_type->record.discriminant_count; pdi++) {
            if (Slice_Equal_Ignore_Case (
                parent_type->record.components[pdi].name,
                cexpr->symbol->name)) {
              val = Emit_Disc_Constraint_Value (parent_type,
                               pdi, dt);
              break;
            }
          }
        }
        if (val == 0) {
          val = Generate_Expression (cexpr);
          val = Emit_Coerce_Default_Int (val, dt);
        }
      } else if (ct->record.disc_constraint_values) {
        val = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", val, dt,
           (long long)ct->record.disc_constraint_values[di]);
      }
      if (val > 0)
        Emit_Constraint_Check (val, dc->component_type, NULL);
    }

    // Recurse into component's own components
    Emit_Nested_Disc_Checks (ct);

    // Close runtime variant guard if opened
    if (rt_skip_lbl) {
      Emit ("  br label %%L%u\n", rt_skip_lbl);
      cg->block_terminated = true;
      Emit_Label_Here (rt_skip_lbl);
    }
  }

  // Also check array components with disc-dependent index bounds.                                  
  // RM 3.6.1(7): when a constrained array's index bounds depend on                                 
  // a discriminant and that discriminant is now known, verify the                                  
  // bounds lie within the index subtype.                                                           
  // Skip variant components not in the selected variant                                            
  //                                                                                                
  for (uint32_t ci = parent_type->record.discriminant_count;
     ci < parent_type->record.component_count; ci++) {
    int32_t comp_vi2 = parent_type->record.components[ci].variant_index;
    if (selected_variant != -2 and comp_vi2 >= 0 and
      comp_vi2 != selected_variant)
      continue;

    // Runtime variant guard for array components
    uint32_t rt_skip_lbl2 = 0;
    if (selected_variant == -2 and runtime_disc_val > 0 and comp_vi2 >= 0 and
      (uint32_t)comp_vi2 < parent_type->record.variant_count) {
      Variant_Info *vinfo = &parent_type->record.variants[comp_vi2];
      if (not vinfo->is_others) {
        uint32_t lo = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", lo, runtime_disc_type,
           (long long)vinfo->disc_value_low);
        uint32_t hi = Emit_Temp ();
        Emit ("  %%t%u = add %s 0, %lld\n", hi, runtime_disc_type,
           (long long)vinfo->disc_value_high);
        uint32_t cmp_lo = Emit_Temp ();
        Emit ("  %%t%u = icmp sge %s %%t%u, %%t%u\n",
           cmp_lo, runtime_disc_type, runtime_disc_val, lo);
        uint32_t cmp_hi = Emit_Temp ();
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n",
           cmp_hi, runtime_disc_type, runtime_disc_val, hi);
        uint32_t in_range = Emit_Temp ();
        Emit ("  %%t%u = and i1 %%t%u, %%t%u\n",
           in_range, cmp_lo, cmp_hi);
        uint32_t check_lbl = cg->label_id++;
        rt_skip_lbl2 = cg->label_id++;
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           in_range, check_lbl, rt_skip_lbl2);
        cg->block_terminated = true;
        Emit_Label_Here (check_lbl);
      }
    }
    Type_Info *ct = parent_type->record.components[ci].component_type;
    if (not ct or not Type_Is_Array_Like (ct) or
      not ct->array.is_constrained) {
      if (rt_skip_lbl2) {
        Emit ("  br label %%L%u\n", rt_skip_lbl2);
        cg->block_terminated = true;
        Emit_Label_Here (rt_skip_lbl2);
      }
      continue;
    }
    for (uint32_t xi = 0; xi < ct->array.index_count; xi++) {
      Type_Info *idx_ty = ct->array.indices[xi].index_type;
      if (not idx_ty or not Type_Is_Scalar (idx_ty)) continue;
      const char *bt = Type_To_Llvm (idx_ty);
      if (not bt or bt[0] == '\0') bt = "i32";
      Type_Bound *lo = &ct->array.indices[xi].low_bound;
      Type_Bound *hi = &ct->array.indices[xi].high_bound;

      // Evaluate each bound, resolving disc references to parent
      uint32_t lo_val = 0, hi_val = 0;
      for (int bx = 0; bx < 2; bx++) {
        Type_Bound *bound = (bx == 0) ? lo : hi;
        uint32_t bval = 0;
        if (bound->kind == BOUND_INTEGER) {
          bval = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %lld\n", bval, bt,
             (long long)bound->int_value);
        } else if (bound->kind == BOUND_EXPR and bound->expr) {
          Syntax_Node *bexpr = bound->expr;
          bool resolved = false;
          if (bexpr->symbol) {
            for (uint32_t pdi = 0;
               pdi < parent_type->record.discriminant_count;
               pdi++) {
              if (Slice_Equal_Ignore_Case (
                  parent_type->record.components[pdi].name,
                  bexpr->symbol->name)) {
                bval = Emit_Disc_Constraint_Value (
                  parent_type, pdi, bt);
                resolved = true;
                break;
              }
            }
          }
          if (not resolved) {
            bval = Generate_Expression (bexpr);
            bval = Emit_Coerce_Default_Int (bval, bt);
          }
        }
        if (bx == 0) lo_val = bval; else hi_val = bval;
      }

      // Only check non-empty ranges (RM 3.6.1)
      if (lo_val > 0 and hi_val > 0) {
        uint32_t cmp = Emit_Temp ();
        uint32_t lbl_chk = cg->label_id++;
        uint32_t lbl_end = cg->label_id++;
        Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n",
           cmp, bt, lo_val, hi_val);
        Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
           cmp, lbl_chk, lbl_end);
        Emit ("L%u:\n", lbl_chk);
        Emit_Constraint_Check (lo_val, idx_ty, NULL);
        Emit_Constraint_Check (hi_val, idx_ty, NULL);
        Emit ("  br label %%L%u\n", lbl_end);
        Emit ("L%u:\n", lbl_end);
      } else if (lo_val > 0) {
        Emit_Constraint_Check (lo_val, idx_ty, NULL);
      } else if (hi_val > 0) {
        Emit_Constraint_Check (hi_val, idx_ty, NULL);
      }
    }

    // Close runtime variant guard for array component
    if (rt_skip_lbl2) {
      Emit ("  br label %%L%u\n", rt_skip_lbl2);
      cg->block_terminated = true;
      Emit_Label_Here (rt_skip_lbl2);
    }
  }
}

// Find discriminant symbols referenced in an expression tree.
// Used by nested default initialization to set disc_agg_temp.
void Collect_Disc_Symbols_In_Expr (Syntax_Node *node,
  Symbol **found, uint32_t *count, uint32_t max)
{
  if (not node or *count >= max) return;
  if (node->symbol and node->symbol->kind == SYMBOL_DISCRIMINANT) {
    bool already = false;
    for (uint32_t i = 0; i < *count; i++)
      if (found[i] == node->symbol) { already = true; break; }
    if (not already) found[(*count)++] = node->symbol;
  }

  // Recurse into children based on node kind
  switch (node->kind) {

    // cg->current_nesting_level is repurposed: 1 = has nested functions, use frame
    case NK_APPLY:
      Collect_Disc_Symbols_In_Expr (node->apply.prefix, found, count, max);
      for (uint32_t i = 0; i < node->apply.arguments.count; i++)
        Collect_Disc_Symbols_In_Expr (node->apply.arguments.items[i], found, count, max);
      break;
    case NK_BINARY_OP:
      Collect_Disc_Symbols_In_Expr (node->binary.left, found, count, max);
      Collect_Disc_Symbols_In_Expr (node->binary.right, found, count, max);
      break;
    case NK_UNARY_OP:
      Collect_Disc_Symbols_In_Expr (node->unary.operand, found, count, max);
      break;
    case NK_SELECTED:
      Collect_Disc_Symbols_In_Expr (node->selected.prefix, found, count, max);
      break;
    case NK_ATTRIBUTE:
      Collect_Disc_Symbols_In_Expr (node->attribute.prefix, found, count, max);
      for (uint32_t i = 0; i < node->attribute.arguments.count; i++)
        Collect_Disc_Symbols_In_Expr (node->attribute.arguments.items[i], found, count, max);
      break;
    case NK_QUALIFIED:
      Collect_Disc_Symbols_In_Expr (node->qualified.expression, found, count, max);
      break;
    case NK_AGGREGATE:
      for (uint32_t i = 0; i < node->aggregate.items.count; i++)
        Collect_Disc_Symbols_In_Expr (node->aggregate.items.items[i], found, count, max);
      break;
    case NK_ASSOCIATION:
      Collect_Disc_Symbols_In_Expr (node->association.expression, found, count, max);
      break;
    case NK_RANGE:
      Collect_Disc_Symbols_In_Expr (node->range.low, found, count, max);
      Collect_Disc_Symbols_In_Expr (node->range.high, found, count, max);
      break;
    case NK_ALLOCATOR:
      Collect_Disc_Symbols_In_Expr (node->allocator.expression, found, count, max);
      break;
    default:
      break;
  }
}
void Generate_Object_Declaration (Syntax_Node *node) {
  bool use_frame = cg->current_nesting_level > 0;
  bool is_package_level = (cg->current_function == NULL);

  // Emit source location for the object declaration
  Emit_Location (node->location);
  for (uint32_t i = 0; i < node->object_decl.names.count; i++) {
    Syntax_Node *name = node->object_decl.names.items[i];
    Symbol *sym = name->symbol;
    if (not sym) continue;

    // RM 3.2.1: For multi-name declarations (S1, S2 : T := ...),                                   
    // each name gets independent evaluation of the subtype indication                              
    // and init expression.  Clear cached temps before each subsequent                              
    // name so bounds are re-evaluated from scratch.                                                
    //                                                                                              
    if (i > 0 and node->object_decl.names.count > 1) {
      Type_Info *prev_ty = node->object_decl.names.items[0]->symbol ?
                 node->object_decl.names.items[0]->symbol->type : NULL;
      if (prev_ty) {
        prev_ty->low_bound.cached_temp = 0;
        prev_ty->high_bound.cached_temp = 0;
        if (prev_ty->kind == TYPE_ARRAY or prev_ty->kind == TYPE_STRING) {
          for (uint32_t d = 0; d < prev_ty->array.index_count; d++) {
            prev_ty->array.indices[d].low_bound.cached_temp = 0;
            prev_ty->array.indices[d].high_bound.cached_temp = 0;
          }
          if (prev_ty->array.element_type) {
            prev_ty->array.element_type->low_bound.cached_temp = 0;
            prev_ty->array.element_type->high_bound.cached_temp = 0;
            if (prev_ty->array.element_type->kind == TYPE_RECORD)
              prev_ty->array.element_type->record.disc_constraint_preeval = NULL;
            if (Type_Is_Array_Like (prev_ty->array.element_type))
              for (uint32_t d = 0; d < prev_ty->array.element_type->array.index_count; d++) {
                prev_ty->array.element_type->array.indices[d].low_bound.cached_temp = 0;
                prev_ty->array.element_type->array.indices[d].high_bound.cached_temp = 0;
              }
          }
        }
        if (prev_ty->kind == TYPE_RECORD)
          prev_ty->record.disc_constraint_preeval = NULL;
      }
    }

    // Emit declaration info comment
    Emit ("  ; %s %.*s : %s\n",
       node->object_decl.is_constant ? "CONST" : "VAR",
       (int)sym->name.length, sym->name.data,
       sym->type ? Type_To_Llvm (sym->type) : "?");
    Type_Info *ty = sym->type;
    Type_Info *orig_ty = ty;  // Save for disc constraint transfer
    if (cg->current_instance)
      ty = Resolve_Generic_Actual_Type (ty);

    // Transfer discriminant constraints from the formal type's constrained                         
    // subtype to the resolved actual type (RM 12.3).  When a generic body                          
    // declares P2 : PRIV (T'VAL (F * 100)), the formal PRIV type gets disc                         
    // constraints, but Resolve_Generic_Actual_Type returns the base actual                         
    // type (REC) which has no constraints.  We must carry them over.                               
    // For generic instances: if the resolved actual type is a record with                          
    // discriminants, extract disc constraint expressions from the object                           
    // declaration's AST (RM 12.3).  The formal type (PRIV) doesn't carry                           
    // constraints, but the AST for "P2 : PRIV (T'VAL (F*100))" has them.                           
    //                                                                                              
    Syntax_Node **decl_disc_exprs = NULL;
    uint32_t decl_disc_count = 0;
    if (cg->current_instance and ty != orig_ty and
      Type_Is_Record (ty) and ty->record.has_discriminants and
      not ty->record.has_disc_constraints and
      node->object_decl.object_type) {
      Syntax_Node *ot = node->object_decl.object_type;
      Syntax_Node *constraint = NULL;
      if (ot->kind == NK_SUBTYPE_INDICATION and ot->subtype_ind.constraint)
        constraint = ot->subtype_ind.constraint;
      Node_List *disc_args = NULL;
      if (constraint and constraint->kind == NK_INDEX_CONSTRAINT)
        disc_args = &constraint->index_constraint.ranges;
      else if (constraint and constraint->kind == NK_DISCRIMINANT_CONSTRAINT)
        disc_args = &constraint->discriminant_constraint.associations;
      if (disc_args and disc_args->count > 0) {
        decl_disc_count = disc_args->count;
        if (decl_disc_count > ty->record.discriminant_count)
          decl_disc_count = ty->record.discriminant_count;
        decl_disc_exprs = disc_args->items;
      }
    }

    // Named numbers (constants without explicit type) don't need storage.                          
    // They are compile-time values that get inlined when referenced.                               
    // Per RM 3.2.2: Named numbers are not objects and have no storage.                             
    //                                                                                              
    if (sym->is_named_number) {
      continue;  // Skip storage allocation for named numbers
    }
    const char *type_str = Type_To_Llvm (ty);

    // Check if this is an array type (constrained or unconstrained).                               
    // is_any_array: true for any array-like type INCLUDING TYPE_STRING,                            
    // used for aggregate/string initialization.                                                    
    // is_constrained_array: true only for constrained, used for allocation                         
    //                                                                                              
    bool is_any_array = ty and (ty->kind == TYPE_ARRAY or ty->kind == TYPE_STRING);
    bool is_constrained_array = is_any_array and ty->array.is_constrained;
    int128_t array_count = is_constrained_array ? Array_Element_Count (ty) : 0;
    const char *elem_type = NULL;
    uint32_t elem_size = 0;
    bool elem_is_composite = false;
    bool elem_has_dynamic_size = false;
    if (is_any_array and ty->array.element_type) {
      Type_Info *et = ty->array.element_type;

      // Check if element is record or another constrained array
      if (Type_Is_Record (et) or Type_Is_Constrained_Array (et)) {
        elem_is_composite = true;
        elem_size = et->size;

        // Element with zero size means dynamic bounds - the outer                                  
        // array must use fat-pointer storage so runtime code can                                   
        // compute element size and lay out storage properly.                                       
        //                                                                                          
        if (elem_size == 0) {
          elem_has_dynamic_size = true;
          elem_type = Type_To_Llvm (et);
        }
      } else {
        elem_type = Type_To_Llvm (et);
      }
    }

    // Check if this is a record type
    bool is_record = Type_Is_Record (ty);
    uint32_t record_size = is_record ? ty->size : 0;

    // Package-level variables are globals, local variables use alloca
    // Skip if global was already emitted (e.g., from spec + body)
    if (is_package_level) {
      if (sym->extern_emitted) continue;
      sym->extern_emitted = true;

      // Global variable at package level
      Emit ("@");
      Emit_Symbol_Name (sym);

      // Constant with initializer - emit as constant
      if (node->object_decl.is_constant and node->object_decl.init) {
        if (node->object_decl.init->kind == NK_INTEGER) {
          sym->extern_emitted = true;
          Emit (" = linkonce_odr constant %s %lld\n", type_str,
             (long long)node->object_decl.init->integer_lit.value);
          continue;
        }

        // String constant - emit fat pointer global
        if (node->object_decl.init->kind == NK_STRING) {
          sym->extern_emitted = true;
          String_Slice str = node->object_decl.init->string_val.text;
          int64_t str_len = (int64_t)str.length;

          // Emit string data first: @SYMNAME.data = ...
          Emit (".data = linkonce_odr constant [%lld x i8] c\"",
             (long long)str_len);

          // Emit escaped string contents
          for (uint32_t j = 0; j < str.length; j++) {
            char ch = str.data[j];
            if (ch >= 32 and ch < 127 and ch != '"' and ch != '\\') {
              Emit ("%c", ch);
            } else {
              Emit ("\\%02X", (unsigned char)ch);
            }
          }
          Emit ("\"\n");

          // Emit bounds global: @SYMNAME.bounds = { i64 1, i64 N }
          Emit ("@");
          Emit_Symbol_Name (sym);
          Emit (".bounds = linkonce_odr constant { i64, i64 } "
             "{ i64 1, i64 %d }\n", (int)str_len);

          // Emit fat pointer global: { ptr @X.data, ptr @X.bounds }
          Emit ("@");
          Emit_Symbol_Name (sym);
          Emit (" = linkonce_odr constant " FAT_PTR_TYPE " "
             "{ ptr @");
          Emit_Symbol_Name (sym);
          Emit (".data, ptr @");
          Emit_Symbol_Name (sym);
          Emit (".bounds }\n");
          continue;
        }
      }

      // Mark as emitted so extern declarations are suppressed
      sym->extern_emitted = true;

      // Variable - emit as global, using static initializer when available.                        
      // Per Ada RM 3.2.1: object declarations with static init expressions                         
      // can be folded into the global initializer directly.                                        
      //                                                                                            
      int64_t  init_ival = 0;
      double   init_fval = 0.0;
      bool     has_static_init = false;
      if (node->object_decl.init) {
        Syntax_Node *init = node->object_decl.init;
        if (init->kind == NK_INTEGER) {
          init_ival = init->integer_lit.value;
          has_static_init = true;
        } else if (init->kind == NK_REAL) {
          init_fval = init->real_lit.value;
          has_static_init = true;
        } else if (init->kind == NK_IDENTIFIER and init->symbol
               and init->symbol->is_named_number) {
          init_ival = (int64_t)Eval_Const_Numeric (init);
          has_static_init = true;
        }
      }
      if (is_constrained_array and array_count > 0) {
        if (elem_is_composite and elem_size > 0) {
          Emit (" = linkonce_odr global [%s x [%u x i8]] zeroinitializer\n",
             I128_Decimal (array_count), elem_size);
        } else {
          Emit (" = linkonce_odr global [%s x %s] zeroinitializer\n",
             I128_Decimal (array_count), elem_type);
        }
      } else if (is_record and record_size > 0) {
        Emit (" = linkonce_odr global [%u x i8] zeroinitializer\n", record_size);
      } else {
        if (Llvm_Type_Is_Fat_Pointer (type_str)) {
          Emit (" = linkonce_odr global %s zeroinitializer\n", type_str);
        } else if (Type_Is_Float_Representation (ty)) {
          Emit (" = linkonce_odr global %s %a\n", type_str,
             has_static_init ? init_fval : 0.0);
        } else if (strcmp (type_str, "ptr") == 0) {
          Emit (" = linkonce_odr global ptr null\n");
        } else {
          Emit (" = linkonce_odr global %s %lld\n", type_str,
             (long long)(has_static_init ? init_ival : 0));
        }
      }
      continue;
    }

    // RM 7.4: Deferred constant and its completion share one symbol after                          
    // Scope_Insert merge.  The deferred decl (no init) emits storage first;                        
    // the completion (has init) must reuse that storage, not re-emit it.                           
    //                                                                                              
    if (sym->extern_emitted) goto obj_decl_init;
    sym->extern_emitted = true;

    // Local variable allocation                                                                    
    // Skip symbols that aren't actually in the current function's scope.                           
    // This happens with package spec private part declarations: the AST                            
    // nodes have orphaned symbol references (parent==NULL, frame_offset==0)                        
    // while the proper symbols are already allocated in the frame under                            
    // their package-qualified names. Emitting these would create duplicate                         
    // or conflicting IR definitions.                                                               
    //                                                                                              
    if (use_frame) {
      if (cg->current_function and cg->current_function->scope and
        not sym->parent and sym->frame_offset == 0) {
        bool found_in_scope = false;
        Scope *scope = cg->current_function->scope;
        for (uint32_t j = 0; j < scope->symbol_count; j++) {
          if (scope->symbols[j] == sym) { found_in_scope = true; break; }
        }
        if (not found_in_scope) {
          for (uint32_t j = 0; j < scope->frame_var_count; j++) {
            if (scope->frame_vars[j] == sym) { found_in_scope = true; break; }
          }
        }
        if (not found_in_scope) continue;
      }

      // In generic instance bodies, find the instance symbol which has
      // the correct frame_offset (template symbols have offset 0).
      int64_t offset = sym->frame_offset;
      if (cg->current_instance) {
        Symbol *inst_sym = Find_Instance_Local (sym);
        if (inst_sym) offset = inst_sym->frame_offset;
      }
      Emit ("  %%");
      Emit_Symbol_Name (sym);
      Emit (" = getelementptr i8, ptr %%__frame_base, i64 %lld\n",
         (long long)offset);

    // Constrained outer array whose element type is a dynamic-bound                                
    // array/string: each element is stored as a fat pointer pair.                                  
    // Allocate [N x { ptr, ptr }] so each slot holds one fat ptr.                                  
    //                                                                                              
    } else if (elem_has_dynamic_size and is_constrained_array and array_count > 0) {
      Emit ("  %%");
      Emit_Symbol_Name (sym);
      Emit (" = alloca [%s x " FAT_PTR_TYPE "]\n", I128_Decimal (array_count));
    } else if (sym->needs_fat_ptr_storage or
           (is_any_array and Type_Has_Dynamic_Bounds (ty)) or
           elem_has_dynamic_size) {

      // Dynamic-bound arrays - or arrays whose element type has                                    
      // dynamic size - need fat pointer storage ({ ptr, ptr }).                                    
      // Runtime code computes actual sizes and allocates data.                                     
      //                                                                                            
      Emit ("  %%");
      Emit_Symbol_Name (sym);
      Emit (" = alloca " FAT_PTR_TYPE "\n");
      sym->needs_fat_ptr_storage = true;

    // Constrained array with static bounds: allocate [N x element_type]
    } else if (is_constrained_array and array_count > 0) {
      Emit ("  %%");
      Emit_Symbol_Name (sym);

      // Element is record or constrained array - use byte array
      if (elem_is_composite and elem_size > 0) {
        Emit (" = alloca [%s x [%u x i8]]\n", I128_Decimal (array_count), elem_size);
      } else {
        Emit (" = alloca [%s x %s]\n", I128_Decimal (array_count), elem_type);
      }

    // Record with dynamic-sized components: load runtime size
    } else if (is_record and ty and ty->rt_global_id > 0) {
      uint32_t rtsz = Emit_Temp ();
      Emit ("  %%t%u = load i64, ptr @__rt_rec_%u_size\n",
         rtsz, ty->rt_global_id);
      Emit ("  %%");
      Emit_Symbol_Name (sym);
      Emit (" = alloca i8, i64 %%t%u  ; record rt alloca\n", rtsz);

      // Zero-init so memcmp-based equality sees clean padding
      Emit ("  call void @llvm.memset.p0.i64(ptr %%");
      Emit_Symbol_Name (sym);
      Emit (", i8 0, i64 %%t%u, i1 false)\n", rtsz);

    // Record type: allocate [N x i8] for the record size
    } else if (is_record and record_size > 0) {
      Emit ("  %%");
      Emit_Symbol_Name (sym);
      Emit (" = alloca [%u x i8]  ; record type\n", record_size);

      // Zero-init all record types so that padding bytes are clean                                 
      // (for memcmp equality) and nested record component defaults                                 
      // start from a known state (RM 3.3.1, 4.5.2).                                                
      //                                                                                            
      if (ty and Type_Is_Record (ty)) {
        Emit ("  call void @llvm.memset.p0.i64(ptr %%");
        Emit_Symbol_Name (sym);
        Emit (", i8 0, i64 %u, i1 false)\n", record_size);
      }
    } else {
      Emit ("  %%");
      Emit_Symbol_Name (sym);
      Emit (" = alloca %s\n", type_str);

      // For unconstrained array/string variables with explicit index                               
      // constraints (e.g. S : STRING (1..N)), allocate data storage                                
      // and initialize the fat pointer so it's valid before any use.                               
      // Without this, the fat pointer is { null, null } and any                                    
      // access to bounds causes a crash. (RM 3.6.1)                                                
      //                                                                                            
      if (is_any_array and not is_constrained_array and
        node->object_decl.object_type and
        node->object_decl.object_type->kind == NK_SUBTYPE_INDICATION and
        node->object_decl.object_type->subtype_ind.constraint and
        node->object_decl.object_type->subtype_ind.constraint->kind == NK_INDEX_CONSTRAINT) {
        Syntax_Node *constraint = node->object_decl.object_type->subtype_ind.constraint;
        Node_List *ranges = &constraint->index_constraint.ranges;
        if (ranges->count > 0) {
          const char *bt = Array_Bound_Llvm_Type (ty);
          uint32_t ndims = ranges->count;
          uint32_t dim_lo[8], dim_hi[8];  // support up to 8 dimensions
          if (ndims > 8) ndims = 8;
          bool all_ok = true;
          for (uint32_t d = 0; d < ndims; d++) {
            Syntax_Node *rng = ranges->items[d];
            if (rng and rng->kind == NK_RANGE and rng->range.low and rng->range.high) {
              dim_lo[d] = Generate_Expression (rng->range.low);
              dim_hi[d] = Generate_Expression (rng->range.high);
            } else {
              all_ok = false;
              break;
            }
          }

          // Compute total element count = product of all dimension lengths
          if (all_ok) {
            uint32_t total_len = 0;
            for (uint32_t d = 0; d < ndims; d++) {
              uint32_t diff_d = Emit_Temp ();
              Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", diff_d, bt, dim_hi[d], dim_lo[d]);
              uint32_t len_d = Emit_Temp ();
              Emit ("  %%t%u = add %s %%t%u, 1\n", len_d, bt, diff_d);
              if (d == 0) {
                total_len = len_d;
              } else {
                uint32_t prod = Emit_Temp ();
                Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", prod, bt, total_len, len_d);
                total_len = prod;
              }
            }

            // Multiply by element size
            uint32_t elem_sz = ty->array.element_type ? ty->array.element_type->size : 4;
            if (elem_sz == 0) elem_sz = 4;
            uint32_t byte_len = Emit_Temp ();
            Emit ("  %%t%u = mul %s %%t%u, %u  ; total bytes\n",
               byte_len, bt, total_len, elem_sz);
            uint32_t byte_len64 = Emit_Extend_To_I64 (byte_len, bt);

            // Allocate data storage
            uint32_t data_alloc = Emit_Temp ();
            Emit ("  %%t%u = alloca i8, i64 %%t%u  ; constrained uncon array data\n",
               data_alloc, byte_len64);

            // Build fat pointer with multi-dim bounds
            uint32_t fat;
            if (ndims > 1) {
              fat = Emit_Fat_Pointer_MultiDim (data_alloc, dim_lo, dim_hi, ndims, bt);
            } else {
              fat = Emit_Fat_Pointer_Dynamic (data_alloc, dim_lo[0], dim_hi[0], bt);
            }
            Emit_Store_Fat_Pointer_To_Symbol (fat, sym, bt);
          }
        }
      }
    }

    // Start task if this is a task type object                                                     
    // Task objects are started immediately at elaboration.                                         
    // The task body function is named @task_TYPENAME where TYPENAME                                
    // is the task type name (not the object name).                                                 
    // For task types defined outside the generic, no instance prefix.                              
    // For single tasks inside generics, the body has an instance prefix.                           
    //                                                                                              
    if (Type_Is_Task (ty)) {
      uint32_t handle_tmp = Emit_Temp ();
      Emit ("  %%t%u = call ptr @__ada_task_start(ptr @", handle_tmp);
      Emit_Task_Function_Name (ty->defining_symbol, ty->name);
      Emit (", ");

      // Pass parent frame for uplevel access, or null if at package level.
      // use_frame indicates the parent is using frame-based allocation.
      if (use_frame) {
        Emit ("ptr %%__frame_base)\n");
      } else {
        Emit ("ptr null)\n");
      }

      // Store thread handle in task object for later join/abort
      Emit ("  store ptr %%t%u, ptr %%", handle_tmp);
      Emit_Symbol_Name (sym);
      Emit ("\n");
    }
obj_decl_init:

    // RM 3.3.1(5) + RM 3.6(5): Elaborate subtype indication by                                     
    // evaluating all dynamic constraint expressions, even for                                      
    // uninitialized objects.  For arrays, the elaboration order is:                                
    //   1. Index discrete ranges (array bounds)                                                    
    //   2. Component subtype indication (element type constraints)                                 
    // This ordering is critical because expressions may have side                                  
    // effects (function calls setting globals read by later exprs).                                
    //                                                                                              
    if (ty and Type_Is_Scalar (ty)) {
      if (ty->low_bound.kind == BOUND_EXPR and ty->low_bound.expr
        and not ty->low_bound.cached_temp) {
        ty->low_bound.cached_temp = Generate_Expression (ty->low_bound.expr);
        const char *ety = Expression_Llvm_Type (ty->low_bound.expr);
        if (ety and ety[0]) Temp_Set_Type (ty->low_bound.cached_temp, ety);
      }
      if (ty->high_bound.kind == BOUND_EXPR and ty->high_bound.expr
        and not ty->high_bound.cached_temp) {
        ty->high_bound.cached_temp = Generate_Expression (ty->high_bound.expr);
        const char *ety = Expression_Llvm_Type (ty->high_bound.expr);
        if (ety and ety[0]) Temp_Set_Type (ty->high_bound.cached_temp, ety);
      }
    }

    // Step 1: evaluate array index bounds (discrete ranges)
    if (is_any_array and ty) {
      for (uint32_t d = 0; d < ty->array.index_count; d++) {
        Index_Info *idx = &ty->array.indices[d];
        if (idx->low_bound.kind == BOUND_EXPR and idx->low_bound.expr
          and not idx->low_bound.cached_temp) {
          idx->low_bound.cached_temp = Generate_Expression (idx->low_bound.expr);
          const char *ety = Expression_Llvm_Type (idx->low_bound.expr);
          if (ety and ety[0]) Temp_Set_Type (idx->low_bound.cached_temp, ety);
        }
        if (idx->high_bound.kind == BOUND_EXPR and idx->high_bound.expr
          and not idx->high_bound.cached_temp) {
          idx->high_bound.cached_temp = Generate_Expression (idx->high_bound.expr);
          const char *ety = Expression_Llvm_Type (idx->high_bound.expr);
          if (ety and ety[0]) Temp_Set_Type (idx->high_bound.cached_temp, ety);
        }
      }

      // Step 2: evaluate element subtype constraints.                                              
      // For record elements with disc constraints: pre-evaluate once                               
      // so aggregate disc checks reuse cached values (RM 3.7.1).                                   
      // For scalar elements with dynamic range: cache bounds.                                      
      //                                                                                            
      if (ty->array.element_type and
        Type_Is_Record (ty->array.element_type)) {
        Type_Info *elt = ty->array.element_type;
        if (elt->record.has_disc_constraints and
          elt->record.disc_constraint_exprs and
          not elt->record.disc_constraint_preeval) {
          elt->record.disc_constraint_preeval = Arena_Allocate (
            elt->record.discriminant_count * sizeof (uint32_t));
          memset (elt->record.disc_constraint_preeval, 0,
               elt->record.discriminant_count * sizeof (uint32_t));
          for (uint32_t di = 0; di < elt->record.discriminant_count; di++) {
            if (not elt->record.disc_constraint_exprs[di]) continue;
            Component_Info *dc = &elt->record.components[di];
            const char *ddt = dc->component_type ?
              Type_To_Llvm (dc->component_type) : "i32";
            uint32_t disc_val = Generate_Expression (elt->record.disc_constraint_exprs[di]);
            disc_val = Emit_Coerce_Default_Int (disc_val, ddt);
            uint32_t at = Emit_Temp ();
            Emit ("  %%t%u = alloca %s  ; preeval array elem disc\n",
               at, ddt);
            Emit ("  store %s %%t%u, ptr %%t%u\n", ddt, disc_val, at);
            elt->record.disc_constraint_preeval[di] = at;
          }
        }
      } else if (ty->array.element_type and
             Type_Is_Scalar (ty->array.element_type)) {
        Type_Info *elt = ty->array.element_type;
        if (elt->low_bound.kind == BOUND_EXPR and elt->low_bound.expr
          and not elt->low_bound.cached_temp) {
          elt->low_bound.cached_temp = Generate_Expression (elt->low_bound.expr);
          const char *ety = Expression_Llvm_Type (elt->low_bound.expr);
          if (ety and ety[0]) Temp_Set_Type (elt->low_bound.cached_temp, ety);
        }
        if (elt->high_bound.kind == BOUND_EXPR and elt->high_bound.expr
          and not elt->high_bound.cached_temp) {
          elt->high_bound.cached_temp = Generate_Expression (elt->high_bound.expr);
          const char *ety = Expression_Llvm_Type (elt->high_bound.expr);
          if (ety and ety[0]) Temp_Set_Type (elt->high_bound.cached_temp, ety);
        }
      } else if (ty->array.element_type and
             Type_Is_Array_Like (ty->array.element_type)) {

        // RM 3.6(5): For array-of-array, cache the element type's                                  
        // index bounds so inner aggregates reuse them instead of                                   
        // re-evaluating side-effectful expressions per outer element.                              
        //                                                                                          
        Type_Info *elt = ty->array.element_type;
        for (uint32_t d = 0; d < elt->array.index_count; d++) {
          Index_Info *eidx = &elt->array.indices[d];
          if (eidx->low_bound.kind == BOUND_EXPR and eidx->low_bound.expr
            and not eidx->low_bound.cached_temp) {
            eidx->low_bound.cached_temp = Generate_Expression (eidx->low_bound.expr);
            const char *ety = Expression_Llvm_Type (eidx->low_bound.expr);
            if (ety and ety[0]) Temp_Set_Type (eidx->low_bound.cached_temp, ety);
          }
          if (eidx->high_bound.kind == BOUND_EXPR and eidx->high_bound.expr
            and not eidx->high_bound.cached_temp) {
            eidx->high_bound.cached_temp = Generate_Expression (eidx->high_bound.expr);
            const char *ety = Expression_Llvm_Type (eidx->high_bound.expr);
            if (ety and ety[0]) Temp_Set_Type (eidx->high_bound.cached_temp, ety);
          }
        }
      }
    }

    // Initialize if provided
    if (node->object_decl.init) {

      // String/character array initialization.                                                     
      // NK_STRING always yields a fat pointer.                                                     
      // Unconstrained array identifiers yield fat pointer values.                                  
      // Constrained array identifiers yield plain ptr - use memcpy.                                
      //                                                                                            
      // CRITICAL: When the destination is unconstrained (STRING variable),                         
      // the alloca holds a fat pointer descriptor { ptr, { bound, bound } }.                       
      // We must NOT memcpy data into that descriptor.  Instead:                                    
      //   1. Allocate separate local data storage (dynamic alloca)                                 
      //   2. Copy data from source to local storage                                                
      //   3. Build a fat pointer { local_data, { low, high } }                                     
      //   4. Store the fat pointer into the variable                                               
      // This is "constrained by initialization" - GNAT does the same.                              
      //                                                                                            
      if (is_any_array and ty->array.element_type == sm->type_character) {
        Syntax_Node *init = node->object_decl.init;
        Type_Info *init_ty = init->type;
        int init_is_constrained = Type_Is_Constrained_Array (init_ty);
        bool dest_needs_fat_storage = not is_constrained_array or
          sym->needs_fat_ptr_storage;

        // Source produces a fat pointer value
        if (init->kind == NK_STRING or not init_is_constrained) {
          uint32_t fat_ptr = Generate_Expression (init);
          const char *init_bt = Array_Bound_Llvm_Type (ty);

          // Destination needs fat pointer storage (unconstrained or dynamic bounds).               
          // Storage is { ptr, { bound, bound } }.  We need separate                                
          // data storage on the stack, then store the fat pointer.                                 
          //                                                                                        
          if (dest_needs_fat_storage) {
            uint32_t src_data = Emit_Fat_Pointer_Data (fat_ptr, init_bt);
            uint32_t src_low  = Emit_Fat_Pointer_Low (fat_ptr, init_bt);
            uint32_t src_high = Emit_Fat_Pointer_High (fat_ptr, init_bt);
            uint32_t len      = Emit_Fat_Pointer_Length (fat_ptr, init_bt);
            uint32_t len_64   = Emit_Extend_To_I64 (len, init_bt);

            // Allocate local data storage sized by source bounds
            uint32_t local_data = Emit_Temp ();
            Emit ("  %%t%u = alloca i8, i64 %%t%u"
               "  ; constrained-by-init data\n", local_data, len_64);

            // Copy source data to local storage
            Emit ("  call void @llvm.memcpy.p0.p0.i64("
               "ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
               local_data, src_data, len_64);

            // Build fat pointer pointing to local data with source bounds
            uint32_t new_fat = Emit_Fat_Pointer_Dynamic (local_data, src_low, src_high, init_bt);

            // Store fat pointer into variable
            Emit_Store_Fat_Pointer_To_Symbol (new_fat, sym, init_bt);

          // Destination is constrained - just copy data bytes
          } else {
            Emit_Fat_Pointer_Copy_To_Name (fat_ptr, sym, init_bt);
          }

        // Source is a constrained character array - usually plain ptr.
        // But may be a fat pointer if source has dynamic bounds.
        } else {
          uint32_t src_ptr = Generate_Expression (init);
          const char *src_llvm_init = Expression_Llvm_Type (init);
          bool src_is_fat = Llvm_Type_Is_Fat_Pointer (src_llvm_init);

          // Source produced a fat pointer - extract data pointer
          if (src_is_fat) {
            const char *init_bt2 = Array_Bound_Llvm_Type (ty);
            src_ptr = Emit_Fat_Pointer_Data (src_ptr, init_bt2);
          }
          if (dest_needs_fat_storage and init_ty and
            init_ty->array.index_count > 0) {

            // Dest is unconstrained but source is constrained.                                     
            // Build fat pointer from source's static bounds.                                       
            // For multidimensional arrays, use product of all extents.                             
            //                                                                                      
            int128_t byte_len = 1;
            for (uint32_t d = 0; d < init_ty->array.index_count; d++) {
              int128_t dlo = Type_Bound_Value (init_ty->array.indices[d].low_bound);
              int128_t dhi = Type_Bound_Value (init_ty->array.indices[d].high_bound);
              int128_t dim_cnt = (dhi >= dlo) ? (dhi - dlo + 1) : 0;
              byte_len *= dim_cnt;
            }
            uint32_t el_sz_c2u = (init_ty->array.element_type and
              init_ty->array.element_type->size > 0) ?
              init_ty->array.element_type->size : 1;
            byte_len *= el_sz_c2u;
            int128_t lo = Type_Bound_Value (
              init_ty->array.indices[0].low_bound);
            int128_t hi = Type_Bound_Value (
              init_ty->array.indices[0].high_bound);

            // Allocate local data storage
            uint32_t local_data = Emit_Temp ();
            Emit ("  %%t%u = alloca i8, i64 %s"
               "  ; con-to-uncon data\n",
               local_data, I128_Decimal (byte_len));

            // Copy data
            Emit ("  call void @llvm.memcpy.p0.p0.i64("
               "ptr %%t%u, ptr %%t%u, i64 %s, i1 false)\n",
               local_data, src_ptr, I128_Decimal (byte_len));

            // Build fat pointer
            const char *ci_bt = Array_Bound_Llvm_Type (ty);
            uint32_t new_fat = Emit_Fat_Pointer (local_data, lo, hi, ci_bt);

            // Store fat pointer into variable
            Emit_Store_Fat_Pointer_To_Symbol (new_fat, sym, ci_bt);

          // Both constrained - simple memcpy using target bounds.                                  
          // For multidimensional arrays, compute total byte count                                  
          // as product of all dimension extents * element size.                                    
          //                                                                                        
          } else {
            int128_t total_elems = 1;
            for (uint32_t d = 0; d < ty->array.index_count; d++) {
              int128_t lo = Type_Bound_Value (ty->array.indices[d].low_bound);
              int128_t hi = Type_Bound_Value (ty->array.indices[d].high_bound);
              int128_t dim_cnt = (hi >= lo) ? (hi - lo + 1) : 0;
              total_elems *= dim_cnt;
            }
            uint32_t el_sz = (ty->array.element_type and
              ty->array.element_type->size > 0) ?
              ty->array.element_type->size : 1;
            int128_t byte_len = total_elems * el_sz;
            Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
            Emit_Symbol_Name (sym);
            Emit (", ptr %%t%u, i64 %s, i1 false)\n",
               src_ptr, I128_Decimal (byte_len));
          }
        }
      } else if (Type_Is_Fixed_Point (ty) and
             node->object_decl.init->kind == NK_REAL) {

        // Fixed-point initialization from real literal:                                            
        // Convert real value to scaled integer at compile time                                     
        // Use Big_Real for precise scaling when available                                          
        //                                                                                          
        double real_val = node->object_decl.init->real_lit.value;
        double small = ty->fixed.small > 0 ? ty->fixed.small : ty->fixed.delta;
        int64_t scaled_val;
        Big_Real *big_val = node->object_decl.init->real_lit.big_value;

        // Precise scaling: scaled = significand * 10^exponent / small
        // For best precision, compute in arbitrary precision then round
        if (big_val and small != 0) {
          Big_Real *small_br = Big_Real_New ();
          small_br->significand = Big_Integer_New (4);
          small_br->significand->limbs[0] = (uint64_t)(small * 1e15 + 0.5);
          small_br->significand->count = 1;
          small_br->exponent = -15;

          // Use double for final division - Big_Real provides precise numerator.
          // Use round() for correct rounding of negative values.
          scaled_val = (int64_t)round(Big_Real_To_Double (big_val) / small);
        } else {
          scaled_val = (int64_t)round(real_val / small);
        }
        uint32_t init = Emit_Temp ();
        const char *fix_store_type = Type_To_Llvm (ty);
        Emit ("  %%t%u = add %s 0, %lld  ; fixed-point scaled (small=%g)\n",
           init, fix_store_type, (long long)scaled_val, small);

        // RM 3.3.2: Scalar constraint check on initialization
        Emit_Constraint_Check_With_Type (init, ty, NULL, fix_store_type);
        Emit ("  store %s %%t%u, ptr %%", fix_store_type, init);
        Emit_Symbol_Name (sym);
        Emit ("\n");

      // RM 3.2.2(5): Evaluate discriminant constraint expressions BEFORE                           
      // the aggregate init.  The subtype indication must be evaluated first                        
      // so that side effects (function calls that set variables read by the                        
      // aggregate) occur in the correct order.  Cache the values so they                           
      // aren't re-evaluated during the discriminant store.                                         
      //                                                                                            
      } else if (is_record and node->object_decl.init->kind == NK_AGGREGATE) {
        uint32_t disc_cached[16] = {0};
        uint32_t disc_count = 0;
        if (ty->record.has_disc_constraints and ty->record.disc_constraint_values) {
          disc_count = ty->record.discriminant_count;
          if (disc_count > 16) disc_count = 16;
          for (uint32_t di = 0; di < disc_count; di++) {
            Component_Info *dci = &ty->record.components[di];
            const char *ddt = dci->component_type ?
              Type_To_Llvm (dci->component_type) : "i32";
            uint32_t disc_val = Emit_Disc_Constraint_Value (ty, di, ddt);
            if (disc_val) disc_cached[di] = disc_val;
          }

          // Set up cg->disc_cache so Generate_Aggregate can use cached
          // discriminant values instead of re-evaluating expressions.
          for (uint32_t di = 0; di < disc_count and di < MAX_DISC_CACHE; di++)
            cg->disc_cache[di] = disc_cached[di];
          cg->disc_cache_count = disc_count;
          cg->disc_cache_type = ty;
        }

        // Record aggregate initialization - copy from aggregate to variable
        uint32_t agg_ptr = Generate_Expression (node->object_decl.init);

        // Clear disc cache after aggregate generation
        cg->disc_cache_count = 0;
        cg->disc_cache_type = NULL;
        Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
        Emit_Symbol_Name (sym);
        Emit (", ptr %%t%u, i64 %u, i1 false)\n", agg_ptr, record_size);

        // If the type has discriminant constraints, store constraint values (RM 3.7.2)             
        // This ensures discriminant fields are correctly set even after aggregate copy.            
        // Use pre-cached expression results to avoid re-evaluation.                                
        //                                                                                          
        if (ty->record.has_disc_constraints and ty->record.disc_constraint_values) {
          for (uint32_t di = 0; di < ty->record.discriminant_count; di++) {
            Component_Info *dc = &ty->record.components[di];
            uint32_t dp = Emit_Temp ();
            Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
            Emit_Symbol_Name (sym);
            Emit (", i64 %u  ; disc %.*s\n", dc->byte_offset,
               (int)dc->name.length, dc->name.data);
            const char *dt = Type_To_Llvm (dc->component_type);
            {
              uint32_t val;
              if (di < disc_count and disc_cached[di]) {
                val = Emit_Coerce_Default_Int (disc_cached[di], dt);
              } else {
                val = Emit_Disc_Constraint_Value (ty, di, dt);
                if (val == 0) val = Emit_Static_Int (0, dt);
              }
              Emit ("  store %s %%t%u, ptr %%t%u  ; disc store\n", dt, val, dp);
            }
          }
          sym->is_disc_constrained = true;
        }

      // Array aggregate initialization - copy from aggregate to variable.                          
      // Works for both constrained and unconstrained arrays with aggregate initializers.           
      // For arrays with dynamic bounds, the aggregate already returns a fat pointer.               
      //                                                                                            
      } else if (is_any_array and node->object_decl.init->kind == NK_AGGREGATE) {
        Type_Info *agg_type = node->object_decl.init->type;
        bool dest_needs_fat = Type_Has_Dynamic_Bounds (ty) or Type_Is_Unconstrained_Array (ty);

        // Generate_Aggregate returns a fat ptr alloca when the aggregate                           
        // type is unconstrained OR has dynamic bounds in ANY dimension.                            
        // Detect this so we copy the fat ptr instead of re-wrapping.                               
        //                                                                                          
        bool agg_returns_fat = agg_type and
          (not agg_type->array.is_constrained or
           Type_Has_Dynamic_Bounds (agg_type));
        uint32_t agg_ptr = Generate_Expression (node->object_decl.init);

        // Aggregate already returns a fat pointer alloca.
        // Load it and store to the destination variable.
        if (dest_needs_fat and agg_returns_fat) {
          uint32_t loaded = Emit_Temp ();
          Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u  ; load agg fat ptr\n",
             loaded, agg_ptr);
          Emit ("  store " FAT_PTR_TYPE " %%t%u, ptr %%", loaded);
          Emit_Symbol_Name (sym);
          Emit ("  ; store fat ptr to var\n");

        // Destination needs a fat pointer { ptr, { bound, bound, ... } }.
        // agg_ptr is the data pointer (static bounds), construct the fat pointer.
        } else if (dest_needs_fat and agg_type and agg_type->array.index_count > 0) {
          const char *iat_agg = Integer_Arith_Type ();
          const char *agg_abt = Array_Bound_Llvm_Type (ty);
          uint32_t agg_ndims = agg_type->array.index_count;

          // Multi-dim: store all dimension bounds
          if (agg_ndims > 1) {
            uint32_t agg_mlo[8], agg_mhi[8];
            if (agg_ndims > 8) agg_ndims = 8;
            for (uint32_t d = 0; d < agg_ndims; d++) {
              Type_Bound lo_b = agg_type->array.indices[d].low_bound;
              Type_Bound hi_b = agg_type->array.indices[d].high_bound;
              agg_mlo[d] = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %s\n", agg_mlo[d], iat_agg,
                 lo_b.kind == BOUND_INTEGER ? I128_Decimal (lo_b.int_value) : "1");
              Temp_Set_Type (agg_mlo[d], iat_agg);
              agg_mhi[d] = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %s\n", agg_mhi[d], iat_agg,
                 hi_b.kind == BOUND_INTEGER ? I128_Decimal (hi_b.int_value) : "0");
              Temp_Set_Type (agg_mhi[d], iat_agg);
            }
            uint32_t fat = Emit_Fat_Pointer_MultiDim (agg_ptr, agg_mlo, agg_mhi, agg_ndims, agg_abt);
            Emit_Store_Fat_Pointer_To_Symbol (fat, sym, agg_abt);
          } else {
            Type_Bound low_b = agg_type->array.indices[0].low_bound;
            Type_Bound high_b = agg_type->array.indices[0].high_bound;
            uint32_t low_val, high_val;
            if (low_b.kind == BOUND_INTEGER) {
              low_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %s\n", low_val, iat_agg, I128_Decimal (low_b.int_value));
            } else {
              low_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, 1\n", low_val, iat_agg);
            }
            Temp_Set_Type (low_val, iat_agg);
            if (high_b.kind == BOUND_INTEGER) {
              high_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %s\n", high_val, iat_agg, I128_Decimal (high_b.int_value));
            } else {
              high_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, 0\n", high_val, iat_agg);
            }
            Temp_Set_Type (high_val, iat_agg);
            Emit_Store_Fat_Pointer_Fields_To_Symbol (agg_ptr, low_val, high_val, sym, agg_abt);
          }

        // Static size known at compile time
        } else if (ty->size > 0) {
          Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
          Emit_Symbol_Name (sym);
          Emit (", ptr %%t%u, i64 %u, i1 false)  ; array init\n", agg_ptr, ty->size);

        // Dynamic size - compute from bounds at runtime
        } else {
          uint32_t elem_sz = elem_size > 0 ? elem_size :
                     (ty->array.element_type ? ty->array.element_type->size : 8);
          if (elem_sz == 0) elem_sz = 8;

          // Get bounds from aggregate type if available
          if (agg_type and agg_type->array.index_count > 0 and
            agg_type->array.indices[0].low_bound.kind == BOUND_INTEGER and
            agg_type->array.indices[0].high_bound.kind == BOUND_INTEGER) {

            // Bounds are static integers in the aggregate - use ALL dimensions
            int64_t count = 1;
            for (uint32_t d = 0; d < agg_type->array.index_count; d++) {
              if (agg_type->array.indices[d].low_bound.kind == BOUND_INTEGER and
                agg_type->array.indices[d].high_bound.kind == BOUND_INTEGER) {
                int64_t lo = agg_type->array.indices[d].low_bound.int_value;
                int64_t hi = agg_type->array.indices[d].high_bound.int_value;
                int64_t dim_count = hi - lo + 1;
                if (dim_count > 0) count *= dim_count;
              }
            }
            if (count > 0) {
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
              Emit_Symbol_Name (sym);
              Emit (", ptr %%t%u, i64 %lld, i1 false)  ; array init\n",
                 agg_ptr, (long long)(count * elem_sz));
            }

          // Fallback: use element size as minimum
          } else {
            Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
            Emit_Symbol_Name (sym);
            Emit (", ptr %%t%u, i64 %u, i1 false)  ; array init (min)\n",
               agg_ptr, elem_sz);
          }
        }
      } else if (is_any_array and not is_constrained_array and
             node->object_decl.init->kind != NK_AGGREGATE) {

        // Unconstrained non-character array init from expression (e.g. function call).             
        // The source produces a fat pointer.  We need to:                                          
        // 1. Allocate local data storage from source bounds                                        
        // 2. Copy data to local storage                                                            
        // 3. Build new fat pointer with local data                                                 
        // 4. Store fat pointer into variable                                                       
        //                                                                                          
        uint32_t fat_ptr = Generate_Expression (node->object_decl.init);
        const char *src_llvm = Expression_Llvm_Type (node->object_decl.init);
        const char *uai_bt = Array_Bound_Llvm_Type (ty);

        // Source is fat pointer - unpack, alloca, copy, rebuild
        if (Llvm_Type_Is_Fat_Pointer (src_llvm)) {
          uint32_t src_data = Emit_Fat_Pointer_Data (fat_ptr, uai_bt);
          uint32_t src_low  = Emit_Fat_Pointer_Low (fat_ptr, uai_bt);
          uint32_t src_high = Emit_Fat_Pointer_High (fat_ptr, uai_bt);
          uint32_t len      = Emit_Fat_Pointer_Length (fat_ptr, uai_bt);
          uint32_t e_sz = elem_size > 0 ? elem_size :
            (ty->array.element_type ? ty->array.element_type->size : 1);
          if (e_sz == 0) e_sz = 1;
          const char *uai_iat = Integer_Arith_Type ();
          uint32_t len_cvt = Emit_Convert (len, uai_bt, uai_iat);
          uint32_t byte_len = Emit_Temp ();
          if (e_sz == 1) {
            Emit ("  %%t%u = add %s %%t%u, 0  ; byte_len\n",
               byte_len, uai_iat, len_cvt);
          } else {
            Emit ("  %%t%u = mul %s %%t%u, %u  ; byte_len\n",
               byte_len, uai_iat, len_cvt, e_sz);
          }

          // Widen byte_len to i64 for alloca and memcpy intrinsics
          uint32_t byte_len_64 = Emit_Extend_To_I64 (byte_len, uai_iat);
          uint32_t local_data = Emit_Temp ();
          Emit ("  %%t%u = alloca i8, i64 %%t%u"
             "  ; uncon array init data\n", local_data, byte_len_64);
          Emit ("  call void @llvm.memcpy.p0.p0.i64("
             "ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)\n",
             local_data, src_data, byte_len_64);
          uint32_t new_fat = Emit_Fat_Pointer_Dynamic (local_data, src_low, src_high, uai_bt);
          Emit_Store_Fat_Pointer_To_Symbol (new_fat, sym, uai_bt);

        // Source is ptr (constrained) - wrap with bounds
        } else {
          Type_Info *init_ty = node->object_decl.init->type;
          if (init_ty and init_ty->array.index_count > 0) {
            int128_t lo = Type_Bound_Value (
              init_ty->array.indices[0].low_bound);
            int128_t hi = Type_Bound_Value (
              init_ty->array.indices[0].high_bound);
            uint32_t new_fat = Emit_Fat_Pointer (fat_ptr, lo, hi, uai_bt);
            Emit_Store_Fat_Pointer_To_Symbol (new_fat, sym, uai_bt);

          // Fallback: store as fat pointer with unknown bounds
          } else {
            Emit_Store_Fat_Pointer_To_Symbol (fat_ptr, sym, uai_bt);
          }
        }

      // Record initialized with non-aggregate expression (function call,                           
      // qualified expression, type conversion, etc.).  Generate_Expression                         
      // returns a ptr to the result; memcpy into the variable.  RM 3.3.1                           
      //                                                                                            
      } else if (is_record) {

        // RM 3.2.2(5): Pre-evaluate discriminant constraint expressions                            
        // BEFORE the init expression.  The subtype indication must be                              
        // evaluated first so that side effects (function calls that set                            
        // variables read by the init) occur in the correct order.                                  
        //                                                                                          
        uint32_t rec_disc_cached[MAX_DISC_CACHE] = {0};
        uint32_t rec_disc_count = 0;
        if (ty->record.has_disc_constraints and ty->record.disc_constraint_values) {
          rec_disc_count = ty->record.discriminant_count;
          if (rec_disc_count > MAX_DISC_CACHE) rec_disc_count = MAX_DISC_CACHE;
          for (uint32_t di = 0; di < rec_disc_count; di++) {
            Component_Info *dci = &ty->record.components[di];
            const char *ddt = dci->component_type ?
              Type_To_Llvm (dci->component_type) : "i32";
            uint32_t disc_val = Emit_Disc_Constraint_Value (ty, di, ddt);
            if (disc_val) rec_disc_cached[di] = disc_val;
          }
        }
        uint32_t init_ptr = Generate_Expression (node->object_decl.init);

        // RM 3.3.2: If target has discriminant constraints, verify that the                        
        // initial value's discriminants match.  Raise CONSTRAINT_ERROR on                          
        // mismatch before the memcpy.                                                              
        //                                                                                          
        if (ty->record.has_disc_constraints and ty->record.disc_constraint_values) {
          for (uint32_t di = 0; di < ty->record.discriminant_count; di++) {
            Component_Info *dc = &ty->record.components[di];
            const char *dt = Type_To_Llvm (dc->component_type);
            uint32_t sdp = Emit_Temp ();
            Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
               sdp, init_ptr, dc->byte_offset);
            uint32_t sv = Emit_Temp ();
            Emit ("  %%t%u = load %s, ptr %%t%u  ; init disc %.*s\n",
               sv, dt, sdp, (int)dc->name.length, dc->name.data);
            uint32_t cv;
            if (di < rec_disc_count and rec_disc_cached[di]) {
              cv = Emit_Coerce_Default_Int (rec_disc_cached[di], dt);
            } else {
              cv = Emit_Disc_Constraint_Value (ty, di, dt);
              if (cv == 0) cv = Emit_Static_Int (0, dt);
            }
            Emit_Discriminant_Check (sv, cv, dt, ty);
          }
        }
        if (record_size > 0) {
          Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
          Emit_Symbol_Name (sym);
          Emit (", ptr %%t%u, i64 %u, i1 false)  ; record init from expr\n",
             init_ptr, record_size);
        }

      // Constrained array initialized with non-aggregate expression                                
      // (function call, slice, type conversion, etc.).  Similar to                                 
      // record case - result is a ptr, memcpy into the variable.                                   
      //                                                                                            
      } else if (is_any_array and is_constrained_array) {
        uint32_t init_ptr = Generate_Expression (node->object_decl.init);
        uint32_t copy_sz = ty->size > 0 ? ty->size : elem_size;
        if (copy_sz > 0) {
          Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
          Emit_Symbol_Name (sym);
          Emit (", ptr %%t%u, i64 %u, i1 false)  ; constrained array init from expr\n",
             init_ptr, copy_sz);
        }

      // RM 3.2.2(5): For scalar subtypes with dynamic bounds, the                                  
      // subtype indication must be evaluated BEFORE the init expression.                           
      // Pre-evaluate dynamic bounds and cache them so they are not                                 
      // re-evaluated by constraint checks (which would cause side-                                 
      // effectful expressions like function calls to be invoked                                    
      // multiple times, breaking the c32001 family of ACATS tests).                                
      //                                                                                            
      } else if (not is_any_array and not is_record) {
        bool has_cached_bounds = false;
        if (ty and Type_Is_Scalar (ty)) {
          if (ty->low_bound.kind == BOUND_EXPR and ty->low_bound.expr
            and not ty->low_bound.cached_temp) {
            ty->low_bound.cached_temp = Generate_Expression (ty->low_bound.expr);
            const char *ety = Expression_Llvm_Type (ty->low_bound.expr);
            if (ety and ety[0])
              Temp_Set_Type (ty->low_bound.cached_temp, ety);
          }
          has_cached_bounds = (ty->low_bound.cached_temp != 0);
          if (ty->high_bound.kind == BOUND_EXPR and ty->high_bound.expr
            and not ty->high_bound.cached_temp) {
            ty->high_bound.cached_temp = Generate_Expression (ty->high_bound.expr);
            const char *ety = Expression_Llvm_Type (ty->high_bound.expr);
            if (ety and ety[0])
              Temp_Set_Type (ty->high_bound.cached_temp, ety);
          }
          has_cached_bounds = has_cached_bounds or (ty->high_bound.cached_temp != 0);
        }

        // RM 3.2.2(5): For access types with constrained designated record,                        
        // pre-evaluate discriminant constraint expressions before the init.                        
        // This ensures side effects occur in the correct order and cached                          
        // values are used by both aggregate generation and constraint checks.                      
        //                                                                                          
        uint32_t acc_disc_cached[MAX_DISC_CACHE] = {0};
        uint32_t acc_disc_count = 0;
        if (ty and Type_Is_Access (ty) and ty->access.designated_type) {
          Type_Info *des = ty->access.designated_type;
          if (des->kind == TYPE_RECORD and des->record.has_disc_constraints
            and des->record.disc_constraint_exprs
            and des->record.discriminant_count > 0) {
            acc_disc_count = des->record.discriminant_count;
            if (acc_disc_count > MAX_DISC_CACHE) acc_disc_count = MAX_DISC_CACHE;
            for (uint32_t di = 0; di < acc_disc_count; di++) {
              Component_Info *dci = &des->record.components[di];
              const char *ddt = dci->component_type ?
                Type_To_Llvm (dci->component_type) : "i32";
              uint32_t disc_val = Emit_Disc_Constraint_Value (des, di, ddt);
              if (disc_val) acc_disc_cached[di] = disc_val;
            }

            // Set up cg->disc_cache for aggregate generation
            for (uint32_t di = 0; di < acc_disc_count; di++)
              cg->disc_cache[di] = acc_disc_cached[di];
            cg->disc_cache_count = acc_disc_count;
            cg->disc_cache_type = des;
            has_cached_bounds = true;
          }
        }

        // RM 3.5(3): Check dynamic subtype constraint bounds against base type.
        // Must happen before init to respect evaluation order.
        Emit_Subtype_Constraint_Compat_Check (ty);
        uint32_t init = Generate_Expression (node->object_decl.init);

        // Clear disc cache after init generation
        if (acc_disc_count > 0) {
          cg->disc_cache_count = 0;
          cg->disc_cache_type = NULL;
        }

        // Use Expression_Llvm_Type to get correct type for all expressions
        // including pointers, floats, and integers
        const char *src_type_str = Expression_Llvm_Type (node->object_decl.init);

        // RM 3.3.2: Scalar constraint check on initialization.
        // check BEFORE conversion so value is at source type.
        if (ty and Type_Is_Scalar (ty)) {
          Type_Info *init_src_type = node->object_decl.init->type;
          Emit_Constraint_Check_With_Type (init, ty, init_src_type, src_type_str);
        }

        // Float > fixed-point: scale by SMALL before integer conversion.                           
        // Fixed-point values are stored as scaled integers: val / SMALL.                           
        // Without this, fptosi would just truncate the float value.                                
        //                                                                                          
        if (ty and Type_Is_Fixed_Point (ty) and Is_Float_Type (src_type_str)) {
          double small = ty->fixed.small;
          if (small <= 0) small = ty->fixed.delta > 0 ? ty->fixed.delta : 1.0;
          uint64_t sb; memcpy (&sb, &small, sizeof (sb));
          uint32_t small_t = Emit_Temp ();
          Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small=%g\n",
             small_t, (unsigned long long)sb, small);
          uint32_t div_t = Emit_Temp ();
          Emit ("  %%t%u = fdiv %s %%t%u, %%t%u  ; float/SMALL for fixed-point\n",
             div_t, src_type_str, init, small_t);
          init = div_t;
        }

        // RM 4.8(6): Access type constraint check.  When assigning                                 
        // an allocator result to a constrained access type variable,                               
        // verify that the designated object's discriminants or bounds                              
        // match the access subtype's constraints.                                                  
        //                                                                                          
        if (ty and Type_Is_Access (ty) and ty->access.designated_type) {
          Type_Info *des = ty->access.designated_type;

          // Check discriminant constraints on designated record type
          if (des->kind == TYPE_RECORD and des->record.has_disc_constraints
            and des->record.disc_constraint_values
            and des->record.discriminant_count > 0) {

            // RM 4.8(6): Extract data ptr from fat-pointer access,
            // null-check, then verify discriminants.
            uint32_t init_ptr = init;
            { const char *ity = Temp_Get_Type (init);
              if (ity and Llvm_Type_Is_Fat_Pointer (ity)) {
                init_ptr = Emit_Temp ();
                Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n",
                   init_ptr, init);
              }
            }
            uint32_t is_nul = Emit_Temp ();
            Emit ("  %%t%u = icmp eq ptr %%t%u, null\n", is_nul, init_ptr);
            uint32_t lbl_chk = cg->label_id++;
            uint32_t lbl_end = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               is_nul, lbl_end, lbl_chk);
            Emit ("L%u:\n", lbl_chk);
            for (uint32_t di = 0; di < des->record.discriminant_count; di++) {
              Component_Info *dc = &des->record.components[di];
              const char *dt = Type_To_Llvm (dc->component_type);
              uint32_t dp = Emit_Temp ();
              Emit ("  %%t%u = getelementptr i8, ptr %%t%u, i64 %u\n",
                 dp, init_ptr, dc->byte_offset);
              uint32_t dv = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u  ; access disc %.*s\n",
                 dv, dt, dp, (int)dc->name.length, dc->name.data);
              uint32_t cv;

              // Use pre-cached disc values if available
              if (di < acc_disc_count and acc_disc_cached[di]) {
                cv = Emit_Coerce_Default_Int (acc_disc_cached[di], dt);
              } else {
                cv = Emit_Disc_Constraint_Value (des, di, dt);
                if (cv == 0) cv = Emit_Static_Int (0, dt);
              }
              Emit_Discriminant_Check (dv, cv, dt, des);
            }
            Emit ("  br label %%L%u\n", lbl_end);
            Emit ("L%u:\n", lbl_end);
          }

          // Check index constraints on designated array type
          if (Type_Is_Array_Like (des) and des->array.is_constrained
            and des->array.index_count > 0) {
            const char *acc_llvm = src_type_str;
            bool is_fat = Llvm_Type_Is_Fat_Pointer (acc_llvm);

            // Fat pointer - extract bounds and compare
            if (is_fat) {
              const char *abt = Array_Bound_Llvm_Type (des);
              for (uint32_t xi = 0; xi < des->array.index_count; xi++) {
                Type_Bound *lo_b = &des->array.indices[xi].low_bound;
                Type_Bound *hi_b = &des->array.indices[xi].high_bound;

                // Get expected bounds (may be i32) and coerce to abt
                uint32_t exp_lo = Emit_Bound_Value (lo_b);
                uint32_t exp_hi = Emit_Bound_Value (hi_b);
                exp_lo = Emit_Coerce_Default_Int (exp_lo, abt);
                exp_hi = Emit_Coerce_Default_Int (exp_hi, abt);

                // Get actual bounds from fat pointer
                uint32_t act_lo = Emit_Fat_Pointer_Low (init, abt);
                uint32_t act_hi = Emit_Fat_Pointer_High (init, abt);

                // Check lo match
                uint32_t clo = Emit_Temp ();
                Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                   clo, abt, act_lo, exp_lo);
                Emit_Check_With_Raise (clo, true, "access array lo bound");

                // Check hi match
                uint32_t chi = Emit_Temp ();
                Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                   chi, abt, act_hi, exp_hi);
                Emit_Check_With_Raise (chi, true, "access array hi bound");
              }
            } else if (node->object_decl.init and
                   node->object_decl.init->kind == NK_ALLOCATOR) {

              // RM 4.8(6): Non-fat-pointer allocator - compare type-level                          
              // bounds from the allocator's designated type against the                            
              // constrained access subtype's designated type bounds.                               
              // E.g. AC : ACCA (1..2) := NEW ARR (1..1) must raise CE.                             
              //                                                                                    
              Syntax_Node *alloc_node = node->object_decl.init;
              Type_Info *alloc_des = NULL;
              if (alloc_node->type and Type_Is_Access (alloc_node->type))
                alloc_des = alloc_node->type->access.designated_type;
              if (not alloc_des and alloc_node->allocator.subtype_mark)
                alloc_des = alloc_node->allocator.subtype_mark->type;
              if (alloc_des and Type_Is_Array_Like (alloc_des)
                and alloc_des->array.index_count > 0) {
                const char *abt = Array_Bound_Llvm_Type (des);

                // Null check first
                uint32_t is_nul = Emit_Temp ();
                Emit ("  %%t%u = icmp eq ptr %%t%u, null\n", is_nul, init);
                uint32_t lbl_achk = cg->label_id++;
                uint32_t lbl_aend = cg->label_id++;
                Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                   is_nul, lbl_aend, lbl_achk);
                Emit ("L%u:\n", lbl_achk);
                uint32_t ndims = des->array.index_count;
                if (ndims > alloc_des->array.index_count)
                  ndims = alloc_des->array.index_count;
                for (uint32_t xi = 0; xi < ndims; xi++) {
                  Type_Bound *exp_lo_b = &des->array.indices[xi].low_bound;
                  Type_Bound *exp_hi_b = &des->array.indices[xi].high_bound;
                  Type_Bound *act_lo_b = &alloc_des->array.indices[xi].low_bound;
                  Type_Bound *act_hi_b = &alloc_des->array.indices[xi].high_bound;
                  uint32_t exp_lo = Emit_Bound_Value (exp_lo_b);
                  uint32_t exp_hi = Emit_Bound_Value (exp_hi_b);
                  uint32_t act_lo = Emit_Bound_Value (act_lo_b);
                  uint32_t act_hi = Emit_Bound_Value (act_hi_b);

                  // Skip check if any bound produced temp 0 (unresolved)
                  if (exp_lo == 0 or exp_hi == 0 or act_lo == 0 or act_hi == 0)
                    continue;

                  // Coerce to array bound type (may differ from i32)
                  exp_lo = Emit_Coerce_Default_Int (exp_lo, abt);
                  exp_hi = Emit_Coerce_Default_Int (exp_hi, abt);
                  act_lo = Emit_Coerce_Default_Int (act_lo, abt);
                  act_hi = Emit_Coerce_Default_Int (act_hi, abt);

                  // Check lo match
                  uint32_t clo = Emit_Temp ();
                  Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                     clo, abt, act_lo, exp_lo);
                  Emit_Check_With_Raise (clo, true, "access array lo bound");

                  // Check hi match
                  uint32_t chi = Emit_Temp ();
                  Emit ("  %%t%u = icmp ne %s %%t%u, %%t%u\n",
                     chi, abt, act_hi, exp_hi);
                  Emit_Check_With_Raise (chi, true, "access array hi bound");
                }
                Emit ("  br label %%L%u\n", lbl_aend);
                Emit ("L%u:\n", lbl_aend);
              }
            }
          }
        }

        // Convert if types differ, then store
        init = Emit_Convert (init, src_type_str, type_str);
        Emit ("  store %s %%t%u, ptr %%", type_str, init);
        Emit_Symbol_Name (sym);
        Emit ("\n");

        // Clear cached bound temps so next object in a multi-declaration
        // re-evaluates them independently (RM 3.2.2).
        if (has_cached_bounds and ty) {
          ty->low_bound.cached_temp = 0;
          ty->high_bound.cached_temp = 0;
        }
      }

    // Uninitialized array with dynamic bounds - still need to set up fat pointer.                  
    // The array contents are uninitialized but bounds are known from the type.                     
    // This handles cases like: A2 : ARR1 (1 .. F * 1000);                                          
    //                                                                                              
    } else if (is_any_array and Type_Has_Dynamic_Bounds (ty) and ty->array.index_count > 0) {
      const char *iat_decl = Integer_Arith_Type ();
      uint32_t ndims_decl = ty->array.index_count;
      if (ndims_decl > 8) ndims_decl = 8;
      uint32_t dim_lo_decl[8], dim_hi_decl[8];
      for (uint32_t d = 0; d < ndims_decl; d++) {
        Type_Bound low_b = ty->array.indices[d].low_bound;
        Type_Bound high_b = ty->array.indices[d].high_bound;

        // Get low bound
        if (low_b.kind == BOUND_INTEGER) {
          dim_lo_decl[d] = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %s\n", dim_lo_decl[d], iat_decl, I128_Decimal (low_b.int_value));
          Temp_Set_Type (dim_lo_decl[d], iat_decl);

        // RM 3.3.1: Reuse cached temp from subtype elaboration
        // to avoid re-evaluating side-effectful expressions.
        } else if (low_b.kind == BOUND_EXPR and low_b.expr) {
          dim_lo_decl[d] = low_b.cached_temp ? low_b.cached_temp
            : Generate_Expression (low_b.expr);
          if (not low_b.cached_temp and
            not Type_Is_Float_Representation (low_b.expr->type)) {
            const char *low_llvm = Expression_Llvm_Type (low_b.expr);
            if (strcmp (low_llvm, iat_decl) != 0 and not Llvm_Type_Is_Pointer (low_llvm))
              dim_lo_decl[d] = Emit_Convert (dim_lo_decl[d], low_llvm, iat_decl);
          }
          Temp_Set_Type (dim_lo_decl[d], iat_decl);

        // Derive from index type's low bound (e.g., BOOLEAN'FIRST=0, NI'FIRST=-3)
        } else if (ty->array.indices[d].index_type) {
          Type_Info *idx_ty = ty->array.indices[d].index_type;
          if (idx_ty->low_bound.kind == BOUND_EXPR and idx_ty->low_bound.expr) {
            dim_lo_decl[d] = Generate_Expression (idx_ty->low_bound.expr);
            const char *vty = Expression_Llvm_Type (idx_ty->low_bound.expr);
            if (strcmp (vty, iat_decl) != 0 and vty[0] == 'i')
              dim_lo_decl[d] = Emit_Convert (dim_lo_decl[d], vty, iat_decl);
          } else {
            dim_lo_decl[d] = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %s\n", dim_lo_decl[d], iat_decl,
               I128_Decimal (Type_Bound_Value (idx_ty->low_bound)));
          }
          Temp_Set_Type (dim_lo_decl[d], iat_decl);
        } else {
          dim_lo_decl[d] = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, 1\n", dim_lo_decl[d], iat_decl);
          Temp_Set_Type (dim_lo_decl[d], iat_decl);
        }

        // Get high bound
        if (high_b.kind == BOUND_INTEGER) {
          dim_hi_decl[d] = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, %s\n", dim_hi_decl[d], iat_decl, I128_Decimal (high_b.int_value));
          Temp_Set_Type (dim_hi_decl[d], iat_decl);
        } else if (high_b.kind == BOUND_EXPR and high_b.expr) {
          dim_hi_decl[d] = high_b.cached_temp ? high_b.cached_temp
            : Generate_Expression (high_b.expr);
          if (not high_b.cached_temp and
            not Type_Is_Float_Representation (high_b.expr->type)) {
            const char *high_llvm = Expression_Llvm_Type (high_b.expr);
            if (strcmp (high_llvm, iat_decl) != 0 and not Llvm_Type_Is_Pointer (high_llvm))
              dim_hi_decl[d] = Emit_Convert (dim_hi_decl[d], high_llvm, iat_decl);
          }
          Temp_Set_Type (dim_hi_decl[d], iat_decl);

        // Derive from index type's high bound
        } else if (ty->array.indices[d].index_type) {
          Type_Info *idx_ty = ty->array.indices[d].index_type;
          if (idx_ty->high_bound.kind == BOUND_EXPR and idx_ty->high_bound.expr) {
            dim_hi_decl[d] = Generate_Expression (idx_ty->high_bound.expr);
            const char *vty = Expression_Llvm_Type (idx_ty->high_bound.expr);
            if (strcmp (vty, iat_decl) != 0 and vty[0] == 'i')
              dim_hi_decl[d] = Emit_Convert (dim_hi_decl[d], vty, iat_decl);
          } else {
            dim_hi_decl[d] = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %s\n", dim_hi_decl[d], iat_decl,
               I128_Decimal (Type_Bound_Value (idx_ty->high_bound)));
          }
          Temp_Set_Type (dim_hi_decl[d], iat_decl);
        } else {
          dim_hi_decl[d] = Emit_Temp ();
          Emit ("  %%t%u = add %s 0, 0\n", dim_hi_decl[d], iat_decl);
          Temp_Set_Type (dim_hi_decl[d], iat_decl);
        }
      }

      // Allocate array data: total_bytes = product(len_d) * elem_size
      uint32_t elem_sz = elem_size > 0 ? elem_size :
                 (ty->array.element_type ? ty->array.element_type->size : 8);
      if (elem_sz == 0) elem_sz = 8;
      uint32_t total_count = 0;
      for (uint32_t d = 0; d < ndims_decl; d++) {
        uint32_t count_d = Emit_Temp ();
        Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", count_d, iat_decl, dim_hi_decl[d], dim_lo_decl[d]);
        uint32_t len_d = Emit_Temp ();
        Emit ("  %%t%u = add %s %%t%u, 1\n", len_d, iat_decl, count_d);
        if (d == 0) {
          total_count = len_d;
        } else {
          uint32_t prod = Emit_Temp ();
          Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", prod, iat_decl, total_count, len_d);
          total_count = prod;
        }
      }
      uint32_t byte_size_raw = Emit_Temp ();
      Emit ("  %%t%u = mul %s %%t%u, %u\n", byte_size_raw, iat_decl, total_count, elem_sz);

      // Clamp to 0 for null ranges: hi < lo → negative count → huge unsigned alloca
      uint32_t neg_chk = Emit_Temp ();
      Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", neg_chk, iat_decl, byte_size_raw);
      uint32_t byte_size = Emit_Temp ();
      Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
         byte_size, neg_chk, iat_decl, iat_decl, byte_size_raw);
      uint32_t data_ptr = Emit_Temp ();
      Emit ("  %%t%u = alloca i8, %s %%t%u  ; dynamic uninit array\n", data_ptr, iat_decl, byte_size);

      // Construct fat pointer with multi-dim bounds
      if (ndims_decl > 1) {
        uint32_t fat = Emit_Fat_Pointer_MultiDim (data_ptr, dim_lo_decl, dim_hi_decl, ndims_decl,
                              Array_Bound_Llvm_Type (ty));
        Emit_Store_Fat_Pointer_To_Symbol (fat, sym, Array_Bound_Llvm_Type (ty));
      } else {
        Emit_Store_Fat_Pointer_Fields_To_Symbol (data_ptr, dim_lo_decl[0], dim_hi_decl[0], sym, Array_Bound_Llvm_Type (ty));
      }

    // Record without explicit initializer (RM 3.7):                                                
    // 1. If constrained subtype, initialize discriminants from constraints                         
    // 2. Apply component defaults for discriminants and regular components                         
    //                                                                                              
    } else if (is_record and ty->record.component_count > 0) {

      // Initialize discriminant constraints if type is constrained (RM 3.7.2)
      Symbol *disc_temps[16]; uint32_t disc_temp_count = 0;
      bool has_decl_disc = decl_disc_exprs and decl_disc_count > 0;
      if (ty->record.has_disc_constraints and ty->record.disc_constraint_values) {
        for (uint32_t di = 0; di < ty->record.discriminant_count; di++) {
          Component_Info *dc = &ty->record.components[di];
          uint32_t dp = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
          Emit_Symbol_Name (sym);
          Emit (", i64 %u  ; disc %.*s init\n", dc->byte_offset,
             (int)dc->name.length, dc->name.data);
          const char *dt = Type_To_Llvm (dc->component_type);
          uint32_t val;

          // Use pre-evaluated / runtime / static constraint value
          val = Emit_Disc_Constraint_Value (ty, di, dt);
          if (val == 0) val = Emit_Static_Int (0, dt);
          Emit ("  store %s %%t%u, ptr %%t%u  ; disc init\n", dt, val, dp);

          // RM 3.7.2: Check discriminant value against disc subtype
          if (dc->component_type and Type_Is_Scalar (dc->component_type))
            Emit_Constraint_Check (val, dc->component_type, NULL);

          // Find discriminant symbols referenced by dependent array                                
          // bounds and store the constraint value into a temp alloca.                              
          // Needed so Generate_Expression can load disc values for                                 
          // dependent component size computation.                                                  
          //                                                                                        
          for (uint32_t ci2 = 0; ci2 < ty->record.component_count; ci2++) {
            Type_Info *ct2 = ty->record.components[ci2].component_type;
            if (not ct2 or not Type_Is_Array_Like (ct2)) continue;
            for (uint32_t xi = 0; xi < ct2->array.index_count; xi++) {
              Type_Bound *bds[2] = { &ct2->array.indices[xi].low_bound,
                           &ct2->array.indices[xi].high_bound };
              for (int bi2 = 0; bi2 < 2; bi2++) {
                if (bds[bi2]->kind == BOUND_EXPR and bds[bi2]->expr and
                  bds[bi2]->expr->symbol and
                  Slice_Equal_Ignore_Case (bds[bi2]->expr->symbol->name, dc->name)) {
                  Symbol *dsym = bds[bi2]->expr->symbol;
                  if (dsym->disc_agg_temp == 0) {
                    uint32_t dt2 = Emit_Temp ();
                    Emit ("  %%t%u = alloca %s  ; decl disc temp\n", dt2, dt);
                    Emit ("  store %s %%t%u, ptr %%t%u\n", dt, val, dt2);
                    dsym->disc_agg_temp = dt2;
                    if (disc_temp_count < 16) disc_temps[disc_temp_count++] = dsym;
                  }
                }
              }
            }
          }
        }
        sym->is_disc_constrained = true;

      // Generic instantiation: discriminant constraints from the AST                               
      // (e.g., P2 : PRIV (T'VAL (F*100))) override defaults (RM 12.3).                             
      // The formal type doesn't carry constraints, but the declaration                             
      // AST has the constraint expressions.                                                        
      //                                                                                            
      } else if (has_decl_disc and ty->record.has_discriminants) {
        for (uint32_t di = 0; di < ty->record.discriminant_count and di < decl_disc_count; di++) {
          Component_Info *dc = &ty->record.components[di];
          uint32_t dp = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
          Emit_Symbol_Name (sym);
          Emit (", i64 %u  ; disc %.*s (generic constraint)\n", dc->byte_offset,
             (int)dc->name.length, dc->name.data);
          const char *dt = Type_To_Llvm (dc->component_type);
          Syntax_Node *expr = decl_disc_exprs[di];

          // Unwrap NK_ASSOCIATION if present
          if (expr and expr->kind == NK_ASSOCIATION)
            expr = expr->association.expression;
          uint32_t val = Generate_Expression (expr);
          val = Emit_Coerce_Default_Int (val, dt);
          Emit ("  store %s %%t%u, ptr %%t%u  ; disc from decl constraint\n", dt, val, dp);
        }
        sym->is_disc_constrained = true;
      }

      // RM 3.7.3: Determine selected variant so we skip absent components
      // during subcomponent init and default init.
      int32_t decl_sel_variant = -2;  // -2 = unknown
      uint32_t decl_rt_disc_val = 0;
      const char *decl_rt_disc_type = NULL;

      // Try static disc value first
      if (ty->record.variant_count > 0 and ty->record.has_discriminants) {
        bool dv_known = false;
        int64_t dv = 0;
        if (ty->record.disc_constraint_values and
          (not ty->record.disc_constraint_exprs or
           not ty->record.disc_constraint_exprs[0])) {
          dv = ty->record.disc_constraint_values[0];
          dv_known = true;
        } else if (ty->record.disc_constraint_values and
               not ty->record.disc_constraint_exprs) {
          dv = ty->record.disc_constraint_values[0];
          dv_known = true;
        }
        if (dv_known) {
          decl_sel_variant = -1;
          for (uint32_t vi = 0; vi < ty->record.variant_count; vi++) {
            if (dv >= ty->record.variants[vi].disc_value_low and
              dv <= ty->record.variants[vi].disc_value_high) {
              decl_sel_variant = (int32_t)vi;
              break;
            }
            if (ty->record.variants[vi].is_others)
              decl_sel_variant = (int32_t)vi;
          }
        } else if (ty->record.disc_constraint_exprs and
               ty->record.disc_constraint_exprs[0]) {

          // Dynamic: load the disc value that was just stored
          decl_rt_disc_type = Type_To_Llvm (
            ty->record.components[0].component_type);
          if (not decl_rt_disc_type) decl_rt_disc_type = "i32";
          decl_rt_disc_val = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%", decl_rt_disc_val);
          Emit_Symbol_Name (sym);
          Emit (", i64 %u\n",
             ty->record.components[0].byte_offset);
          uint32_t ld = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u\n",
             ld, decl_rt_disc_type, decl_rt_disc_val);
          decl_rt_disc_val = ld;
        }
      }

      // Initialize discriminant constraints of record subcomponents (RM 3.7.2).                    
      // E.g., for TYPE R2 (D2: POSITIVE) IS RECORD C : R1 (2); END RECORD;                         
      // we must also initialize C.D1 = 2 at the appropriate offset.                                
      //                                                                                            
      for (uint32_t ci = ty->record.discriminant_count;
         ci < ty->record.component_count; ci++) {
        Component_Info *comp = &ty->record.components[ci];

        // RM 3.7.3: Skip components in unselected variants
        if (decl_sel_variant != -2 and comp->variant_index >= 0 and
          comp->variant_index != decl_sel_variant)
          continue;

        // Runtime variant guard for dynamic disc values
        uint32_t decl_rt_skip = 0;
        if (decl_sel_variant == -2 and decl_rt_disc_val > 0 and
          comp->variant_index >= 0 and
          (uint32_t)comp->variant_index < ty->record.variant_count) {
          Variant_Info *vinfo = &ty->record.variants[comp->variant_index];
          if (not vinfo->is_others) {
            uint32_t lo = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %lld\n", lo, decl_rt_disc_type,
               (long long)vinfo->disc_value_low);
            uint32_t hi = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %lld\n", hi, decl_rt_disc_type,
               (long long)vinfo->disc_value_high);
            uint32_t cmp_lo = Emit_Temp ();
            Emit ("  %%t%u = icmp sge %s %%t%u, %%t%u\n",
               cmp_lo, decl_rt_disc_type, decl_rt_disc_val, lo);
            uint32_t cmp_hi = Emit_Temp ();
            Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n",
               cmp_hi, decl_rt_disc_type, decl_rt_disc_val, hi);
            uint32_t in_range = Emit_Temp ();
            Emit ("  %%t%u = and i1 %%t%u, %%t%u\n",
               in_range, cmp_lo, cmp_hi);
            uint32_t check_lbl = cg->label_id++;
            decl_rt_skip = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               in_range, check_lbl, decl_rt_skip);
            cg->block_terminated = true;
            Emit_Label_Here (check_lbl);
          }
        }
        Type_Info *ct = comp->component_type;
        if (not ct or ct->kind != TYPE_RECORD) {
          if (decl_rt_skip) {
            Emit ("  br label %%L%u\n", decl_rt_skip);
            cg->block_terminated = true;
            Emit_Label_Here (decl_rt_skip);
          }
          continue;
        }
        if (not ct->record.has_disc_constraints or
          not ct->record.disc_constraint_values) {
          if (decl_rt_skip) {
            Emit ("  br label %%L%u\n", decl_rt_skip);
            cg->block_terminated = true;
            Emit_Label_Here (decl_rt_skip);
          }
          continue;
        }
        for (uint32_t di = 0; di < ct->record.discriminant_count; di++) {
          Component_Info *dc = &ct->record.components[di];
          uint32_t dp = Emit_Temp ();
          uint32_t abs_offset = comp->byte_offset + dc->byte_offset;
          Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
          Emit_Symbol_Name (sym);
          Emit (", i64 %u  ; subcomponent disc %.*s.%.*s init\n",
             abs_offset,
             (int)comp->name.length, comp->name.data,
             (int)dc->name.length, dc->name.data);
          const char *dt = Type_To_Llvm (dc->component_type);
          uint32_t disc_val = 0;
          if (ct->record.disc_constraint_exprs and
            ct->record.disc_constraint_exprs[di]) {

            // Check if the constraint expression references a discriminant                         
            // of the parent record - if so, load from the record variable                          
            // at the discriminant's offset rather than via Generate_Expression                     
            // (the discriminant has no standalone alloca, RM 3.7.2).                               
            //                                                                                      
            Syntax_Node *cexpr = ct->record.disc_constraint_exprs[di];
            bool found_parent_disc = false;
            if (cexpr->kind == NK_IDENTIFIER and cexpr->symbol) {
              for (uint32_t pdi = 0; pdi < ty->record.discriminant_count; pdi++) {
                Component_Info *pdc = &ty->record.components[pdi];
                if (Slice_Equal_Ignore_Case (pdc->name, cexpr->symbol->name)) {
                  disc_val = Emit_Temp ();
                  Emit ("  %%t%u = getelementptr i8, ptr %%", disc_val);
                  Emit_Symbol_Name (sym);
                  Emit (", i64 %u\n", pdc->byte_offset);
                  uint32_t ld = Emit_Temp ();
                  Emit ("  %%t%u = load %s, ptr %%t%u\n", ld, dt, disc_val);
                  disc_val = ld;
                  found_parent_disc = true;
                  break;
                }
              }
            }

            // Use pre-evaluated value if available (RM 3.7.1:                                      
            // non-disc expressions in disc-dependent constraints                                   
            // are evaluated once at type elaboration).                                             
            //                                                                                      
            if (not found_parent_disc) {
              if (ct->record.disc_constraint_preeval and
                ct->record.disc_constraint_preeval[di]) {
                disc_val = Emit_Temp ();
                Emit ("  %%t%u = load %s, ptr %%t%u  ; preeval disc\n",
                   disc_val, dt, ct->record.disc_constraint_preeval[di]);
              } else {
                disc_val = Generate_Expression (cexpr);
              }
            }
            disc_val = Emit_Coerce_Default_Int (disc_val, dt);
          } else if (ct->record.disc_constraint_values) {
            disc_val = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %lld\n", disc_val, dt,
               (long long)ct->record.disc_constraint_values[di]);
          }
          if (disc_val > 0) {
            Emit ("  store %s %%t%u, ptr %%t%u  ; subcomp disc init\n",
               dt, disc_val, dp);

            // RM 3.7.2: Check nested disc value against disc subtype
            if (dc->component_type and Type_Is_Scalar (dc->component_type))
              Emit_Constraint_Check (disc_val, dc->component_type, NULL);
          }
        }

        // Close runtime variant guard
        if (decl_rt_skip) {
          Emit ("  br label %%L%u\n", decl_rt_skip);
          cg->block_terminated = true;
          Emit_Label_Here (decl_rt_skip);
        }
      }

      // Apply component defaults (RM 3.7) - includes discriminant defaults
      // for mutable records (all discriminants have defaults, no explicit constraint)
      bool has_any_default = false;
      for (uint32_t ci = 0; ci < ty->record.component_count; ci++) {
        if (ty->record.components[ci].default_expr) {
          has_any_default = true;
          break;
        }
      }
      if (has_any_default) {
        for (uint32_t ci = 0; ci < ty->record.component_count; ci++) {
          Component_Info *comp = &ty->record.components[ci];
          if (not comp->default_expr) continue;

          // Skip discriminant defaults if constraint values already set
          if (comp->is_discriminant and (ty->record.has_disc_constraints or has_decl_disc)) continue;

          // RM 3.7.3: Skip default init for components in unselected variants
          if (decl_sel_variant != -2 and comp->variant_index >= 0 and
            comp->variant_index != decl_sel_variant)
            continue;

          // Generate value for default expression
          uint32_t val = Generate_Expression (comp->default_expr);
          if (val == 0) continue;  // Expression generation failed

          // Check default value against component subtype constraint (RM 3.3.2).                   
          // E.g., A : INTEGER RANGE 1..10 := IDENT_INT (0) must raise                              
          // CONSTRAINT_ERROR because 0 is not in 1..10.                                            
          //                                                                                        
          Emit_Constraint_Check (val, comp->component_type,
                      comp->default_expr->type);

          // Get pointer to component within record - use runtime
          // offset for records with dynamic-sized components.
          uint32_t comp_ptr = Emit_Temp ();
          if (ty and ty->rt_global_id > 0) {
            uint32_t rt_off = Emit_Temp ();
            Emit ("  %%t%u = load i64, ptr @__rt_rec_%u_off%u\n",
               rt_off, ty->rt_global_id, ci);
            Emit ("  %%t%u = getelementptr i8, ptr %%", comp_ptr);
            Emit_Symbol_Name (sym);
            Emit (", i64 %%t%u  ; %.*s rt default\n", rt_off,
               (int)comp->name.length, comp->name.data);
          } else {
            Emit ("  %%t%u = getelementptr i8, ptr %%", comp_ptr);
            Emit_Symbol_Name (sym);
            Emit (", i64 %u  ; %.*s default\n", comp->byte_offset,
               (int)comp->name.length, comp->name.data);
          }

          // Store default value into component, handling composites
          // (arrays/records) with memcpy instead of scalar store.
          Type_Info *comp_type = comp->component_type;
          bool comp_is_composite = Type_Is_Composite (comp_type);
          bool has_rt_size = comp_type and comp_type->rt_global_id > 0;

          // Composite default: expression returns a data pointer.                                  
          // For dynamic-bound aggregates that produce fat pointers,                                
          // extract the data pointer first.                                                        
          //                                                                                        
          if (comp_is_composite and (comp_type->size > 0 or has_rt_size)) {
            uint32_t data_ptr = val;
            bool is_fat_agg = comp->default_expr->kind == NK_AGGREGATE and
              comp_type and Type_Is_Array_Like (comp_type) and
              (Type_Is_Unconstrained_Array (comp_type) or
               Aggregate_Produces_Fat_Pointer (comp_type));
            if (is_fat_agg) {
              uint32_t loaded = Emit_Temp ();
              Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n",
                 loaded, val);
              data_ptr = Emit_Temp ();
              Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE
                 " %%t%u, 0\n", data_ptr, loaded);
            }
            if (has_rt_size) {
              uint32_t rtsz = Emit_Temp ();
              Emit ("  %%t%u = load i64, ptr @__rt_type_%u_size\n",
                 rtsz, comp_type->rt_global_id);
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
                 "  ; %.*s rt default memcpy\n",
                 comp_ptr, data_ptr, rtsz,
                 (int)comp->name.length, comp->name.data);
            } else {
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)"
                 "  ; %.*s default memcpy\n",
                 comp_ptr, data_ptr, comp_type->size,
                 (int)comp->name.length, comp->name.data);
            }

          // Dynamic-size composite (disc-dependent array):                                         
          // extract data from fat-pointer aggregate, compute                                       
          // byte size from bounds, and memcpy.                                                     
          //                                                                                        
          } else if (comp_is_composite) {
            bool is_fat_agg = comp->default_expr->kind == NK_AGGREGATE and
              comp_type and Type_Is_Array_Like (comp_type) and
              (Type_Is_Unconstrained_Array (comp_type) or
               Aggregate_Produces_Fat_Pointer (comp_type));
            if (is_fat_agg) {
              uint32_t loaded = Emit_Temp ();
              Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n", loaded, val);
              uint32_t data_ptr = Emit_Temp ();
              Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n",
                 data_ptr, loaded);
              uint32_t bnds = Emit_Temp ();
              Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n",
                 bnds, loaded);
              uint32_t ndim = comp_type->array.index_count;
              uint32_t esz = (comp_type->array.element_type and
                comp_type->array.element_type->size > 0)
                ? comp_type->array.element_type->size : 4;
              const char *bt = Array_Bound_Llvm_Type (comp_type);
              uint32_t tsz = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %u\n", tsz, bt, esz);
              for (uint32_t d = 0; d < ndim; d++) {
                uint32_t lp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
                   lp, bt, bnds, d * 2);
                uint32_t lo = Emit_Temp ();
                Emit ("  %%t%u = load %s, ptr %%t%u\n", lo, bt, lp);
                uint32_t hp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
                   hp, bt, bnds, d * 2 + 1);
                uint32_t hi = Emit_Temp ();
                Emit ("  %%t%u = load %s, ptr %%t%u\n", hi, bt, hp);
                uint32_t cnt = Emit_Temp ();
                Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", cnt, bt, hi, lo);
                uint32_t c1 = Emit_Temp ();
                Emit ("  %%t%u = add %s %%t%u, 1\n", c1, bt, cnt);
                uint32_t neg = Emit_Temp ();
                Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", neg, bt, c1);
                uint32_t cl = Emit_Temp ();
                Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
                   cl, neg, bt, bt, c1);
                uint32_t nt = Emit_Temp ();
                Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", nt, bt, tsz, cl);
                tsz = nt;
              }
              uint32_t sz64 = tsz;
              if (strcmp (bt, "i64") != 0) {
                sz64 = Emit_Temp ();
                Emit ("  %%t%u = sext %s %%t%u to i64\n", sz64, bt, tsz);
              }
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
                 "  ; %.*s dynamic default memcpy\n",
                 comp_ptr, data_ptr, sz64,
                 (int)comp->name.length, comp->name.data);
            }
          } else {
            const char *store_type = Type_To_Llvm (comp_type);
            const char *val_type = Temp_Get_Type (val);
            if (not val_type or strlen (val_type) == 0) {
              Type_Info *expr_type = comp->default_expr->type;
              if (expr_type)
                val_type = Type_To_Llvm (expr_type);
            }
            if (not val_type or strlen (val_type) == 0)
              val_type = Expression_Llvm_Type (comp->default_expr);
            if (not store_type or strlen (store_type) == 0)
              store_type = val_type;
            if (Type_Is_Float (comp_type)) {
              const char *flt_ty = Float_Llvm_Type_Of (comp_type);
              val = Emit_Convert (val, val_type, flt_ty);
              Emit ("  store %s %%t%u, ptr %%t%u\n", flt_ty, val, comp_ptr);
            } else {
              if (comp_type and comp_type->kind == TYPE_FIXED and
                val_type and Is_Float_Type (val_type)) {
                double small = comp_type->fixed.small;
                if (small <= 0) small = comp_type->fixed.delta > 0 ? comp_type->fixed.delta : 1.0;
                uint64_t sb; memcpy (&sb, &small, sizeof (sb));
                uint32_t st = Emit_Temp ();
                Emit ("  %%t%u = fadd double 0.0, 0x%016llX  ; small\n", st, (unsigned long long)sb);
                uint32_t dv = Emit_Temp ();
                Emit ("  %%t%u = fdiv %s %%t%u, %%t%u  ; rec default/small\n", dv, val_type, val, st);
                val = dv;
              }
              val = Emit_Convert (val, val_type, store_type);
              Emit ("  store %s %%t%u, ptr %%t%u\n", store_type, val, comp_ptr);
            }
          }
        }
      }

      // Apply defaults from nested record-typed components (RM 3.3.1).                             
      // When a component has no explicit initializer but its type is a                             
      // record with default expressions, apply those defaults recursively.                         
      //                                                                                            
      for (uint32_t ci = 0; ci < ty->record.component_count; ci++) {
        Component_Info *comp = &ty->record.components[ci];
        if (comp->default_expr) continue;  // Already handled above
        if (comp->is_discriminant) continue;
        Type_Info *ct = comp->component_type;
        if (not ct or ct->kind != TYPE_RECORD) continue;

        // Check if this nested record type has any component defaults
        bool has_nested_defaults = false;
        for (uint32_t si = 0; si < ct->record.component_count; si++) {
          if (ct->record.components[si].default_expr) {
            has_nested_defaults = true;
            break;
          }
        }
        if (not has_nested_defaults) continue;

        // Set up disc_agg_temp for nested record discriminants so that                             
        // discriminant-dependent default expressions can load disc values.                         
        // Collect disc symbols from default expr ASTs, then provide values                         
        // from either disc constraints (constrained) or defaults (unconstrained).                  
        //                                                                                          
        Symbol *nested_disc_temps[16]; uint32_t nested_disc_count = 0;

        // Phase 1: Collect discriminant symbols referenced in default exprs
        Symbol *disc_syms[16]; uint32_t disc_sym_count = 0;
        for (uint32_t si2 = 0; si2 < ct->record.component_count; si2++) {
          if (ct->record.components[si2].default_expr)
            Collect_Disc_Symbols_In_Expr (ct->record.components[si2].default_expr,
              disc_syms, &disc_sym_count, 16);
        }

        // Also check array bound expressions for disc refs
        for (uint32_t ci2 = 0; ci2 < ct->record.component_count; ci2++) {
          Type_Info *ct2 = ct->record.components[ci2].component_type;
          if (not ct2 or not Type_Is_Array_Like (ct2)) continue;
          for (uint32_t xi = 0; xi < ct2->array.index_count; xi++) {
            Type_Bound *bds[2] = { &ct2->array.indices[xi].low_bound,
                         &ct2->array.indices[xi].high_bound };
            for (int bi2 = 0; bi2 < 2; bi2++) {
              if (bds[bi2]->kind == BOUND_EXPR and bds[bi2]->expr)
                Collect_Disc_Symbols_In_Expr (bds[bi2]->expr,
                  disc_syms, &disc_sym_count, 16);
            }
          }
        }

        // Phase 2: For each disc of the nested record, generate its value
        // and store both to memory and to disc_agg_temp.
        for (uint32_t di = 0; di < ct->record.discriminant_count; di++) {
          Component_Info *dc = &ct->record.components[di];
          const char *dt = Type_To_Llvm (dc->component_type);
          if (not dt or dt[0] == '\0') dt = "i32";
          uint32_t val = 0;

          // Constrained nested record: value from constraint.                                      
          // RM 3.7.2: If the constraint expression references a                                    
          // discriminant of the parent record (ty), load from                                      
          // the parent variable at its offset - the discriminant                                   
          // has no standalone alloca.                                                              
          //                                                                                        
          if (ct->record.has_disc_constraints) {
            bool resolved = false;
            if (ct->record.disc_constraint_exprs and
              ct->record.disc_constraint_exprs[di]) {
              Syntax_Node *cexpr = ct->record.disc_constraint_exprs[di];
              if (cexpr->kind == NK_IDENTIFIER and cexpr->symbol) {
                for (uint32_t pdi = 0; pdi < ty->record.discriminant_count; pdi++) {
                  Component_Info *pdc = &ty->record.components[pdi];
                  if (Slice_Equal_Ignore_Case (pdc->name, cexpr->symbol->name)) {
                    uint32_t gp = Emit_Temp ();
                    Emit ("  %%t%u = getelementptr i8, ptr %%", gp);
                    Emit_Symbol_Name (sym);
                    Emit (", i64 %u\n", pdc->byte_offset);
                    val = Emit_Temp ();
                    Emit ("  %%t%u = load %s, ptr %%t%u  ; disc %.*s from parent\n",
                       val, dt, gp,
                       (int)pdc->name.length, pdc->name.data);
                    resolved = true;
                    break;
                  }
                }
              }
            }
            if (not resolved)
              val = Emit_Disc_Constraint_Value (ct, di, dt);

          // Unconstrained nested record: value from default
          } else if (dc->default_expr) {
            val = Generate_Expression (dc->default_expr);
            val = Emit_Coerce_Default_Int (val, dt);
          }
          if (val == 0) continue;

          // Store discriminant to memory
          uint32_t abs_off = comp->byte_offset + dc->byte_offset;
          uint32_t dp = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
          Emit_Symbol_Name (sym);
          Emit (", i64 %u  ; nested disc %.*s.%.*s\n",
             abs_off,
             (int)comp->name.length, comp->name.data,
             (int)dc->name.length, dc->name.data);
          Emit ("  store %s %%t%u, ptr %%t%u\n", dt, val, dp);

          // Set disc_agg_temp for matching symbols found in defaults
          for (uint32_t ds = 0; ds < disc_sym_count; ds++) {
            Symbol *dsym = disc_syms[ds];
            if (dsym->disc_agg_temp != 0) continue;
            if (not Slice_Equal_Ignore_Case (dsym->name, dc->name)) continue;
            uint32_t dt3 = Emit_Temp ();
            Emit ("  %%t%u = alloca %s  ; nested disc_agg_temp\n", dt3, dt);
            Emit ("  store %s %%t%u, ptr %%t%u\n", dt, val, dt3);
            dsym->disc_agg_temp = dt3;
            if (nested_disc_count < 16)
              nested_disc_temps[nested_disc_count++] = dsym;
          }
        }

        // Apply non-discriminant component defaults
        for (uint32_t si = ct->record.discriminant_count;
           si < ct->record.component_count; si++) {
          Component_Info *sc = &ct->record.components[si];
          if (not sc->default_expr) continue;
          uint32_t val = Generate_Expression (sc->default_expr);
          if (val == 0) continue;
          uint32_t abs_off = comp->byte_offset + sc->byte_offset;
          uint32_t sp = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%", sp);
          Emit_Symbol_Name (sym);
          Emit (", i64 %u  ; nested default %.*s.%.*s\n",
             abs_off,
             (int)comp->name.length, comp->name.data,
             (int)sc->name.length, sc->name.data);
          Type_Info *sc_type = sc->component_type;
          bool sc_composite = Type_Is_Composite (sc_type);
          bool sc_rt = sc_type and sc_type->rt_global_id > 0;
          if (sc_composite and (sc_type->size > 0 or sc_rt)) {
            uint32_t data_ptr = val;
            bool is_fat = sc->default_expr->kind == NK_AGGREGATE and
              sc_type and Type_Is_Array_Like (sc_type) and
              (Type_Is_Unconstrained_Array (sc_type) or
               Aggregate_Produces_Fat_Pointer (sc_type));
            if (is_fat) {
              uint32_t loaded = Emit_Temp ();
              Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n", loaded, val);
              data_ptr = Emit_Temp ();
              Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE
                 " %%t%u, 0\n", data_ptr, loaded);
            }
            if (sc_rt) {
              uint32_t rtsz = Emit_Temp ();
              Emit ("  %%t%u = load i64, ptr @__rt_type_%u_size\n",
                 rtsz, sc_type->rt_global_id);
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
                 "  ; %.*s.%.*s nested rt default\n",
                 sp, data_ptr, rtsz,
                 (int)comp->name.length, comp->name.data,
                 (int)sc->name.length, sc->name.data);
            } else {
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %u, i1 false)"
                 "  ; %.*s.%.*s nested default\n",
                 sp, data_ptr, sc_type->size,
                 (int)comp->name.length, comp->name.data,
                 (int)sc->name.length, sc->name.data);
            }

          // Dynamic-size composite (disc-dependent array):
          // extract data from fat-pointer aggregate and memcpy.
          } else if (sc_composite) {
            bool is_fat = sc->default_expr->kind == NK_AGGREGATE and
              sc_type and Type_Is_Array_Like (sc_type) and
              (Type_Is_Unconstrained_Array (sc_type) or
               Type_Has_Dynamic_Bounds (sc_type) or
               Aggregate_Produces_Fat_Pointer (sc_type));
            if (is_fat) {
              uint32_t loaded = Emit_Temp ();
              Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n", loaded, val);
              uint32_t data_ptr = Emit_Temp ();
              Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n",
                 data_ptr, loaded);
              uint32_t bnds = Emit_Temp ();
              Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 1\n",
                 bnds, loaded);
              uint32_t ndim = sc_type->array.index_count;
              uint32_t esz = (sc_type->array.element_type and
                sc_type->array.element_type->size > 0)
                ? sc_type->array.element_type->size : 4;
              const char *bt = Array_Bound_Llvm_Type (sc_type);
              uint32_t tsz = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %u\n", tsz, bt, esz);
              for (uint32_t d = 0; d < ndim; d++) {
                uint32_t lp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
                   lp, bt, bnds, d * 2);
                uint32_t lo = Emit_Temp ();
                Emit ("  %%t%u = load %s, ptr %%t%u\n", lo, bt, lp);
                uint32_t hp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr %s, ptr %%t%u, i32 %u\n",
                   hp, bt, bnds, d * 2 + 1);
                uint32_t hi = Emit_Temp ();
                Emit ("  %%t%u = load %s, ptr %%t%u\n", hi, bt, hp);
                uint32_t cnt = Emit_Temp ();
                Emit ("  %%t%u = sub %s %%t%u, %%t%u\n", cnt, bt, hi, lo);
                uint32_t c1 = Emit_Temp ();
                Emit ("  %%t%u = add %s %%t%u, 1\n", c1, bt, cnt);
                uint32_t neg = Emit_Temp ();
                Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", neg, bt, c1);
                uint32_t cl = Emit_Temp ();
                Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n",
                   cl, neg, bt, bt, c1);
                uint32_t nt = Emit_Temp ();
                Emit ("  %%t%u = mul %s %%t%u, %%t%u\n", nt, bt, tsz, cl);
                tsz = nt;
              }
              uint32_t sz64 = tsz;
              if (strcmp (bt, "i64") != 0) {
                sz64 = Emit_Temp ();
                Emit ("  %%t%u = sext %s %%t%u to i64\n", sz64, bt, tsz);
              }
              Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%t%u, ptr %%t%u, i64 %%t%u, i1 false)"
                 "  ; %.*s.%.*s dynamic nested default\n",
                 sp, data_ptr, sz64,
                 (int)comp->name.length, comp->name.data,
                 (int)sc->name.length, sc->name.data);
            }
          } else {
            const char *store_type = Type_To_Llvm (sc_type);
            const char *val_type = Temp_Get_Type (val);
            if (not val_type or strlen (val_type) == 0) {
              Type_Info *et = sc->default_expr->type;
              if (et) val_type = Type_To_Llvm (et);
            }
            if (not val_type or strlen (val_type) == 0)
              val_type = Expression_Llvm_Type (sc->default_expr);
            if (not store_type or strlen (store_type) == 0)
              store_type = val_type;
            if (Type_Is_Float (sc_type)) {
              const char *flt_ty = Float_Llvm_Type_Of (sc_type);
              val = Emit_Convert (val, val_type, flt_ty);
              Emit ("  store %s %%t%u, ptr %%t%u\n", flt_ty, val, sp);
            } else {
              val = Emit_Convert (val, val_type, store_type);
              Emit ("  store %s %%t%u, ptr %%t%u\n", store_type, val, sp);
            }
          }
        }

        // Clean up disc_agg_temp for nested record
        for (uint32_t ndi = 0; ndi < nested_disc_count; ndi++)
          nested_disc_temps[ndi]->disc_agg_temp = 0;
      }

      // RM 3.3.2: After applying discriminant defaults, verify that                                
      // discriminant-dependent component constraints are satisfiable.                              
      // Load disc values directly from the record storage (disc symbols                            
      // don't have standalone allocas).                                                            
      //                                                                                            
      for (uint32_t ci = ty->record.discriminant_count;
         ci < ty->record.component_count; ci++) {
        Component_Info *comp = &ty->record.components[ci];

        // RM 3.7.3: Skip components in unselected variants
        if (decl_sel_variant != -2 and comp->variant_index >= 0 and
          comp->variant_index != decl_sel_variant)
          continue;

        // Runtime variant guard for dynamic disc values
        uint32_t post_rt_skip = 0;
        if (decl_sel_variant == -2 and decl_rt_disc_val > 0 and
          comp->variant_index >= 0 and
          (uint32_t)comp->variant_index < ty->record.variant_count) {
          Variant_Info *vinfo = &ty->record.variants[comp->variant_index];
          if (not vinfo->is_others) {
            uint32_t lo = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %lld\n", lo, decl_rt_disc_type,
               (long long)vinfo->disc_value_low);
            uint32_t hi = Emit_Temp ();
            Emit ("  %%t%u = add %s 0, %lld\n", hi, decl_rt_disc_type,
               (long long)vinfo->disc_value_high);
            uint32_t cmp_lo = Emit_Temp ();
            Emit ("  %%t%u = icmp sge %s %%t%u, %%t%u\n",
               cmp_lo, decl_rt_disc_type, decl_rt_disc_val, lo);
            uint32_t cmp_hi = Emit_Temp ();
            Emit ("  %%t%u = icmp sle %s %%t%u, %%t%u\n",
               cmp_hi, decl_rt_disc_type, decl_rt_disc_val, hi);
            uint32_t in_range = Emit_Temp ();
            Emit ("  %%t%u = and i1 %%t%u, %%t%u\n",
               in_range, cmp_lo, cmp_hi);
            uint32_t check_lbl = cg->label_id++;
            post_rt_skip = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               in_range, check_lbl, post_rt_skip);
            cg->block_terminated = true;
            Emit_Label_Here (check_lbl);
          }
        }
        Type_Info *ct = comp->component_type;
        if (not ct) {
          if (post_rt_skip) {
            Emit ("  br label %%L%u\n", post_rt_skip);
            cg->block_terminated = true;
            Emit_Label_Here (post_rt_skip);
          }
          continue;
        }

        // Check array component with disc-dependent bounds.                                        
        // RM 3.6.1: Null ranges (low > high) need not satisfy                                      
        // index subtype constraints, so guard with a null check.                                   
        //                                                                                          
        if (Type_Is_Array_Like (ct) and ct->array.index_count > 0) {
          for (uint32_t xi = 0; xi < ct->array.index_count; xi++) {
            Type_Bound *lo_bd = &ct->array.indices[xi].low_bound;
            Type_Bound *hi_bd = &ct->array.indices[xi].high_bound;
            Type_Info *idx_ty = ct->array.indices[xi].index_type;
            if (not idx_ty or not Type_Is_Scalar (idx_ty)) continue;
            bool lo_is_disc = (lo_bd->kind == BOUND_EXPR and lo_bd->expr
                       and lo_bd->expr->symbol);
            bool hi_is_disc = (hi_bd->kind == BOUND_EXPR and hi_bd->expr
                       and hi_bd->expr->symbol);
            if (not lo_is_disc and not hi_is_disc) continue;
            const char *iat = Type_To_Llvm (idx_ty);
            if (not iat or strlen (iat) == 0) iat = "i32";

            // Resolve low bound value
            uint32_t lo_val = 0;
            const char *lo_dt = iat;
            if (lo_is_disc) {
              for (uint32_t di = 0; di < ty->record.discriminant_count; di++) {
                if (not Slice_Equal_Ignore_Case (lo_bd->expr->symbol->name,
                    ty->record.components[di].name)) continue;
                Component_Info *dc = &ty->record.components[di];
                lo_dt = Type_To_Llvm (dc->component_type);
                uint32_t dp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
                Emit_Symbol_Name (sym);
                Emit (", i64 %u\n", dc->byte_offset);
                lo_val = Emit_Temp ();
                Emit ("  %%t%u = load %s, ptr %%t%u  ; disc %.*s lo\n",
                   lo_val, lo_dt, dp, (int)dc->name.length, dc->name.data);
                break;
              }
            } else if (lo_bd->kind == BOUND_INTEGER) {
              lo_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld\n", lo_val, iat,
                 (long long)lo_bd->int_value);
            }

            // Resolve high bound value
            uint32_t hi_val = 0;
            const char *hi_dt = iat;
            if (hi_is_disc) {
              for (uint32_t di = 0; di < ty->record.discriminant_count; di++) {
                if (not Slice_Equal_Ignore_Case (hi_bd->expr->symbol->name,
                    ty->record.components[di].name)) continue;
                Component_Info *dc = &ty->record.components[di];
                hi_dt = Type_To_Llvm (dc->component_type);
                uint32_t dp = Emit_Temp ();
                Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
                Emit_Symbol_Name (sym);
                Emit (", i64 %u\n", dc->byte_offset);
                hi_val = Emit_Temp ();
                Emit ("  %%t%u = load %s, ptr %%t%u  ; disc %.*s hi\n",
                   hi_val, hi_dt, dp, (int)dc->name.length, dc->name.data);
                break;
              }
            } else if (hi_bd->kind == BOUND_INTEGER) {
              hi_val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld\n", hi_val, iat,
                 (long long)hi_bd->int_value);
            }
            if (lo_val == 0 or hi_val == 0) continue;

            // Widen both to same comparison type
            lo_val = Emit_Convert (lo_val, lo_dt, iat);
            hi_val = Emit_Convert (hi_val, hi_dt, iat);

            // Skip check if null range (low > high)
            uint32_t is_null = Emit_Temp ();
            Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n",
               is_null, iat, lo_val, hi_val);
            uint32_t lbl_chk = cg->label_id++;
            uint32_t lbl_end = cg->label_id++;
            Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
               is_null, lbl_end, lbl_chk);
            Emit ("L%u:\n", lbl_chk);
            if (lo_is_disc)
              Emit_Constraint_Check (lo_val, idx_ty, NULL);
            if (hi_is_disc)
              Emit_Constraint_Check (hi_val, idx_ty, NULL);
            Emit ("  br label %%L%u\n", lbl_end);
            Emit ("L%u:\n", lbl_end);
          }
        }

        // Check nested unconstrained discriminated record components.                              
        // If a component is an unconstrained disc record type with                                 
        // default disc values, those defaults may create invalid                                   
        // constraints on the nested record's own subcomponents.                                    
        // E.g., REC4 (D:=-1) has C:REC where REC (D:=-1) creates                                   
        // invalid STRING (-1..3).                                                                  
        //                                                                                          
        if (ct->kind == TYPE_RECORD and ct->record.discriminant_count > 0
          and not ct->record.has_disc_constraints) {
          uint32_t nest_off = comp->byte_offset;
          for (uint32_t nci = ct->record.discriminant_count;
             nci < ct->record.component_count; nci++) {
            Component_Info *ncomp = &ct->record.components[nci];
            Type_Info *nct = ncomp->component_type;
            if (not nct) continue;

            // Nested array with disc-dependent bounds.
            // Same null-range guard as the outer check.
            if (Type_Is_Array_Like (nct) and nct->array.index_count > 0) {
              for (uint32_t xi = 0; xi < nct->array.index_count; xi++) {
                Type_Bound *nlo = &nct->array.indices[xi].low_bound;
                Type_Bound *nhi = &nct->array.indices[xi].high_bound;
                Type_Info *idx_ty2 = nct->array.indices[xi].index_type;
                if (not idx_ty2 or not Type_Is_Scalar (idx_ty2)) continue;
                bool nlo_d = (nlo->kind == BOUND_EXPR and nlo->expr
                        and nlo->expr->symbol);
                bool nhi_d = (nhi->kind == BOUND_EXPR and nhi->expr
                        and nhi->expr->symbol);
                if (not nlo_d and not nhi_d) continue;
                const char *niat = Type_To_Llvm (idx_ty2);
                if (not niat or strlen (niat) == 0) niat = "i32";

                // Resolve low bound
                uint32_t nlo_v = 0; const char *nlo_t = niat;
                if (nlo_d) {
                  for (uint32_t ndi = 0; ndi < ct->record.discriminant_count; ndi++) {
                    if (not Slice_Equal_Ignore_Case (nlo->expr->symbol->name,
                        ct->record.components[ndi].name)) continue;
                    Component_Info *ndc = &ct->record.components[ndi];
                    nlo_t = Type_To_Llvm (ndc->component_type);
                    uint32_t ndp = Emit_Temp ();
                    Emit ("  %%t%u = getelementptr i8, ptr %%", ndp);
                    Emit_Symbol_Name (sym);
                    Emit (", i64 %u\n", nest_off + ndc->byte_offset);
                    nlo_v = Emit_Temp ();
                    Emit ("  %%t%u = load %s, ptr %%t%u  ; nest disc %.*s lo\n",
                       nlo_v, nlo_t, ndp, (int)ndc->name.length, ndc->name.data);
                    break;
                  }
                } else if (nlo->kind == BOUND_INTEGER) {
                  nlo_v = Emit_Temp ();
                  Emit ("  %%t%u = add %s 0, %lld\n", nlo_v, niat,
                     (long long)nlo->int_value);
                }

                // Resolve high bound
                uint32_t nhi_v = 0; const char *nhi_t = niat;
                if (nhi_d) {
                  for (uint32_t ndi = 0; ndi < ct->record.discriminant_count; ndi++) {
                    if (not Slice_Equal_Ignore_Case (nhi->expr->symbol->name,
                        ct->record.components[ndi].name)) continue;
                    Component_Info *ndc = &ct->record.components[ndi];
                    nhi_t = Type_To_Llvm (ndc->component_type);
                    uint32_t ndp = Emit_Temp ();
                    Emit ("  %%t%u = getelementptr i8, ptr %%", ndp);
                    Emit_Symbol_Name (sym);
                    Emit (", i64 %u\n", nest_off + ndc->byte_offset);
                    nhi_v = Emit_Temp ();
                    Emit ("  %%t%u = load %s, ptr %%t%u  ; nest disc %.*s hi\n",
                       nhi_v, nhi_t, ndp, (int)ndc->name.length, ndc->name.data);
                    break;
                  }
                } else if (nhi->kind == BOUND_INTEGER) {
                  nhi_v = Emit_Temp ();
                  Emit ("  %%t%u = add %s 0, %lld\n", nhi_v, niat,
                     (long long)nhi->int_value);
                }
                if (nlo_v == 0 or nhi_v == 0) continue;
                nlo_v = Emit_Convert (nlo_v, nlo_t, niat);
                nhi_v = Emit_Convert (nhi_v, nhi_t, niat);

                // Skip if null range
                uint32_t nn = Emit_Temp ();
                Emit ("  %%t%u = icmp sgt %s %%t%u, %%t%u\n",
                   nn, niat, nlo_v, nhi_v);
                uint32_t nc = cg->label_id++;
                uint32_t ne = cg->label_id++;
                Emit ("  br i1 %%t%u, label %%L%u, label %%L%u\n",
                   nn, ne, nc);
                Emit ("L%u:\n", nc);
                if (nlo_d)
                  Emit_Constraint_Check (nlo_v, idx_ty2, NULL);
                if (nhi_d)
                  Emit_Constraint_Check (nhi_v, idx_ty2, NULL);
                Emit ("  br label %%L%u\n", ne);
                Emit ("L%u:\n", ne);
              }
            }

            // Nested record subcomponent with disc constraints
            if (nct->kind == TYPE_RECORD and nct->record.has_disc_constraints and
              nct->record.disc_constraint_exprs) {
              for (uint32_t nsdi = 0; nsdi < nct->record.discriminant_count; nsdi++) {
                if (not nct->record.disc_constraint_exprs[nsdi]) continue;
                Syntax_Node *ncexpr = nct->record.disc_constraint_exprs[nsdi];
                for (uint32_t npdi = 0; npdi < ct->record.discriminant_count; npdi++) {
                  if (not ncexpr->symbol) continue;
                  if (not Slice_Equal_Ignore_Case (ncexpr->symbol->name,
                      ct->record.components[npdi].name)) continue;
                  Component_Info *npdc = &ct->record.components[npdi];
                  const char *ndt2 = Type_To_Llvm (npdc->component_type);
                  uint32_t ndp2 = Emit_Temp ();
                  Emit ("  %%t%u = getelementptr i8, ptr %%", ndp2);
                  Emit_Symbol_Name (sym);
                  Emit (", i64 %u\n", nest_off + npdc->byte_offset);
                  uint32_t ndv2 = Emit_Temp ();
                  Emit ("  %%t%u = load %s, ptr %%t%u  ; nested disc %.*s sub\n",
                     ndv2, ndt2, ndp2, (int)npdc->name.length, npdc->name.data);
                  Component_Info *nsub = &nct->record.components[nsdi];
                  if (nsub->component_type and Type_Is_Scalar (nsub->component_type))
                    Emit_Constraint_Check (ndv2, nsub->component_type,
                      npdc->component_type);
                  break;
                }
              }
            }
          }
        }

        // Check record subcomponent with disc constraints
        if (ct->kind == TYPE_RECORD and ct->record.has_disc_constraints and
          ct->record.disc_constraint_exprs) {
          for (uint32_t sdi = 0; sdi < ct->record.discriminant_count; sdi++) {
            if (not ct->record.disc_constraint_exprs[sdi]) continue;
            Syntax_Node *cexpr = ct->record.disc_constraint_exprs[sdi];
            for (uint32_t pdi = 0; pdi < ty->record.discriminant_count; pdi++) {
              if (not cexpr->symbol) continue;
              if (not Slice_Equal_Ignore_Case (cexpr->symbol->name,
                  ty->record.components[pdi].name)) continue;

              // Load parent disc from record
              Component_Info *pdc = &ty->record.components[pdi];
              const char *dt = Type_To_Llvm (pdc->component_type);
              uint32_t dp = Emit_Temp ();
              Emit ("  %%t%u = getelementptr i8, ptr %%", dp);
              Emit_Symbol_Name (sym);
              Emit (", i64 %u\n", pdc->byte_offset);
              uint32_t dv = Emit_Temp ();
              Emit ("  %%t%u = load %s, ptr %%t%u  ; disc %.*s for subcomp check\n",
                 dv, dt, dp, (int)pdc->name.length, pdc->name.data);
              Component_Info *sub_dc = &ct->record.components[sdi];
              if (sub_dc->component_type and Type_Is_Scalar (sub_dc->component_type))
                Emit_Constraint_Check (dv, sub_dc->component_type,
                  pdc->component_type);
              break;
            }
          }
        }

        // Close runtime variant guard
        if (post_rt_skip) {
          Emit ("  br label %%L%u\n", post_rt_skip);
          cg->block_terminated = true;
          Emit_Label_Here (post_rt_skip);
        }
      }

      // Clear disc_agg_temp so it doesn't leak to other contexts
      for (uint32_t di = 0; di < disc_temp_count; di++)
        disc_temps[di]->disc_agg_temp = 0;
    }
  }

  // RM 3.2.1: Clear cached_temps after the entire declaration so that                              
  // subsequent code (comparisons, assignments) doesn't use stale bound                             
  // values from BOUND_EXPR cached during elaboration.  Each object stores                          
  // its actual bounds in its fat pointer at runtime.                                               
  //                                                                                                
  if (node->object_decl.names.count > 0) {
    Symbol *first_sym = node->object_decl.names.items[0]->symbol;
    Type_Info *cty = first_sym ? first_sym->type : NULL;
    if (cty) {
      cty->low_bound.cached_temp = 0;
      cty->high_bound.cached_temp = 0;
      if (cty->kind == TYPE_ARRAY or cty->kind == TYPE_STRING) {
        for (uint32_t d = 0; d < cty->array.index_count; d++) {
          cty->array.indices[d].low_bound.cached_temp = 0;
          cty->array.indices[d].high_bound.cached_temp = 0;
        }
        if (cty->array.element_type) {
          cty->array.element_type->low_bound.cached_temp = 0;
          cty->array.element_type->high_bound.cached_temp = 0;
          if (Type_Is_Array_Like (cty->array.element_type))
            for (uint32_t d = 0; d < cty->array.element_type->array.index_count; d++) {
              cty->array.element_type->array.indices[d].low_bound.cached_temp = 0;
              cty->array.element_type->array.indices[d].high_bound.cached_temp = 0;
            }
        }
      }
    }
  }
}

bool Has_Nested_In_Statements (Node_List *statements) {
  if (not statements) return false;
  for (uint32_t i = 0; i < statements->count; i++) {
    Syntax_Node *stmt = statements->items[i];
    if (not stmt) continue;

    // DECLARE block - check its declarations and nested statements
    if (stmt->kind == NK_BLOCK) {
      if (Has_Nested_Subprograms (&stmt->block_stmt.declarations,
                    &stmt->block_stmt.statements)) {
        return true;
      }

    // Check all branches of IF
    } else if (stmt->kind == NK_IF) {
      if (Has_Nested_In_Statements (&stmt->if_stmt.then_stmts)) return true;
      for (uint32_t j = 0; j < stmt->if_stmt.elsif_parts.count; j++) {
        Syntax_Node *elsif = stmt->if_stmt.elsif_parts.items[j];
        if (elsif and Has_Nested_In_Statements (&elsif->if_stmt.then_stmts)) return true;
      }
      if (Has_Nested_In_Statements (&stmt->if_stmt.else_stmts)) return true;
    } else if (stmt->kind == NK_LOOP) {
      if (Has_Nested_In_Statements (&stmt->loop_stmt.statements)) return true;
    } else if (stmt->kind == NK_CASE) {
      for (uint32_t j = 0; j < stmt->case_stmt.alternatives.count; j++) {
        Syntax_Node *alt = stmt->case_stmt.alternatives.items[j];
        if (alt and alt->kind == NK_ASSOCIATION and
          alt->association.expression and
          alt->association.expression->kind == NK_BLOCK) {
          if (Has_Nested_In_Statements (&alt->association.expression->block_stmt.statements)) {
            return true;
          }
        }
      }
    }
  }
  return false;
}
bool Has_Nested_Subprograms (Node_List *declarations, Node_List *statements) {

  // Check declarations for procedure/function/task bodies.                                         
  // Task bodies access enclosing scope variables just like nested                                  
  // subprograms (RM 9.1), so the enclosing scope needs frame allocation.                           
  // Also check inside nested packages - their subprograms need frame access too.                   
  //                                                                                                
  if (declarations) {
    for (uint32_t i = 0; i < declarations->count; i++) {
      Syntax_Node *decl = declarations->items[i];
      if (not decl) continue;
      if (decl->kind == NK_PROCEDURE_BODY or
        decl->kind == NK_FUNCTION_BODY or
        decl->kind == NK_TASK_BODY or
        decl->kind == NK_GENERIC_INST) {
        return true;
      }

      // Check inside nested package specs for procedure/function declarations
      if (decl->kind == NK_PACKAGE_SPEC) {
        Node_List *pkg_visible = &decl->package_spec.visible_decls;
        Node_List *pkg_private = &decl->package_spec.private_decls;
        for (uint32_t j = 0; j < pkg_visible->count; j++) {
          Syntax_Node *pd = pkg_visible->items[j];
          if (pd and (pd->kind == NK_PROCEDURE_SPEC or
                pd->kind == NK_FUNCTION_SPEC or
                pd->kind == NK_PROCEDURE_BODY or
                pd->kind == NK_FUNCTION_BODY))
            return true;
        }
        for (uint32_t j = 0; j < pkg_private->count; j++) {
          Syntax_Node *pd = pkg_private->items[j];
          if (pd and (pd->kind == NK_PROCEDURE_SPEC or
                pd->kind == NK_FUNCTION_SPEC or
                pd->kind == NK_PROCEDURE_BODY or
                pd->kind == NK_FUNCTION_BODY))
            return true;
        }
      }

      // Check inside nested package bodies for procedure/function bodies
      if (decl->kind == NK_PACKAGE_BODY) {
        Node_List *pkg_decls = &decl->package_body.declarations;
        for (uint32_t j = 0; j < pkg_decls->count; j++) {
          Syntax_Node *pd = pkg_decls->items[j];
          if (pd and (pd->kind == NK_PROCEDURE_BODY or
                pd->kind == NK_FUNCTION_BODY))
            return true;
        }
      }
    }
  }

  // Check statements for DECLARE blocks that might contain nested subprograms
  return Has_Nested_In_Statements (statements);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Emit LLVM function header: define <ret> @<name>([ptr %__parent_frame,] params...) {              
// Extracted from three near-identical blocks in Generate_Subprogram_Body,                          
// Generate_Generic_Instance_Body, and Generate_Task_Body.                                          
//                                                                                                  
// For BIP functions (returning limited types), we prepend extra parameters:                        
//   i32 %__BIPalloc   - Allocation form selector                                                   
//   ptr %__BIPaccess  - Pointer to result destination                                              
// The function returns void since result is built into __BIPaccess.                                
//                                                                                                  
void Emit_Function_Header (Symbol *sym, bool is_nested) {
  bool is_function = (sym->kind == SYMBOL_FUNCTION);
  bool is_bip = BIP_Is_BIP_Function (sym);

  // BIP functions return void - result is built into __BIPaccess
  if (is_bip) {
    Emit ("define void @");
  } else {
    Emit ("define %s @", is_function ? Type_To_Llvm_Sig (sym->return_type) : "void");
  }
  Emit_Symbol_Name (sym);
  Emit ("(");
  bool need_comma = false;

  // Static chain for nested functions
  if (is_nested) {
    Emit ("ptr %%__parent_frame");
    need_comma = true;
  }

  // BIP extra formals: __BIPalloc (allocation form) and __BIPaccess (dest ptr)
  // Additional formals for task components: __BIPmaster, __BIPchain
  if (is_bip) {
    uint32_t bip_count = BIP_Extra_Formal_Count (sym);
    if (need_comma) Emit (", ");
    Emit ("i32 %%__BIPalloc, ptr %%__BIPaccess");
    need_comma = true;

    // Emit task formals if return type has task components
    if (bip_count > 2) {
      Emit (", i32 %%__BIPmaster, ptr %%__BIPchain");
    }
  }

  // Regular parameters
  for (uint32_t i = 0; i < sym->parameter_count; i++) {
    if (need_comma) Emit (", ");
    need_comma = true;
    if (Param_Is_By_Reference (sym->parameters[i].mode))
      Emit ("ptr %%p%u", i);
    else
      Emit ("%s %%p%u", Type_To_Llvm_Sig (sym->parameters[i].param_type), i);
  }
  Emit (") {\nentry:\n");

  // Initialize BIP state for code generator
  BIP_Begin_Function (sym);
}

// Find the Nth homograph body matching export name in a package body.
// Extracted from two identical 25-line blocks in Generate_Declaration.
Syntax_Node *Find_Homograph_Body (Symbol **exports, uint32_t idx,
                     String_Slice name, Node_List *body_decls) {

  // Count preceding exports with the same name to get homograph index
  uint32_t homograph_idx = 0;
  for (uint32_t k = 0; k < idx; k++) {
    Symbol *prev = exports[k];
    if (prev and (prev->kind == SYMBOL_FUNCTION or prev->kind == SYMBOL_PROCEDURE) and
      Slice_Equal_Ignore_Case (prev->name, name))
      homograph_idx++;
  }

  // Walk body declarations to find the Nth body with this name
  uint32_t body_idx = 0;
  for (uint32_t j = 0; j < body_decls->count; j++) {
    Syntax_Node *decl = body_decls->items[j];
    if (not decl) continue;
    if (decl->kind == NK_PROCEDURE_BODY or decl->kind == NK_FUNCTION_BODY)
      if (Slice_Equal_Ignore_Case (
          decl->subprogram_body.specification->subprogram_spec.name, name))
        if (body_idx++ == homograph_idx) return decl;
  }
  return NULL;
}

// Process_Deferred_Bodies: Emit deferred nested subprogram/task/generic bodies.                    
// Called at end of enclosing function/task/generic instance after restoring context.               
// Processes all bodies deferred since saved_deferred_count.                                        
//                                                                                                  
void Process_Deferred_Bodies (uint32_t saved_deferred_count) {
  while (cg->deferred_count > saved_deferred_count) {
    Syntax_Node *deferred = cg->deferred_bodies[--cg->deferred_count];
    if (deferred->kind == NK_GENERIC_INST) {
      Symbol *inst = deferred->symbol;
      if (inst and inst->generic_template and inst->generic_template->generic_body) {
        Symbol *saved = cg->current_instance;
        cg->current_instance = inst;
        Set_Generic_Type_Map (inst);
        Generate_Generic_Instance_Body (inst, inst->generic_template->generic_body);
        cg->current_instance = saved;
        Set_Generic_Type_Map (saved);
      }
    } else if (deferred->kind == NK_TASK_BODY) {
      Generate_Task_Body (deferred);
    } else {
      Generate_Subprogram_Body (deferred);
    }
  }
}
void Generate_Subprogram_Body (Syntax_Node *node) {

  // Skip stub bodies (PROCEDURE X IS SEPARATE;) - the actual body
  // will be provided by a separate subunit compilation
  if (node->subprogram_body.is_separate) {
    return;
  }

  // Skip if code already generated for this body (prevents duplicates)
  if (node->subprogram_body.code_generated) return;
  node->subprogram_body.code_generated = true;
  Syntax_Node *spec = node->subprogram_body.specification;
  Symbol *sym = spec ? spec->symbol : NULL;
  if (not sym) return;

  // Mark this symbol as having been defined - prevents duplicate
  // 'declare' statements for functions defined in the same file
  sym->extern_emitted = true;
  bool is_function = sym->kind == SYMBOL_FUNCTION;
  uint32_t saved_deferred_count = cg->deferred_count;

  // Check if this is a nested function (has enclosing function).                                   
  // This handles nested packages: a subprogram inside a package inside                             
  // a procedure needs static chain access to the outermost procedure's frame.                      
  //                                                                                                
  Symbol *saved_enclosing = cg->enclosing_function;
  bool saved_is_nested = cg->is_nested;

  // Find nearest enclosing function/procedure (walks through packages)
  Symbol *enclosing_subprog = Find_Enclosing_Subprogram (sym);
  bool is_nested = (enclosing_subprog != NULL);
  cg->is_nested = is_nested;
  cg->enclosing_function = enclosing_subprog;

  // Emit comment showing function/procedure being generated
  Emit ("\n; ============================================================\n");
  Emit ("; %s %.*s", is_function ? "FUNCTION" : "PROCEDURE",
     (int)sym->name.length, sym->name.data);
  if (is_nested)
    Emit (" (nested)");
  Emit ("\n");
  Emit_Location (node->location);
  Emit ("; ============================================================\n");
  Emit_Function_Header (sym, is_nested);
  Symbol *saved_current_function = cg->current_function;
  cg->current_function = sym;
  cg->has_return = false;
  cg->block_terminated = false;  // Reset for new function

  // Check if this function has nested subprograms (in declarations or DECLARE blocks)
  bool has_nested = Has_Nested_Subprograms (&node->subprogram_body.declarations,
                        &node->subprogram_body.statements);

  // If this function has nested subprograms, allocate a frame base.                                
  // When also nested itself, reserve 8 extra bytes at the end to store                             
  // the received __parent_frame (static chain pointer for RM 8.3).                                 
  //                                                                                                
  if (has_nested) {
    int64_t frame_size = (sym->scope and sym->scope->frame_size > 0)
      ? sym->scope->frame_size : 8;
    int64_t alloca_size = is_nested ? frame_size + 8 : frame_size;
    Emit ("  ; Frame for nested function access\n");
    Emit ("  %%__frame_base = alloca i8, i64 %lld\n", (long long)alloca_size);

    // Store static chain: parent's frame at offset frame_size
    if (is_nested) {
      uint32_t slot = Emit_Temp ();
      Emit ("  %%t%u = getelementptr i8, ptr %%__frame_base, i64 %lld\n",
         slot, (long long)frame_size);
      Emit ("  store ptr %%__parent_frame, ptr %%t%u\n", slot);
    }
  }

  // If nested, create aliases for accessing enclosing scope variables via frame                    
  // Create pointer aliases to parent scope variables.                                              
  // Must include all storage-bearing symbol kinds: variables, parameters,                          
  // discriminants, and constants (non-named-number constants like                                  
  // "X : INTEGER := 2" have stack storage and can be modified).                                    
  // Track emitted names to avoid duplicate definitions when the same                               
  // mangled name appears from both symbols[] and frame_vars[].                                     
  //                                                                                                
  if (is_nested and enclosing_subprog and enclosing_subprog->scope) {
    Scope *parent_scope = enclosing_subprog->scope;

    // Track emitted frame alias unique_ids to prevent duplicates.                                  
    // Note: cannot store String_Slice from Symbol_Mangle_Name because                              
    // it returns pointers into a rotating static buffer (4 slots).                                 
    //                                                                                              
    #define MAX_FRAME_ALIASES 512
    uint32_t emitted_ids[MAX_FRAME_ALIASES];
    uint32_t emitted_count = 0;
    for (uint32_t i = 0; i < parent_scope->symbol_count; i++) {
      Symbol *var = parent_scope->symbols[i];
      if (var and (var->kind == SYMBOL_VARIABLE or var->kind == SYMBOL_PARAMETER or
            var->kind == SYMBOL_DISCRIMINANT or
            (var->kind == SYMBOL_CONSTANT and not var->is_named_number))) {

        // Check for duplicate unique_id
        bool already_emitted = false;
        for (uint32_t j = 0; j < emitted_count; j++) {
          if (var->unique_id == emitted_ids[j]) {
            already_emitted = true;
            break;
          }
        }
        if (already_emitted) continue;
        if (emitted_count < MAX_FRAME_ALIASES) {
          emitted_ids[emitted_count++] = var->unique_id;
        }
        Emit ("  %%__frame.");
        Emit_Symbol_Name (var);
        Emit (" = getelementptr i8, ptr %%__parent_frame, i64 %lld\n",
           (long long)(var->frame_offset));
      }
    }

    // Also create aliases for variables from child scopes (DECLARE blocks, etc.)                   
    // that share the same function frame but were in deeper scopes.                                
    // Skip symbols whose mangled name was already emitted above.                                   
    //                                                                                              
    for (uint32_t i = 0; i < parent_scope->frame_var_count; i++) {
      Symbol *var = parent_scope->frame_vars[i];
      if (not var) continue;
      bool already_emitted = false;
      for (uint32_t j = 0; j < emitted_count; j++) {
        if (var->unique_id == emitted_ids[j]) {
          already_emitted = true;
          break;
        }
      }
      if (not already_emitted) {
        if (emitted_count < MAX_FRAME_ALIASES) {
          emitted_ids[emitted_count++] = var->unique_id;
        }
        Emit ("  %%__frame.");
        Emit_Symbol_Name (var);
        Emit (" = getelementptr i8, ptr %%__parent_frame, i64 %lld\n",
           (long long)(var->frame_offset));
      }
    }
    #undef MAX_FRAME_ALIASES
  }

  // Allocate and store parameters to local stack slots                                             
  // For OUT/IN OUT: param is already a pointer, use directly                                       
  // For IN: allocate local slot and copy value                                                     
  //                                                                                                
  for (uint32_t i = 0; i < sym->parameter_count; i++) {
    Symbol *param_sym = sym->parameters[i].param_sym;
    if (param_sym) {
      const char *type_str = Type_To_Llvm_Sig (sym->parameters[i].param_type);
      Parameter_Mode mode = sym->parameters[i].mode;

      // Check if this IN parameter is a constrained composite type                                 
      // (array/record/string) passed as a raw pointer. These need by-ref                           
      // treatment since the caller passes a pointer to the data, not the                           
      // data itself. Unconstrained arrays/strings use fat pointers                                 
      // {ptr, ptr} and should be stored in alloca like scalars.                                    
      //                                                                                            
      Type_Info *pt = sym->parameters[i].param_type;
      bool is_composite_in = false;
      if (mode == PARAM_IN and pt) {
        if (pt->kind == TYPE_RECORD or pt->kind == TYPE_TASK) {
          is_composite_in = true;
        } else if ((pt->kind == TYPE_ARRAY or pt->kind == TYPE_STRING) and
               pt->array.is_constrained and not Type_Has_Dynamic_Bounds (pt)) {
          is_composite_in = true;
        }
      }

      // OUT/IN OUT or IN composite: %p is already a pointer to data.
      // Create an alias so the parameter name points to caller's storage
      if (Param_Is_By_Reference (mode) or is_composite_in) {
        Emit ("  %%");
        Emit_Symbol_Name (param_sym);
        Emit (" = getelementptr i8, ptr %%p%u, i64 0  ; by-ref param\n", i);

      // IN param with nested functions: allocate in frame
      } else if (has_nested and sym->scope) {
        Emit ("  %%");
        Emit_Symbol_Name (param_sym);
        Emit (" = getelementptr i8, ptr %%__frame_base, i64 %lld\n",
           (long long)param_sym->frame_offset);
        Emit ("  store %s %%p%u, ptr %%", type_str, i);
        Emit_Symbol_Name (param_sym);
        Emit ("\n");

      // IN param: allocate local and copy value
      } else {
        Emit ("  %%");
        Emit_Symbol_Name (param_sym);
        Emit (" = alloca %s\n", type_str);
        Emit ("  store %s %%p%u, ptr %%", type_str, i);
        Emit_Symbol_Name (param_sym);
        Emit ("\n");
      }
    }
  }

  // Generate local declarations, passing has_nested flag via cg for frame allocation.
  // Keep has_nested active for statements too (DECLARE blocks need it).
  bool saved_has_nested = cg->current_nesting_level > 0;  // Repurpose field temporarily
  cg->current_nesting_level = has_nested ? 1 : 0;
  Generate_Declaration_List (&node->subprogram_body.declarations);

  // Don't reset yet - statements may have DECLARE blocks with tasks needing frame

  // Check if subprogram has exception handlers
  bool has_exc_handlers = node->subprogram_body.handlers.count > 0;

  // Setup exception handling using consolidated helper
  if (has_exc_handlers) {
    uint32_t end_label = Emit_Label ();
    Exception_Setup exc = Emit_Exception_Handler_Setup ();

    // Normal execution path
    Emit_Label_Here (exc.normal_label);
    Generate_Statement_List (&node->subprogram_body.statements);
    Emit ("  call void @__ada_pop_handler()\n");
    Emit ("  br label %%L%u\n", end_label);

    // Exception handler entry
    Emit_Label_Here (exc.handler_label);
    cg->block_terminated = false;
    Emit ("  call void @__ada_pop_handler()\n");

    // Dispatch to exception handlers
    uint32_t exc_id = Emit_Current_Exception_Id ();
    Generate_Exception_Dispatch (&node->subprogram_body.handlers, exc_id, end_label);

    // End label - normal return point
    Emit_Label_Here (end_label);

  // Generate statements without exception handling
  } else {
    Generate_Statement_List (&node->subprogram_body.statements);
  }

  // Restore nesting level now that all code for this subprogram is generated
  cg->current_nesting_level = saved_has_nested ? 1 : 0;

  // Default return if block is not terminated
  if (not cg->block_terminated) {

    // BIP functions return void, but missing return is still PROGRAM_ERROR
    if (is_function) {
      bool func_is_bip = BIP_Is_BIP_Function (sym);

      // For BIP: raise error first, then ret void (unreachable)
      if (func_is_bip) {
        Emit_Raise_Program_Error ("missing return");
        Emit ("  ret void  ; unreachable after raise\n");

      // RM 6.4(11): raise PROGRAM_ERROR if function completes without RETURN
      } else {
        Emit_Raise_Program_Error ("missing return");
      }
    } else {
      Emit ("  ret void\n");
    }
  }

  // Clean up BIP state
  BIP_End_Function ();
  Emit ("}\n\n");
  cg->current_function = saved_current_function;
  cg->is_nested = saved_is_nested;
  cg->enclosing_function = saved_enclosing;
  Process_Deferred_Bodies (saved_deferred_count);
}

// Generate code for a generic instance body
void Generate_Generic_Instance_Body (Symbol *inst_sym,
                       Syntax_Node *template_body) {
  if (not inst_sym or not template_body) return;

  // Handle package instantiation specially: generate package body declarations                     
  // (global variables), then iterate through exported subprograms.                                 
  // Set current instance so names are prefixed with instance name                                  
  //                                                                                                
  if (inst_sym->kind == SYMBOL_PACKAGE and template_body->kind == NK_PACKAGE_BODY) {
    Symbol *saved_instance = cg->current_instance;
    cg->current_instance = inst_sym;
    Set_Generic_Type_Map (inst_sym);

    // First, emit any global declarations from the package body                                    
    // (e.g., FILES, BUFFERS, NEXT_FD in DIRECT_IO). These need unique names                        
    // based on the instance to avoid collisions.                                                   
    //                                                                                              
    for (uint32_t i = 0; i < template_body->package_body.declarations.count; i++) {
      Syntax_Node *decl = template_body->package_body.declarations.items[i];
      if (not decl) continue;

      // Only generate non-subprogram declarations (variables, types)
      if (decl->kind != NK_PROCEDURE_BODY and decl->kind != NK_FUNCTION_BODY and
        decl->kind != NK_PROCEDURE_SPEC and decl->kind != NK_FUNCTION_SPEC) {
        Generate_Declaration (decl);
      }
    }

    // Then generate the subprogram bodies
    for (uint32_t i = 0; i < inst_sym->exported_count; i++) {
      Symbol *exp = inst_sym->exported[i];
      if (not exp) continue;
      if (exp->kind != SYMBOL_FUNCTION and exp->kind != SYMBOL_PROCEDURE)
        continue;
      Syntax_Node *subp_body = Find_Homograph_Body (
        inst_sym->exported, i, exp->name,
        &template_body->package_body.declarations);
      if (subp_body)
        Generate_Generic_Instance_Body (exp, subp_body);
    }

    // Restore previous instance context
    cg->current_instance = saved_instance;
    Set_Generic_Type_Map (saved_instance);
    return;
  }
  bool is_function = inst_sym->kind == SYMBOL_FUNCTION;
  uint32_t saved_deferred_count = cg->deferred_count;

  // Set current instance for formal subprogram substitution during codegen
  Symbol *saved_current_instance = cg->current_instance;
  cg->current_instance = inst_sym;
  Set_Generic_Type_Map (inst_sym);

  // For generic instances, determine nesting from instance parent
  Symbol *saved_enclosing = cg->enclosing_function;
  bool saved_is_nested = cg->is_nested;

  // Find nearest enclosing function/procedure (walks through packages)
  Symbol *enclosing_subprog = Find_Enclosing_Subprogram (inst_sym);
  bool is_nested = (enclosing_subprog != NULL);
  cg->is_nested = is_nested;
  cg->enclosing_function = enclosing_subprog;
  Emit_Function_Header (inst_sym, is_nested);
  Symbol *saved_current_function = cg->current_function;
  cg->current_function = inst_sym;
  cg->has_return = false;
  cg->block_terminated = false;  // Reset for new function

  // Get parameter symbols from template body's specification (they have
  // the unique_ids that match the body's code references)
  Syntax_Node *body_spec = template_body->subprogram_body.specification;
  uint32_t param_idx = 0;
  if (body_spec) {
    Node_List *params = &body_spec->subprogram_spec.parameters;
    for (uint32_t i = 0; i < params->count and param_idx < inst_sym->parameter_count; i++) {
      Syntax_Node *ps = params->items[i];
      if (ps and ps->kind == NK_PARAM_SPEC) {
        for (uint32_t j = 0; j < ps->param_spec.names.count and param_idx < inst_sym->parameter_count; j++) {
          Syntax_Node *pname = ps->param_spec.names.items[j];
          Symbol *param_sym = pname->symbol;
          if (param_sym) {
            const char *type_str = Type_To_Llvm (inst_sym->parameters[param_idx].param_type);
            Parameter_Mode mode = inst_sym->parameters[param_idx].mode;

            // OUT/IN OUT: %p is already a pointer to caller's variable
            // Create an alias so the parameter name points to caller's storage
            if (Param_Is_By_Reference (mode)) {
              Emit ("  %%");
              Emit_Symbol_Name (param_sym);
              Emit (" = getelementptr i8, ptr %%p%u, i64 0  ; by-ref param\n", param_idx);

            // IN param: allocate local and copy value
            } else {
              Emit ("  %%");
              Emit_Symbol_Name (param_sym);
              Emit (" = alloca %s\n", type_str);
              Emit ("  store %s %%p%u, ptr %%", type_str, param_idx);
              Emit_Symbol_Name (param_sym);
              Emit ("\n");
            }
          }
          param_idx++;
        }
      }
    }
  }

  // Allocate and initialize generic formal object parameters (RM 12.4).                            
  // These are variables visible inside the generic body but not subprogram                         
  // parameters - they carry the actual values provided at instantiation.                           
  //                                                                                                
  // Storage type must match the expression's value representation:                                 
  //   - String literals and unconstrained arrays produce {ptr,ptr} fat ptrs                        
  //   - Scalar values and constrained arrays produce their base type                               
  // We use Expression_Llvm_Type to determine the correct storage width.                            
  //                                                                                                
  {
    Symbol *tmpl = inst_sym->generic_template;
    Syntax_Node *gen_decl = tmpl ? tmpl->declaration : NULL;
    if (gen_decl and gen_decl->kind == NK_GENERIC_DECL) {
      Node_List *formals = &gen_decl->generic_decl.formals;
      uint32_t ai = 0;
      for (uint32_t fi = 0; fi < formals->count; fi++) {
        Syntax_Node *formal = formals->items[fi];
        if (formal->kind == NK_GENERIC_OBJECT_PARAM) {
          for (uint32_t j = 0; j < formal->generic_object_param.names.count; j++) {
            Syntax_Node *fname = formal->generic_object_param.names.items[j];
            Symbol *obj_sym = fname ? fname->symbol : NULL;
            Syntax_Node *actual_expr = (ai < inst_sym->generic_actual_count)
              ? inst_sym->generic_actuals[ai].actual_expr : NULL;

            // Use default expression if no actual provided (RM 12.4)
            if (not actual_expr)
              actual_expr = formal->generic_object_param.default_expr;

            // Resolve type from formal's type mark (RM 12.4)
            if (obj_sym and actual_expr) {
              Type_Info *obj_type = obj_sym->type;
              if (not obj_type) {
                Syntax_Node *ot = formal->generic_object_param.object_type;
                if (ot) {
                  if (ot->type) obj_type = ot->type;
                  else if (ot->symbol and ot->symbol->type)
                    obj_type = ot->symbol->type;
                  else {
                    Symbol *ts = Symbol_Find (ot->string_val.text);
                    if (ts and ts->type) obj_type = ts->type;
                  }
                }
              }

              // Set type on expression for codegen (aggregates need it)
              if (obj_type and not actual_expr->type)
                actual_expr->type = obj_type;

              // Determine storage type for formal object (RM 12.4):                                
              // - Unconstrained/dynamic arrays: fat pointer {ptr,ptr}                              
              // - Constrained arrays: [N x i8] flat alloca, memcpy data                            
              // - Scalars/records: native type                                                     
              //                                                                                    
              bool needs_fat = false;
              if (obj_type and Type_Is_Array_Like (obj_type) and
                (Type_Has_Dynamic_Bounds (obj_type) or
                 Type_Is_Unconstrained_Array (obj_type)))
                needs_fat = true;

              // For constrained arrays, compute storage size from bounds
              // (obj_type->size may be 0 if bounds use expressions)
              bool is_constrained_arr = obj_type and
                Type_Is_Constrained_Array (obj_type) and not needs_fat;
              uint32_t arr_storage_sz = 0;
              if (is_constrained_arr) {
                arr_storage_sz = obj_type->size;

                // Recompute from type bounds (RM 3.6.1)
                if (arr_storage_sz == 0) {
                  int128_t total = 1;
                  for (uint32_t d = 0; d < obj_type->array.index_count; d++) {
                    int128_t lo = Type_Bound_Value (obj_type->array.indices[d].low_bound);
                    int128_t hi = Type_Bound_Value (obj_type->array.indices[d].high_bound);
                    int128_t dim = hi - lo + 1;
                    if (dim < 0) dim = 0;
                    total *= dim;
                  }
                  uint32_t esz = obj_type->array.element_type
                    ? obj_type->array.element_type->size : 1;
                  arr_storage_sz = (uint32_t)(total * esz);
                }
              }
              obj_sym->needs_fat_ptr_storage = needs_fat;
              Emit ("  %%");
              Emit_Symbol_Name (obj_sym);
              const char *ts = needs_fat ? FAT_PTR_TYPE
                : (obj_type ? Type_To_Llvm (obj_type)
                      : Expression_Llvm_Type (actual_expr));
              if (is_constrained_arr and arr_storage_sz > 0) {
                Emit (" = alloca [%u x i8]  ; generic formal object\n",
                   arr_storage_sz);
              } else {
                Emit (" = alloca %s  ; generic formal object\n", ts);
              }
              uint32_t val = Generate_Expression (actual_expr);
              if (val > 0) {
                if (needs_fat) {
                  val = Fat_Ptr_As_Value (val);
                  Emit ("  store " FAT_PTR_TYPE " %%t%u, ptr %%", val);

                // Constrained array: extract data ptr from fat ptr if needed
                } else if (obj_type and Type_Is_Constrained_Array (obj_type)) {
                  const char *vty = Temp_Get_Type (val);
                  bool is_fat_val = (vty and strcmp (vty, FAT_PTR_TYPE) == 0) or
                            Temp_Is_Fat_Alloca (val);
                  if (is_fat_val) {
                    if (Temp_Is_Fat_Alloca (val)) {
                      uint32_t lv = Emit_Temp ();
                      Emit ("  %%t%u = load " FAT_PTR_TYPE ", ptr %%t%u\n", lv, val);
                      val = lv;
                    }
                    uint32_t dp = Emit_Temp ();
                    Emit ("  %%t%u = extractvalue " FAT_PTR_TYPE " %%t%u, 0\n", dp, val);
                    val = dp;
                  }
                  uint32_t sz = arr_storage_sz > 0 ? arr_storage_sz
                    : (obj_type->size > 0 ? obj_type->size : 1);
                  Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
                  Emit_Symbol_Name (obj_sym);
                  Emit (", ptr %%t%u, i64 %u, i1 false)\n", val, sz);
                  val = 0;  // Skip the store below
                } else {
                  Emit ("  store %s %%t%u, ptr %%", ts, val);
                }
                if (val > 0) {
                  Emit_Symbol_Name (obj_sym);
                  Emit ("\n");
                }
              }
            }
            ai++;
          }
        } else {
          ai++;
        }
      }
    }
  }

  // Generate the body statements with type substitution.                                           
  // Per Ada RM 11.4, exception handlers in the generic template body                               
  // apply to each instantiation.  Emit_Subprogram_Body                                             
  // handles exception parts identically for generic instances.                                     
  //                                                                                                
  Syntax_Node *body = template_body;

  // Generate local declarations
  Generate_Declaration_List (&body->subprogram_body.declarations);

  // Check for exception handlers in the template body
  bool has_exc = body->subprogram_body.handlers.count > 0;
  if (has_exc) {
    Exception_Setup setup = Emit_Exception_Handler_Setup ();
    uint32_t end_lbl = Emit_Label ();

    // Normal execution path
    Emit_Label_Here (setup.normal_label);
    Generate_Statement_List (&body->subprogram_body.statements);
    if (not cg->block_terminated)
      Emit ("  call void @__ada_pop_handler()\n");
    Emit_Branch_If_Needed (end_lbl);

    // Exception handler entry
    Emit_Label_Here (setup.handler_label);
    cg->block_terminated = false;
    Emit ("  call void @__ada_pop_handler()\n");
    uint32_t exc_id = Emit_Current_Exception_Id ();
    Generate_Exception_Dispatch (&body->subprogram_body.handlers, exc_id, end_lbl);
    Emit_Label_Here (end_lbl);
    cg->block_terminated = false;
  } else {
    for (uint32_t i = 0; i < body->subprogram_body.statements.count; i++) {
      Syntax_Node *stmt = body->subprogram_body.statements.items[i];
      if (stmt) Generate_Statement (stmt);
    }
  }

  // Default return if block is not terminated
  if (not cg->block_terminated) {

    // BIP functions return void, but missing return is still PROGRAM_ERROR
    if (is_function) {
      bool inst_is_bip = BIP_Is_BIP_Function (inst_sym);
      if (inst_is_bip) {
        Emit_Raise_Program_Error ("missing return");
        Emit ("  ret void  ; unreachable after raise\n");

      // RM 6.4(11): raise PROGRAM_ERROR if function completes without RETURN
      } else {
        Emit_Raise_Program_Error ("missing return");
      }
    } else {
      Emit ("  ret void\n");
    }
  }

  // Clean up BIP state
  BIP_End_Function ();
  Emit ("}\n\n");
  cg->current_function = saved_current_function;
  cg->is_nested = saved_is_nested;
  cg->enclosing_function = saved_enclosing;
  cg->current_instance = saved_current_instance;
  Set_Generic_Type_Map (saved_current_instance);
  Process_Deferred_Bodies (saved_deferred_count);
}

// Emit the task body function name for a task type symbol.                                         
// Resolves to the task TYPE's defining_symbol for consistency between                              
// task body definitions and task_start call sites.                                                 
// The name format is: task_MANGLED_NAME                                                            
//                                                                                                  
void Emit_Task_Function_Name (Symbol *task_sym, String_Slice fallback_name) {
  Emit ("task_");

  // Prefer the task TYPE's defining_symbol for consistency.                                        
  // The task body's own symbol and the task type's defining_symbol                                 
  // may have different _sN suffixes; using the type's symbol ensures                               
  // the define and call sites match.                                                               
  //                                                                                                
  Symbol *resolved = NULL;
  if (task_sym and task_sym->type and task_sym->type->defining_symbol) {
    resolved = task_sym->type->defining_symbol;
  } else if (task_sym) {
    resolved = task_sym;
  }
  if (resolved) {
    Emit_Symbol_Name (resolved);
  } else {
    for (uint32_t i = 0; i < fallback_name.length; i++) {
      char ch = fallback_name.data[i];
      if (ch >= 'A' and ch <= 'Z') ch = ch - 'A' + 'a';
      fputc (ch, cg->output);
    }
  }
}
void Generate_Task_Body (Syntax_Node *node) {

  // Skip stub bodies (TASK BODY X IS SEPARATE;) - the actual body
  // will be provided by a separate subunit compilation
  if (node->task_body.is_separate) {
    Emit ("\n; Task body stub: %.*s (defined in separate subunit)\n",
       (int)node->task_body.name.length, node->task_body.name.data);
    return;
  }
  Emit ("\n; Task body: %.*s\n",
     (int)node->task_body.name.length, node->task_body.name.data);

  // Generate task entry point function - receives parent frame pointer                             
  // for uplevel variable access (task bodies access enclosing scope).                              
  // For task bodies inside generic instances, prefix with instance name                            
  // to make the function name unique across multiple instantiations.                               
  //                                                                                                
  Emit ("define ptr @");
  Emit_Task_Function_Name (node->symbol, node->task_body.name);
  Emit ("(ptr %%__parent_frame) {\n");
  Emit ("entry:\n");

  // Save and set context - task body is like a nested function
  Symbol *saved_current_function = cg->current_function;
  bool saved_is_nested = cg->is_nested;
  Symbol *saved_enclosing = cg->enclosing_function;
  bool saved_in_task_body = cg->in_task_body;
  cg->current_function = node->symbol;
  cg->is_nested = true;  // Task bodies are nested in enclosing scope
  cg->enclosing_function = saved_current_function;  // Access parent's vars
  cg->in_task_body = true;  // Task entry points return ptr for pthread

  // Reset temp counter for new function
  uint32_t saved_temp = cg->temp_id;
  cg->temp_id = 1;
  memset (cg->temp_types, 0, sizeof (cg->temp_types));
  memset (cg->temp_type_keys, 0, sizeof (cg->temp_type_keys));
  memset (cg->temp_is_fat_alloca, 0, sizeof (cg->temp_is_fat_alloca));

  // Create frame aliases for accessing enclosing scope variables.                                  
  // Task bodies can reference variables from the enclosing scope                                   
  // (RM 9.1). The parent passed %__parent_frame pointing to its frame.                             
  // Use the task symbol's defining_scope to get the correct scope                                  
  // (important for tasks in DECLARE blocks which have their own scope).                            
  //                                                                                                
  Scope *parent_scope = node->symbol ? node->symbol->defining_scope : NULL;

  // Dedup by unique_id (same approach as Generate_Subprogram_Body)
  if (parent_scope) {
    #define MAX_TASK_FRAME_ALIASES 512
    uint32_t task_emitted_ids[MAX_TASK_FRAME_ALIASES];
    uint32_t task_emitted_count = 0;
    for (uint32_t i = 0; i < parent_scope->symbol_count; i++) {
      Symbol *var = parent_scope->symbols[i];
      if (var and (var->kind == SYMBOL_VARIABLE or var->kind == SYMBOL_PARAMETER or
            var->kind == SYMBOL_DISCRIMINANT)) {
        bool dup = false;
        for (uint32_t j = 0; j < task_emitted_count; j++)
          if (var->unique_id == task_emitted_ids[j]) { dup = true; break; }
        if (dup) continue;
        if (task_emitted_count < MAX_TASK_FRAME_ALIASES)
          task_emitted_ids[task_emitted_count++] = var->unique_id;
        Emit ("  %%__frame.");
        Emit_Symbol_Name (var);
        Emit (" = getelementptr i8, ptr %%__parent_frame, i64 %lld\n",
           (long long)(var->frame_offset));
      }
    }

    // Also include variables from child scopes (DECLARE blocks).
    for (uint32_t i = 0; i < parent_scope->frame_var_count; i++) {
      Symbol *var = parent_scope->frame_vars[i];
      if (not var) continue;
      bool dup = false;
      for (uint32_t j = 0; j < task_emitted_count; j++)
        if (var->unique_id == task_emitted_ids[j]) { dup = true; break; }
      if (dup) continue;
      if (task_emitted_count < MAX_TASK_FRAME_ALIASES)
        task_emitted_ids[task_emitted_count++] = var->unique_id;
      Emit ("  %%__frame.");
      Emit_Symbol_Name (var);
      Emit (" = getelementptr i8, ptr %%__parent_frame, i64 %lld\n",
         (long long)(var->frame_offset));
    }
    #undef MAX_TASK_FRAME_ALIASES
  }

  // Push exception handler for task using consolidated helper
  Exception_Setup exc = Emit_Exception_Handler_Setup ();

  // Normal execution path
  Emit_Label_Here (exc.normal_label);
  cg->block_terminated = false;  // Reset after br target label
  Generate_Declaration_List (&node->task_body.declarations);
  Generate_Statement_List (&node->task_body.statements);
  Emit ("  call void @__ada_pop_handler()\n");
  Emit ("  ret ptr null\n");

  // Exception handler path
  Emit_Label_Here (exc.handler_label);
  Emit ("  call void @__ada_pop_handler()\n");

  // Task terminates silently on unhandled exception
  Emit ("  ret ptr null\n");
  Emit ("}\n\n");

  // Restore context
  cg->temp_id = saved_temp;
  cg->current_function = saved_current_function;
  cg->is_nested = saved_is_nested;
  cg->enclosing_function = saved_enclosing;
  cg->in_task_body = saved_in_task_body;
}
void Generate_Declaration (Syntax_Node *node) {
  if (not node) return;
  switch (node->kind) {
    case NK_OBJECT_DECL:
      Generate_Object_Declaration (node);
      break;
    case NK_PROCEDURE_SPEC:
    case NK_FUNCTION_SPEC:

      // Forward declaration - if imported, emit extern declaration
      if (node->symbol and node->symbol->is_imported) {
        Emit_Extern_Subprogram (node->symbol);
      }
      break;
    case NK_PACKAGE_SPEC:

      // Nested package spec: emit object declarations for variables and                            
      // constants declared in the visible and private parts.                                       
      // Without this, variables from package specs without bodies                                  
      // would never be allocated, causing undefined value errors.                                  
      //                                                                                            
      {
        for (uint32_t j = 0; j < node->package_spec.visible_decls.count; j++) {
          Syntax_Node *decl = node->package_spec.visible_decls.items[j];
          if (decl) Generate_Declaration (decl);
        }
        for (uint32_t j = 0; j < node->package_spec.private_decls.count; j++) {
          Syntax_Node *decl = node->package_spec.private_decls.items[j];
          if (decl) Generate_Declaration (decl);
        }

        // RM 7.4: Deferred constant completion - copy the private part                             
        // completion's value to the visible part deferred constant's                               
        // storage so that references from outside the package see the                              
        // correct initialized value.  Match by base name (case-insensitive).                       
        //                                                                                          
        for (uint32_t pj = 0; pj < node->package_spec.private_decls.count; pj++) {
          Syntax_Node *priv = node->package_spec.private_decls.items[pj];
          if (not priv or priv->kind != NK_OBJECT_DECL) continue;
          if (not priv->object_decl.is_constant or not priv->object_decl.init) continue;
          for (uint32_t pi = 0; pi < priv->object_decl.names.count; pi++) {
            Syntax_Node *pn = priv->object_decl.names.items[pi];
            if (not pn or not pn->symbol) continue;
            Symbol *priv_sym = pn->symbol;

            // Search visible_decls for a matching deferred constant
            for (uint32_t vj = 0; vj < node->package_spec.visible_decls.count; vj++) {
              Syntax_Node *vis = node->package_spec.visible_decls.items[vj];
              if (not vis or vis->kind != NK_OBJECT_DECL) continue;
              if (not vis->object_decl.is_constant) continue;
              if (vis->object_decl.init) continue;  // not deferred
              for (uint32_t vi = 0; vi < vis->object_decl.names.count; vi++) {
                Syntax_Node *vn = vis->object_decl.names.items[vi];
                if (not vn or not vn->symbol) continue;
                Symbol *vis_sym = vn->symbol;
                if (vis_sym == priv_sym) continue;  // same symbol
                if (not Slice_Equal_Ignore_Case (vis_sym->name, priv_sym->name)) continue;

                // Found matching deferred constant - emit memcpy
                uint32_t sz = priv_sym->type ? priv_sym->type->size : 0;
                if (sz == 0 and vis_sym->type) sz = vis_sym->type->size;
                if (sz > 0) {
                  Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%");
                  Emit_Symbol_Name (vis_sym);
                  Emit (", ptr %%");
                  Emit_Symbol_Name (priv_sym);
                  Emit (", i64 %u, i1 false)  ; deferred const completion\n", sz);
                }
              }
            }
          }
        }
      }
      break;

    // Check for duplicate in deferred list
    case NK_PROCEDURE_BODY:
    case NK_FUNCTION_BODY:

      // Defer nested subprogram bodies - emit after enclosing function
      // Skip if already generated (prevents duplicates from re-processing)
      if (node->subprogram_body.code_generated) break;
      if (cg->current_function and cg->deferred_count < 64) {
        bool already_deferred = false;
        for (uint32_t d = 0; d < cg->deferred_count; d++) {
          if (cg->deferred_bodies[d] == node) {
            already_deferred = true;
            break;
          }
        }
        if (not already_deferred) {
          cg->deferred_bodies[cg->deferred_count++] = node;
        }
      } else {
        Generate_Subprogram_Body (node);
      }
      break;
    case NK_PACKAGE_BODY:
      {
        Symbol *pkg_sym = node->symbol;  // Use symbol from semantic analysis

        // Skip generic package bodies - code is generated only for instances
        if (pkg_sym and pkg_sym->kind == SYMBOL_GENERIC) {
          break;
        }

        // For library-level package bodies (separate compilation), emit the                        
        // associated spec's visible/private declarations as globals.  The spec                     
        // is loaded from a separate .ads file and never visited by codegen,                        
        // so constants like LEGAL_FILE_NAME must be emitted here.                                  
        // Skip this for nested packages - their specs are already in the                           
        // enclosing scope's declaration list and have already been emitted.                        
        //                                                                                          
        if (not cg->current_function and pkg_sym and
          pkg_sym->declaration and
          pkg_sym->declaration->kind == NK_PACKAGE_SPEC) {
          Syntax_Node *spec = pkg_sym->declaration;
          Generate_Declaration_List (&spec->package_spec.visible_decls);
          Generate_Declaration_List (&spec->package_spec.private_decls);
        }
        Generate_Declaration_List (&node->package_body.declarations);

      // Check if the package spec has any single task declarations that                            
      // need starting at elaboration (RM 9.2: tasks are activated at the                           
      // end of the declarative region containing the task declaration).                            
      //                                                                                            
      bool has_pkg_tasks = false;
      Syntax_Node *pkg_spec_node = (pkg_sym and pkg_sym->declaration and
        pkg_sym->declaration->kind == NK_PACKAGE_SPEC) ? pkg_sym->declaration : NULL;
      if (not cg->current_function and pkg_spec_node) {
        for (uint32_t ti = 0; ti < pkg_spec_node->package_spec.visible_decls.count; ti++) {
          Syntax_Node *decl = pkg_spec_node->package_spec.visible_decls.items[ti];
          if (decl and decl->kind == NK_TASK_SPEC and not decl->task_spec.is_type) {
            has_pkg_tasks = true;
            break;
          }
        }
      }

      // Generate initialization/elaboration function if the package has                            
      // init statements OR package-level tasks that need starting.                                 
      // For nested packages (inside a function), emit statements inline                            
      // in the enclosing function - Ada RM 7.2: elaboration occurs at the                          
      // point of the package body in the enclosing declarative region.                             
      // For library-level packages, create a separate __elab function.                             
      //                                                                                            
      bool has_init_stmts = node->package_body.statements.count > 0;

      // Nested package: emit initialization inline.                                                
      // If the package body has exception handlers (RM 11.4),                                      
      // wrap the statements in setjmp/longjmp like subprograms.                                    
      //                                                                                            
      if (has_init_stmts and cg->current_function) {
        bool has_pkg_exc = node->package_body.handlers.count > 0;
        Emit ("  ; Package body initialization (inline)\n");
        if (has_pkg_exc) {
          Exception_Setup setup = Emit_Exception_Handler_Setup ();
          uint32_t end_lbl = Emit_Label ();

          // Normal execution path
          Emit_Label_Here (setup.normal_label);
          Generate_Statement_List (&node->package_body.statements);
          if (not cg->block_terminated)
            Emit ("  call void @__ada_pop_handler()\n");
          Emit_Branch_If_Needed (end_lbl);

          // Exception handler entry
          Emit_Label_Here (setup.handler_label);
          cg->block_terminated = false;
          Emit ("  call void @__ada_pop_handler()\n");
          uint32_t exc_id = Emit_Current_Exception_Id ();
          Generate_Exception_Dispatch (&node->package_body.handlers, exc_id, end_lbl);
          Emit_Label_Here (end_lbl);
          cg->block_terminated = false;
        } else {
          Generate_Statement_List (&node->package_body.statements);
        }

      // Library-level package: emit elaboration function that starts
      // package-level tasks and runs any initialization statements.
      } else if (not cg->current_function and (has_init_stmts or has_pkg_tasks)) {
        Emit ("\n; Package body elaboration\n");
        Emit ("define void @");
        if (pkg_sym) {
          Emit_Symbol_Name (pkg_sym);
        } else {
          for (uint32_t i = 0; i < node->package_body.name.length; i++) {
            char c = node->package_body.name.data[i];
            Emit ("%c", (c >= 'A' and c <= 'Z') ? c + 32 : c);
          }
        }
        Emit ("___elab() {\n");
        Emit ("entry:\n");
        Symbol *saved_current_function = cg->current_function;
        uint32_t saved_temp = cg->temp_id;
        cg->current_function = pkg_sym;
        cg->block_terminated = false;
        cg->temp_id = 1;
        memset (cg->temp_types, 0, sizeof (cg->temp_types));
  memset (cg->temp_type_keys, 0, sizeof (cg->temp_type_keys));
  memset (cg->temp_is_fat_alloca, 0, sizeof (cg->temp_is_fat_alloca));

        // Start package-level tasks
        if (has_pkg_tasks and pkg_spec_node) {
          for (uint32_t ti = 0; ti < pkg_spec_node->package_spec.visible_decls.count; ti++) {
            Syntax_Node *decl = pkg_spec_node->package_spec.visible_decls.items[ti];
            if (not decl or decl->kind != NK_TASK_SPEC or decl->task_spec.is_type or not decl->symbol)
              continue;

            // Find the task object variable symbol
            Symbol *task_obj = NULL;
            Scope *tscope = decl->symbol->defining_scope;
            if (tscope) {
              for (uint32_t si = 0; si < tscope->symbol_count; si++) {
                Symbol *cur_sym = tscope->symbols[si];
                if (cur_sym and cur_sym->kind == SYMBOL_VARIABLE and
                  Type_Is_Task (cur_sym->type) and
                  Slice_Equal_Ignore_Case (cur_sym->name, decl->task_spec.name)) {
                  task_obj = cur_sym;
                  break;
                }
              }
            }
            if (not task_obj) continue;

            // Start the task and store handle in the global
            uint32_t handle_tmp = Emit_Temp ();
            Emit ("  %%t%u = call ptr @__ada_task_start(ptr @", handle_tmp);
            Emit_Task_Function_Name (decl->symbol, decl->task_spec.name);
            Emit (", ptr null)\n");
            Emit ("  store ptr %%t%u, ptr @", handle_tmp);
            Emit_Symbol_Name (task_obj);
            Emit ("\n");
          }
        }

        // Run initialization statements if any
        if (has_init_stmts) {
          Generate_Statement_List (&node->package_body.statements);
        }
        if (not cg->block_terminated) {
          Emit ("  ret void\n");
        }
        Emit ("}\n\n");
        cg->temp_id = saved_temp;
        cg->current_function = saved_current_function;

        // Track this elaboration function for calling from main.                                   
        // Also register with the §15 elaboration graph for                                         
        // proper dependency-ordered elaboration.                                                   
        //                                                                                          
        if (pkg_sym and cg->elab_func_count < 64) {
          cg->elab_funcs[cg->elab_func_count++] = pkg_sym;

          // Register unit in elaboration graph (§15)
          Elab_Register_Unit (pkg_sym->name, true, pkg_sym,  // is_body

                     false,  // is_preelaborate
                     false,  // is_pure
                     true);  // has_elab_code
        }
      }
      }
      break;
    case NK_GENERIC_INST:

      // Generate code for generic instantiation. Per Ada RM 12.3: "The
      // elaboration of a generic instantiation declares an instance"
      {
        Symbol *inst_sym = node->symbol;
        if (not inst_sym or not inst_sym->generic_template) break;
        Symbol *template = inst_sym->generic_template;
        Symbol *saved_instance = cg->current_instance;
        cg->current_instance = inst_sym;
        Set_Generic_Type_Map (inst_sym);

        // Spec reference and elab flag, used both in global emission
        // and in the spec-only elaboration path below.
        Syntax_Node *gen_spec = inst_sym->expanded_spec;
        if (not gen_spec) gen_spec = template->generic_unit;
        bool needs_elab = false;

        // For library-level package instances, emit global variables for                           
        // exported objects REGARDLESS of whether there's a body. Generic                           
        // packages may have just a spec with object declarations.                                  
        //                                                                                          
        if (not cg->current_function and inst_sym->kind == SYMBOL_PACKAGE) {
          for (uint32_t i = 0; i < inst_sym->exported_count; i++) {
            Symbol *exp = inst_sym->exported[i];
            if (not exp) continue;
            if (exp->kind != SYMBOL_VARIABLE and exp->kind != SYMBOL_CONSTANT)
              continue;
            Type_Info *ty = exp->type;
            const char *type_str = Type_To_Llvm (ty);
            bool is_array = Type_Is_Constrained_Array (ty);
            bool is_record = Type_Is_Record (ty);

            // Look for initializer in the expanded spec's visible decls.                           
            // With expanded_spec, formal params are already substituted                            
            // with actual expressions, so we can evaluate directly.                                
            //                                                                                      
            int64_t init_val = 0;
            bool has_init = false;
            bool this_needs_elab = false;
            if (gen_spec and gen_spec->kind == NK_PACKAGE_SPEC) {
              for (uint32_t j = 0; j < gen_spec->package_spec.visible_decls.count; j++) {
                Syntax_Node *decl = gen_spec->package_spec.visible_decls.items[j];
                if (not decl or decl->kind != NK_OBJECT_DECL) continue;
                for (uint32_t k = 0; k < decl->object_decl.names.count; k++) {
                  Syntax_Node *nm = decl->object_decl.names.items[k];
                  if (nm and Slice_Equal_Ignore_Case (nm->string_val.text, exp->name)) {
                    Syntax_Node *init = decl->object_decl.init;
                    if (init and init->kind == NK_INTEGER) {
                      init_val = init->integer_lit.value;
                      has_init = true;
                    } else if (init) {
                      this_needs_elab = true;
                      needs_elab = true;
                    }
                    break;
                  }
                }
                if (has_init or this_needs_elab) break;
              }
            }
            exp->extern_emitted = true;
            Emit ("@");
            Emit_Symbol_Name (exp);
            if (is_array) {
              int128_t cnt = Array_Element_Count (ty);
              const char *elem_type = Type_To_Llvm (ty->array.element_type);
              Emit (" = linkonce_odr global [%s x %s] zeroinitializer\n",
                 I128_Decimal (cnt), elem_type);
            } else if (is_record) {
              Emit (" = linkonce_odr global [%u x i8] zeroinitializer\n", ty->size);
            } else if (Type_Is_Float_Representation (ty)) {
              Emit (" = linkonce_odr global %s 0.0\n", type_str);
            } else if (strcmp (type_str, "ptr") == 0) {
              Emit (" = linkonce_odr global ptr null\n");
            } else {
              Emit (" = linkonce_odr global %s %lld\n", type_str,
                 (long long)init_val);
            }
          }
        }

        // Prefer expanded_body (with substitutions already applied)
        // over template->generic_body (requires runtime substitution)
        Syntax_Node *generic_body = inst_sym->expanded_body;
        if (not generic_body) generic_body = template->generic_body;

        // Spec-only generic: if any init needs runtime evaluation,                                 
        // emit an elab function. Use generic_actuals (resolved in the                              
        // instantiation context) rather than cloned template AST.                                  
        //                                                                                          
        if (not generic_body) {

          // Build map: for each variable whose init references a
          // generic formal, pair it with the formal's actual_expr.
          if (needs_elab) {
            Emit ("\n; Generic instance spec elaboration\n");
            Emit ("define void @");
            Emit_Symbol_Name (inst_sym);
            Emit ("___elab() {\nentry:\n");
            Symbol *saved_func = cg->current_function;
            uint32_t saved_temp = cg->temp_id;
            cg->current_function = inst_sym;
            cg->block_terminated = false;
            cg->temp_id = 1;
            memset (cg->temp_types, 0, sizeof (cg->temp_types));
            memset (cg->temp_type_keys, 0, sizeof (cg->temp_type_keys));
            memset (cg->temp_is_fat_alloca, 0, sizeof (cg->temp_is_fat_alloca));

            // For each non-static exported var, find the generic actual
            // that corresponds to its init expression and evaluate it.
            Syntax_Node *tmpl_spec = template->generic_unit;
            if (tmpl_spec and tmpl_spec->kind == NK_PACKAGE_SPEC) {
              for (uint32_t j = 0; j < tmpl_spec->package_spec.visible_decls.count; j++) {
                Syntax_Node *decl = tmpl_spec->package_spec.visible_decls.items[j];
                if (not decl or decl->kind != NK_OBJECT_DECL or not decl->object_decl.init)
                  continue;
                Syntax_Node *init = decl->object_decl.init;
                if (init->kind == NK_INTEGER) continue;

                // init is likely NK_IDENTIFIER referencing formal C.
                // Find the actual expression from generic_actuals.
                Syntax_Node *actual_expr = NULL;
                if (init->kind == NK_IDENTIFIER) {
                  for (uint32_t ai = 0; ai < inst_sym->generic_actual_count; ai++) {
                    if (Slice_Equal_Ignore_Case (
                        inst_sym->generic_actuals[ai].formal_name,
                        init->string_val.text)) {
                      actual_expr = inst_sym->generic_actuals[ai].actual_expr;
                      break;
                    }
                  }
                }
                if (not actual_expr) continue;

                // Find exported symbol for this variable
                for (uint32_t k = 0; k < decl->object_decl.names.count; k++) {
                  Syntax_Node *nm = decl->object_decl.names.items[k];
                  if (not nm) continue;
                  Symbol *target = NULL;
                  for (uint32_t ei = 0; ei < inst_sym->exported_count; ei++) {
                    if (inst_sym->exported[ei] and
                      Slice_Equal_Ignore_Case (inst_sym->exported[ei]->name,
                                  nm->string_val.text)) {
                      target = inst_sym->exported[ei];
                      break;
                    }
                  }
                  if (not target) continue;
                  uint32_t val = Generate_Expression (actual_expr);
                  const char *ts = Type_To_Llvm (target->type);
                  Emit ("  store %s %%t%u, ptr @", ts, val);
                  Emit_Symbol_Name (target);
                  Emit ("\n");
                }
              }
            }
            if (not cg->block_terminated) Emit ("  ret void\n");
            Emit ("}\n\n");
            cg->temp_id = saved_temp;
            cg->current_function = saved_func;
            if (cg->elab_func_count < 64) {
              cg->elab_funcs[cg->elab_func_count++] = inst_sym;

              // Register generic instance in elaboration graph (§15)
              Elab_Register_Unit (inst_sym->name, true, inst_sym,  // is_body

                         false,  // is_preelaborate
                         false,  // is_pure
                         true);  // has_elab_code
            }
          }
          cg->current_instance = saved_instance;
          Set_Generic_Type_Map (saved_instance);
          break;
        }

        // Generate instantiated body using the instance's symbol                                   
        // Local generic package instance: allocate storage for exported                            
        // variables/constants NOW (as local allocas), but defer subprogram                         
        // bodies for later. Per Ada RM 12.3: "The elaboration of a generic                         
        // instantiation declares an instance... and elaborates the instance"                       
        //                                                                                          
        if (cg->current_function and inst_sym->kind == SYMBOL_PACKAGE) {
          for (uint32_t i = 0; i < inst_sym->exported_count; i++) {
            Symbol *exp = inst_sym->exported[i];
            if (not exp) continue;

            // Allocate local storage for package variable
            if (exp->kind == SYMBOL_VARIABLE or exp->kind == SYMBOL_CONSTANT) {
              Type_Info *ty = exp->type;
              const char *type_str = Type_To_Llvm (ty);
              bool is_array = Type_Is_Constrained_Array (ty);
              bool is_record = Type_Is_Record (ty);
              Emit ("  %%");
              Emit_Symbol_Name (exp);
              if (is_array) {
                int128_t cnt = Array_Element_Count (ty);
                const char *elem_type = Type_To_Llvm (ty->array.element_type);
                Emit (" = alloca [%s x %s]  ; local pkg var\n", I128_Decimal (cnt), elem_type);
              } else if (is_record) {
                Emit (" = alloca [%u x i8]  ; local pkg record\n", ty->size);
              } else {
                Emit (" = alloca %s  ; local pkg var\n", type_str);
              }

              // Initialize from package body if present                                            
              // Look for assignment in BEGIN..END section targeting this var.                      
              // Compare by name since body symbol differs from exported symbol.                    
              //                                                                                    
              if (generic_body and generic_body->kind == NK_PACKAGE_BODY) {
                for (uint32_t j = 0; j < generic_body->package_body.statements.count; j++) {
                  Syntax_Node *stmt = generic_body->package_body.statements.items[j];
                  if (not stmt or stmt->kind != NK_ASSIGNMENT) continue;
                  Syntax_Node *tgt = stmt->assignment.target;
                  if (not tgt) continue;
                  String_Slice tgt_name = {0};
                  if (tgt->kind == NK_IDENTIFIER)
                    tgt_name = tgt->string_val.text;
                  else if (tgt->symbol)
                    tgt_name = tgt->symbol->name;

                  // Redirect target to use local exported symbol
                  if (tgt_name.data and Slice_Equal_Ignore_Case (tgt_name, exp->name)) {
                    tgt->symbol = exp;
                    Generate_Statement (stmt);
                    break;
                  }
                }
              }
            }
          }

          // Start any task objects declared in the generic spec.                                   
          // Task variables (SYMBOL_VARIABLE with TYPE_TASK) need                                   
          // __ada_task_start called after their alloca.                                            
          //                                                                                        
          for (uint32_t i = 0; i < inst_sym->exported_count; i++) {
            Symbol *exp = inst_sym->exported[i];
            if (not exp or exp->kind != SYMBOL_VARIABLE) continue;
            if (not Type_Is_Task (exp->type)) continue;

            // Start the task
            uint32_t handle_tmp = Emit_Temp ();
            Emit ("  %%t%u = call ptr @__ada_task_start(ptr @", handle_tmp);
            Emit_Task_Function_Name (exp->type ? exp->type->defining_symbol : NULL,
                        exp->type ? exp->type->name : exp->name);
            Emit (", ");
            if (cg->current_nesting_level > 0) {
              Emit ("ptr %%__frame_base)\n");
            } else {
              Emit ("ptr null)\n");
            }
            Emit ("  store ptr %%t%u, ptr %%", handle_tmp);
            Emit_Symbol_Name (exp);
            Emit ("\n");
          }

          // Defer subprogram bodies for later
          if (cg->deferred_count < 64)
            cg->deferred_bodies[cg->deferred_count++] = node;

        // Defer nested generic subprogram instance
        } else if (cg->current_function and cg->deferred_count < 64) {
          cg->deferred_bodies[cg->deferred_count++] = node;

        // Global variables already emitted above. Now generate subprogram
        // bodies, matching by homograph index.
        } else if (inst_sym->kind == SYMBOL_PACKAGE) {
          if (generic_body->kind == NK_PACKAGE_BODY) {
            for (uint32_t i = 0; i < inst_sym->exported_count; i++) {
              Symbol *exp = inst_sym->exported[i];
              if (not exp) continue;
              if (exp->kind != SYMBOL_FUNCTION and exp->kind != SYMBOL_PROCEDURE)
                continue;
              Syntax_Node *subp_body = Find_Homograph_Body (
                inst_sym->exported, i, exp->name,
                &generic_body->package_body.declarations);
              if (subp_body)
                Generate_Generic_Instance_Body (exp, subp_body);
            }
          }
        } else {
          Generate_Generic_Instance_Body (inst_sym, generic_body);
        }
        cg->current_instance = saved_instance;
        Set_Generic_Type_Map (saved_instance);
      }
      break;

    // Find the variable symbol in the type symbol's defining scope.                                
    // The type and variable were both added to the same scope during                               
    // semantic analysis. Use defining_scope instead of sm->current_scope                           
    // since current_scope may have changed during code generation.                                 
    //                                                                                              
    case NK_GENERIC_DECL:

      // Generic declarations don't generate code - only instances do
      break;
    case NK_TASK_SPEC:

      // Task type/object specification - record entry points
      Emit ("; Task spec: %.*s (entries registered at runtime)\n",
         (int)node->task_spec.name.length, node->task_spec.name.data);

      // For single tasks (not task types), allocate task control block storage
      // and start the task body in a separate thread
      if (not node->task_spec.is_type and node->symbol) {
        Symbol *obj_sym = NULL;
        Scope *scope = node->symbol->defining_scope;
        if (scope) {
          for (uint32_t i = 0; i < scope->symbol_count; i++) {
            Symbol *cur_sym = scope->symbols[i];
            if (cur_sym and cur_sym->kind == SYMBOL_VARIABLE and
              Type_Is_Task (cur_sym->type) and
              Slice_Equal_Ignore_Case (cur_sym->name, node->task_spec.name)) {
              obj_sym = cur_sym;
              break;
            }
          }
        }
        if (obj_sym) {
          if (obj_sym->unique_id == 0) {
            obj_sym->unique_id = sm->next_unique_id++;
          }

          // Package-level tasks need global storage, not alloca.                                   
          // alloca is only valid inside a function.  For package-level                             
          // tasks, emit a global variable here and defer the task_start                            
          // to the package body elaboration function (RM 9.2).                                     
          // Emit global variable for the task control block (once only)                            
          //                                                                                        
          if (not cg->current_function) {
            if (not obj_sym->extern_emitted) {
              obj_sym->extern_emitted = true;
              Emit ("@");
              Emit_Symbol_Name (obj_sym);
              Emit (" = global ptr null  ; package-level task object\n");
            }

            // Task start is deferred to package body elaboration
          // Allocate task object - use frame if nested, else stack
          } else {
          bool use_frame = cg->current_nesting_level > 0;
          if (use_frame) {
            Emit ("  %%");
            Emit_Symbol_Name (obj_sym);
            Emit (" = getelementptr i8, ptr %%__frame_base, i64 %lld  ; task in frame\n",
               (long long)obj_sym->frame_offset);
          } else {
            Emit ("  %%");
            Emit_Symbol_Name (obj_sym);
            Emit (" = alloca ptr  ; task control block\n");
          }

          // Start the task body in a separate thread.
          // Pass frame_base if nested, or null if at module level.
          uint32_t handle_tmp = Emit_Temp ();
          Emit ("  %%t%u = call ptr @__ada_task_start(ptr @", handle_tmp);
          Emit_Task_Function_Name (node->symbol, node->task_spec.name);
          Emit (", ");

          // Pass parent frame for uplevel access, or null if at module level.
          // Use frame_base only if the current function has nested subprograms.
          if (cg->current_nesting_level > 0) {
            Emit ("ptr %%__frame_base)\n");
          } else {
            Emit ("ptr null)\n");
          }

          // Store thread handle in task control block
          Emit ("  store ptr %%t%u, ptr %%", handle_tmp);
          Emit_Symbol_Name (obj_sym);
          Emit ("\n");
          }
        }
      }
      break;
    case NK_TASK_BODY:

      // Defer task body generation when inside another function
      if (cg->current_function and cg->deferred_count < 64) {
        cg->deferred_bodies[cg->deferred_count++] = node;
      } else {
        Generate_Task_Body (node);
      }
      break;

    // RM §3.3.1: Type elaboration.  For types with static bounds,                                  
    // no code is needed.  For constrained array types whose bounds are                             
    // runtime expressions (BOUND_EXPR), evaluate bounds at elaboration                             
    // time and store in globals for use by record layouts, alloca, etc.                            
    // For record types with dynamic-sized components, compute and store                            
    // cumulative byte offsets and total size in globals.                                           
    //                                                                                              
    case NK_TYPE_DECL:
    {
      Symbol *type_sym = node->symbol;
      Type_Info *elab_ty = type_sym ? type_sym->type : NULL;

      // ── Array type with expression bounds ──────────────────────────────────────────────────────

      if (elab_ty and Type_Is_Array_Like (elab_ty) and
        elab_ty->array.is_constrained and Type_Has_Dynamic_Bounds (elab_ty)) {
        Ensure_Runtime_Type_Globals (elab_ty);
        if (elab_ty->rt_global_id > 0) {
          uint32_t rtid = elab_ty->rt_global_id;
          uint32_t ndims = elab_ty->array.index_count;
          uint32_t esz = (elab_ty->array.element_type and
                  elab_ty->array.element_type->size > 0)
                   ? elab_ty->array.element_type->size : 4;
          uint32_t count_reg = 0;
          for (uint32_t d = 0; d < ndims; d++) {
            Type_Bound lo = elab_ty->array.indices[d].low_bound;
            Type_Bound hi = elab_ty->array.indices[d].high_bound;
            if (lo.kind == BOUND_NONE and elab_ty->array.indices[d].index_type)
              lo = elab_ty->array.indices[d].index_type->low_bound;
            if (hi.kind == BOUND_NONE and elab_ty->array.indices[d].index_type)
              hi = elab_ty->array.indices[d].index_type->high_bound;

            // Evaluate or materialise each bound as i64
            uint32_t lo_r, hi_r;
            if (lo.kind == BOUND_EXPR and lo.expr) {
              lo_r = Generate_Expression (lo.expr);
              lo_r = Emit_Coerce_Default_Int (lo_r, "i64");
            } else {
              lo_r = Emit_Temp ();
              Emit ("  %%t%u = add i64 0, %lld\n",
                 lo_r, (long long)Type_Bound_Value (lo));
            }
            if (hi.kind == BOUND_EXPR and hi.expr) {
              hi_r = Generate_Expression (hi.expr);
              hi_r = Emit_Coerce_Default_Int (hi_r, "i64");
            } else {
              hi_r = Emit_Temp ();
              Emit ("  %%t%u = add i64 0, %lld\n",
                 hi_r, (long long)Type_Bound_Value (hi));
            }
            Emit ("  store i64 %%t%u, ptr @__rt_type_%u_lo%u\n", lo_r, rtid, d);
            Emit ("  store i64 %%t%u, ptr @__rt_type_%u_hi%u\n", hi_r, rtid, d);

            // dim_count = max(hi - lo + 1, 0)
            uint32_t diff = Emit_Temp ();
            Emit ("  %%t%u = sub i64 %%t%u, %%t%u\n", diff, hi_r, lo_r);
            uint32_t cnt = Emit_Temp ();
            Emit ("  %%t%u = add i64 %%t%u, 1\n", cnt, diff);
            uint32_t neg = Emit_Temp ();
            Emit ("  %%t%u = icmp slt i64 %%t%u, 0\n", neg, cnt);
            uint32_t clamped = Emit_Temp ();
            Emit ("  %%t%u = select i1 %%t%u, i64 0, i64 %%t%u\n",
               clamped, neg, cnt);
            if (d == 0) {
              count_reg = clamped;
            } else {
              uint32_t prod = Emit_Temp ();
              Emit ("  %%t%u = mul i64 %%t%u, %%t%u\n",
                 prod, count_reg, clamped);
              count_reg = prod;
            }
          }
          uint32_t total = Emit_Temp ();
          Emit ("  %%t%u = mul i64 %%t%u, %u  ; type elab size\n",
             total, count_reg, esz);
          Emit ("  store i64 %%t%u, ptr @__rt_type_%u_size\n", total, rtid);
        }
      }

      // ── Record type with dynamic-sized components ──────────────────────────────────────────────

      if (elab_ty and Type_Is_Record (elab_ty)) {
        bool has_dyn = false;
        for (uint32_t ci = 0; ci < elab_ty->record.component_count; ci++) {
          Type_Info *ct = elab_ty->record.components[ci].component_type;
          if (ct and ct->size == 0 and Type_Is_Array_Like (ct)
            and ct->array.is_constrained and Type_Has_Dynamic_Bounds (ct)) {
            Ensure_Runtime_Type_Globals (ct);
            if (ct->rt_global_id > 0)
              has_dyn = true;
          }
        }
        if (has_dyn and elab_ty->rt_global_id == 0) {
          uint32_t rid = ++cg->rt_type_counter;
          elab_ty->rt_global_id = rid;
          Emit_String_Const ("@__rt_rec_%u_size = internal global i64 0\n", rid);
          for (uint32_t ci = 0; ci < elab_ty->record.component_count; ci++)
            Emit_String_Const ("@__rt_rec_%u_off%u = internal global i64 0\n",
                      rid, ci);
        }
        if (elab_ty->rt_global_id > 0) {
          uint32_t rid = elab_ty->rt_global_id;
          uint32_t off_r = Emit_Temp ();
          Emit ("  %%t%u = add i64 0, 0  ; rec elab base\n", off_r);
          for (uint32_t ci = 0; ci < elab_ty->record.component_count; ci++) {
            Emit ("  store i64 %%t%u, ptr @__rt_rec_%u_off%u\n",
               off_r, rid, ci);
            Type_Info *ct = elab_ty->record.components[ci].component_type;
            uint32_t csz;
            if (ct and ct->rt_global_id > 0) {
              csz = Emit_Temp ();
              Emit ("  %%t%u = load i64, ptr @__rt_type_%u_size\n",
                 csz, ct->rt_global_id);
            } else {
              uint32_t s = ct ? ct->size : 0;
              csz = Emit_Temp ();
              Emit ("  %%t%u = add i64 0, %u\n", csz, s);
            }
            uint32_t noff = Emit_Temp ();
            Emit ("  %%t%u = add i64 %%t%u, %%t%u\n", noff, off_r, csz);
            off_r = noff;
          }
          Emit ("  store i64 %%t%u, ptr @__rt_rec_%u_size\n", off_r, rid);
        }
      }

      // RM 3.7.2: During type elaboration, check discriminant constraint                           
      // values for any constrained record types referenced by this type                            
      // (array element types, access designated types, derived types,                              
      // record components).                                                                        
      //                                                                                            
      {
        Type_Info *disc_check_types[8];
        uint32_t disc_check_count = 0;
        #define HAS_DISC_CONSTR(t) ((t) and ((t)->kind == TYPE_RECORD or \
          (t)->kind == TYPE_PRIVATE or (t)->kind == TYPE_LIMITED_PRIVATE) \
          and (t)->record.has_disc_constraints)
        if (elab_ty and Type_Is_Array_Like (elab_ty) and
          HAS_DISC_CONSTR (elab_ty->array.element_type))
          disc_check_types[disc_check_count++] = elab_ty->array.element_type;
        if (elab_ty and Type_Is_Access (elab_ty) and
          HAS_DISC_CONSTR (elab_ty->access.designated_type))
          disc_check_types[disc_check_count++] = elab_ty->access.designated_type;
        if (elab_ty and (elab_ty->kind == TYPE_RECORD or
          elab_ty->kind == TYPE_PRIVATE or
          elab_ty->kind == TYPE_LIMITED_PRIVATE) and
          elab_ty->record.has_disc_constraints and elab_ty->parent_type)
          disc_check_types[disc_check_count++] = elab_ty;
        if (elab_ty and (Type_Is_Record (elab_ty) or Type_Is_Private (elab_ty))) {
          for (uint32_t ci = 0; ci < elab_ty->record.component_count and
             disc_check_count < 8; ci++) {
            Type_Info *ct = elab_ty->record.components[ci].component_type;
            if (HAS_DISC_CONSTR (ct))
              disc_check_types[disc_check_count++] = ct;
          }
        }
        #undef HAS_DISC_CONSTR
        for (uint32_t ti = 0; ti < disc_check_count; ti++) {
          Type_Info *dct = disc_check_types[ti];

          // RM 3.7.2: If ANY disc constraint depends on a discriminant,                            
          // the whole constraint is disc-dependent.  Pre-evaluate                                  
          // non-disc expressions for later use but defer all checks                                
          // until the constraint is applied explicitly (RM 3.7.2(3)).                              
          //                                                                                        
          bool any_refs_disc = false;
          if (dct->record.disc_constraint_exprs) {
            for (uint32_t di = 0; di < dct->record.discriminant_count; di++) {
              if (not dct->record.disc_constraint_exprs[di]) continue;
              Symbol *disc_refs[8]; uint32_t drc = 0;
              Collect_Disc_Symbols_In_Expr (
                dct->record.disc_constraint_exprs[di],
                disc_refs, &drc, 8);
              if (drc > 0) { any_refs_disc = true; break; }
            }
          }

          // Pre-evaluate non-disc expressions and cache in allocas.                                
          // This ensures they are evaluated once at type elaboration                               
          // (RM 3.7.1) but not checked until constraint application.                               
          // Skip if already pre-evaluated (same constrained type                                   
          // referenced by multiple enclosing types).                                               
          //                                                                                        
          if (any_refs_disc) {
            if (dct->record.disc_constraint_preeval) {
              continue;  // Already pre-evaluated
            }
            dct->record.disc_constraint_preeval = Arena_Allocate (
              dct->record.discriminant_count * sizeof (uint32_t));
            memset (dct->record.disc_constraint_preeval, 0,
                 dct->record.discriminant_count * sizeof (uint32_t));
            for (uint32_t di = 0; di < dct->record.discriminant_count; di++) {
              if (not dct->record.disc_constraint_exprs or
                not dct->record.disc_constraint_exprs[di]) continue;
              Symbol *disc_refs[8]; uint32_t drc = 0;
              Collect_Disc_Symbols_In_Expr (
                dct->record.disc_constraint_exprs[di],
                disc_refs, &drc, 8);

              // Non-disc expression: evaluate once, store to alloca
              if (drc == 0) {
                Component_Info *dc = &dct->record.components[di];
                const char *dt = Type_To_Llvm (dc->component_type);
                uint32_t val = Generate_Expression (dct->record.disc_constraint_exprs[di]);
                val = Emit_Coerce_Default_Int (val, dt);
                uint32_t alloca_t = Emit_Temp ();
                Emit ("  %%t%u = alloca %s  ; preeval disc constraint\n",
                   alloca_t, dt);
                Emit ("  store %s %%t%u, ptr %%t%u\n", dt, val, alloca_t);
                dct->record.disc_constraint_preeval[di] = alloca_t;
              }
            }
            continue;  // Skip all checks for disc-dependent types
          }
          for (uint32_t di = 0; di < dct->record.discriminant_count; di++) {
            Component_Info *dc = &dct->record.components[di];
            Type_Info *disc_type = dc->component_type;
            if (not disc_type or not Type_Is_Scalar (disc_type)) continue;
            const char *dt = Type_To_Llvm (disc_type);
            uint32_t val = 0;
            if (dct->record.disc_constraint_exprs and
              dct->record.disc_constraint_exprs[di]) {
              val = Generate_Expression (dct->record.disc_constraint_exprs[di]);
              val = Emit_Coerce_Default_Int (val, dt);
            } else if (dct->record.disc_constraint_values) {
              val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld\n", val, dt,
                 (long long)dct->record.disc_constraint_values[di]);
            }
            if (val > 0)
              Emit_Constraint_Check (val, disc_type, NULL);
          }

          // RM 3.7.2(3): Also check nested component disc constraints
          Emit_Nested_Disc_Checks (dct);
        }
      }
      break;
    }

    // Subtype elaboration constraint check (RM 3.3.2(7)):                                          
    // For "subtype S is T range L..H", check that L and H are                                      
    // within T's range. Raise CONSTRAINT_ERROR if not.                                             
    //                                                                                              
    case NK_SUBTYPE_DECL:
    {
      Symbol *sub_sym = node->symbol;
      Type_Info *sub_type = sub_sym ? sub_sym->type : NULL;
      if (sub_type and sub_type->base_type) {
        Type_Info *parent = sub_type->base_type;
        bool is_scalar = (sub_type->kind == TYPE_INTEGER or
                  sub_type->kind == TYPE_ENUMERATION or
                  sub_type->kind == TYPE_FLOAT or
                  sub_type->kind == TYPE_FIXED or
                  sub_type->kind == TYPE_CHARACTER or
                  sub_type->kind == TYPE_MODULAR);

        // Only check scalar subtypes with both bounds known
        bool sub_lo_ok = (sub_type->low_bound.kind == BOUND_INTEGER or
                  sub_type->low_bound.kind == BOUND_FLOAT);
        bool sub_hi_ok = (sub_type->high_bound.kind == BOUND_INTEGER or
                  sub_type->high_bound.kind == BOUND_FLOAT);
        bool par_lo_ok = (parent->low_bound.kind == BOUND_INTEGER or
                  parent->low_bound.kind == BOUND_FLOAT);
        bool par_hi_ok = (parent->high_bound.kind == BOUND_INTEGER or
                  parent->high_bound.kind == BOUND_FLOAT);

        // Check if subtype range is a non-null range
        if (is_scalar and sub_lo_ok and sub_hi_ok and par_lo_ok and par_hi_ok) {
          int64_t sub_lo = sub_type->low_bound.kind == BOUND_INTEGER ?
            (int64_t)sub_type->low_bound.int_value : (int64_t)sub_type->low_bound.float_value;
          int64_t sub_hi = sub_type->high_bound.kind == BOUND_INTEGER ?
            (int64_t)sub_type->high_bound.int_value : (int64_t)sub_type->high_bound.float_value;
          int64_t par_lo = parent->low_bound.kind == BOUND_INTEGER ?
            (int64_t)parent->low_bound.int_value : (int64_t)parent->low_bound.float_value;
          int64_t par_hi = parent->high_bound.kind == BOUND_INTEGER ?
            (int64_t)parent->high_bound.int_value : (int64_t)parent->high_bound.float_value;

          // Only non-null ranges need checking (RM 3.3.2(8))
          if (sub_lo <= sub_hi) {

            // Static violation: always raise
            if (sub_lo < par_lo or sub_hi > par_hi) {
              Emit_Raise_Constraint_Error ("subtype elaboration check");
              uint32_t cont = Emit_Label ();
              Emit ("L%u:\n", cont);
              cg->block_terminated = false;
            }
          }
        }
      }

      // RM 3.7.2: For record subtypes with discriminant constraints,                               
      // check that each constraint value lies within the discriminant's                            
      // subtype range. Raise CONSTRAINT_ERROR if not.                                              
      //                                                                                            
      if (sub_type and (sub_type->kind == TYPE_RECORD or
        sub_type->kind == TYPE_PRIVATE or
        sub_type->kind == TYPE_LIMITED_PRIVATE) and
        sub_type->record.has_disc_constraints) {

        // RM 3.7.2: If ANY disc constraint depends on a discriminant,
        // pre-evaluate non-disc parts and defer checks.
        bool any_refs_disc = false;
        if (sub_type->record.disc_constraint_exprs) {
          for (uint32_t di = 0; di < sub_type->record.discriminant_count; di++) {
            if (not sub_type->record.disc_constraint_exprs[di]) continue;
            Symbol *disc_refs[8]; uint32_t drc = 0;
            Collect_Disc_Symbols_In_Expr (
              sub_type->record.disc_constraint_exprs[di],
              disc_refs, &drc, 8);
            if (drc > 0) { any_refs_disc = true; break; }
          }
        }

        // Pre-evaluate non-disc expressions (RM 3.7.1).
        // Skip if already pre-evaluated.
        if (any_refs_disc) {
          if (sub_type->record.disc_constraint_preeval) {

            // Already done
          } else {
            sub_type->record.disc_constraint_preeval = Arena_Allocate (
              sub_type->record.discriminant_count * sizeof (uint32_t));
            memset (sub_type->record.disc_constraint_preeval, 0,
                 sub_type->record.discriminant_count * sizeof (uint32_t));
          for (uint32_t di = 0; di < sub_type->record.discriminant_count; di++) {
            if (not sub_type->record.disc_constraint_exprs or
              not sub_type->record.disc_constraint_exprs[di]) continue;
            Symbol *disc_refs[8]; uint32_t drc = 0;
            Collect_Disc_Symbols_In_Expr (
              sub_type->record.disc_constraint_exprs[di],
              disc_refs, &drc, 8);
            if (drc == 0) {
              Component_Info *dc = &sub_type->record.components[di];
              const char *dt = Type_To_Llvm (dc->component_type);
              uint32_t val = Generate_Expression (sub_type->record.disc_constraint_exprs[di]);
              val = Emit_Coerce_Default_Int (val, dt);
              uint32_t alloca_t = Emit_Temp ();
              Emit ("  %%t%u = alloca %s  ; preeval subtype disc\n",
                 alloca_t, dt);
              Emit ("  store %s %%t%u, ptr %%t%u\n", dt, val, alloca_t);
              sub_type->record.disc_constraint_preeval[di] = alloca_t;
            }
          }
          }  // end else (not already preeval'd)
        } else {
          for (uint32_t di = 0; di < sub_type->record.discriminant_count; di++) {
            Component_Info *dc = &sub_type->record.components[di];
            Type_Info *disc_type = dc->component_type;
            if (not disc_type or not Type_Is_Scalar (disc_type)) continue;
            const char *dt = Type_To_Llvm (disc_type);
            uint32_t val = 0;
            if (sub_type->record.disc_constraint_exprs and
              sub_type->record.disc_constraint_exprs[di]) {
              val = Generate_Expression (sub_type->record.disc_constraint_exprs[di]);
              val = Emit_Coerce_Default_Int (val, dt);
            } else if (sub_type->record.disc_constraint_values) {
              val = Emit_Temp ();
              Emit ("  %%t%u = add %s 0, %lld\n", val, dt,
                 (long long)sub_type->record.disc_constraint_values[di]);
            }
            if (val > 0)
              Emit_Constraint_Check (val, disc_type, NULL);
          }

          // RM 3.7.2(3): Also check nested component disc constraints
          Emit_Nested_Disc_Checks (sub_type);
        }
      }
      break;
    }

    // Other declaration kinds that need no codegen
    case NK_EXCEPTION_DECL:
    case NK_USE_CLAUSE:
    case NK_WITH_CLAUSE:
    case NK_PRAGMA:
    case NK_REPRESENTATION_CLAUSE:
    case NK_EXCEPTION_HANDLER:
    case NK_ENTRY_DECL:
    case NK_SUBPROGRAM_RENAMING:
    case NK_PACKAGE_RENAMING:
    case NK_EXCEPTION_RENAMING:
      break;
    default:
      fprintf (stderr, "warning: unhandled declaration kind %d at %s:%u\n",
          node->kind,
          node->location.filename ? node->location.filename : "<unknown>",
          node->location.line);
      break;
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.6 Implicit Equality Function Generation                                                      
//                                                                                                  
// Generate equality functions for composite types at freeze points.                                
// Per RM 4.5.2, equality is predefined for all non-limited types.                                  
// The RM specifies the semantics and the compiler provides the implementation.                     
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Generate_Type_Equality_Function (Type_Info *t) {
  if (not t or not t->equality_func_name) return;
  const char *func_name = t->equality_func_name;

  // Determine parameter type: unconstrained and dynamic-bound constrained
  // arrays are represented as fat pointers { ptr, ptr } at runtime.
  bool is_fat = Type_Is_Unconstrained_Array (t) or
          (Type_Is_Array_Like (t) and Type_Has_Dynamic_Bounds (t));
  const char *eq_bt = is_fat ? Array_Bound_Llvm_Type (t) : "i32";
  const char *param_type = is_fat ? FAT_PTR_TYPE : "ptr";

  // Emit function definition with linkonce_odr for linker deduplication
  Emit ("\n; Implicit equality for type %.*s\n",
     (int)t->name.length, t->name.data);
  Emit ("define linkonce_odr i1 @%s(%s %%0, %s %%1) {\n", func_name, param_type, param_type);
  Emit ("entry:\n");

  // Save and reset temp counter for this function
  uint32_t saved_temp = cg->temp_id;
  cg->temp_id = 2;  // Start after %0 and %1
  memset (cg->temp_types, 0, sizeof (cg->temp_types));
  memset (cg->temp_type_keys, 0, sizeof (cg->temp_type_keys));
  memset (cg->temp_is_fat_alloca, 0, sizeof (cg->temp_is_fat_alloca));
  if (Type_Is_Record (t)) {

    // Empty record - always equal
    if (t->record.component_count == 0) {
      Emit ("  ret i1 1\n");
    } else {
      uint32_t result = 0;
      for (uint32_t i = 0; i < t->record.component_count; i++) {
        Component_Info *comp = &t->record.components[i];
        const char *comp_llvm_type = Type_To_Llvm (comp->component_type);

        // Get pointers to components
        uint32_t left_gep = Emit_Temp ();
        uint32_t right_gep = Emit_Temp ();
        Emit ("  %%t%u = getelementptr i8, ptr %%0, i64 %u\n",
           left_gep, comp->byte_offset);
        Emit ("  %%t%u = getelementptr i8, ptr %%1, i64 %u\n",
           right_gep, comp->byte_offset);

        // Compare component - handle arrays/strings specially
        uint32_t cmp;
        Type_Info *ct = comp->component_type;
        bool is_fat_ptr_access = Type_Needs_Fat_Pointer (ct);
        if (Type_Is_Unconstrained_Array (ct) or
          (not Type_Is_Constrained_Array (ct) and Type_Is_String (ct))) {

          // Unconstrained array/string - load fat pointer values from storage
          const char *eqf_bt = Array_Bound_Llvm_Type (ct);
          uint32_t left_fat = Emit_Load_Fat_Pointer_From_Temp (left_gep, eqf_bt);
          uint32_t right_fat = Emit_Load_Fat_Pointer_From_Temp (right_gep, eqf_bt);
          cmp = Generate_Array_Equality (left_fat, right_fat, ct);

        // Constrained array with dynamic bounds (discriminant-dependent).
        // Compute runtime byte size from discriminant value.
        } else if (Type_Is_Constrained_Array (ct) and Type_Has_Dynamic_Bounds (ct)) {
          uint32_t elem_size = ct->array.element_type ? ct->array.element_type->size : 1;
          if (elem_size == 0) elem_size = 1;
          int64_t low_val = 1;
          if (ct->array.index_count > 0 and ct->array.indices[0].low_bound.kind == BOUND_INTEGER)
            low_val = (int64_t)ct->array.indices[0].low_bound.int_value;
          uint32_t disc_offset = 0;
          const char *disc_llvm = "i32";
          if (ct->array.index_count > 0 and ct->array.indices[0].high_bound.kind == BOUND_EXPR
            and ct->array.indices[0].high_bound.expr) {
            Symbol *bsym = ct->array.indices[0].high_bound.expr->symbol;
            for (uint32_t d = 0; d < t->record.discriminant_count; d++) {
              Component_Info *dc = &t->record.components[d];
              if (bsym and Slice_Equal_Ignore_Case (dc->name, bsym->name)) {
                disc_offset = dc->byte_offset;
                disc_llvm = Type_To_Llvm (dc->component_type);
                if (not disc_llvm) disc_llvm = "i32";
                break;
              }
            }
          }
          uint32_t dp = Emit_Temp (), dv = Emit_Temp ();
          Emit ("  %%t%u = getelementptr i8, ptr %%0, i64 %u\n", dp, disc_offset);
          Emit ("  %%t%u = load %s, ptr %%t%u\n", dv, disc_llvm, dp);
          uint32_t cnt = Emit_Temp ();
          Emit ("  %%t%u = sub %s %%t%u, %lld\n", cnt, disc_llvm, dv, (long long)(low_val - 1));
          if (elem_size > 1) {
            uint32_t mul = Emit_Temp ();
            Emit ("  %%t%u = mul %s %%t%u, %u\n", mul, disc_llvm, cnt, elem_size);
            cnt = mul;
          }
          uint32_t is_neg = Emit_Temp (), clamped = Emit_Temp (), sz64 = Emit_Temp ();
          Emit ("  %%t%u = icmp slt %s %%t%u, 0\n", is_neg, disc_llvm, cnt);
          Emit ("  %%t%u = select i1 %%t%u, %s 0, %s %%t%u\n", clamped, is_neg, disc_llvm, disc_llvm, cnt);
          Emit ("  %%t%u = sext %s %%t%u to i64\n", sz64, disc_llvm, clamped);
          uint32_t mc_result = Emit_Temp ();
          uint32_t mc_cmp = Emit_Temp ();
          Emit ("  %%t%u = call i32 @memcmp (ptr %%t%u, ptr %%t%u, i64 %%t%u)\n",
             mc_result, left_gep, right_gep, sz64);
          Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", mc_cmp, mc_result);
          cmp = mc_cmp;

        // Constrained array with static bounds - use array equality
        } else if (Type_Is_Constrained_Array (ct)) {
          cmp = Generate_Array_Equality (left_gep, right_gep, ct);

        // Nested record - recurse
        } else if (Type_Is_Record (ct)) {
          cmp = Generate_Record_Equality (left_gep, right_gep, ct);

        // ACCESS to unconstrained array - compare fat pointer identity
        } else if (is_fat_ptr_access) {
          const char *acc_eqf_bt = Array_Bound_Llvm_Type (ct->access.designated_type);
          uint32_t left_val = Emit_Load_Fat_Pointer_From_Temp (left_gep, acc_eqf_bt);
          uint32_t right_val = Emit_Load_Fat_Pointer_From_Temp (right_gep, acc_eqf_bt);
          cmp = Emit_Fat_Pointer_Compare (left_val, right_val, acc_eqf_bt);

        // Scalar type - load and compare
        } else {
          uint32_t left_val = Emit_Temp ();
          uint32_t right_val = Emit_Temp ();
          Emit ("  %%t%u = load %s, ptr %%t%u\n",
             left_val, comp_llvm_type, left_gep);
          Emit ("  %%t%u = load %s, ptr %%t%u\n",
             right_val, comp_llvm_type, right_gep);
          cmp = Emit_Temp ();
          if (Type_Is_Float_Representation (ct)) {
            Emit ("  %%t%u = fcmp oeq %s %%t%u, %%t%u\n",
               cmp, comp_llvm_type, left_val, right_val);
          } else {
            Emit ("  %%t%u = icmp eq %s %%t%u, %%t%u\n",
               cmp, comp_llvm_type, left_val, right_val);
          }
        }

        // AND with previous results
        if (i == 0) {
          result = cmp;
        } else {
          uint32_t and_result = Emit_Temp ();
          Emit ("  %%t%u = and i1 %%t%u, %%t%u\n",
             and_result, result, cmp);
          result = and_result;
        }
      }
      Emit ("  ret i1 %%t%u\n", result);
    }
  } else if (Type_Is_Array_Like (t)) {

    // Constrained array with static bounds - use memcmp
    if (t->array.is_constrained and not Type_Has_Dynamic_Bounds (t)) {
      int128_t count = Array_Element_Count (t);
      uint32_t elem_size = t->array.element_type ?
                 t->array.element_type->size : 4;
      int64_t total_size = count * elem_size;
      uint32_t result = Emit_Temp ();
      uint32_t cmp = Emit_Temp ();
      Emit ("  %%t%u = call i32 @memcmp (ptr %%0, ptr %%1, i64 %lld)\n",
         result, (long long)total_size);
      Emit ("  %%t%u = icmp eq i32 %%t%u, 0\n", cmp, result);
      Emit ("  ret i1 %%t%u\n", cmp);

    // Unconstrained array equality (per RM 4.5.2):                                                 
    // Fat pointer layout: { ptr data, ptr bounds }                                                 
    // where bounds > { bt low, bt high }                                                           
    // Compare lengths first, then data if lengths match.                                           
    //                                                                                              
    } else {
      uint32_t elem_size = t->array.element_type ?
                 t->array.element_type->size : 1;
      const char *eq_bst = Bounds_Type_For (eq_bt);

      // Extract data pointers (field 0)
      Emit ("  %%left_data = extractvalue " FAT_PTR_TYPE " %%0, 0\n");
      Emit ("  %%right_data = extractvalue " FAT_PTR_TYPE " %%1, 0\n");

      // Extract bounds pointers (field 1)
      Emit ("  %%left_bptr = extractvalue " FAT_PTR_TYPE " %%0, 1\n");
      Emit ("  %%right_bptr = extractvalue " FAT_PTR_TYPE " %%1, 1\n");

      // Load bounds from left fat pointer in native bt
      Emit ("  %%left_lo_gep = getelementptr %s, ptr %%left_bptr, i32 0, i32 0\n", eq_bst);
      Emit ("  %%left_low = load %s, ptr %%left_lo_gep\n", eq_bt);
      Emit ("  %%left_hi_gep = getelementptr %s, ptr %%left_bptr, i32 0, i32 1\n", eq_bst);
      Emit ("  %%left_high = load %s, ptr %%left_hi_gep\n", eq_bt);
      Emit ("  %%left_len = sub %s %%left_high, %%left_low\n", eq_bt);
      Emit ("  %%left_len1 = add %s %%left_len, 1\n", eq_bt);

      // Load bounds from right fat pointer in native bt
      Emit ("  %%right_lo_gep = getelementptr %s, ptr %%right_bptr, i32 0, i32 0\n", eq_bst);
      Emit ("  %%right_low = load %s, ptr %%right_lo_gep\n", eq_bt);
      Emit ("  %%right_hi_gep = getelementptr %s, ptr %%right_bptr, i32 0, i32 1\n", eq_bst);
      Emit ("  %%right_high = load %s, ptr %%right_hi_gep\n", eq_bt);
      Emit ("  %%right_len = sub %s %%right_high, %%right_low\n", eq_bt);
      Emit ("  %%right_len1 = add %s %%right_len, 1\n", eq_bt);

      // Compare lengths in native bt
      Emit ("  %%len_eq = icmp eq %s %%left_len1, %%right_len1\n", eq_bt);

      // Convert to INTEGER width for memcmp byte size computation
      const char *iat_eq = Integer_Arith_Type ();
      if (strcmp (eq_bt, iat_eq) != 0) {
        int eq_bits = Type_Bits (eq_bt), iat_bits = Type_Bits (iat_eq);
        const char *conv_op = (iat_bits > eq_bits) ? "sext" : "trunc";
        Emit ("  %%left_len1_w = %s %s %%left_len1 to %s\n", conv_op, eq_bt, iat_eq);
        Emit ("  %%byte_size = mul %s %%left_len1_w, %u\n", iat_eq, elem_size);
      } else {
        Emit ("  %%byte_size = mul %s %%left_len1, %u\n", iat_eq, elem_size);
      }

      // memcmp requires i64 for size_t - extend if needed
      if (strcmp (iat_eq, "i64") != 0) {
        Emit ("  %%byte_size64 = sext %s %%byte_size to i64\n", iat_eq);
        Emit ("  %%memcmp_res = call i32 @memcmp (ptr %%left_data, ptr %%right_data, i64 %%byte_size64)\n");
      } else {
        Emit ("  %%memcmp_res = call i32 @memcmp (ptr %%left_data, ptr %%right_data, i64 %%byte_size)\n");
      }
      Emit ("  %%data_eq = icmp eq i32 %%memcmp_res, 0\n");

      // Result: lengths match AND data matches
      Emit ("  %%result = and i1 %%len_eq, %%data_eq\n");
      Emit ("  ret i1 %%result\n");
    }

  // Unknown composite type - conservatively return false (not equal).
  // This is safer than the previous "always equal" default.
  } else {
    fprintf (stderr, "warning: equality for unknown composite type '%.*s', assuming not equal\n",
        (int)t->name.length, t->name.data);
    Emit ("  ret i1 0\n");
  }
  Emit ("}\n");
  cg->temp_id = saved_temp;  // Restore temp counter
}
void Generate_Implicit_Operators (void) {

  // Generate equality functions for all frozen composite types
  for (uint32_t i = 0; i < Frozen_Composite_Count; i++) {
    Generate_Type_Equality_Function (Frozen_Composite_Types[i]);
  }
}

// Generate global constants for exception identities
void Generate_Exception_Globals (void) {

  // Generate globals for all registered exceptions (from declarations).                            
  // Dedup by mangled name to avoid redefinition when the same exception                            
  // is declared in multiple WITH'd packages.                                                       
  //                                                                                                
  if (Exception_Symbol_Count > 0) {
    Emit ("; Exception identity globals\n");
    char exc_emitted[256][256];
    uint32_t exc_emitted_count = 0;
    for (uint32_t i = 0; i < Exception_Symbol_Count; i++) {
      Symbol *sym = Exception_Symbols[i];

      // Get mangled name to check for duplicates
      FILE *real_out = cg->output;
      char buf[256];
      FILE *mem = fmemopen (buf, sizeof (buf) - 1, "w");
      cg->output = mem;
      Emit_Symbol_Name (sym);
      fflush (mem);
      long len = ftell (mem);
      fclose (mem);
      buf[len] = '\0';
      cg->output = real_out;
      bool dup = false;
      for (uint32_t j = 0; j < exc_emitted_count; j++) {
        if (strcmp (buf, exc_emitted[j]) == 0) { dup = true; break; }
      }
      if (dup) continue;
      if (exc_emitted_count < 256) {
        strncpy (exc_emitted[exc_emitted_count], buf, 255);
        exc_emitted[exc_emitted_count][255] = '\0';
        exc_emitted_count++;
      }
      Emit ("@__exc.%s = private constant i8 0\n", buf);
    }
    Emit ("\n");
  }

  // Also emit globals for all referenced exception names that weren't                              
  // already emitted above (e.g., instance-prefixed exceptions from                                 
  // generic instantiations like SEQ_IO.NAME_ERROR).                                                
  // Build set of already-emitted names for dedup                                                   
  //                                                                                                
  if (cg->exc_ref_count > 0) {
    char emitted_names[256][256];
    uint32_t emitted_count = 0;
    for (uint32_t i = 0; i < Exception_Symbol_Count and emitted_count < 256; i++) {
      FILE *real_out = cg->output;
      char buf[256];
      FILE *mem = fmemopen (buf, sizeof (buf) - 1, "w");
      cg->output = mem;
      Emit_Symbol_Name (Exception_Symbols[i]);
      fflush (mem);
      long len = ftell (mem);
      fclose (mem);
      buf[len] = '\0';
      cg->output = real_out;
      strncpy (emitted_names[emitted_count], buf, 255);
      emitted_names[emitted_count][255] = '\0';
      emitted_count++;
    }

    // Also include standard exception names
    static const char *std_exc[] = {
      "constraint_error", "numeric_error", "program_error",
      "storage_error", "tasking_error", NULL
    };
    for (uint32_t i = 0; i < cg->exc_ref_count; i++) {
      const char *name = cg->exc_refs[i];
      bool already = false;

      // Check against emitted declaration names
      for (uint32_t j = 0; j < emitted_count; j++) {
        if (strcmp (name, emitted_names[j]) == 0) { already = true; break; }
      }

      // Check against standard exceptions
      if (not already) {
        for (int j = 0; std_exc[j]; j++) {
          if (strcmp (name, std_exc[j]) == 0) { already = true; break; }
        }
      }

      // Also check against previously emitted exc_refs (dedup within the list)
      if (not already) {
        for (uint32_t j = 0; j < i; j++) {
          if (strcmp (name, cg->exc_refs[j]) == 0) { already = true; break; }
        }
      }
      if (not already) {
        Emit ("@__exc.%s = private constant i8 0\n", name);
      }
    }
  }
}

// Generate extern declarations for all loaded package specs
void Generate_Extern_Declarations (Syntax_Node *node) {
  if (not node or not node->compilation_unit.context) return;
  Syntax_Node *ctx = node->compilation_unit.context;
  bool emitted_header = false;

  // Iterate through WITH'd packages
  for (uint32_t i = 0; i < ctx->context.with_clauses.count; i++) {
    Syntax_Node *with_node = ctx->context.with_clauses.items[i];
    for (uint32_t j = 0; j < with_node->use_clause.names.count; j++) {
      Syntax_Node *pkg_name = with_node->use_clause.names.items[j];
      if (pkg_name->kind != NK_IDENTIFIER) continue;
      Symbol *pkg_sym = pkg_name->symbol;
      if (not pkg_sym or pkg_sym->kind != SYMBOL_PACKAGE) continue;

      // Skip extern declarations for packages whose bodies will be code-generated.
      // Those symbols will be defined, not external.
      if (Body_Already_Loaded (pkg_sym->name)) continue;

      // For ALI-loaded packages without declaration, use exported[] array
      if (not pkg_sym->declaration and pkg_sym->exported_count > 0) {
        for (uint32_t k = 0; k < pkg_sym->exported_count; k++) {
          Symbol *sym = pkg_sym->exported[k];
          if (not sym) continue;
          if (sym->kind == SYMBOL_FUNCTION or sym->kind == SYMBOL_PROCEDURE) {
            if (not emitted_header) {
              Emit ("\n; External Ada subprogram declarations\n");
              emitted_header = true;
            }
            Emit_Extern_Subprogram (sym);
          } else if (sym->kind == SYMBOL_VARIABLE or sym->kind == SYMBOL_CONSTANT) {
            if (not sym->is_named_number and not sym->extern_emitted) {
              sym->extern_emitted = true;
              Type_Info *ty = sym->type;
              const char *type_str = Type_To_Llvm (ty);
              bool is_string = (not Type_Is_Constrained_Array (ty) and Type_Is_String (ty)) or
                (Type_Is_Unconstrained_Array (ty) and
                 Type_Is_Character (ty->array.element_type));
              if (is_string) type_str = FAT_PTR_TYPE;
              Emit ("@");
              Emit_Symbol_Name (sym);
              Emit (" = external global %s\n", type_str);
            }
          }
        }
        continue;  // Done with this package
      }
      if (not pkg_sym->declaration) continue;
      Syntax_Node *pkg_decl = pkg_sym->declaration;
      if (pkg_decl->kind != NK_PACKAGE_SPEC) continue;

      // Emit extern for each subprogram and object in the package
      for (uint32_t k = 0; k < pkg_decl->package_spec.visible_decls.count; k++) {
        Syntax_Node *decl = pkg_decl->package_spec.visible_decls.items[k];
        if (not decl) continue;
        if (decl->kind == NK_PROCEDURE_SPEC or decl->kind == NK_FUNCTION_SPEC) {
          if (not emitted_header) {
            Emit ("\n; External Ada subprogram declarations\n");
            emitted_header = true;
          }
          Emit_Extern_Subprogram (decl->symbol);
        }

        // Emit extern for object declarations (constants/variables)
        if (decl->kind == NK_OBJECT_DECL) {
          for (uint32_t m = 0; m < decl->object_decl.names.count; m++) {
            Syntax_Node *name = decl->object_decl.names.items[m];
            Symbol *sym = name ? name->symbol : NULL;
            if (not sym or sym->is_named_number) continue;
            if (sym->extern_emitted) continue;
            sym->extern_emitted = true;
            Type_Info *ty = sym->type;
            const char *type_str = Type_To_Llvm (ty);

            // String constants use fat pointer type
            bool is_string = (not Type_Is_Constrained_Array (ty) and Type_Is_String (ty)) or
              (Type_Is_Unconstrained_Array (ty) and
               Type_Is_Character (ty->array.element_type));
            if (is_string) type_str = FAT_PTR_TYPE;
            Emit ("@");
            Emit_Symbol_Name (sym);
            Emit (" = external global %s\n", type_str);
          }
        }
      }
    }
  }
  if (emitted_header) Emit ("\n");
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.7 Compilation Unit Code Generation                                                           
//                                                                                                  
// A compilation unit is the quantum of separate compilation.                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Generate_Compilation_Unit (Syntax_Node *node) {
  if (not node) return;

  // Generate LLVM module header (only once per file)
  if (not cg->header_emitted) {
  Emit ("; Ada83 Compiler Output\n");
  Emit ("target datalayout = \"e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128\"\n");
  Emit ("target triple = \"x86_64-pc-linux-gnu\"\n\n");

  // External C library and LLVM intrinsic declarations
  Emit ("; External declarations\n");
  Emit ("declare i32 @memcmp (ptr, ptr, i64)\n");
  Emit ("declare i32 @strncasecmp (ptr, ptr, i64)\n");
  Emit ("declare i32 @setjmp(ptr) returns_twice\n");
  Emit ("declare void @longjmp(ptr, i32) noreturn\n");
  Emit ("declare void @exit (i32)\n");
  Emit ("declare ptr @malloc (i64)\n");
  Emit ("declare ptr @realloc (ptr, i64)\n");
  Emit ("declare void @free (ptr)\n");
  Emit ("declare i32 @usleep(i32)\n");
  Emit ("declare i32 @pthread_create(ptr, ptr, ptr, ptr)\n");
  Emit ("declare i32 @pthread_join(ptr, ptr)\n");
  Emit ("declare void @pthread_exit (ptr)\n");
  Emit ("declare i32 @printf (ptr, ...)\n");
  Emit ("declare i32 @putchar(i32)\n");
  Emit ("declare i32 @getchar()\n");

  // C file I/O (for TEXT_IO pragma Import)
  Emit ("declare ptr @fopen (ptr, ptr)\n");
  Emit ("declare i32 @fclose (ptr)\n");
  Emit ("declare i32 @fputc(i32, ptr)\n");
  Emit ("declare i32 @fgetc(ptr)\n");
  Emit ("declare i32 @ungetc(i32, ptr)\n");
  Emit ("declare i32 @feof(ptr)\n");
  Emit ("declare i32 @fflush(ptr)\n");
  Emit ("declare i32 @remove(ptr)\n");
  Emit ("declare i64 @ftell (ptr)\n");
  Emit ("declare i32 @fseek (ptr, i64, i32)\n");
  Emit ("declare void @llvm.memcpy.p0.p0.i64(ptr, ptr, i64, i1)\n");
  Emit ("declare void @llvm.memset.p0.i64(ptr, i8, i64, i1)\n");
  Emit ("declare double @llvm.pow.f64(double, double)\n\n");

  // LLVM overflow intrinsics for GNAT-style arithmetic overflow checks (RM 4.5).
  // Signed add/sub/mul with overflow detection at each standard width.
  Emit ("; Signed overflow intrinsics\n");
  Emit ("declare {i8,  i1} @llvm.sadd.with.overflow.i8(i8, i8)\n");
  Emit ("declare {i8,  i1} @llvm.ssub.with.overflow.i8(i8, i8)\n");
  Emit ("declare {i8,  i1} @llvm.smul.with.overflow.i8(i8, i8)\n");
  Emit ("declare {i16, i1} @llvm.sadd.with.overflow.i16(i16, i16)\n");
  Emit ("declare {i16, i1} @llvm.ssub.with.overflow.i16(i16, i16)\n");
  Emit ("declare {i16, i1} @llvm.smul.with.overflow.i16(i16, i16)\n");
  Emit ("declare {i32, i1} @llvm.sadd.with.overflow.i32(i32, i32)\n");
  Emit ("declare {i32, i1} @llvm.ssub.with.overflow.i32(i32, i32)\n");
  Emit ("declare {i32, i1} @llvm.smul.with.overflow.i32(i32, i32)\n");
  Emit ("declare {i64, i1} @llvm.sadd.with.overflow.i64(i64, i64)\n");
  Emit ("declare {i64, i1} @llvm.ssub.with.overflow.i64(i64, i64)\n");
  Emit ("declare {i64, i1} @llvm.smul.with.overflow.i64(i64, i64)\n");
  Emit ("declare {i128, i1} @llvm.sadd.with.overflow.i128(i128, i128)\n");
  Emit ("declare {i128, i1} @llvm.ssub.with.overflow.i128(i128, i128)\n");
  Emit ("declare {i128, i1} @llvm.smul.with.overflow.i128(i128, i128)\n\n");

  // Integer'VALUE - parse string to integer.
  // Bound type derived from STRING's index type via type system.
  const char *rts_sbt = String_Bound_Type ();
  const char *iat = Integer_Arith_Type ();

  // Integer'VALUE helper - Ada RM 3.5.5: full parsing with based literals.                         
  // Handles: [spaces] [sign] decimal_literal | based_literal [spaces]                              
  // decimal_literal ::= digit {[_] digit} [exponent]                                               
  // based_literal   ::= base # hex_digit {[_] hex_digit} # [exponent]                              
  // exponent        ::= E [+] digit {[_] digit}                                                    
  // Raises CONSTRAINT_ERROR on any malformed input.                                                
  //                                                                                                
  Emit ("; Integer'VALUE helper (based literals + validation)\n");
  Emit ("define linkonce_odr %s @__ada_integer_value(" FAT_PTR_TYPE " %%str) {\n", iat);
  Emit ("entry:\n");
  Emit_Fat_Pointer_Extractvalue_Named ("str", "data", "low_bt", "high_bt", rts_sbt);
  Emit ("  %%len = sub %s %%high_bt, %%low_bt\n", rts_sbt);
  Emit ("  %%len1 = add %s %%len, 1\n", rts_sbt);

  // Copy to NUL-terminated C buffer for sscanf-like processing in C helper
  Emit ("  %%len64 = sext %s %%len1 to i64\n", rts_sbt);
  Emit ("  %%buf = alloca i8, i64 %%len64\n");
  Emit ("  call void @llvm.memcpy.p0.p0.i64(ptr %%buf, ptr %%data, i64 %%len64, i1 false)\n");
  Emit ("  %%result = call %s @__ada_parse_integer(ptr %%buf, %s %%len1)\n", iat, rts_sbt);
  Emit ("  ret %s %%result\n", iat);
  Emit ("}\n\n");

  // The actual parser is a C-callable function emitted as LLVM IR.                                 
  // It handles spaces, sign, decimal, based literals, exponent, underscores,                       
  // and raises CONSTRAINT_ERROR on any validation failure.                                         
  //                                                                                                
  Emit ("define linkonce_odr %s @__ada_parse_integer(ptr %%buf, %s %%len) {\n", iat, rts_sbt);
  Emit ("entry:\n");

  // Phase 1: skip leading spaces (only ASCII 32; HT/other > error)
  Emit ("  br label %%skip_lead\n");
  Emit ("skip_lead:\n");
  Emit ("  %%sl_i = phi %s [ 0, %%entry ], [ %%sl_ni, %%sl_cont ]\n", rts_sbt);
  Emit ("  %%sl_done = icmp sge %s %%sl_i, %%len\n", rts_sbt);
  Emit ("  br i1 %%sl_done, label %%bad, label %%sl_body\n");
  Emit ("sl_body:\n");
  Emit ("  %%sl_p = getelementptr i8, ptr %%buf, %s %%sl_i\n", rts_sbt);
  Emit ("  %%sl_c = load i8, ptr %%sl_p\n");
  Emit ("  %%sl_sp = icmp eq i8 %%sl_c, 32\n");
  Emit ("  br i1 %%sl_sp, label %%sl_cont, label %%got_start\n");
  Emit ("sl_cont:\n");
  Emit ("  %%sl_ni = add %s %%sl_i, 1\n", rts_sbt);
  Emit ("  br label %%skip_lead\n");

  // Phase 2: find end (skip trailing spaces; HT/other at end > error)
  Emit ("got_start:\n");
  Emit ("  %%end_init = sub %s %%len, 1\n", rts_sbt);
  Emit ("  br label %%skip_trail\n");
  Emit ("skip_trail:\n");
  Emit ("  %%st_i = phi %s [ %%end_init, %%got_start ], [ %%st_ni, %%st_cont ]\n", rts_sbt);
  Emit ("  %%st_le = icmp sle %s %%st_i, %%sl_i\n", rts_sbt);
  Emit ("  br i1 %%st_le, label %%bad, label %%st_body\n");
  Emit ("st_body:\n");
  Emit ("  %%st_p = getelementptr i8, ptr %%buf, %s %%st_i\n", rts_sbt);
  Emit ("  %%st_c = load i8, ptr %%st_p\n");
  Emit ("  %%st_sp = icmp eq i8 %%st_c, 32\n");
  Emit ("  br i1 %%st_sp, label %%st_cont, label %%trimmed\n");
  Emit ("st_cont:\n");
  Emit ("  %%st_ni = sub %s %%st_i, 1\n", rts_sbt);
  Emit ("  br label %%skip_trail\n");

  // Now sl_i..st_i is the trimmed content (inclusive)
  Emit ("trimmed:\n");
  Emit ("  %%end = add %s %%st_i, 1\n", rts_sbt);  // exclusive end

  // Phase 3: check for sign
  Emit ("  %%s_p = getelementptr i8, ptr %%buf, %s %%sl_i\n", rts_sbt);
  Emit ("  %%s_c = load i8, ptr %%s_p\n");
  Emit ("  %%is_neg = icmp eq i8 %%s_c, 45\n");
  Emit ("  %%is_pos = icmp eq i8 %%s_c, 43\n");
  Emit ("  %%has_sign = or i1 %%is_neg, %%is_pos\n");
  Emit ("  %%dig_start_s = add %s %%sl_i, 1\n", rts_sbt);
  Emit ("  %%dig_start = select i1 %%has_sign, %s %%dig_start_s, %s %%sl_i\n", rts_sbt, rts_sbt);

  // Phase 4: Parse digit sequence, looking for '#' or ':' (based literal delimiter)
  Emit ("  br label %%d1loop\n");
  Emit ("d1loop:\n");
  Emit ("  %%d1_i = phi %s [ %%dig_start, %%trimmed ], [ %%d1_ni, %%d1_cont ]\n", rts_sbt);
  Emit ("  %%d1_val = phi %s [ 0, %%trimmed ], [ %%d1_nv, %%d1_cont ]\n", iat);
  Emit ("  %%d1_last_under = phi i1 [ false, %%trimmed ], [ %%d1_is_under, %%d1_cont ]\n");
  Emit ("  %%d1_done = icmp sge %s %%d1_i, %%end\n", rts_sbt);
  Emit ("  br i1 %%d1_done, label %%apply_sign, label %%d1_body\n");
  Emit ("d1_body:\n");
  Emit ("  %%d1_p = getelementptr i8, ptr %%buf, %s %%d1_i\n", rts_sbt);
  Emit ("  %%d1_ch = load i8, ptr %%d1_p\n");

  // Check for '#' or ':' > based literal
  Emit ("  %%d1_sharp = icmp eq i8 %%d1_ch, 35\n");
  Emit ("  %%d1_colon = icmp eq i8 %%d1_ch, 58\n");
  Emit ("  %%d1_delim = or i1 %%d1_sharp, %%d1_colon\n");
  Emit ("  br i1 %%d1_delim, label %%start_based, label %%d1_check_exp\n");

  // Check for E/e > exponent
  Emit ("d1_check_exp:\n");
  Emit ("  %%d1_isE = icmp eq i8 %%d1_ch, 69\n");
  Emit ("  %%d1_ise = icmp eq i8 %%d1_ch, 101\n");
  Emit ("  %%d1_exp = or i1 %%d1_isE, %%d1_ise\n");
  Emit ("  br i1 %%d1_exp, label %%exp_start_d, label %%d1_check_under\n");

  // Check for underscore
  Emit ("d1_check_under:\n");
  Emit ("  %%d1_is_under = icmp eq i8 %%d1_ch, 95\n");
  Emit ("  br i1 %%d1_is_under, label %%d1_under, label %%d1_digit\n");
  Emit ("d1_under:\n");

  // Consecutive underscores or leading underscore > error
  Emit ("  %%d1_consec = and i1 %%d1_last_under, %%d1_is_under\n");
  Emit ("  %%d1_leading = icmp eq %s %%d1_i, %%dig_start\n", rts_sbt);
  Emit ("  %%d1_bad_u = or i1 %%d1_consec, %%d1_leading\n");
  Emit ("  br i1 %%d1_bad_u, label %%bad, label %%d1_cont\n");

  // Parse decimal digit
  Emit ("d1_digit:\n");
  Emit ("  %%d1_ge0 = icmp uge i8 %%d1_ch, 48\n");
  Emit ("  %%d1_le9 = icmp ule i8 %%d1_ch, 57\n");
  Emit ("  %%d1_isdig = and i1 %%d1_ge0, %%d1_le9\n");
  Emit ("  br i1 %%d1_isdig, label %%d1_accum, label %%bad\n");
  Emit ("d1_accum:\n");
  Emit ("  %%d1_d = sub i8 %%d1_ch, 48\n");
  Emit ("  %%d1_dw = zext i8 %%d1_d to %s\n", iat);
  Emit ("  %%d1_mul = mul %s %%d1_val, 10\n", iat);
  Emit ("  %%d1_nv_a = add %s %%d1_mul, %%d1_dw\n", iat);
  Emit ("  br label %%d1_cont\n");
  Emit ("d1_cont:\n");
  Emit ("  %%d1_nv = phi %s [ %%d1_nv_a, %%d1_accum ], [ %%d1_val, %%d1_under ]\n", iat);
  Emit ("  %%d1_ni = add %s %%d1_i, 1\n", rts_sbt);
  Emit ("  br label %%d1loop\n");

  // Based literal: d1_val is the base, parse digits in that base until '#'/':'
  Emit ("start_based:\n");
  Emit ("  %%base = add %s %%d1_val, 0\n", iat);  // copy

  // Validate base: 2..16
  Emit ("  %%base_lo = icmp slt %s %%base, 2\n", iat);
  Emit ("  %%base_hi = icmp sgt %s %%base, 16\n", iat);
  Emit ("  %%base_bad = or i1 %%base_lo, %%base_hi\n");
  Emit ("  br i1 %%base_bad, label %%bad, label %%based_ok\n");
  Emit ("based_ok:\n");
  Emit ("  %%b_delim = add i8 %%d1_ch, 0\n");  // remember delimiter
  Emit ("  %%b_start = add %s %%d1_i, 1\n", rts_sbt);
  Emit ("  br label %%bloop\n");
  Emit ("bloop:\n");
  Emit ("  %%b_i = phi %s [ %%b_start, %%based_ok ], [ %%b_ni, %%b_cont ]\n", rts_sbt);
  Emit ("  %%b_val = phi %s [ 0, %%based_ok ], [ %%b_nv, %%b_cont ]\n", iat);
  Emit ("  %%b_last_under = phi i1 [ false, %%based_ok ], [ %%b_is_under, %%b_cont ]\n");
  Emit ("  %%b_done = icmp sge %s %%b_i, %%end\n", rts_sbt);
  Emit ("  br i1 %%b_done, label %%bad, label %%b_body\n");  // missing closing delimiter
  Emit ("b_body:\n");
  Emit ("  %%b_p = getelementptr i8, ptr %%buf, %s %%b_i\n", rts_sbt);
  Emit ("  %%b_ch = load i8, ptr %%b_p\n");

  // Check for closing delimiter
  Emit ("  %%b_close = icmp eq i8 %%b_ch, %%b_delim\n");
  Emit ("  br i1 %%b_close, label %%based_done, label %%b_check_under\n");

  // Check underscore
  Emit ("b_check_under:\n");
  Emit ("  %%b_is_under = icmp eq i8 %%b_ch, 95\n");
  Emit ("  br i1 %%b_is_under, label %%b_under, label %%b_hexdig\n");
  Emit ("b_under:\n");
  Emit ("  %%b_consec = and i1 %%b_last_under, %%b_is_under\n");
  Emit ("  %%b_leading_u = icmp eq %s %%b_i, %%b_start\n", rts_sbt);
  Emit ("  %%b_bad_u = or i1 %%b_consec, %%b_leading_u\n");
  Emit ("  br i1 %%b_bad_u, label %%bad, label %%b_cont\n");

  // Parse hex digit (0-9, A-F, a-f)
  Emit ("b_hexdig:\n");
  Emit ("  %%b_ge0 = icmp uge i8 %%b_ch, 48\n");
  Emit ("  %%b_le9 = icmp ule i8 %%b_ch, 57\n");
  Emit ("  %%b_09 = and i1 %%b_ge0, %%b_le9\n");
  Emit ("  br i1 %%b_09, label %%b_d09, label %%b_check_af\n");
  Emit ("b_d09:\n");
  Emit ("  %%b_d09v = sub i8 %%b_ch, 48\n");
  Emit ("  br label %%b_accum\n");
  Emit ("b_check_af:\n");
  Emit ("  %%b_geA = icmp uge i8 %%b_ch, 65\n");
  Emit ("  %%b_leF = icmp ule i8 %%b_ch, 70\n");
  Emit ("  %%b_AF = and i1 %%b_geA, %%b_leF\n");
  Emit ("  br i1 %%b_AF, label %%b_dAF, label %%b_check_af2\n");
  Emit ("b_dAF:\n");
  Emit ("  %%b_dAFv = sub i8 %%b_ch, 55\n");  // 'A'-10 = 55
  Emit ("  br label %%b_accum\n");
  Emit ("b_check_af2:\n");
  Emit ("  %%b_gea = icmp uge i8 %%b_ch, 97\n");
  Emit ("  %%b_lef = icmp ule i8 %%b_ch, 102\n");
  Emit ("  %%b_af = and i1 %%b_gea, %%b_lef\n");
  Emit ("  br i1 %%b_af, label %%b_daf, label %%bad\n");
  Emit ("b_daf:\n");
  Emit ("  %%b_dafv = sub i8 %%b_ch, 87\n");  // 'a'-10 = 87
  Emit ("  br label %%b_accum\n");
  Emit ("b_accum:\n");
  Emit ("  %%b_digit = phi i8 [ %%b_d09v, %%b_d09 ], [ %%b_dAFv, %%b_dAF ], [ %%b_dafv, %%b_daf ]\n");
  Emit ("  %%b_dw = zext i8 %%b_digit to %s\n", iat);

  // Check digit < base
  Emit ("  %%b_inrange = icmp slt %s %%b_dw, %%base\n", iat);
  Emit ("  br i1 %%b_inrange, label %%b_ok_dig, label %%bad\n");
  Emit ("b_ok_dig:\n");
  Emit ("  %%b_mul = mul %s %%b_val, %%base\n", iat);
  Emit ("  %%b_nv_a = add %s %%b_mul, %%b_dw\n", iat);
  Emit ("  br label %%b_cont\n");
  Emit ("b_cont:\n");
  Emit ("  %%b_nv = phi %s [ %%b_nv_a, %%b_ok_dig ], [ %%b_val, %%b_under ]\n", iat);
  Emit ("  %%b_ni = add %s %%b_i, 1\n", rts_sbt);
  Emit ("  br label %%bloop\n");

  // After closing '#'/':' - check for optional exponent
  Emit ("based_done:\n");
  Emit ("  %%bd_ni = add %s %%b_i, 1\n", rts_sbt);
  Emit ("  %%bd_more = icmp slt %s %%bd_ni, %%end\n", rts_sbt);
  Emit ("  br i1 %%bd_more, label %%bd_check_exp, label %%apply_sign_based\n");
  Emit ("bd_check_exp:\n");
  Emit ("  %%bd_ep = getelementptr i8, ptr %%buf, %s %%bd_ni\n", rts_sbt);
  Emit ("  %%bd_ec = load i8, ptr %%bd_ep\n");
  Emit ("  %%bd_isE = icmp eq i8 %%bd_ec, 69\n");
  Emit ("  %%bd_ise = icmp eq i8 %%bd_ec, 101\n");
  Emit ("  %%bd_exp = or i1 %%bd_isE, %%bd_ise\n");
  Emit ("  br i1 %%bd_exp, label %%exp_start_b, label %%bad\n");  // trailing junk

  // Exponent for based literal: exp_start_b, base is still %%base
  Emit ("exp_start_b:\n");
  Emit ("  %%eb_s = add %s %%bd_ni, 1\n", rts_sbt);
  Emit ("  br label %%ebloop\n");
  Emit ("ebloop:\n");
  Emit ("  %%eb_i = phi %s [ %%eb_s, %%exp_start_b ], [ %%eb_ni, %%eb_cont ]\n", rts_sbt);
  Emit ("  %%eb_v = phi %s [ 0, %%exp_start_b ], [ %%eb_nv, %%eb_cont ]\n", iat);
  Emit ("  %%eb_done = icmp sge %s %%eb_i, %%end\n", rts_sbt);
  Emit ("  br i1 %%eb_done, label %%apply_exp_based, label %%eb_body\n");
  Emit ("eb_body:\n");
  Emit ("  %%eb_p = getelementptr i8, ptr %%buf, %s %%eb_i\n", rts_sbt);
  Emit ("  %%eb_c = load i8, ptr %%eb_p\n");
  Emit ("  %%eb_ge0 = icmp uge i8 %%eb_c, 48\n");
  Emit ("  %%eb_le9 = icmp ule i8 %%eb_c, 57\n");
  Emit ("  %%eb_isdig = and i1 %%eb_ge0, %%eb_le9\n");
  Emit ("  br i1 %%eb_isdig, label %%eb_accum, label %%eb_skip\n");
  Emit ("eb_accum:\n");
  Emit ("  %%eb_d = sub i8 %%eb_c, 48\n");
  Emit ("  %%eb_dw = zext i8 %%eb_d to %s\n", iat);
  Emit ("  %%eb_mul = mul %s %%eb_v, 10\n", iat);
  Emit ("  %%eb_nv_a = add %s %%eb_mul, %%eb_dw\n", iat);
  Emit ("  br label %%eb_cont\n");
  Emit ("eb_skip:\n");  // skip + sign
  Emit ("  br label %%eb_cont\n");
  Emit ("eb_cont:\n");
  Emit ("  %%eb_nv = phi %s [ %%eb_nv_a, %%eb_accum ], [ %%eb_v, %%eb_skip ]\n", iat);
  Emit ("  %%eb_ni = add %s %%eb_i, 1\n", rts_sbt);
  Emit ("  br label %%ebloop\n");

  // Apply exponent for based literal: result = b_val * base^exp
  Emit ("apply_exp_based:\n");
  Emit ("  br label %%bpow_loop\n");
  Emit ("bpow_loop:\n");
  Emit ("  %%bp_acc = phi %s [ 1, %%apply_exp_based ], [ %%bp_next, %%bpow_body ]\n", iat);
  Emit ("  %%bp_rem = phi %s [ %%eb_v, %%apply_exp_based ], [ %%bp_dec, %%bpow_body ]\n", iat);
  Emit ("  %%bp_done = icmp sle %s %%bp_rem, 0\n", iat);
  Emit ("  br i1 %%bp_done, label %%apply_sign_based_e, label %%bpow_body\n");
  Emit ("bpow_body:\n");
  Emit ("  %%bp_next = mul %s %%bp_acc, %%base\n", iat);
  Emit ("  %%bp_dec = sub %s %%bp_rem, 1\n", iat);
  Emit ("  br label %%bpow_loop\n");
  Emit ("apply_sign_based_e:\n");
  Emit ("  %%based_scaled = mul %s %%b_val, %%bp_acc\n", iat);
  Emit ("  %%neg_based_e = sub %s 0, %%based_scaled\n", iat);
  Emit ("  %%ret_based_e = select i1 %%is_neg, %s %%neg_based_e, %s %%based_scaled\n", iat, iat);
  Emit ("  ret %s %%ret_based_e\n", iat);
  Emit ("apply_sign_based:\n");
  Emit ("  %%neg_based = sub %s 0, %%b_val\n", iat);
  Emit ("  %%ret_based = select i1 %%is_neg, %s %%neg_based, %s %%b_val\n", iat, iat);
  Emit ("  ret %s %%ret_based\n", iat);

  // Decimal exponent path
  Emit ("exp_start_d:\n");

  // Check for trailing underscore before E
  Emit ("  br i1 %%d1_last_under, label %%bad, label %%exp_d_ok\n");
  Emit ("exp_d_ok:\n");
  Emit ("  %%ed_s = add %s %%d1_i, 1\n", rts_sbt);

  // Check first char after E: must be digit or '+'; '-' > error
  Emit ("  %%ed_first_p = getelementptr i8, ptr %%buf, %s %%ed_s\n", rts_sbt);
  Emit ("  %%ed_first_done = icmp sge %s %%ed_s, %%end\n", rts_sbt);
  Emit ("  br i1 %%ed_first_done, label %%bad, label %%ed_check_first\n");
  Emit ("ed_check_first:\n");
  Emit ("  %%ed_fc = load i8, ptr %%ed_first_p\n");
  Emit ("  %%ed_fc_minus = icmp eq i8 %%ed_fc, 45\n");
  Emit ("  br i1 %%ed_fc_minus, label %%bad, label %%ed_fc_plus\n");
  Emit ("ed_fc_plus:\n");
  Emit ("  %%ed_fc_isplus = icmp eq i8 %%ed_fc, 43\n");
  Emit ("  %%ed_s2 = add %s %%ed_s, 1\n", rts_sbt);
  Emit ("  %%ed_real_s = select i1 %%ed_fc_isplus, %s %%ed_s2, %s %%ed_s\n", rts_sbt, rts_sbt);

  // First char after E (or E+) must be a digit, not underscore or other
  Emit ("  %%ed_fc_ge0 = icmp uge i8 %%ed_fc, 48\n");
  Emit ("  %%ed_fc_le9 = icmp ule i8 %%ed_fc, 57\n");
  Emit ("  %%ed_fc_isdig = and i1 %%ed_fc_ge0, %%ed_fc_le9\n");
  Emit ("  %%ed_fc_ok = or i1 %%ed_fc_isplus, %%ed_fc_isdig\n");
  Emit ("  br i1 %%ed_fc_ok, label %%edloop, label %%bad\n");
  Emit ("edloop:\n");
  Emit ("  %%ed_i = phi %s [ %%ed_real_s, %%ed_fc_plus ], [ %%ed_ni, %%ed_cont ]\n", rts_sbt);
  Emit ("  %%ed_v = phi %s [ 0, %%ed_fc_plus ], [ %%ed_nv, %%ed_cont ]\n", iat);
  Emit ("  %%ed_had_dig = phi i1 [ false, %%ed_fc_plus ], [ true, %%ed_cont ]\n");
  Emit ("  %%ed_done = icmp sge %s %%ed_i, %%end\n", rts_sbt);
  Emit ("  br i1 %%ed_done, label %%ed_check_end, label %%ed_body\n");
  Emit ("ed_check_end:\n");

  // Must have seen at least one digit
  Emit ("  br i1 %%ed_had_dig, label %%apply_exp_d, label %%bad\n");
  Emit ("ed_body:\n");
  Emit ("  %%ed_p = getelementptr i8, ptr %%buf, %s %%ed_i\n", rts_sbt);
  Emit ("  %%ed_c = load i8, ptr %%ed_p\n");
  Emit ("  %%ed_ge0 = icmp uge i8 %%ed_c, 48\n");
  Emit ("  %%ed_le9 = icmp ule i8 %%ed_c, 57\n");
  Emit ("  %%ed_isdig = and i1 %%ed_ge0, %%ed_le9\n");
  Emit ("  br i1 %%ed_isdig, label %%ed_accum, label %%ed_check_u\n");
  Emit ("ed_check_u:\n");

  // Only underscore allowed in exponent digits (not consecutive/leading/trailing)
  Emit ("  %%ed_is_u = icmp eq i8 %%ed_c, 95\n");
  Emit ("  br i1 %%ed_is_u, label %%ed_cont, label %%bad\n");
  Emit ("ed_accum:\n");
  Emit ("  %%ed_d = sub i8 %%ed_c, 48\n");
  Emit ("  %%ed_dw = zext i8 %%ed_d to %s\n", iat);
  Emit ("  %%ed_mul = mul %s %%ed_v, 10\n", iat);
  Emit ("  %%ed_nv_a = add %s %%ed_mul, %%ed_dw\n", iat);
  Emit ("  br label %%ed_cont\n");
  Emit ("ed_cont:\n");
  Emit ("  %%ed_nv = phi %s [ %%ed_nv_a, %%ed_accum ], [ %%ed_v, %%ed_check_u ]\n", iat);
  Emit ("  %%ed_ni = add %s %%ed_i, 1\n", rts_sbt);
  Emit ("  br label %%edloop\n");

  // Apply decimal exponent: result = mantissa * 10^exp
  Emit ("apply_exp_d:\n");
  Emit ("  br label %%dpow_loop\n");
  Emit ("dpow_loop:\n");
  Emit ("  %%dp_acc = phi %s [ 1, %%apply_exp_d ], [ %%dp_next, %%dpow_body ]\n", iat);
  Emit ("  %%dp_rem = phi %s [ %%ed_v, %%apply_exp_d ], [ %%dp_dec, %%dpow_body ]\n", iat);
  Emit ("  %%dp_done = icmp sle %s %%dp_rem, 0\n", iat);
  Emit ("  br i1 %%dp_done, label %%apply_sign_de, label %%dpow_body\n");
  Emit ("dpow_body:\n");
  Emit ("  %%dp_next = mul %s %%dp_acc, 10\n", iat);
  Emit ("  %%dp_dec = sub %s %%dp_rem, 1\n", iat);
  Emit ("  br label %%dpow_loop\n");
  Emit ("apply_sign_de:\n");
  Emit ("  %%de_scaled = mul %s %%d1_val, %%dp_acc\n", iat);
  Emit ("  %%neg_de = sub %s 0, %%de_scaled\n", iat);
  Emit ("  %%ret_de = select i1 %%is_neg, %s %%neg_de, %s %%de_scaled\n", iat, iat);
  Emit ("  ret %s %%ret_de\n", iat);

  // Simple decimal result (no exponent, no base) - apply sign
  Emit ("apply_sign:\n");

  // Check for trailing underscore
  Emit ("  br i1 %%d1_last_under, label %%bad, label %%apply_sign_ok\n");
  Emit ("apply_sign_ok:\n");
  Emit ("  %%neg_d = sub %s 0, %%d1_val\n", iat);
  Emit ("  %%ret_d = select i1 %%is_neg, %s %%neg_d, %s %%d1_val\n", iat, iat);
  Emit ("  ret %s %%ret_d\n", iat);

  // Error: raise CONSTRAINT_ERROR
  Emit ("bad:\n");
  Emit ("  %%exc_ptr_bad = ptrtoint ptr @__exc.constraint_error to i64\n");
  Emit ("  call void @__ada_raise(i64 %%exc_ptr_bad)\n");
  Emit ("  unreachable\n");
  Emit ("}\n\n");

  // Float'VALUE - declare as external for now (can be implemented similarly)
  Emit ("; Float'VALUE declaration (to be implemented)\n");
  Emit ("declare double @strtod(ptr, ptr)\n");
  Emit ("define linkonce_odr double @__ada_float_value(" FAT_PTR_TYPE " %%str) {\n");
  Emit ("entry:\n");
  Emit_Fat_Pointer_Extractvalue_Named ("str", "data", "fv_low", "fv_high", rts_sbt);
  Emit ("  %%result = call double @strtod(ptr %%data, ptr null)\n");
  Emit ("  ret double %%result\n");
  Emit ("}\n\n");

  // Integer power function - signed, overflow-checked (RM 4.5.6).                                  
  // Uses binary exponentiation (O(log n)) with overflow checking.                                  
  // Raises Constraint_Error on negative exponent or overflow.                                      
  //                                                                                                
  Emit ("; Integer exponentiation helper (signed, overflow-checked, binary exp)\n");
  Emit ("define linkonce_odr %s @__ada_integer_pow (%s %%base, %s %%exp) {\n", iat, iat, iat);
  Emit ("entry:\n");

  // Check for negative exponent
  Emit ("  %%is_neg = icmp slt %s %%exp, 0\n", iat);
  Emit ("  br i1 %%is_neg, label %%neg_exp, label %%check_zero\n");
  Emit ("neg_exp:\n");
  Emit ("  %%exc_ptr_neg = ptrtoint ptr @__exc.constraint_error to i64\n");
  Emit ("  call void @__ada_raise(i64 %%exc_ptr_neg)\n");
  Emit ("  unreachable  ; negative exponent for integer (RM 4.5.6)\n");
  Emit ("check_zero:\n");

  // exp == 0 -> return 1
  Emit ("  %%is_zero = icmp eq %s %%exp, 0\n", iat);
  Emit ("  br i1 %%is_zero, label %%ret_one, label %%loop\n");
  Emit ("ret_one:\n");
  Emit ("  ret %s 1\n", iat);

  // Binary exponentiation loop: result *= b when e is odd, b *= b, e /= 2
  Emit ("loop:\n");
  Emit ("  %%result = phi %s [ 1, %%check_zero ], [ %%new_result, %%cont ]\n", iat);
  Emit ("  %%b = phi %s [ %%base, %%check_zero ], [ %%new_b, %%cont ]\n", iat);
  Emit ("  %%e = phi %s [ %%exp, %%check_zero ], [ %%new_e, %%cont ]\n", iat);

  // Check if e is odd: e & 1
  Emit ("  %%is_odd = and %s %%e, 1\n", iat);
  Emit ("  %%odd_bit = icmp ne %s %%is_odd, 0\n", iat);
  Emit ("  br i1 %%odd_bit, label %%mult_result, label %%square_base\n");

  // Multiply result by base (overflow checked)
  Emit ("mult_result:\n");
  Emit ("  %%pair_r = call {%s, i1} @llvm.smul.with.overflow.%s(%s %%result, %s %%b)\n", iat, iat, iat, iat);
  Emit ("  %%mult_r = extractvalue {%s, i1} %%pair_r, 0\n", iat);
  Emit ("  %%ovf_r = extractvalue {%s, i1} %%pair_r, 1\n", iat);
  Emit ("  br i1 %%ovf_r, label %%overflow, label %%square_base\n");

  // Square the base for next iteration
  Emit ("square_base:\n");
  Emit ("  %%result_sq = phi %s [ %%result, %%loop ], [ %%mult_r, %%mult_result ]\n", iat);

  // Halve exponent
  Emit ("  %%new_e = lshr %s %%e, 1\n", iat);

  // Check if done (e == 0 after shift)
  Emit ("  %%done = icmp eq %s %%new_e, 0\n", iat);
  Emit ("  br i1 %%done, label %%exit, label %%do_square\n");

  // Actually compute b*b (only if we need it for next iteration)
  Emit ("do_square:\n");
  Emit ("  %%pair_b = call {%s, i1} @llvm.smul.with.overflow.%s(%s %%b, %s %%b)\n", iat, iat, iat, iat);
  Emit ("  %%sq_b = extractvalue {%s, i1} %%pair_b, 0\n", iat);
  Emit ("  %%ovf_b = extractvalue {%s, i1} %%pair_b, 1\n", iat);
  Emit ("  br i1 %%ovf_b, label %%overflow, label %%cont\n");
  Emit ("cont:\n");
  Emit ("  %%new_result = phi %s [ %%result_sq, %%do_square ]\n", iat);
  Emit ("  %%new_b = phi %s [ %%sq_b, %%do_square ]\n", iat);
  Emit ("  br label %%loop\n");
  Emit ("overflow:\n");
  Emit ("  %%exc_ptr = ptrtoint ptr @__exc.constraint_error to i64\n");
  Emit ("  call void @__ada_raise(i64 %%exc_ptr)\n");
  Emit ("  unreachable\n");
  Emit ("exit:\n");
  Emit ("  ret %s %%result_sq\n", iat);
  Emit ("}\n\n");

  // Modular power function - unsigned, wrapping (RM 3.5.4).
  // Uses binary exponentiation (O(log n)), no overflow check as modular wraps.
  Emit ("; Modular exponentiation helper (unsigned, wrapping, binary exp)\n");
  Emit ("define linkonce_odr %s @__ada_modular_pow (%s %%base, %s %%exp) {\n", iat, iat, iat);
  Emit ("entry:\n");
  Emit ("  %%is_zero = icmp eq %s %%exp, 0\n", iat);
  Emit ("  br i1 %%is_zero, label %%ret_one, label %%loop\n");
  Emit ("ret_one:\n");
  Emit ("  ret %s 1\n", iat);

  // Binary exponentiation loop
  Emit ("loop:\n");
  Emit ("  %%result = phi %s [ 1, %%entry ], [ %%new_result, %%cont ]\n", iat);
  Emit ("  %%b = phi %s [ %%base, %%entry ], [ %%new_b, %%cont ]\n", iat);
  Emit ("  %%e = phi %s [ %%exp, %%entry ], [ %%new_e, %%cont ]\n", iat);
  Emit ("  %%is_odd = and %s %%e, 1\n", iat);
  Emit ("  %%odd_bit = icmp ne %s %%is_odd, 0\n", iat);
  Emit ("  br i1 %%odd_bit, label %%mult, label %%square\n");
  Emit ("mult:\n");
  Emit ("  %%mult_r = mul %s %%result, %%b\n", iat);
  Emit ("  br label %%square\n");
  Emit ("square:\n");
  Emit ("  %%result_s = phi %s [ %%result, %%loop ], [ %%mult_r, %%mult ]\n", iat);
  Emit ("  %%new_e = lshr %s %%e, 1\n", iat);
  Emit ("  %%done = icmp eq %s %%new_e, 0\n", iat);
  Emit ("  br i1 %%done, label %%exit, label %%cont\n");
  Emit ("cont:\n");
  Emit ("  %%new_result = phi %s [ %%result_s, %%square ]\n", iat);
  Emit ("  %%new_b = mul %s %%b, %%b\n", iat);
  Emit ("  br label %%loop\n");
  Emit ("exit:\n");
  Emit ("  ret %s %%result_s\n", iat);
  Emit ("}\n\n");

  // String trim helpers for Enum'VALUE (Ada RM 3.5): count leading/trailing
  // spaces so the comparison uses the trimmed content only.
  {
      // Always emitted - linkonce_odr deduplicates, and codegen sets
      // needs_trim_helpers too late (after header emission).
    Emit ("; String trim helpers for VALUE attribute\n");

    // count_leading_spaces(ptr data, iN len) -> iN
    Emit ("define linkonce_odr %s @__ada_count_leading_spaces(ptr %%d, %s %%len) {\n", iat, iat);
    Emit ("entry:\n  br label %%loop\n");
    Emit ("loop:\n");
    Emit ("  %%i = phi %s [ 0, %%entry ], [ %%ni, %%cont ]\n", iat);
    Emit ("  %%done = icmp sge %s %%i, %%len\n", iat);
    Emit ("  br i1 %%done, label %%exit, label %%body\n");
    Emit ("body:\n");
    Emit ("  %%p = getelementptr i8, ptr %%d, %s %%i\n", iat);
    Emit ("  %%c = load i8, ptr %%p\n");
    Emit ("  %%sp = icmp eq i8 %%c, 32\n");
    Emit ("  br i1 %%sp, label %%cont, label %%exit\n");
    Emit ("cont:\n");
    Emit ("  %%ni = add %s %%i, 1\n", iat);
    Emit ("  br label %%loop\n");
    Emit ("exit:\n");
    Emit ("  %%r = phi %s [ %%i, %%loop ], [ %%i, %%body ]\n", iat);
    Emit ("  ret %s %%r\n", iat);
    Emit ("}\n\n");

    // count_trailing_spaces(ptr data, iN len) -> iN
    Emit ("define linkonce_odr %s @__ada_count_trailing_spaces(ptr %%d, %s %%len) {\n", iat, iat);
    Emit ("entry:\n");
    Emit ("  %%start = sub %s %%len, 1\n", iat);
    Emit ("  br label %%loop\n");
    Emit ("loop:\n");
    Emit ("  %%i = phi %s [ %%start, %%entry ], [ %%ni, %%cont ]\n", iat);
    Emit ("  %%done = icmp slt %s %%i, 0\n", iat);
    Emit ("  br i1 %%done, label %%exit, label %%body\n");
    Emit ("body:\n");
    Emit ("  %%p = getelementptr i8, ptr %%d, %s %%i\n", iat);
    Emit ("  %%c = load i8, ptr %%p\n");
    Emit ("  %%sp = icmp eq i8 %%c, 32\n");
    Emit ("  br i1 %%sp, label %%cont, label %%exit\n");
    Emit ("cont:\n");
    Emit ("  %%ni = sub %s %%i, 1\n", iat);
    Emit ("  br label %%loop\n");
    Emit ("exit:\n");

    // trailing count = len - 1 - i (when i is last non-space index) or len (all spaces)
    Emit ("  %%last = phi %s [ -1, %%loop ], [ %%i, %%body ]\n", iat);
    Emit ("  %%r = sub %s %%len, %%last\n", iat);
    Emit ("  %%r2 = sub %s %%r, 1\n", iat);
    Emit ("  ret %s %%r2\n", iat);
    Emit ("}\n\n");
  }

  // Runtime globals
  Emit ("; Runtime globals\n");
  Emit ("@__ss_base = linkonce_odr global ptr null\n");
  Emit ("@__ss_ptr = linkonce_odr global i64 0\n");
  Emit ("@__ss_size = linkonce_odr global i64 0\n");
  Emit ("@__eh_cur = linkonce_odr global ptr null\n");
  Emit ("@__ex_cur = linkonce_odr global i64 0\n");
  Emit ("@__fin_list = linkonce_odr global ptr null\n");
  Emit ("@__entry_queue = linkonce_odr global ptr null\n");
  Emit ("@.fmt_ue = linkonce_odr constant [27 x i8] c\"Unhandled exception: %%lld\\0A\\00\"\n\n");

  // Standard exceptions (RM 11.1) - names lowercase to match mangled symbols
  Emit ("; Standard exception identities\n");
  Emit ("@__exc.constraint_error = linkonce_odr constant i64 1\n");
  Emit ("@__exc.numeric_error = linkonce_odr constant i64 2\n");
  Emit ("@__exc.program_error = linkonce_odr constant i64 3\n");
  Emit ("@__exc.storage_error = linkonce_odr constant i64 4\n");
  Emit ("@__exc.tasking_error = linkonce_odr constant i64 5\n\n");

  // Secondary stack initialization
  Emit ("; Secondary stack runtime\n");
  Emit ("define linkonce_odr void @__ada_ss_init() {\n");
  Emit ("  %%p = call ptr @malloc (i64 1048576)\n");
  Emit ("  store ptr %%p, ptr @__ss_base\n");
  Emit ("  store i64 1048576, ptr @__ss_size\n");
  Emit ("  store i64 0, ptr @__ss_ptr\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Secondary stack mark
  Emit ("define linkonce_odr i64 @__ada_sec_stack_mark() {\n");
  Emit ("  %%m = load i64, ptr @__ss_ptr\n");
  Emit ("  ret i64 %%m\n");
  Emit ("}\n\n");

  // Secondary stack release
  Emit ("define linkonce_odr void @__ada_sec_stack_release(i64 %%m) {\n");
  Emit ("  store i64 %%m, ptr @__ss_ptr\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Secondary stack allocate
  Emit ("define linkonce_odr ptr @__ada_sec_stack_alloc(i64 %%sz) {\n");
  Emit ("entry:\n");
  Emit ("  %%1 = load ptr, ptr @__ss_base\n");
  Emit ("  %%2 = icmp eq ptr %%1, null\n");
  Emit ("  br i1 %%2, label %%init, label %%alloc\n");
  Emit ("init:\n");
  Emit ("  call void @__ada_ss_init()\n");
  Emit ("  %%3 = load ptr, ptr @__ss_base\n");
  Emit ("  br label %%alloc\n");
  Emit ("alloc:\n");
  Emit ("  %%p = phi ptr [%%1, %%entry], [%%3, %%init]\n");
  Emit ("  %%4 = load i64, ptr @__ss_ptr\n");
  Emit ("  %%5 = add i64 %%sz, 7\n");
  Emit ("  %%6 = and i64 %%5, -8\n");
  Emit ("  %%7 = add i64 %%4, %%6\n");
  Emit ("  %%8 = load i64, ptr @__ss_size\n");
  Emit ("  %%9 = icmp ult i64 %%7, %%8\n");
  Emit ("  br i1 %%9, label %%ok, label %%grow\n");
  Emit ("grow:\n");
  Emit ("  %%10 = mul i64 %%8, 2\n");
  Emit ("  store i64 %%10, ptr @__ss_size\n");
  Emit ("  %%11 = call ptr @realloc (ptr %%p, i64 %%10)\n");
  Emit ("  store ptr %%11, ptr @__ss_base\n");
  Emit ("  br label %%ok\n");
  Emit ("ok:\n");
  Emit ("  %%12 = phi ptr [%%p, %%alloc], [%%11, %%grow]\n");
  Emit ("  %%13 = getelementptr i8, ptr %%12, i64 %%4\n");
  Emit ("  store i64 %%7, ptr @__ss_ptr\n");
  Emit ("  ret ptr %%13\n");
  Emit ("}\n\n");

  // Exception handling: push handler                                                               
  // Handler frame structure: { ptr prev, [200 x i8] jmp_buf }                                      
  // Field 0 = link to previous handler                                                             
  // Field 1 = jmp_buf for setjmp/longjmp                                                           
  //                                                                                                
  Emit ("; Exception handling runtime\n");
  Emit ("define linkonce_odr void @__ada_push_handler(ptr %%h) {\n");
  Emit ("  %%old = load ptr, ptr @__eh_cur\n");
  Emit ("  %%link = getelementptr { ptr, [200 x i8] }, ptr %%h, i32 0, i32 0\n");
  Emit ("  store ptr %%old, ptr %%link\n");
  Emit ("  store ptr %%h, ptr @__eh_cur\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Exception handling: pop handler
  Emit ("define linkonce_odr void @__ada_pop_handler() {\n");
  Emit ("  %%cur = load ptr, ptr @__eh_cur\n");
  Emit ("  %%is_null = icmp eq ptr %%cur, null\n");
  Emit ("  br i1 %%is_null, label %%done, label %%pop\n");
  Emit ("pop:\n");
  Emit ("  %%link = getelementptr { ptr, [200 x i8] }, ptr %%cur, i32 0, i32 0\n");
  Emit ("  %%prev = load ptr, ptr %%link\n");
  Emit ("  store ptr %%prev, ptr @__eh_cur\n");
  Emit ("  br label %%done\n");
  Emit ("done:\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Exception handling: raise
  Emit ("define linkonce_odr void @__ada_raise(i64 %%exc_id) {\n");
  Emit ("  store i64 %%exc_id, ptr @__ex_cur\n");
  Emit ("  %%frame = load ptr, ptr @__eh_cur\n");
  Emit ("  %%is_null = icmp eq ptr %%frame, null\n");
  Emit ("  br i1 %%is_null, label %%unhandled, label %%jump\n");
  Emit ("jump:\n");
  Emit ("  %%jb = getelementptr { ptr, [200 x i8] }, ptr %%frame, i32 0, i32 1\n");
  Emit ("  call void @longjmp(ptr %%jb, i32 1)\n");
  Emit ("  unreachable\n");
  Emit ("unhandled:\n");
  Emit ("  call i32 (ptr, ...) @printf (ptr @.fmt_ue, i64 %%exc_id)\n");
  Emit ("  call void @exit (i32 1)\n");
  Emit ("  unreachable\n");
  Emit ("}\n\n");

  // Exception handling: reraise
  Emit ("define linkonce_odr void @__ada_reraise() {\n");
  Emit ("  %%exc = load i64, ptr @__ex_cur\n");
  Emit ("  call void @__ada_raise(i64 %%exc)\n");
  Emit ("  unreachable\n");
  Emit ("}\n\n");

  // Exception handling: get current exception
  Emit ("define linkonce_odr i64 @__ada_current_exception() {\n");
  Emit ("  %%exc = load i64, ptr @__ex_cur\n");
  Emit ("  ret i64 %%exc\n");
  Emit ("}\n\n");

  // Integer power function
  Emit ("; Arithmetic runtime\n");
  Emit ("define linkonce_odr i64 @__ada_powi(i64 %%base, i64 %%exp) {\n");
  Emit ("entry:\n");
  Emit ("  %%result = alloca i64\n");
  Emit ("  store i64 1, ptr %%result\n");
  Emit ("  %%e = alloca i64\n");
  Emit ("  store i64 %%exp, ptr %%e\n");
  Emit ("  br label %%loop\n");
  Emit ("loop:\n");
  Emit ("  %%ev = load i64, ptr %%e\n");
  Emit ("  %%cmp = icmp sgt i64 %%ev, 0\n");
  Emit ("  br i1 %%cmp, label %%body, label %%done\n");
  Emit ("body:\n");
  Emit ("  %%rv = load i64, ptr %%result\n");
  Emit ("  %%nv = mul i64 %%rv, %%base\n");
  Emit ("  store i64 %%nv, ptr %%result\n");
  Emit ("  %%ev2 = load i64, ptr %%e\n");
  Emit ("  %%ev3 = sub i64 %%ev2, 1\n");
  Emit ("  store i64 %%ev3, ptr %%e\n");
  Emit ("  br label %%loop\n");
  Emit ("done:\n");
  Emit ("  %%final = load i64, ptr %%result\n");
  Emit ("  ret i64 %%final\n");
  Emit ("}\n\n");

  // Delay statement support
  Emit ("; Tasking runtime\n");
  Emit ("define linkonce_odr void @__ada_delay(i64 %%us) {\n");
  Emit ("  %%t = trunc i64 %%us to i32\n");
  Emit ("  call i32 @usleep(i32 %%t)\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Task abort: signal task to terminate (simplified: sets abort-pending flag)
  Emit ("define linkonce_odr void @__ada_task_abort (ptr %%task) {\n");
  Emit ("entry:\n");
  Emit ("  %%1 = icmp eq ptr %%task, null\n");
  Emit ("  br i1 %%1, label %%done, label %%abort\n");
  Emit ("abort:\n");
  Emit ("  ; In full impl: set abort flag, signal condition\n");
  Emit ("  store i8 1, ptr %%task  ; Mark abort pending\n");
  Emit ("  br label %%done\n");
  Emit ("done:\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Task terminate: graceful task termination (for terminate alternative).                         
  // Per RM 9.7.1: a terminate alternative is selected when the task's                              
  // master has completed and all sibling tasks are terminated or waiting                           
  // at terminate alternatives.  We terminate just this task's thread.                              
  //                                                                                                
  Emit ("define linkonce_odr void @__ada_task_terminate() {\n");
  Emit ("  call void @pthread_exit (ptr null)\n");
  Emit ("  unreachable\n");
  Emit ("}\n\n");

  // Task Control Block (TCB) layout:                                                               
  //   offset  0: ptr  func          (task body function)                                           
  //   offset  8: ptr  parent_frame  (enclosing scope frame)                                        
  //   offset 16: ptr  thread_handle (pthread_t)                                                    
  //   offset 24: i8   completed     (0=running, 1=done)                                            
  // Total 25 bytes, rounded to 32 by malloc.                                                       
  //                                                                                                

  // Task wrapper: calls actual task body then sets completed flag.
  Emit ("define linkonce_odr ptr @__ada_task_wrapper(ptr %%tcb) {\n");
  Emit ("entry:\n");
  Emit ("  %%func = load ptr, ptr %%tcb\n");
  Emit ("  %%1 = getelementptr ptr, ptr %%tcb, i64 1\n");
  Emit ("  %%frame = load ptr, ptr %%1\n");
  Emit ("  %%_r = call ptr %%func(ptr %%frame)\n");
  Emit ("  %%2 = getelementptr i8, ptr %%tcb, i64 24\n");
  Emit ("  store i8 1, ptr %%2  ; mark completed\n");
  Emit ("  ret ptr null\n");
  Emit ("}\n\n");

  // Task start: allocate TCB, spawn wrapper thread. Returns TCB ptr.
  Emit ("define linkonce_odr ptr @__ada_task_start(ptr %%task_func, ptr %%parent_frame) {\n");
  Emit ("entry:\n");
  Emit ("  %%tcb = call ptr @malloc (i64 32)\n");
  Emit ("  store ptr %%task_func, ptr %%tcb\n");
  Emit ("  %%1 = getelementptr ptr, ptr %%tcb, i64 1\n");
  Emit ("  store ptr %%parent_frame, ptr %%1\n");
  Emit ("  %%2 = getelementptr i8, ptr %%tcb, i64 24\n");
  Emit ("  store i8 0, ptr %%2  ; not completed\n");
  Emit ("  %%tid_slot = getelementptr ptr, ptr %%tcb, i64 2\n");
  Emit ("  %%_rc = call i32 @pthread_create(ptr %%tid_slot, ptr null, ptr @__ada_task_wrapper, ptr %%tcb)\n");
  Emit ("  ret ptr %%tcb\n");
  Emit ("}\n\n");

  // T'CALLABLE (RM 9.9): task is callable iff not completed
  Emit ("define linkonce_odr i8 @__ada_task_callable(ptr %%tcb) {\n");
  Emit ("entry:\n");
  Emit ("  %%1 = icmp eq ptr %%tcb, null\n");
  Emit ("  br i1 %%1, label %%nil, label %%check\n");
  Emit ("nil:\n  ret i8 0\n");
  Emit ("check:\n");
  Emit ("  %%2 = getelementptr i8, ptr %%tcb, i64 24\n");
  Emit ("  %%3 = load i8, ptr %%2\n");
  Emit ("  %%4 = xor i8 %%3, 1\n");
  Emit ("  ret i8 %%4\n");
  Emit ("}\n\n");

  // Entry call: caller side of rendezvous (blocks until accept completes)
  Emit ("define linkonce_odr void @__ada_entry_call(ptr %%task, i64 %%entry_idx, ptr %%params) {\n");
  Emit ("entry:\n");
  Emit ("  ; Allocate rendezvous record: { task_ptr, entry_idx, params, complete_flag, next }\n");
  Emit ("  %%rv = call ptr @malloc (i64 40)\n");
  Emit ("  store ptr %%task, ptr %%rv\n");
  Emit ("  %%1 = getelementptr i64, ptr %%rv, i64 1\n");
  Emit ("  store i64 %%entry_idx, ptr %%1\n");
  Emit ("  %%2 = getelementptr ptr, ptr %%rv, i64 2\n");
  Emit ("  store ptr %%params, ptr %%2\n");
  Emit ("  %%3 = getelementptr i8, ptr %%rv, i64 24\n");
  Emit ("  store i8 0, ptr %%3  ; complete = false\n");
  Emit ("  ; Enqueue to task's entry queue (append to @__entry_queue)\n");
  Emit ("  %%4 = load ptr, ptr @__entry_queue\n");
  Emit ("  %%5 = getelementptr ptr, ptr %%rv, i64 4\n");
  Emit ("  store ptr %%4, ptr %%5\n");
  Emit ("  store ptr %%rv, ptr @__entry_queue\n");
  Emit ("  br label %%wait\n");
  Emit ("wait:\n");
  Emit ("  ; Spin-wait for complete flag (yield to scheduler)\n");
  Emit ("  %%_u1 = call i32 @usleep(i32 100)\n");
  Emit ("  %%6 = load i8, ptr %%3\n");
  Emit ("  %%7 = icmp eq i8 %%6, 0\n");
  Emit ("  br i1 %%7, label %%wait, label %%done\n");
  Emit ("done:\n");
  Emit ("  call void @free (ptr %%rv)\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Accept wait: acceptor blocks until entry call arrives
  Emit ("define linkonce_odr ptr @__ada_accept_wait(i64 %%entry_idx) {\n");
  Emit ("entry:\n");
  Emit ("  br label %%wait\n");
  Emit ("wait:\n");
  Emit ("  ; Scan entry queue for matching entry index\n");
  Emit ("  %%q = load ptr, ptr @__entry_queue\n");
  Emit ("  %%is_empty = icmp eq ptr %%q, null\n");
  Emit ("  br i1 %%is_empty, label %%spin, label %%check\n");
  Emit ("spin:\n");
  Emit ("  %%_u2 = call i32 @usleep(i32 100)\n");
  Emit ("  br label %%wait\n");
  Emit ("check:\n");
  Emit ("  ; Check if first entry matches\n");
  Emit ("  %%1 = getelementptr i64, ptr %%q, i64 1\n");
  Emit ("  %%2 = load i64, ptr %%1\n");
  Emit ("  %%3 = icmp eq i64 %%2, %%entry_idx\n");
  Emit ("  br i1 %%3, label %%found, label %%spin\n");
  Emit ("found:\n");
  Emit ("  ; Dequeue and return caller's parameter block\n");
  Emit ("  %%4 = getelementptr ptr, ptr %%q, i64 4\n");
  Emit ("  %%5 = load ptr, ptr %%4\n");
  Emit ("  store ptr %%5, ptr @__entry_queue\n");
  Emit ("  %%6 = getelementptr ptr, ptr %%q, i64 2\n");
  Emit ("  %%params = load ptr, ptr %%6\n");
  Emit ("  ret ptr %%q\n");
  Emit ("}\n\n");

  // Accept try: non-blocking check for pending entry call (for SELECT)
  Emit ("define linkonce_odr ptr @__ada_accept_try(i64 %%entry_idx) {\n");
  Emit ("entry:\n");
  Emit ("  %%q = load ptr, ptr @__entry_queue\n");
  Emit ("  %%is_empty = icmp eq ptr %%q, null\n");
  Emit ("  br i1 %%is_empty, label %%none, label %%check\n");
  Emit ("check:\n");
  Emit ("  %%1 = getelementptr i64, ptr %%q, i64 1\n");
  Emit ("  %%2 = load i64, ptr %%1\n");
  Emit ("  %%3 = icmp eq i64 %%2, %%entry_idx\n");
  Emit ("  br i1 %%3, label %%found, label %%none\n");
  Emit ("found:\n");
  Emit ("  ; Dequeue and return caller's parameter block\n");
  Emit ("  %%4 = getelementptr ptr, ptr %%q, i64 4\n");
  Emit ("  %%5 = load ptr, ptr %%4\n");
  Emit ("  store ptr %%5, ptr @__entry_queue\n");
  Emit ("  ret ptr %%q\n");
  Emit ("none:\n");
  Emit ("  ret ptr null\n");
  Emit ("}\n\n");

  // Accept complete: signal rendezvous completion to caller
  Emit ("define linkonce_odr void @__ada_accept_complete(ptr %%rv) {\n");
  Emit ("entry:\n");
  Emit ("  %%1 = getelementptr i8, ptr %%rv, i64 24\n");
  Emit ("  store i8 1, ptr %%1  ; complete = true\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // Finalization support
  Emit ("; Finalization runtime\n");
  Emit ("define linkonce_odr void @__ada_finalize(ptr %%obj, ptr %%fn) {\n");
  Emit ("  %%1 = call ptr @malloc (i64 24)\n");
  Emit ("  %%2 = getelementptr ptr, ptr %%1, i64 0\n");
  Emit ("  store ptr %%obj, ptr %%2\n");
  Emit ("  %%3 = getelementptr ptr, ptr %%1, i64 1\n");
  Emit ("  store ptr %%fn, ptr %%3\n");
  Emit ("  %%4 = load ptr, ptr @__fin_list\n");
  Emit ("  %%5 = getelementptr ptr, ptr %%1, i64 2\n");
  Emit ("  store ptr %%4, ptr %%5\n");
  Emit ("  store ptr %%1, ptr @__fin_list\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");
  Emit ("define linkonce_odr void @__ada_finalize_all() {\n");
  Emit ("entry:\n");
  Emit ("  %%1 = load ptr, ptr @__fin_list\n");
  Emit ("  br label %%loop\n");
  Emit ("loop:\n");
  Emit ("  %%p = phi ptr [%%1, %%entry], [%%9, %%fin]\n");
  Emit ("  %%2 = icmp eq ptr %%p, null\n");
  Emit ("  br i1 %%2, label %%done, label %%fin\n");
  Emit ("fin:\n");
  Emit ("  %%3 = getelementptr ptr, ptr %%p, i64 0\n");
  Emit ("  %%4 = load ptr, ptr %%3\n");
  Emit ("  %%5 = getelementptr ptr, ptr %%p, i64 1\n");
  Emit ("  %%6 = load ptr, ptr %%5\n");
  Emit ("  call void %%6(ptr %%4)\n");
  Emit ("  %%8 = getelementptr ptr, ptr %%p, i64 2\n");
  Emit ("  %%9 = load ptr, ptr %%8\n");
  Emit ("  call void @free (ptr %%p)\n");
  Emit ("  br label %%loop\n");
  Emit ("done:\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // TEXT_IO inline implementation (Ada 83 RM §14)
  Emit ("; TEXT_IO runtime\n");
  Emit ("@stdin = external global ptr\n");
  Emit ("@stdout = external global ptr\n");
  Emit ("@stderr = external global ptr\n");

  // Wrapper functions for pragma Import (C, ..., "__ada_stdin") etc. in text_io.adb
  Emit ("define linkonce_odr i64 @__ada_stdin() {\n");
  Emit ("  %%1 = load ptr, ptr @stdin\n");
  Emit ("  %%2 = ptrtoint ptr %%1 to i64\n");
  Emit ("  ret i64 %%2\n");
  Emit ("}\n\n");
  Emit ("define linkonce_odr i64 @__ada_stdout() {\n");
  Emit ("  %%1 = load ptr, ptr @stdout\n");
  Emit ("  %%2 = ptrtoint ptr %%1 to i64\n");
  Emit ("  ret i64 %%2\n");
  Emit ("}\n\n");
  Emit ("define linkonce_odr i64 @__ada_stderr() {\n");
  Emit ("  %%1 = load ptr, ptr @stderr\n");
  Emit ("  %%2 = ptrtoint ptr %%1 to i64\n");
  Emit ("  ret i64 %%2\n");
  Emit ("}\n\n");

  // Note: fputc, fgetc declared in main preamble; add remaining here
  Emit ("declare i32 @fputs(ptr, ptr)\n");
  Emit ("declare ptr @fgets(ptr, i32, ptr)\n");
  Emit ("declare i32 @fprintf (ptr, ptr, ...)\n");

  // Format string depends on INTEGER width: %d for i32, %lld for i64
  if (strcmp (iat, "i32") == 0)
    Emit ("@.fmt_d = linkonce_odr constant [3 x i8] c\"%%d\\00\"\n");
  else
    Emit ("@.fmt_d = linkonce_odr constant [5 x i8] c\"%%lld\\00\"\n");
  Emit ("@.fmt_s = linkonce_odr constant [3 x i8] c\"%%s\\00\"\n");
  Emit ("@.fmt_f = linkonce_odr constant [3 x i8] c\"%%g\\00\"\n");
  Emit ("@.fmt_c = linkonce_odr constant [3 x i8] c\"%%c\\00\"\n\n");

  // NEW_LINE: output line terminator
  Emit ("define linkonce_odr void @__text_io_new_line() {\n");
  Emit ("  %%out = load ptr, ptr @stdout\n");
  Emit ("  call i32 @fputc(i32 10, ptr %%out)\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // PUT_CHAR: output single character
  Emit ("define linkonce_odr void @__text_io_put_char(i8 %%c) {\n");
  Emit ("  %%out = load ptr, ptr @stdout\n");
  Emit ("  %%ci = zext i8 %%c to i32\n");
  Emit ("  call i32 @fputc(i32 %%ci, ptr %%out)\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // PUT: output string (ptr + bounds in native STRING bound type).
  // Bound type derived from STRING's index type via type system.
  Emit ("define linkonce_odr void @__text_io_put(ptr %%data, %s %%lo, %s %%hi) {\n",
     rts_sbt, rts_sbt);
  Emit ("entry:\n");
  Emit ("  %%out = load ptr, ptr @stdout\n");
  Emit ("  %%i.init = sub %s %%lo, 1\n", rts_sbt);
  Emit ("  br label %%loop\n");
  Emit ("loop:\n");
  Emit ("  %%i = phi %s [ %%i.init, %%entry ], [ %%i.next, %%body ]\n", rts_sbt);
  Emit ("  %%i.next = add %s %%i, 1\n", rts_sbt);
  Emit ("  %%done = icmp sgt %s %%i.next, %%hi\n", rts_sbt);
  Emit ("  br i1 %%done, label %%exit, label %%body\n");
  Emit ("body:\n");
  Emit ("  %%idx_bt = sub %s %%i.next, %%lo\n", rts_sbt);
  Emit_Widen_Named_For_Intrinsic ("idx_bt", "idx", rts_sbt);
  Emit ("  %%ptr = getelementptr i8, ptr %%data, %s %%idx\n", iat);
  Emit ("  %%ch = load i8, ptr %%ptr\n");
  Emit ("  %%chi = zext i8 %%ch to i32\n");
  Emit ("  call i32 @fputc(i32 %%chi, ptr %%out)\n");
  Emit ("  br label %%loop\n");
  Emit ("exit:\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // PUT_LINE: output string followed by newline
  Emit ("define linkonce_odr void @__text_io_put_line(ptr %%data, %s %%lo, %s %%hi) {\n",
     rts_sbt, rts_sbt);
  Emit ("  call void @__text_io_put(ptr %%data, %s %%lo, %s %%hi)\n",
     rts_sbt, rts_sbt);
  Emit ("  call void @__text_io_new_line()\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // PUT_INTEGER: output integer with optional width
  Emit ("define linkonce_odr void @__text_io_put_int(%s %%val, i32 %%width) {\n", iat);
  Emit ("  %%out = load ptr, ptr @stdout\n");
  Emit ("  call i32 (ptr, ptr, ...) @fprintf (ptr %%out, ptr @.fmt_d, %s %%val)\n", iat);
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // PUT_FLOAT: output float value
  Emit ("define linkonce_odr void @__text_io_put_float(double %%val) {\n");
  Emit ("  %%out = load ptr, ptr @stdout\n");
  Emit ("  call i32 (ptr, ptr, ...) @fprintf (ptr %%out, ptr @.fmt_f, double %%val)\n");
  Emit ("  ret void\n");
  Emit ("}\n\n");

  // GET_CHAR: read single character
  Emit ("define linkonce_odr i8 @__text_io_get_char() {\n");
  Emit ("  %%inp = load ptr, ptr @stdin\n");
  Emit ("  %%c = call i32 @fgetc(ptr %%inp)\n");
  Emit ("  %%c8 = trunc i32 %%c to i8\n");
  Emit ("  ret i8 %%c8\n");
  Emit ("}\n\n");

  // GET_LINE: read line into buffer, return fat pointer.
  // Bounds use STRING bound type derived from type system.
  {
    char lo_expr[64], hi_empty_expr[64];
    snprintf (lo_expr, sizeof (lo_expr), "%s 1", rts_sbt);
    snprintf (hi_empty_expr, sizeof (hi_empty_expr), "%s 0", rts_sbt);
    Emit ("define linkonce_odr " FAT_PTR_TYPE " @__text_io_get_line() {\n");
    Emit ("entry:\n");
    Emit ("  %%buf = call ptr @__ada_sec_stack_alloc(i64 256)\n");
    Emit ("  %%inp = load ptr, ptr @stdin\n");
    Emit ("  %%res = call ptr @fgets(ptr %%buf, i32 255, ptr %%inp)\n");
    Emit ("  %%iseof = icmp eq ptr %%res, null\n");
    Emit ("  br i1 %%iseof, label %%empty, label %%gotline\n");
    Emit ("empty:\n");
    Emit_Fat_Pointer_Insertvalue_Named ("e", "ptr %buf", lo_expr, hi_empty_expr, rts_sbt);
    Emit ("  ret " FAT_PTR_TYPE " %%e2\n");
    Emit ("gotline:\n");
    Emit ("  %%len = call i64 @strlen (ptr %%buf)\n");
    Emit ("  ; Strip trailing newline if present\n");
    Emit ("  %%lastidx = sub i64 %%len, 1\n");
    Emit ("  %%lastptr = getelementptr i8, ptr %%buf, i64 %%lastidx\n");
    Emit ("  %%lastch = load i8, ptr %%lastptr\n");
    Emit ("  %%isnl = icmp eq i8 %%lastch, 10\n");
    Emit ("  %%adjlen = select i1 %%isnl, i64 %%lastidx, i64 %%len\n");

    // adjlen is i64 from strlen; trunc directly to bound type
    Emit ("  %%adjlen_bt = trunc i64 %%adjlen to %s\n", rts_sbt);
    {
      char hi_expr[64];
      snprintf (hi_expr, sizeof (hi_expr), "%s %%adjlen_bt", rts_sbt);
      Emit_Fat_Pointer_Insertvalue_Named ("f", "ptr %buf", lo_expr, hi_expr, rts_sbt);
    }
    Emit ("  ret " FAT_PTR_TYPE " %%f2\n");
    Emit ("}\n\n");
  }

  // 'IMAGE runtime: Integer'IMAGE (x) returns string representation
  Emit ("; Attribute runtime support\n");
  Emit ("declare i32 @snprintf (ptr, i64, ptr, ...)\n");
  Emit ("declare i64 @strlen (ptr)\n");
  if (strcmp (iat, "i32") == 0)
    Emit ("@.img_fmt_d = linkonce_odr constant [4 x i8] c\"%% d\\00\"\n");
  else
    Emit ("@.img_fmt_d = linkonce_odr constant [6 x i8] c\"%% lld\\00\"\n");
  Emit ("@.img_fmt_c = linkonce_odr constant [3 x i8] c\"%%c\\00\"\n");
  Emit ("@.img_fmt_f = linkonce_odr constant [5 x i8] c\"%%.6g\\00\"\n\n");

  // Integer'IMAGE - convert integer to string fat pointer.
  // Bounds use STRING bound type derived from type system.
  {
    char lo_expr[64], hi_expr[64], const3_expr[64];
    snprintf (lo_expr, sizeof (lo_expr), "%s 1", rts_sbt);
    Emit ("define linkonce_odr " FAT_PTR_TYPE " @__ada_integer_image(%s %%val) {\n", iat);
    Emit ("entry:\n");
    Emit ("  %%buf = call ptr @__ada_sec_stack_alloc(i64 24)\n");
    Emit ("  %%len32 = call i32 (ptr, i64, ptr, ...) @snprintf (ptr %%buf, i64 24, ptr @.img_fmt_d, %s %%val)\n", iat);
    Emit_Widen_Named_For_Intrinsic ("len32", "len64", "i32");  // snprintf returns i32
    Emit_Narrow_Named_From_Intrinsic ("len64", "len_bt", rts_sbt);
    snprintf (hi_expr, sizeof (hi_expr), "%s %%len_bt", rts_sbt);
    Emit_Fat_Pointer_Insertvalue_Named ("fat", "ptr %buf", lo_expr, hi_expr, rts_sbt);
    Emit ("  ret " FAT_PTR_TYPE " %%fat2\n");
    Emit ("}\n\n");

    // Character'IMAGE - single char to string (3 chars: 'x')
    snprintf (const3_expr, sizeof (const3_expr), "%s 3", rts_sbt);
    Emit ("define linkonce_odr " FAT_PTR_TYPE " @__ada_character_image(i8 %%val) {\n");
    Emit ("entry:\n");
    Emit ("  %%buf = call ptr @__ada_sec_stack_alloc(i64 4)\n");
    Emit ("  %%p0 = getelementptr i8, ptr %%buf, i64 0\n");
    Emit ("  store i8 39, ptr %%p0  ; single quote\n");
    Emit ("  %%p1 = getelementptr i8, ptr %%buf, i64 1\n");
    Emit ("  store i8 %%val, ptr %%p1\n");
    Emit ("  %%p2 = getelementptr i8, ptr %%buf, i64 2\n");
    Emit ("  store i8 39, ptr %%p2  ; single quote\n");
    Emit_Fat_Pointer_Insertvalue_Named ("fat", "ptr %buf", lo_expr, const3_expr, rts_sbt);
    Emit ("  ret " FAT_PTR_TYPE " %%fat2\n");
    Emit ("}\n\n");

    // Float'IMAGE - convert float to string
    Emit ("define linkonce_odr " FAT_PTR_TYPE " @__ada_float_image(double %%val) {\n");
    Emit ("entry:\n");
    Emit ("  %%buf = call ptr @__ada_sec_stack_alloc(i64 32)\n");
    Emit ("  %%len32 = call i32 (ptr, i64, ptr, ...) @snprintf (ptr %%buf, i64 32, ptr @.img_fmt_f, double %%val)\n");
    Emit_Widen_Named_For_Intrinsic ("len32", "flen64", "i32");  // snprintf returns i32
    Emit_Narrow_Named_From_Intrinsic ("flen64", "flen_bt", rts_sbt);
    snprintf (hi_expr, sizeof (hi_expr), "%s %%flen_bt", rts_sbt);
    Emit_Fat_Pointer_Insertvalue_Named ("fat", "ptr %buf", lo_expr, hi_expr, rts_sbt);
    Emit ("  ret " FAT_PTR_TYPE " %%fat2\n");
    Emit ("}\n\n");
  }

  // Generate exception identity globals
  Generate_Exception_Globals ();

  // Generate implicit operators for frozen composite types
  Generate_Implicit_Operators ();
  Emit ("\n");
  cg->header_emitted = true;
  }  // End of header emission block

  // Generate extern declarations for WITH'd packages
  Generate_Extern_Declarations (node);

  // Generate declarations
  if (node->compilation_unit.unit) {
    Generate_Declaration (node->compilation_unit.unit);
  }

  // Emit buffered string constants at module level
  if (cg->string_const_size > 0) {
    Emit ("\n; String constants\n");
    fprintf (cg->output, "%s", cg->string_const_buffer);
    Emit ("\n");
    cg->string_const_size = 0;  // Reset buffer for next compilation unit
  }

  // Generate main function if this is a main program (library-level procedure).                    
  // Emit @main() for the LAST parameterless library-level procedure in the file.                   
  // We track main_candidate and emit at end of Compile_File instead.                               
  //                                                                                                
  Syntax_Node *unit = node->compilation_unit.unit;
  if (unit and unit->kind == NK_PROCEDURE_BODY and unit->symbol) {
    Symbol *main_sym = unit->symbol;

    // Check if this is a library-level procedure (no parameters)
    // and NOT a SEPARATE subunit
    if (main_sym->parameter_count == 0 and not unit->subprogram_body.is_separate) {
      cg->main_candidate = main_sym;
    }
  }
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §13.17 BUILD-IN-PLACE - Limited Type Function Returns                                            
//                                                                                                  
// Ada limited types cannot be copied (RM 7.5). Functions returning limited                         
// types must construct the result directly in caller-provided space-the                            
// "Build-in-Place" (BIP) protocol. This eliminates intermediate temporaries.                       
//                                                                                                  
// The protocol passes extra hidden parameters to BIP functions:                                    
//   __BIPalloc  - Allocation form selector (caller space, heap, pool, etc.)                        
//   __BIPaccess - Pointer to destination where result is constructed                               
//   __BIPfinal  - Finalization collection (for controlled components)                              
//   __BIPmaster - Task master ID (for task components)                                             
//   __BIPchain  - Activation chain (for task components)                                           
//                                                                                                  
// Reference: Ada RM 7.5 (Limited Types), RM 6.5 (Return Statements)                                
// ═════════════════════════════════════════════════════════════════════════════════════════════════

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.17.1 Algebraic Types - Sum types for BIP protocol                                            
//                                                                                                  
// BIP_Alloc_Form determines where the function result is allocated:                                
//   - CALLER: Caller provides stack/object space (most common)                                     
//   - SECONDARY_STACK: Use secondary stack for dynamic-sized returns                               
//   - GLOBAL_HEAP: Allocate on heap (from 'new' expression)                                        
//   - USER_POOL: Use user-defined storage pool                                                     
//                                                                                                  
// BIP_Formal_Kind identifies which extra formal parameter is being accessed.                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.17.3 Type Predicates - Pure functions for BIP decisions                                      
//                                                                                                  
// These predicates determine whether a type requires BIP handling. Per Ada RM 7.5, limited types
// include:                                                           
//   - Task types (always limited)                                                                  
//   - Types with "limited" in their declaration                                                    
//   - Private types declared "limited private"                                                     
//   - Composite types with limited components                                                      
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Check if type is explicitly marked limited (not just by composition)
bool BIP_Is_Explicitly_Limited (const Type_Info *t) {
  if (not t) return false;
  return t->kind == TYPE_LIMITED_PRIVATE or t->kind == TYPE_TASK;
}

// Check if type is a task type
bool BIP_Is_Task_Type (const Type_Info *t) {
  return t and t->kind == TYPE_TASK;
}

// Check if record type has any limited components (recursive)
bool BIP_Record_Has_Limited_Component (const Type_Info *t) {
  if (not t or t->kind != TYPE_RECORD) return false;
  for (uint32_t i = 0; i < t->record.component_count; i++) {
    const Type_Info *ft = t->record.components[i].component_type;
    if (not ft) continue;

    // Task component makes the record limited
    if (ft->kind == TYPE_TASK) return true;

    // Limited private component makes the record limited
    if (ft->kind == TYPE_LIMITED_PRIVATE) return true;

    // Recursively check nested records
    if (ft->kind == TYPE_RECORD and BIP_Record_Has_Limited_Component (ft))
      return true;
  }
  return false;
}

// Master predicate: Is this type limited? (RM 7.5)
bool BIP_Is_Limited_Type (const Type_Info *t) {
  if (not t) return false;

  // Explicitly marked as limited (from type declaration)
  if (t->is_limited) return true;

  // Task types are always limited
  if (t->kind == TYPE_TASK) return true;

  // Limited private types
  if (t->kind == TYPE_LIMITED_PRIVATE) return true;

  // Records with limited components
  if (t->kind == TYPE_RECORD and BIP_Record_Has_Limited_Component (t))
    return true;

  // Arrays of limited element type
  if (t->kind == TYPE_ARRAY and t->array.element_type and
    BIP_Is_Limited_Type (t->array.element_type))
    return true;
  return false;
}

// Does this function return a type requiring BIP?
bool BIP_Is_BIP_Function (const Symbol *func) {
  if (not func or func->kind != SYMBOL_FUNCTION) return false;
  return func->return_type and BIP_Is_Limited_Type (func->return_type);
}

// Does type have task components (needs activation chain)?
bool BIP_Type_Has_Task_Component (const Type_Info *t) {
  if (not t) return false;
  if (t->kind == TYPE_TASK) return true;
  if (t->kind == TYPE_RECORD) {
    for (uint32_t i = 0; i < t->record.component_count; i++) {
      if (BIP_Type_Has_Task_Component (t->record.components[i].component_type))
        return true;
    }
  }
  if (t->kind == TYPE_ARRAY and t->array.element_type)
    return BIP_Type_Has_Task_Component (t->array.element_type);
  return false;
}

// Does function need allocation form parameter? (unconstrained result)
bool BIP_Needs_Alloc_Form (const Symbol *func) {
  if (not func or not func->return_type) return false;
  const Type_Info *rt = func->return_type;

  // Unconstrained arrays need runtime size determination
  if (rt->kind == TYPE_ARRAY and not rt->array.is_constrained)
    return true;
  return false;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.17.4 Extra Formal Parameters - Hidden BIP parameters                                         
//                                                                                                  
// BIP functions receive extra hidden parameters prepended to their formals:                        
//   __BIPalloc  : i32     (BIP_Alloc_Form enum value)                                              
//   __BIPaccess : ptr     (pointer to result destination)                                          
//   __BIPmaster : i32     (task master ID, if tasks)                                               
//   __BIPchain  : ptr     (activation chain, if tasks)                                             
//                                                                                                  
// These are added during code generation, not during semantic analysis,                            
// so the Symbol structure remains unchanged.                                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// BIP extra formal names (matched by code generator)
#define BIP_ALLOC_NAME   "__BIPalloc"
#define BIP_ACCESS_NAME  "__BIPaccess"
#define BIP_MASTER_NAME  "__BIPmaster"
#define BIP_CHAIN_NAME   "__BIPchain"
#define BIP_FINAL_NAME   "__BIPfinal"

// Count of BIP extra formals for a given function
uint32_t BIP_Extra_Formal_Count (const Symbol *func) {
  if (not BIP_Is_BIP_Function (func)) return 0;
  uint32_t count = 2;  // alloc_form + object_access always present
  if (BIP_Type_Has_Task_Component (func->return_type))
    count += 2;  // task_master + activation_chain

  // Future: finalization collection for controlled types
  return count;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.17.5 Call-Site Transformation - Expanding BIP function calls                                 
//                                                                                                  
// When calling a BIP function, the caller must:                                                    
//   1. Determine allocation form (usually CALLER for declarations)                                 
//   2. Allocate destination space if CALLER                                                        
//   3. Pass extra BIP actuals before regular arguments                                             
//                                                                                                  
// Transform: X : Limited_Type := F(args);                                                          
// Into:      space = alloca(sizeof (Limited_Type))                                                 
//            F(__BIPalloc => CALLER, __BIPaccess => space, args)                                   
//            X = *space  (or X IS space if we alias)                                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Determine allocation form from call context
BIP_Alloc_Form BIP_Determine_Alloc_Form (bool is_allocator,
                        bool in_return_stmt,
                        bool has_target) {
  if (is_allocator)   return BIP_ALLOC_GLOBAL_HEAP;
  if (in_return_stmt) return BIP_ALLOC_UNSPECIFIED;  // Propagate caller's
  if (has_target)     return BIP_ALLOC_CALLER;
  return BIP_ALLOC_SECONDARY;  // Temp needed
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §13.17.6 Return Statement Expansion - Building result in place                                   
//                                                                                                  
// In a BIP function, return statements build directly into __BIPaccess:                            
//                                                                                                  
//   return (Field1 => V1, Field2 => V2);                                                           
//                                                                                                  
// Becomes (for CALLER allocation):                                                                 
//   __BIPaccess->Field1 = V1;                                                                      
//   __BIPaccess->Field2 = V2;                                                                      
//   return;                                                                                        
//                                                                                                  
// For HEAP allocation, we allocate first then build:                                               
//   tmp = malloc (sizeof (T));                                                                     
//   tmp->Field1 = V1;                                                                              
//   tmp->Field2 = V2;                                                                              
//   *__BIPaccess = tmp;  // Return allocated pointer                                               
//   return;                                                                                        
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Global BIP state for current function being generated
BIP_Function_State g_bip_state = {0};

// Initialize BIP state for a new function
void BIP_Begin_Function (const Symbol *func) {
  g_bip_state = (BIP_Function_State){0};
  if (BIP_Is_BIP_Function (func)) {
    g_bip_state.is_bip_function = true;
    g_bip_state.has_task_components =
      BIP_Type_Has_Task_Component (func->return_type);
  }
}

// Check if we're in a BIP function
bool BIP_In_BIP_Function (void) {
  return g_bip_state.is_bip_function;
}

// End BIP state for function
void BIP_End_Function (void) {
  g_bip_state = (BIP_Function_State){0};
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §14. LIBRARY MANAGEMENT - GNAT-Compatible Library Information                                    
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.1 Unit_Info - Compilation unit metadata collector                                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────
// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.2 CRC32 - Fast checksum for source identity                                                  
//                                                                                                  
// Standard CRC-32/ISO-HDLC polynomial: 0xEDB88320 (bit-reversed 0x04C11DB7)                        
// ─────────────────────────────────────────────────────────────────────────────────────────────────

uint32_t Crc32_Table[256];
bool Crc32_Table_Initialized = false;
void Crc32_Init_Table (void) {
  if (Crc32_Table_Initialized) return;
  for (uint32_t i = 0; i < 256; i++) {
    uint32_t crc = i;
    for (int j = 0; j < 8; j++)
      crc = (crc >> 1) ^ (0xEDB88320 & -(crc & 1));
    Crc32_Table[i] = crc;
  }
  Crc32_Table_Initialized = true;
}
uint32_t Crc32 (const char *data, size_t length) {
  Crc32_Init_Table ();
  uint32_t crc = 0xFFFFFFFF;
  for (size_t i = 0; i < length; i++)
    crc = Crc32_Table[(crc ^ (uint8_t)data[i]) & 0xFF] ^ (crc >> 8);
  return ~crc;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.3 Unit_Name_To_File - GNAT naming convention                                                 
//                                                                                                  
// Maps Ada unit names to file names:                                                               
//   Package_Name      > package_name.ads                                                           
//   Package_Name%b    > package_name.adb                                                           
//   Parent.Child      > parent-child.ads                                                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Unit_Name_To_File (String_Slice unit_name, bool is_body,
                char *out, size_t out_size) {
  size_t j = 0;
  for (size_t i = 0; i < unit_name.length and j < out_size - 5; i++) {
    char c = unit_name.data[i];
    if (c == '.') {
      out[j++] = '-';  // Dots become hyphens
    } else if (c >= 'A' and c <= 'Z') {
      out[j++] = c - 'A' + 'a';  // Lowercase
    } else {
      out[j++] = c;
    }
  }

  // Append extension
  const char *ext = is_body ? ".adb" : ".ads";
  for (int k = 0; ext[k] and j < out_size - 1; k++)
    out[j++] = ext[k];
  out[j] = '\0';
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.4 ALI_Collect - Gather unit info from parsed AST                                             
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void ALI_Collect_Withs (ALI_Info *ali, Syntax_Node *ctx) {
  if (not ctx) return;
  for (uint32_t i = 0; i < ctx->context.with_clauses.count; i++) {
    Syntax_Node *with_node = ctx->context.with_clauses.items[i];
    if (not with_node) continue;

    // Each WITH clause may have multiple names
    for (uint32_t j = 0; j < with_node->use_clause.names.count; j++) {
      Syntax_Node *name = with_node->use_clause.names.items[j];
      if (not name or ali->with_count >= 64) continue;
      With_Info *w = &ali->withs[ali->with_count++];
      w->name = name->kind == NK_IDENTIFIER ?
            name->string_val.text : (String_Slice){0};
      w->is_limited = false;  // TODO: detect LIMITED WITH
      w->elaborate = false;
      w->elaborate_all = false;

      // Derive file names from unit name
      char file_buf[256];
      if (w->name.data) {
        Unit_Name_To_File (w->name, false, file_buf, sizeof (file_buf));
        w->source_file = Slice_Duplicate ((String_Slice){file_buf, strlen (file_buf)});
        size_t len = strlen (file_buf);
        if (len > 4) {
          file_buf[len-3] = 'a';
          file_buf[len-2] = 'l';
          file_buf[len-1] = 'i';
        }
        w->ali_file = Slice_Duplicate ((String_Slice){file_buf, strlen (file_buf)});
      }
    }
  }
}

// Helper: extract unit name from a subprogram spec/body
String_Slice Get_Subprogram_Name (Syntax_Node *node) {
  if (not node) return (String_Slice){"UNKNOWN", 7};
  if (node->kind == NK_PROCEDURE_SPEC or node->kind == NK_FUNCTION_SPEC)
    return node->subprogram_spec.name;

  // Body has spec nested inside
  if (node->kind == NK_PROCEDURE_BODY or node->kind == NK_FUNCTION_BODY) {
    if (node->subprogram_body.specification)
      return Get_Subprogram_Name (node->subprogram_body.specification);
  }
  return (String_Slice){"UNKNOWN", 7};
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.4.2 ALI_Collect_Exports - Gather exported symbols from package spec                          
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void ALI_Collect_Exports (ALI_Info *ali, Syntax_Node *unit) {
  if (not unit or unit->kind != NK_PACKAGE_SPEC) return;
  String_Slice pkg_name = unit->package_spec.name;
  Node_List *decls = &unit->package_spec.visible_decls;
  for (uint32_t i = 0; i < decls->count and ali->export_count < 256; i++) {
    Syntax_Node *decl = decls->items[i];
    if (not decl) continue;
    Export_Info *exp = &ali->exports[ali->export_count];
    exp->line = decl->location.line;
    exp->param_count = 0;
    exp->type_name = (String_Slice){0};
    exp->mangled_name = (String_Slice){0};
    exp->llvm_type = (String_Slice){0};
    switch (decl->kind) {
      case NK_TYPE_DECL:
        exp->name = decl->type_decl.name;
        exp->kind = 'T';
        exp->mangled_name = Mangle_Qualified_Name (pkg_name, exp->name);
        exp->llvm_type = LLVM_Type_Basic (exp->name);
        ali->export_count++;
        break;
      case NK_SUBTYPE_DECL:
        exp->name = decl->type_decl.name;
        exp->kind = 'S';
        exp->mangled_name = Mangle_Qualified_Name (pkg_name, exp->name);
        if (decl->type_decl.definition) {
          Syntax_Node *def = decl->type_decl.definition;
          if (def->kind == NK_IDENTIFIER) {
            exp->type_name = def->string_val.text;
            exp->llvm_type = LLVM_Type_Basic (exp->type_name);
          } else if (def->kind == NK_SUBTYPE_INDICATION and def->subtype_ind.subtype_mark) {
            if (def->subtype_ind.subtype_mark->kind == NK_IDENTIFIER) {
              exp->type_name = def->subtype_ind.subtype_mark->string_val.text;
              exp->llvm_type = LLVM_Type_Basic (exp->type_name);
            }
          }
        }
        if (not exp->llvm_type.data) {
          const char *int_t = Llvm_Int_Type ((uint32_t)To_Bits (sm->type_integer->size));
          exp->llvm_type = (String_Slice){int_t, strlen (int_t)};
        }
        ali->export_count++;
        break;
      case NK_OBJECT_DECL:
        for (uint32_t j = 0; j < decl->object_decl.names.count and ali->export_count < 256; j++) {
          Syntax_Node *name = decl->object_decl.names.items[j];
          if (name and name->kind == NK_IDENTIFIER) {
            exp = &ali->exports[ali->export_count];
            exp->name = name->string_val.text;
            exp->kind = decl->object_decl.is_constant ? 'C' : 'V';
            exp->line = name->location.line;
            exp->mangled_name = Mangle_Qualified_Name (pkg_name, exp->name);
            if (decl->object_decl.object_type and decl->object_decl.object_type->kind == NK_IDENTIFIER) {
              exp->type_name = decl->object_decl.object_type->string_val.text;
              exp->llvm_type = LLVM_Type_Basic (exp->type_name);
            } else {
              const char *int_t = Llvm_Int_Type ((uint32_t)To_Bits (sm->type_integer->size));
              exp->llvm_type = (String_Slice){int_t, strlen (int_t)};
            }
            ali->export_count++;
          }
        }
        break;
      case NK_PROCEDURE_SPEC:
        exp->name = decl->subprogram_spec.name;
        exp->kind = 'P';
        exp->mangled_name = Mangle_Qualified_Name (pkg_name, exp->name);
        for (uint32_t j = 0; j < decl->subprogram_spec.parameters.count; j++) {
          Syntax_Node *ps = decl->subprogram_spec.parameters.items[j];
          if (ps and ps->kind == NK_PARAM_SPEC)
            exp->param_count += ps->param_spec.names.count;
        }
        exp->llvm_type = (String_Slice){"void", 4};
        ali->export_count++;
        break;
      case NK_FUNCTION_SPEC:
        exp->name = decl->subprogram_spec.name;
        exp->kind = 'F';
        exp->mangled_name = Mangle_Qualified_Name (pkg_name, exp->name);
        for (uint32_t j = 0; j < decl->subprogram_spec.parameters.count; j++) {
          Syntax_Node *ps = decl->subprogram_spec.parameters.items[j];
          if (ps and ps->kind == NK_PARAM_SPEC)
            exp->param_count += ps->param_spec.names.count;
        }
        if (decl->subprogram_spec.return_type and decl->subprogram_spec.return_type->kind == NK_IDENTIFIER) {
          exp->type_name = decl->subprogram_spec.return_type->string_val.text;
          exp->llvm_type = LLVM_Type_Basic (exp->type_name);
        } else {
          const char *int_t = Llvm_Int_Type ((uint32_t)To_Bits (sm->type_integer->size));
          exp->llvm_type = (String_Slice){int_t, strlen (int_t)};
        }
        ali->export_count++;
        break;
      case NK_EXCEPTION_DECL:
        for (uint32_t j = 0; j < decl->exception_decl.names.count and ali->export_count < 256; j++) {
          Syntax_Node *name = decl->exception_decl.names.items[j];
          if (name and name->kind == NK_IDENTIFIER) {
            exp = &ali->exports[ali->export_count];
            exp->name = name->string_val.text;
            exp->kind = 'E';
            exp->line = name->location.line;
            exp->mangled_name = Mangle_Qualified_Name (pkg_name, exp->name);
            exp->llvm_type = (String_Slice){"i8", 2};  // Exception identity
            ali->export_count++;
          }
        }
        break;
      default:
        break;
    }
  }
}
void ALI_Collect_Unit (ALI_Info *ali, Syntax_Node *cu,
               const char *source, size_t source_size) {
  if (not cu or ali->unit_count >= 8) return;
  Syntax_Node *unit = cu->compilation_unit.unit;
  if (not unit) return;
  Unit_Info *u = &ali->units[ali->unit_count++];

  // Extract unit name based on declaration kind
  switch (unit->kind) {
    case NK_PACKAGE_SPEC:
      u->unit_name = unit->package_spec.name;
      u->is_body = false;
      break;
    case NK_PACKAGE_BODY:
      u->unit_name = unit->package_body.name;
      u->is_body = true;
      break;
    case NK_PROCEDURE_BODY:
    case NK_PROCEDURE_SPEC:
      u->unit_name = Get_Subprogram_Name (unit);
      u->is_body = unit->kind == NK_PROCEDURE_BODY;
      break;
    case NK_FUNCTION_BODY:
    case NK_FUNCTION_SPEC:
      u->unit_name = Get_Subprogram_Name (unit);
      u->is_body = unit->kind == NK_FUNCTION_BODY;
      break;
    case NK_GENERIC_DECL:
      u->is_generic = true;
      if (unit->generic_decl.unit) {
        Syntax_Node *inner = unit->generic_decl.unit;
        if (inner->kind == NK_PACKAGE_SPEC)
          u->unit_name = inner->package_spec.name;
        else if (inner->kind == NK_PROCEDURE_SPEC or inner->kind == NK_FUNCTION_SPEC)
          u->unit_name = inner->subprogram_spec.name;
      }
      u->is_body = false;
      break;
    default:
      u->unit_name = (String_Slice){"UNKNOWN", 7};
      u->is_body = false;
  }

  // Compute source checksum
  u->source_checksum = Crc32 (source, source_size);

  // Derive source file name
  char file_buf[256];
  Unit_Name_To_File (u->unit_name, u->is_body, file_buf, sizeof (file_buf));
  u->source_name = Slice_Duplicate ((String_Slice){file_buf, strlen (file_buf)});

  // Check for elaboration pragmas (simplified)
  u->is_preelaborate = false;
  u->is_pure = false;
  u->has_elaboration = true;  // Assume has elaboration unless proven otherwise

  // Collect WITH dependencies
  ALI_Collect_Withs (ali, cu->compilation_unit.context);

  // Collect exported symbols from package specs
  if (unit and unit->kind == NK_PACKAGE_SPEC) {
    ALI_Collect_Exports (ali, unit);
  }
}

// LLVM type signature derived from the type system via Symbol_Manager.                             
// Looks up the Ada type name in the symbol table and uses Type_To_Llvm                             
// to get the correct LLVM representation.                                                          
//                                                                                                  
String_Slice LLVM_Type_Basic (String_Slice ada_type) {

  // Look up in the symbol table for proper type-system derivation
  Symbol *sym = Symbol_Find (ada_type);
  if (sym and sym->type) {
    const char *llvm = Type_To_Llvm (sym->type);
    return (String_Slice){llvm, strlen (llvm)};
  }

  // Error: type not found in symbol table
  fprintf (stderr, "error: LLVM_Type_Basic: type '%.*s' not found in symbol table\n",
      (int)ada_type.length, ada_type.data);
  const char *fallback = Llvm_Int_Type ((uint32_t)To_Bits (sm->type_integer->size));
  return (String_Slice){fallback, strlen (fallback)};
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.5 ALI_Write - Emit .ali file in GNAT format                                                  
//                                                                                                  
// Per lib-writ.ads, the minimum valid ALI file needs:                                              
//   V line (version) - MUST be first                                                               
//   P line (parameters) - MUST be present                                                          
//   At least one U line (unit)                                                                     
// ─────────────────────────────────────────────────────────────────────────────────────────────────

#define ALI_VERSION "Ada83 1.0 built " __DATE__ " " __TIME__
void ALI_Write (FILE *out, ALI_Info *ali) {

  // V line: Version - must be first per GNAT spec
  fprintf (out, "V \"%s\"\n", ALI_VERSION);

  // P line: Parameters/flags - ZX = zero-cost exceptions
  fprintf (out, "P ZX\n");

  // Blank line before restrictions
  fprintf (out, "\n");

  // R line: Restrictions (minimal)
  fprintf (out, "RN\n");

  // U lines: Unit entries
  for (uint32_t i = 0; i < ali->unit_count; i++) {
    Unit_Info *u = &ali->units[i];

    // U unit-name source-name version [flags]
    fprintf (out, "\nU %.*s%s %.*s %08X",
        (int)u->unit_name.length, u->unit_name.data,
        u->is_body ? "%b" : "%s",
        (int)u->source_name.length, u->source_name.data,
        u->source_checksum);

    // Flags
    if (u->is_generic) fprintf (out, " GE");
    if (u->is_preelaborate) fprintf (out, " PR");
    if (u->is_pure) fprintf (out, " PU");
    if (not u->has_elaboration) fprintf (out, " NE");
    if (not u->is_body) fprintf (out, " PK");
    else fprintf (out, " SU");
    fprintf (out, "\n");

    // W lines: WITH dependencies for this unit
    for (uint32_t j = 0; j < ali->with_count; j++) {
      With_Info *w = &ali->withs[j];
      if (not w->name.data) continue;
      char line_type = w->is_limited ? 'Y' : 'W';
      fprintf (out, "%c %.*s%s",
          line_type,
          (int)w->name.length, w->name.data,
          "%s");  // Assume spec dependency
      if (w->source_file.data) {
        fprintf (out, " %.*s %.*s",
            (int)w->source_file.length, w->source_file.data,
            (int)w->ali_file.length, w->ali_file.data);
      }
      if (w->elaborate) fprintf (out, " E");
      if (w->elaborate_all) fprintf (out, " EA");
      fprintf (out, "\n");
    }
  }

  // D lines: Source dependencies
  fprintf (out, "\n");
  for (uint32_t i = 0; i < ali->unit_count; i++) {
    Unit_Info *u = &ali->units[i];

    // Self-dependency
    fprintf (out, "D %.*s 00000000 %08X\n",
        (int)u->source_name.length, u->source_name.data,
        u->source_checksum);
  }
  for (uint32_t i = 0; i < ali->with_count; i++) {
    With_Info *w = &ali->withs[i];
    if (w->source_file.data) {
      fprintf (out, "D %.*s 00000000 00000000\n",
          (int)w->source_file.length, w->source_file.data);
    }
  }

  // X lines: Exported symbols (extended format for Ada83 separate compilation)                     
  //                                                                                                
  // Format: X kind name:line llvm_type @mangled [ada_type] [(params)]                              
  //                                                                                                
  //   kind: T=type, S=subtype, V=variable, C=constant, P=procedure, F=function, E=exception        
  //   llvm_type: LLVM IR type signature (i64, double, void, ptr, etc.)                             
  //   @mangled: LLVM symbol name for linking                                                       
  //   ada_type: Ada type name for typed symbols                                                    
  //   (params): Parameter count for subprograms                                                    
  //                                                                                                
  // This provides everything needed to compile against the package without source:                 
  //   - Type checking via ada_type                                                                 
  //   - Code generation via llvm_type and @mangled                                                 
  //   - Linking via @mangled symbol references                                                     
  //                                                                                                
  if (ali->export_count > 0) {
    fprintf (out, "\n");
    for (uint32_t i = 0; i < ali->export_count; i++) {
      Export_Info *x = &ali->exports[i];

      // X kind name:line
      fprintf (out, "X %c %.*s:%u",
          x->kind,
          (int)x->name.length, x->name.data,
          x->line);

      // llvm_type
      if (x->llvm_type.data) {
        fprintf (out, " %.*s", (int)x->llvm_type.length, x->llvm_type.data);
      } else {
        fprintf (out, " i64");
      }

      // @mangled
      if (x->mangled_name.data) {
        fprintf (out, " @%.*s", (int)x->mangled_name.length, x->mangled_name.data);
      }

      // ada_type (for typed symbols)
      if (x->type_name.data) {
        fprintf (out, " %.*s", (int)x->type_name.length, x->type_name.data);
      }

      // (params) for subprograms
      if (x->param_count > 0) {
        fprintf (out, " (%u)", x->param_count);
      }
      fprintf (out, "\n");
    }
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.6 Generate_ALI_File - Entry point for ALI generation                                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Generate_ALI_File (const char *output_path,
                Syntax_Node **units, int unit_count,
                const char *source, size_t source_size) {

  // Build ALI path from output path (replace .ll with .ali)
  char ali_path[512];
  size_t len = strlen (output_path);
  if (len > 3 and strcmp (output_path + len - 3, ".ll") == 0) {
    snprintf (ali_path, sizeof (ali_path), "%.*s.ali", (int)(len - 3), output_path);
  } else {
    snprintf (ali_path, sizeof (ali_path), "%s.ali", output_path);
  }
  FILE *ali_file = fopen (ali_path, "w");
  if (not ali_file) {
    fprintf (stderr, "Warning: cannot create ALI file '%s'\n", ali_path);
    return;
  }

  // Collect information from all compilation units
  ALI_Info ali = {0};
  for (int i = 0; i < unit_count; i++) {
    ALI_Collect_Unit (&ali, units[i], source, source_size);
  }

  // Write ALI file
  ALI_Write (ali_file, &ali);
  fclose (ali_file);
  fprintf (stderr, "Generated ALI file '%s'\n", ali_path);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §14.7 ALI_Reader - Parse .ali files for dependency management                                    
//                                                                                                  
// We read ALI files to:                                                                            
//   1. Skip recompilation of unchanged units (checksum match)                                      
//   2. Load exported symbols from precompiled packages                                             
//   3. Track dependencies for elaboration ordering                                                 
//   4. Find generic templates for instantiation                                                    
// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Global ALI cache                                                                                 
//                                                                                                  
ALI_Cache_Entry ALI_Cache[256];
uint32_t        ALI_Cache_Count = 0;

// Skip whitespace
const char *ALI_Skip_Ws (const char *cursor) {
  while (*cursor == ' ' or *cursor == '\t') cursor++;
  return cursor;
}

// Read until whitespace or newline, return end pointer
const char *ALI_Read_Token (const char *cursor, char *buf, size_t bufsize) {
  cursor = ALI_Skip_Ws (cursor);
  size_t pos = 0;
  while (*cursor and *cursor != ' ' and *cursor != '\t' and *cursor != '\n' and pos < bufsize - 1) {
    buf[pos++] = *cursor++;
  }
  buf[pos] = '\0';
  return cursor;
}

// Parse a hex value
uint32_t ALI_Parse_Hex (const char *str) {
  uint32_t val = 0;
  while (*str) {
    char ch = *str++;
    if (ch >= '0' and ch <= '9')      val = (val << 4) | (ch - '0');
    else if (ch >= 'A' and ch <= 'F') val = (val << 4) | (ch - 'A' + 10);
    else if (ch >= 'a' and ch <= 'f') val = (val << 4) | (ch - 'a' + 10);
    else break;
  }
  return val;
}

// Read and parse an ALI file, returning cache entry or NULL
ALI_Cache_Entry *ALI_Read (const char *ali_path) {

  // Check if already cached
  for (uint32_t i = 0; i < ALI_Cache_Count; i++) {
    if (ALI_Cache[i].ali_file and strcmp (ALI_Cache[i].ali_file, ali_path) == 0) {
      return &ALI_Cache[i];
    }
  }

  // Read ALI file
  FILE *file = fopen (ali_path, "r");
  if (not file) return NULL;

  // Allocate cache entry
  if (ALI_Cache_Count >= 256) {
    fclose (file);
    return NULL;
  }
  ALI_Cache_Entry *entry = &ALI_Cache[ALI_Cache_Count++];
  memset (entry, 0, sizeof (*entry));
  entry->ali_file = strdup (ali_path);
  char line[1024];
  char token[256];
  while (fgets (line, sizeof (line), file)) {
    const char *cursor = line;

    // Version line: V "version" - reject if compiler build differs.                                
    // This ensures stale ALI files from an older compiler rebuild                                  
    // are not reused; the unit will be reparsed from source.                                       
    //                                                                                              
    if (line[0] == 'V') {
      char ver[256] = {0};
      const char *quote = strchr (line, '"');
      if (quote) {
        quote++;
        const char *end = strchr (quote, '"');
        if (end and (size_t)(end - quote) < sizeof (ver)) {
          memcpy (ver, quote, (size_t)(end - quote));
          ver[end - quote] = '\0';
        }
      }

      // ALI was produced by a different compiler build - stale
      if (strcmp (ver, ALI_VERSION) != 0) {
        fclose (file);
        ALI_Cache_Count--;  // release the cache slot
        return NULL;
      }
      continue;
    }
    else if (line[0] == 'P') {

      // Parameters line: P flags
      continue;
    }
    else if (line[0] == 'U') {

      // Unit line: U name source checksum [flags]
      cursor = ALI_Read_Token (cursor + 1, token, sizeof (token));  // Skip 'U'

      // Unit name (with %s/%b suffix)
      char *pct = strchr (token, '%');
      if (pct) {
        entry->is_spec = (pct[1] == 's');
        *pct = '\0';
      }
      entry->unit_name = strdup (token);

      // Source file
      cursor = ALI_Read_Token (cursor, token, sizeof (token));
      entry->source_file = strdup (token);

      // Checksum
      cursor = ALI_Read_Token (cursor, token, sizeof (token));
      entry->checksum = ALI_Parse_Hex (token);

      // Parse flags
      while (*cursor and *cursor != '\n') {
        cursor = ALI_Read_Token (cursor, token, sizeof (token));
        if (strcmp (token, "GE") == 0) entry->is_generic = true;
        else if (strcmp (token, "PR") == 0) entry->is_preelaborate = true;
        else if (strcmp (token, "PU") == 0) entry->is_pure = true;
      }
    }
    else if (line[0] == 'W' or line[0] == 'Y' or line[0] == 'Z') {

      // With line: W/Y/Z name [source ali] [flags]
      cursor = ALI_Read_Token (cursor + 1, token, sizeof (token));

      // Strip %s/%b suffix
      char *pct = strchr (token, '%');
      if (pct) *pct = '\0';
      if (entry->with_count < 64) {
        entry->withs[entry->with_count++] = strdup (token);
      }
    }
    else if (line[0] == 'D') {

      // Dependency line: D source timestamp checksum - informational
      continue;
    }
    else if (line[0] == 'X') {

      // Export line: X kind name:line llvm_type @mangled [ada_type] [(params)]                     
      //                                                                                            
      // Example: X F Get_Value:9 i64 @test_exports__get_value Counter                              
      //                                                                                            
      if (entry->export_count >= 256) continue;
      ALI_Export *exp = &entry->exports[entry->export_count];
      memset (exp, 0, sizeof (*exp));
      cursor = ALI_Skip_Ws (cursor + 1);  // Skip 'X'

      // Kind (single char: T/S/V/C/P/F/E)
      exp->kind = *cursor++;
      cursor = ALI_Skip_Ws (cursor);

      // Name:line
      cursor = ALI_Read_Token (cursor, token, sizeof (token));
      char *colon = strchr (token, ':');
      if (colon) {
        *colon = '\0';
        exp->name = strdup (token);
        exp->line = (uint32_t)atoi (colon + 1);
      } else {
        exp->name = strdup (token);
        exp->line = 0;
      }

      // llvm_type
      cursor = ALI_Skip_Ws (cursor);
      if (*cursor and *cursor != '\n') {
        cursor = ALI_Read_Token (cursor, token, sizeof (token));
        if (token[0]) exp->llvm_type = strdup (token);
      }

      // @mangled
      cursor = ALI_Skip_Ws (cursor);
      if (*cursor == '@') {
        cursor++;  // Skip '@'
        cursor = ALI_Read_Token (cursor, token, sizeof (token));
        if (token[0]) exp->mangled_name = strdup (token);
      }

      // Remaining tokens: ada_type and/or (params)
      while (*cursor and *cursor != '\n') {
        cursor = ALI_Skip_Ws (cursor);
        if (*cursor == '(') {

          // (params)
          cursor = ALI_Read_Token (cursor, token, sizeof (token));
          exp->param_count = (uint32_t)atoi (token + 1);
        } else if (*cursor and *cursor != '\n') {

          // ada_type
          cursor = ALI_Read_Token (cursor, token, sizeof (token));
          if (token[0] and token[0] != '(') {
            if (exp->type_name) free (exp->type_name);
            exp->type_name = strdup (token);
          }
        }
      }
      entry->export_count++;
    }
  }
  fclose (file);
  return entry;
}

// Check if an ALI file is up-to-date with its source
bool ALI_Is_Current (const char *ali_path, const char *source_path) {
  ALI_Cache_Entry *entry = ALI_Read (ali_path);
  if (not entry) return false;

  // Read source and compute checksum
  size_t   source_size;
  char    *source_text = Read_File (source_path, &source_size);
  if (not source_text) return false;
  uint32_t current_checksum = Crc32 (source_text, source_size);
  free (source_text);
  return (current_checksum == entry->checksum);
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §15. ELABORATION MODEL - Standard-Style Dependency Graph Algorithm                               
//                                                                                                  
// Implements the full Standard elaboration ordering algorithm as described                         
// in bindo-elaborators.adb. This determines the safe order in which library                        
// units must be elaborated at program startup (Ada RM 10.2).                                       
//                                                                                                  
// The algorithm proceeds in phases:                                                                
//   1. BUILD GRAPH: Create vertices for units, edges for dependencies                              
//   2. FIND COMPONENTS: Tarjan's SCC for cyclic dependency handling                                
//   3. ELABORATE: Topological sort with priority ordering                                          
//   4. VALIDATE: Verify all constraints satisfied                                                  
//                                                                                                  
// Key insight from GNAT: Edges are classified as "strong" (must-satisfy) or                        
// "weak" (can-ignore-for-dynamic-model). This allows breaking cycles when                          
// compiled with -gnatE (dynamic elaboration checking).                                             
//                                                                                                  
// Style: Haskell-like C99 with algebraic data types (tagged unions),                               
// pure functions where possible, and composition over mutation.                                    
// ═════════════════════════════════════════════════════════════════════════════════════════════════

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.1 Algebraic Types - Sum types via tagged unions                                              
//                                                                                                  
// Following the Haskell pattern: data Kind = A | B | C                                             
// In C99: enum for tag, union for payload, struct wrapper.                                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Is this edge kind inherently strong?
bool Edge_Kind_Is_Strong (Elab_Edge_Kind k) {
  return k != EDGE_INVOCATION;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.4 Graph Structure - Vertices + Edges + Components                                            
//                                                                                                  
// Uses arena allocation for vertices/edges, dynamic arrays for order.                              
// Maximum capacities chosen to handle large Ada programs.                                          
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.5 Graph Construction - Pure creation functions                                               
//                                                                                                  
// Functions return new graph/vertex/edge without side effects.                                     
// Following functional style: prefer immutable creation over mutation.                             
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Initialize an empty graph
Elab_Graph Elab_Graph_New (void) {
  Elab_Graph g = {0};
  return g;
}

// Add a vertex, returning its ID (or 0 on failure)
uint32_t Elab_Add_Vertex (Elab_Graph *g, String_Slice name,
                 Elab_Unit_Kind kind, Symbol *sym) {
  if (g->vertex_count >= ELAB_MAX_VERTICES) return 0;
  uint32_t id = ++g->vertex_count;  // IDs are 1-based
  Elab_Vertex *v = &g->vertices[id - 1];

  *v = (Elab_Vertex){
    .id              = id,
    .name            = name,
    .kind            = kind,
    .symbol          = sym,
    .tarjan_index    = -1,
    .tarjan_lowlink  = -1,
    .is_predefined   = (name.length >= 4 and
               (strncasecmp (name.data, "Ada.", 4) == 0 or
              strncasecmp (name.data, "System", 6) == 0 or
              strncasecmp (name.data, "Interfaces", 10) == 0)),
    .is_internal     = (name.length >= 5 and
               strncasecmp (name.data, "GNAT.", 5) == 0)
  };
  return id;
}

// Find vertex by name, returning ID (or 0 if not found)
uint32_t Elab_Find_Vertex (const Elab_Graph *g, String_Slice name,
                  Elab_Unit_Kind kind) {
  for (uint32_t i = 0; i < g->vertex_count; i++) {
    const Elab_Vertex *v = &g->vertices[i];
    if (v->kind == kind and v->name.length == name.length and
      strncasecmp (v->name.data, name.data, name.length) == 0) {
      return v->id;
    }
  }
  return 0;
}

// Get vertex by ID (1-based), returns NULL if invalid
Elab_Vertex *Elab_Get_Vertex(Elab_Graph *g, uint32_t id) {
  return (id > 0 and id <= g->vertex_count) ? &g->vertices[id - 1] : NULL;
}
const Elab_Vertex *Elab_Get_Vertex_Const(const Elab_Graph *g, uint32_t id) {
  return (id > 0 and id <= g->vertex_count) ? &g->vertices[id - 1] : NULL;
}

// Add an edge from pred_id to succ_id, returning edge ID (or 0 on failure)
uint32_t Elab_Add_Edge (Elab_Graph *g, uint32_t pred_id, uint32_t succ_id,
                 Elab_Edge_Kind kind) {
  if (g->edge_count >= ELAB_MAX_EDGES) return 0;
  if (pred_id == 0 or succ_id == 0) return 0;
  if (pred_id == succ_id) return 0;  // No self-loops

  // Check for duplicate edge
  Elab_Vertex *pred = Elab_Get_Vertex (g, pred_id);
  if (pred) {
    for (uint32_t e = pred->first_succ_edge; e; ) {
      const Elab_Edge *edge = &g->edges[e - 1];
      if (edge->succ_vertex_id == succ_id and edge->kind == kind)
        return e;  // Already exists
      e = edge->next_pred_edge;
    }
  }
  uint32_t id = ++g->edge_count;
  Elab_Edge *e = &g->edges[id - 1];

  *e = (Elab_Edge){
    .id             = id,
    .kind           = kind,
    .is_strong      = Edge_Kind_Is_Strong (kind),
    .pred_vertex_id = pred_id,
    .succ_vertex_id = succ_id
  };

  // Thread into predecessor's outgoing list
  if (pred) {
    e->next_pred_edge = pred->first_succ_edge;
    pred->first_succ_edge = id;
  }

  // Thread into successor's incoming list
  Elab_Vertex *succ = Elab_Get_Vertex (g, succ_id);
  if (succ) {
    e->next_succ_edge = succ->first_pred_edge;
    succ->first_pred_edge = id;

    // Update pending counts
    if (e->is_strong) succ->pending_strong++;
    else              succ->pending_weak++;
  }
  return id;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.6 Tarjan's SCC Algorithm - Find strongly connected components                                
//                                                                                                  
// Standard O(V+E) algorithm for finding SCCs. Each SCC becomes a component                         
// that must be elaborated together (handles circular dependencies).                                
//                                                                                                  
// Invariant: After completion, every vertex has a non-zero component_id.                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Tarjan_Strongconnect (Elab_Graph *g, Tarjan_State *s, uint32_t v_id) {
  Elab_Vertex *v = Elab_Get_Vertex (g, v_id);
  if (not v) return;
  v->tarjan_index = s->index;
  v->tarjan_lowlink = s->index;
  s->index++;

  // Push onto stack
  s->stack[s->stack_top++] = v_id;
  v->tarjan_on_stack = true;

  // Visit all successors
  for (uint32_t e_id = v->first_succ_edge; e_id; ) {
    const Elab_Edge *e = &g->edges[e_id - 1];
    Elab_Vertex *w = Elab_Get_Vertex (g, e->succ_vertex_id);

    // Successor not yet visited
    if (w and w->tarjan_index < 0) {
      Tarjan_Strongconnect (g, s, e->succ_vertex_id);
      v->tarjan_lowlink = (v->tarjan_lowlink < w->tarjan_lowlink)
                ? v->tarjan_lowlink : w->tarjan_lowlink;

    // Successor is on stack, part of current SCC
    } else if (w and w->tarjan_on_stack) {
      v->tarjan_lowlink = (v->tarjan_lowlink < w->tarjan_index)
                ? v->tarjan_lowlink : w->tarjan_index;
    }
    e_id = e->next_pred_edge;
  }

  // If v is a root node, pop SCC from stack
  if (v->tarjan_lowlink == v->tarjan_index) {
    uint32_t comp_id = ++g->component_count;
    uint32_t w_id;
    do {
      w_id = s->stack[--s->stack_top];
      Elab_Vertex *w = Elab_Get_Vertex (g, w_id);
      if (w) {
        w->tarjan_on_stack = false;
        w->component_id = comp_id;
      }
    } while (w_id != v_id);
  }
}
void Elab_Find_Components (Elab_Graph *g) {
  Tarjan_State s = {.stack_top = 0, .index = 0};

  // Reset Tarjan state
  for (uint32_t i = 0; i < g->vertex_count; i++) {
    g->vertices[i].tarjan_index = -1;
    g->vertices[i].tarjan_lowlink = -1;
    g->vertices[i].tarjan_on_stack = false;
    g->vertices[i].component_id = 0;
  }
  g->component_count = 0;

  // Run Tarjan's algorithm
  for (uint32_t i = 1; i <= g->vertex_count; i++) {
    if (g->vertices[i - 1].tarjan_index < 0) {
      Tarjan_Strongconnect (g, &s, i);
    }
  }

  // Compute component-level predecessor counts
  memset (g->component_pending_strong, 0, sizeof (g->component_pending_strong));
  memset (g->component_pending_weak, 0, sizeof (g->component_pending_weak));
  for (uint32_t i = 0; i < g->edge_count; i++) {
    const Elab_Edge *e = &g->edges[i];
    const Elab_Vertex *pred = Elab_Get_Vertex_Const (g, e->pred_vertex_id);
    const Elab_Vertex *succ = Elab_Get_Vertex_Const (g, e->succ_vertex_id);
    if (pred and succ and pred->component_id != succ->component_id) {
      uint32_t c = succ->component_id;
      if (e->is_strong) g->component_pending_strong[c]++;
      else              g->component_pending_weak[c]++;

      // Check for Elaborate_All edge in cycle (fatal)
      if (e->kind == EDGE_ELABORATE_ALL and
        pred->component_id == succ->component_id) {
        g->has_elaborate_all_cycle = true;
      }
    }
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.7 Vertex Predicates - Pure functions for elaboration decisions                               
//                                                                                                  
// These predicates determine vertex eligibility and priority.                                      
// All are pure (no side effects) for functional composition.                                       
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Can this vertex be elaborated now? (all strong predecessors done)
bool Elab_Is_Elaborable (const Elab_Vertex *v) {
  return v and not v->in_elab_order and v->pending_strong == 0;
}

// Can this vertex be weakly elaborated? (only weak predecessors remain)
bool Elab_Is_Weakly_Elaborable (const Elab_Vertex *v) {
  return v and not v->in_elab_order and
       v->pending_strong == 0 and v->pending_weak > 0;
}

// Does this spec vertex have an elaborable body?
bool Elab_Has_Elaborable_Body (const Elab_Graph *g,
                      const Elab_Vertex *v) {
  (void)g;  // reserved for future use
  if (not v or v->kind != UNIT_SPEC) return false;
  if (not v->body_vertex) return false;
  if (v->has_elab_body) return true;  // pragma Elaborate_Body forces it
  return Elab_Is_Elaborable (v->body_vertex);
}

// Compare two vertices for elaboration priority.                                                   
// Returns PREC_HIGHER if a should elaborate before b.                                              
// Per GNAT bindo-elaborators.adb Is_Better_Elaborable_Vertex.                                      
//                                                                                                  
Elab_Precedence Elab_Compare_Vertices (const Elab_Graph *g,
                        const Elab_Vertex *a,
                        const Elab_Vertex *b) {
  (void)g;  // reserved for future use
  if (not a or not b) return PREC_EQUAL;

  // 1. Prefer spec with Elaborate_Body before its paired body
  if (a->has_elab_body and b->spec_vertex == a) return PREC_HIGHER;
  if (b->has_elab_body and a->spec_vertex == b) return PREC_LOWER;

  // 2. Prefer predefined units (Ada.*, System.*, Interfaces.*)
  if (a->is_predefined and not b->is_predefined) return PREC_HIGHER;
  if (b->is_predefined and not a->is_predefined) return PREC_LOWER;

  // 3. Prefer internal units (GNAT.*)
  if (a->is_internal and not b->is_internal) return PREC_HIGHER;
  if (b->is_internal and not a->is_internal) return PREC_LOWER;

  // 4. Prefer preelaborated units
  if (a->is_preelaborate and not b->is_preelaborate) return PREC_HIGHER;
  if (b->is_preelaborate and not a->is_preelaborate) return PREC_LOWER;

  // 5. Prefer pure units
  if (a->is_pure and not b->is_pure) return PREC_HIGHER;
  if (b->is_pure and not a->is_pure) return PREC_LOWER;

  // 6. Lexicographical tiebreaker for determinism
  size_t min_len = (a->name.length < b->name.length)
           ? a->name.length : b->name.length;
  int cmp = strncasecmp (a->name.data, b->name.data, min_len);
  if (cmp < 0) return PREC_HIGHER;
  if (cmp > 0) return PREC_LOWER;
  if (a->name.length < b->name.length) return PREC_HIGHER;
  if (a->name.length > b->name.length) return PREC_LOWER;
  return PREC_EQUAL;
}

// Compare for weak elaboration: prefer fewer weak predecessors
Elab_Precedence Elab_Compare_Weak (const Elab_Graph *g,
                      const Elab_Vertex *a,
                      const Elab_Vertex *b) {
  if (not a or not b) return PREC_EQUAL;
  if (a->pending_weak < b->pending_weak) return PREC_HIGHER;
  if (a->pending_weak > b->pending_weak) return PREC_LOWER;
  return Elab_Compare_Vertices (g, a, b);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.8 Vertex Set Operations - Functional set manipulation                                        
//                                                                                                  
// Uses bitmap representation for O(1) membership testing.                                          
// Pure functions that return new sets rather than mutating.                                        
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Elab_Vertex_Set Elab_Set_Empty (void) {
  return (Elab_Vertex_Set){0};
}
bool Elab_Set_Contains (const Elab_Vertex_Set *s, uint32_t id) {
  if (id == 0 or id > ELAB_MAX_VERTICES) return false;
  return (s->bits[(id - 1) / 64] >> ((id - 1) % 64)) & 1;
}
void Elab_Set_Insert (Elab_Vertex_Set *s, uint32_t id) {
  if (id > 0 and id <= ELAB_MAX_VERTICES)
    s->bits[(id - 1) / 64] |= (1ULL << ((id - 1) % 64));
}
void Elab_Set_Remove (Elab_Vertex_Set *s, uint32_t id) {
  if (id > 0 and id <= ELAB_MAX_VERTICES)
    s->bits[(id - 1) / 64] &= ~(1ULL << ((id - 1) % 64));
}
uint32_t Elab_Set_Size (const Elab_Vertex_Set *s) {
  uint32_t count = 0;
  for (int i = 0; i < (ELAB_MAX_VERTICES + 63) / 64; i++) {
    uint64_t v = s->bits[i];
    while (v) { count++; v &= v - 1; }  // Brian Kernighan's trick
  }
  return count;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.9 Best Vertex Selection - Find optimal elaboration candidate                                 
//                                                                                                  
// Scans a vertex set to find the best candidate using a comparator.                                
// Pure function with no side effects.                                                              
// ─────────────────────────────────────────────────────────────────────────────────────────────────

typedef bool (*Elab_Vertex_Pred)(const Elab_Vertex *v);
typedef Elab_Precedence (*Elab_Vertex_Cmp)(const Elab_Graph *,
                       const Elab_Vertex *,
                       const Elab_Vertex *);
uint32_t Elab_Find_Best_Vertex (const Elab_Graph *g,
                     const Elab_Vertex_Set *candidates,
                     Elab_Vertex_Pred pred,
                     Elab_Vertex_Cmp cmp) {
  uint32_t best_id = 0;
  const Elab_Vertex *best = NULL;
  for (uint32_t i = 1; i <= g->vertex_count; i++) {
    if (not Elab_Set_Contains (candidates, i)) continue;
    const Elab_Vertex *v = &g->vertices[i - 1];
    if (not pred(v)) continue;
    if (not best or cmp(g, v, best) == PREC_HIGHER) {
      best_id = i;
      best = v;
    }
  }
  return best_id;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.10 Elaboration Core - The main elaboration algorithm                                         
//                                                                                                  
// Implements the Standard elaboration loop:                                                        
//   1. Create elaborable/waiting vertex sets                                                       
//   2. Repeatedly find best elaborable vertex                                                      
//   3. Elaborate it and update successor counts                                                    
//   4. Handle weak elaboration for cycles                                                          
//                                                                                                  
// Per bindo-elaborators.adb Elaborate_Library_Graph.                                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Update successor when predecessor is elaborated
void Elab_Update_Successor (Elab_Graph *g, uint32_t edge_id,
                   Elab_Vertex_Set *elaborable,
                   Elab_Vertex_Set *waiting) {
  Elab_Edge *e = (edge_id > 0 and edge_id <= g->edge_count)
         ? &g->edges[edge_id - 1] : NULL;
  if (not e) return;
  Elab_Vertex *succ = Elab_Get_Vertex (g, e->succ_vertex_id);
  Elab_Vertex *pred = Elab_Get_Vertex (g, e->pred_vertex_id);
  if (not succ or not pred) return;

  // Decrement appropriate predecessor count
  if (e->is_strong and succ->pending_strong > 0)
    succ->pending_strong--;
  else if (not e->is_strong and succ->pending_weak > 0)
    succ->pending_weak--;

  // Update component counts if cross-component edge
  if (pred->component_id != succ->component_id) {
    uint32_t c = succ->component_id;
    if (e->is_strong and g->component_pending_strong[c] > 0)
      g->component_pending_strong[c]--;
    else if (not e->is_strong and g->component_pending_weak[c] > 0)
      g->component_pending_weak[c]--;
  }

  // Move successor from waiting to elaborable if now ready
  if (Elab_Is_Elaborable (succ) and not succ->in_elab_order) {
    Elab_Set_Remove (waiting, succ->id);
    Elab_Set_Insert (elaborable, succ->id);

    // Also update complement (spec/body pair)
    Elab_Vertex *comp = succ->body_vertex ? succ->body_vertex
              : succ->spec_vertex;
    if (comp and Elab_Is_Elaborable (comp) and not comp->in_elab_order) {
      Elab_Set_Remove (waiting, comp->id);
      Elab_Set_Insert (elaborable, comp->id);
    }
  }
}

// Elaborate a single vertex
void Elab_Elaborate_Vertex (Elab_Graph *g, uint32_t v_id,
                   Elab_Vertex_Set *elaborable,
                   Elab_Vertex_Set *waiting) {
  Elab_Vertex *v = Elab_Get_Vertex (g, v_id);
  if (not v or v->in_elab_order) return;

  // Mark as elaborated
  v->in_elab_order = true;
  Elab_Set_Remove (elaborable, v_id);
  Elab_Set_Remove (waiting, v_id);

  // Add to elaboration order
  if (g->order_count < ELAB_MAX_VERTICES)
    g->order[g->order_count++] = v;

  // Update all successors
  for (uint32_t e_id = v->first_succ_edge; e_id; ) {
    Elab_Edge *e = &g->edges[e_id - 1];
    Elab_Update_Successor (g, e_id, elaborable, waiting);
    e_id = e->next_pred_edge;
  }

  // If this is a spec with Elaborate_Body, immediately elaborate the body
  if (v->has_elab_body and v->body_vertex and
    not v->body_vertex->in_elab_order) {
    Elab_Elaborate_Vertex (g, v->body_vertex->id, elaborable, waiting);
  }

  // If this spec has an elaborable body, elaborate it too
  if (Elab_Has_Elaborable_Body (g, v)) {
    Elab_Elaborate_Vertex (g, v->body_vertex->id, elaborable, waiting);
  }
}

// Main elaboration algorithm
Elab_Order_Status Elab_Elaborate_Graph (Elab_Graph *g) {

  // Initialize vertex sets
  Elab_Vertex_Set elaborable = Elab_Set_Empty ();
  Elab_Vertex_Set waiting = Elab_Set_Empty ();
  for (uint32_t i = 1; i <= g->vertex_count; i++) {
    Elab_Vertex *v = &g->vertices[i - 1];
    v->in_elab_order = false;
    if (Elab_Is_Elaborable (v))
      Elab_Set_Insert (&elaborable, i);
    else
      Elab_Set_Insert (&waiting, i);
  }
  g->order_count = 0;

  // Main elaboration loop
  // Find best elaborable vertex
  while (Elab_Set_Size (&elaborable) > 0 or Elab_Set_Size (&waiting) > 0) {
    uint32_t best_id = Elab_Find_Best_Vertex (
      g, &elaborable, Elab_Is_Elaborable,
      (Elab_Vertex_Cmp)Elab_Compare_Vertices);

    // If no strongly elaborable vertex, try weak elaboration
    if (best_id == 0) {
      best_id = Elab_Find_Best_Vertex (
        g, &waiting, Elab_Is_Weakly_Elaborable,
        (Elab_Vertex_Cmp)Elab_Compare_Weak);
    }

    // If still nothing, we have an unresolvable cycle
    if (best_id == 0) {
      if (g->has_elaborate_all_cycle)
        return ELAB_ORDER_HAS_ELABORATE_ALL_CYCLE;
      return ELAB_ORDER_HAS_CYCLE;
    }
    Elab_Elaborate_Vertex (g, best_id, &elaborable, &waiting);
  }
  return ELAB_ORDER_OK;
}

// Pair spec and body vertices after all vertices are added
void Elab_Pair_Specs_Bodies (Elab_Graph *g) {
  for (uint32_t i = 0; i < g->vertex_count; i++) {
    Elab_Vertex *v = &g->vertices[i];
    if (v->kind != UNIT_SPEC) continue;

    // Find matching body
    for (uint32_t j = 0; j < g->vertex_count; j++) {
      Elab_Vertex *b = &g->vertices[j];
      if (b->kind != UNIT_BODY) continue;
      if (b->name.length != v->name.length) continue;
      if (strncasecmp (b->name.data, v->name.data, v->name.length) != 0)
        continue;

      // Found the pair
      v->body_vertex = b;
      b->spec_vertex = v;

      // Add spec-before-body edge
      Elab_Add_Edge (g, v->id, b->id, EDGE_SPEC_BEFORE_BODY);
      break;
    }
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §15.12 Elaboration Order API - Public interface                                                  
//                                                                                                  
// These functions are called from the main driver (§19) and code generator                         
// (§13) to determine and use the elaboration order.                                                
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Global elaboration graph (initialized during compilation)
Elab_Graph g_elab_graph;
bool g_elab_graph_initialized = false;

// Initialize the global elaboration graph
void Elab_Init (void) {
  if (not g_elab_graph_initialized) {
    g_elab_graph = Elab_Graph_New ();
    g_elab_graph_initialized = true;
  }
}

// Add a unit to the elaboration graph
uint32_t Elab_Register_Unit (String_Slice name, bool is_body,
                  Symbol *sym, bool is_preelaborate,
                  bool is_pure, bool has_elab_code) {
  Elab_Init ();
  Elab_Unit_Kind kind = is_body ? UNIT_BODY : UNIT_SPEC;
  uint32_t id = Elab_Find_Vertex (&g_elab_graph, name, kind);
  if (id == 0) {
    id = Elab_Add_Vertex (&g_elab_graph, name, kind, sym);
  }
  Elab_Vertex *v = Elab_Get_Vertex (&g_elab_graph, id);
  if (v) {
    v->symbol = sym;
    v->is_preelaborate = is_preelaborate;
    v->is_pure = is_pure;
    v->needs_elab_code = has_elab_code;
  }
  return id;
}

// Compute the elaboration order (call after all units registered)
Elab_Order_Status Elab_Compute_Order (void) {
  Elab_Init ();

  // Pair specs with their bodies
  Elab_Pair_Specs_Bodies (&g_elab_graph);

  // Find strongly connected components
  Elab_Find_Components (&g_elab_graph);

  // Compute elaboration order
  return Elab_Elaborate_Graph (&g_elab_graph);
}

// Get the computed elaboration order
uint32_t Elab_Get_Order_Count (void) {
  return g_elab_graph.order_count;
}
Symbol *Elab_Get_Order_Symbol(uint32_t index) {
  if (index >= g_elab_graph.order_count) return NULL;
  const Elab_Vertex *v = g_elab_graph.order[index];
  return v ? v->symbol : NULL;
}
bool Elab_Needs_Elab_Call (uint32_t index) {
  if (index >= g_elab_graph.order_count) return false;
  const Elab_Vertex *v = g_elab_graph.order[index];
  if (not v) return false;

  // Pure units never need elaboration calls
  if (v->is_pure) return false;

  // Preelaborate units without explicit elab code don't need calls
  if (v->is_preelaborate and not v->needs_elab_code) return false;

  // Only bodies generate __elab functions
  return v->kind == UNIT_BODY or v->kind == UNIT_BODY_ONLY;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §16. GENERIC EXPANSION - Macro-style instantiation                                               
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §16.1 Instantiation_Env - Formal-to-actual mapping                                               
//                                                                                                  
// Instead of mutating nodes, we carry substitution environment through.                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §16.2 Instantiation_Env helpers                                                                  
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Type_Info *Env_Lookup_Type (Instantiation_Env *env, String_Slice name) {
  for (uint32_t i = 0; i < env->count; i++) {
    if (Slice_Equal_Ignore_Case (env->mappings[i].formal_name, name))
      return env->mappings[i].actual_type;
  }
  return NULL;
}
Syntax_Node *Env_Lookup_Expr (Instantiation_Env *env, String_Slice name) {
  for (uint32_t i = 0; i < env->count; i++) {
    if (Slice_Equal_Ignore_Case (env->mappings[i].formal_name, name))
      return env->mappings[i].actual_expr;
  }
  return NULL;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §16.3 Node_Deep_Clone - Deep copy with environment substitution                                  
//                                                                                                  
// Unlike the existing node_clone_substitute, this:                                                 
//   • ALWAYS allocates new nodes (no aliasing)                                                     
//   • Uses recursion depth tracking with proper error                                              
//   • Carries environment for type substitution                                                    
// ─────────────────────────────────────────────────────────────────────────────────────────────────

Syntax_Node *Node_Deep_Clone (Syntax_Node *node, Instantiation_Env *env,
                  int depth);

// Clone a node list
void Node_List_Clone (Node_List *dst, Node_List *src,
              Instantiation_Env *env, int depth) {
  dst->count = 0;
  dst->capacity = 0;
  dst->items = NULL;
  for (uint32_t i = 0; i < src->count; i++) {
    Syntax_Node *cloned = Node_Deep_Clone (src->items[i], env, depth);
    Node_List_Push (dst, cloned);
  }
}
Syntax_Node *Node_Deep_Clone (Syntax_Node *node, Instantiation_Env *env,
                  int depth) {
  if (not node) return NULL;

  // Depth limit with REAL error, not silent aliasing
  if (depth > 500) {
    Report_Error (node->location, "generic instantiation too deeply nested");
    return NULL;
  }

  // Allocate fresh node
  Syntax_Node *n = Arena_Allocate (sizeof (Syntax_Node));
  memset (n, 0, sizeof (Syntax_Node));  // Zero-init ALL fields
  n->kind = node->kind;
  n->location = node->location;
  n->type = node->type;
  n->symbol = NULL;  // Symbols will be re-resolved

  // Substitute generic formal types throughout the cloned tree.                                    
  // When a node carries a TYPE_PRIVATE/TYPE_LIMITED_PRIVATE type whose                             
  // name matches a generic formal parameter, replace it with the actual                            
  // type.  This ensures that expressions, declarations, and statements                             
  // all use the concrete type (e.g., FLOAT) rather than the opaque                                 
  // formal type (e.g., ELEMENT_TYPE).                                                              
  //                                                                                                
  if (env and n->type and Type_Is_Private (n->type) and n->type->name.data) {
    Type_Info *subst = Env_Lookup_Type (env, n->type->name);
    if (subst) n->type = subst;
  }
  switch (node->kind) {
    case NK_IDENTIFIER:

      // Check for expression substitution (formal object parameters)
      if (env) {
        Syntax_Node *expr_subst = Env_Lookup_Expr (env, node->string_val.text);

        // Return a clone of the actual expression instead.
        // The 'n' node becomes garbage but arena will reclaim it.
        if (expr_subst) {
          return Node_Deep_Clone (expr_subst, env, depth + 1);
        }
      }
      n->string_val = node->string_val;

      // Check for type substitution
      if (env) {
        Type_Info *subst = Env_Lookup_Type (env, node->string_val.text);
        if (subst) n->type = subst;
      }
      break;
    case NK_INTEGER:
      n->integer_lit = node->integer_lit;
      break;
    case NK_REAL:
      n->real_lit = node->real_lit;
      break;
    case NK_STRING:
    case NK_CHARACTER:
      n->string_val = node->string_val;
      break;
    case NK_BINARY_OP:
      n->binary.op = node->binary.op;
      n->binary.left = Node_Deep_Clone (node->binary.left, env, depth + 1);
      n->binary.right = Node_Deep_Clone (node->binary.right, env, depth + 1);
      break;
    case NK_UNARY_OP:
      n->unary.op = node->unary.op;
      n->unary.operand = Node_Deep_Clone (node->unary.operand, env, depth + 1);
      break;
    case NK_ATTRIBUTE:
      n->attribute.prefix = Node_Deep_Clone (node->attribute.prefix, env, depth + 1);
      n->attribute.name = node->attribute.name;
      Node_List_Clone (&n->attribute.arguments, &node->attribute.arguments, env, depth + 1);
      break;
    case NK_APPLY:
      n->apply.prefix = Node_Deep_Clone (node->apply.prefix, env, depth + 1);
      Node_List_Clone (&n->apply.arguments, &node->apply.arguments, env, depth + 1);
      break;
    case NK_SELECTED:
      n->selected.prefix = Node_Deep_Clone (node->selected.prefix, env, depth + 1);
      n->selected.selector = node->selected.selector;  // String_Slice, not a node
      break;
    case NK_AGGREGATE:
      Node_List_Clone (&n->aggregate.items, &node->aggregate.items, env, depth + 1);
      n->aggregate.is_named = node->aggregate.is_named;
      break;
    case NK_ASSOCIATION:
      Node_List_Clone (&n->association.choices, &node->association.choices, env, depth + 1);
      n->association.expression = Node_Deep_Clone (node->association.expression, env, depth + 1);
      break;
    case NK_RANGE:
      n->range.low = Node_Deep_Clone (node->range.low, env, depth + 1);
      n->range.high = Node_Deep_Clone (node->range.high, env, depth + 1);
      break;
    case NK_OBJECT_DECL:
      Node_List_Clone (&n->object_decl.names, &node->object_decl.names, env, depth + 1);
      n->object_decl.object_type = Node_Deep_Clone (node->object_decl.object_type, env, depth + 1);
      n->object_decl.init = Node_Deep_Clone (node->object_decl.init, env, depth + 1);
      n->object_decl.is_constant = node->object_decl.is_constant;
      n->object_decl.is_rename = node->object_decl.is_rename;
      break;
    case NK_TYPE_DECL:
    case NK_SUBTYPE_DECL:
      n->type_decl.name = node->type_decl.name;
      n->type_decl.definition = Node_Deep_Clone (node->type_decl.definition, env, depth + 1);
      Node_List_Clone (&n->type_decl.discriminants, &node->type_decl.discriminants, env, depth + 1);
      break;
    case NK_PROCEDURE_BODY:
    case NK_FUNCTION_BODY:
      n->subprogram_body.specification = Node_Deep_Clone (node->subprogram_body.specification, env, depth + 1);
      Node_List_Clone (&n->subprogram_body.declarations, &node->subprogram_body.declarations, env, depth + 1);
      Node_List_Clone (&n->subprogram_body.statements, &node->subprogram_body.statements, env, depth + 1);
      Node_List_Clone (&n->subprogram_body.handlers, &node->subprogram_body.handlers, env, depth + 1);
      n->subprogram_body.is_separate = node->subprogram_body.is_separate;
      break;
    case NK_PROCEDURE_SPEC:
    case NK_FUNCTION_SPEC:
      n->subprogram_spec.name = node->subprogram_spec.name;
      Node_List_Clone (&n->subprogram_spec.parameters, &node->subprogram_spec.parameters, env, depth + 1);
      n->subprogram_spec.return_type = Node_Deep_Clone (node->subprogram_spec.return_type, env, depth + 1);
      n->subprogram_spec.renamed = Node_Deep_Clone (node->subprogram_spec.renamed, env, depth + 1);
      break;
    case NK_PARAM_SPEC:
      Node_List_Clone (&n->param_spec.names, &node->param_spec.names, env, depth + 1);
      n->param_spec.mode = node->param_spec.mode;
      n->param_spec.param_type = Node_Deep_Clone (node->param_spec.param_type, env, depth + 1);
      n->param_spec.default_expr = Node_Deep_Clone (node->param_spec.default_expr, env, depth + 1);
      break;
    case NK_PACKAGE_SPEC:
      n->package_spec.name = node->package_spec.name;
      Node_List_Clone (&n->package_spec.visible_decls, &node->package_spec.visible_decls, env, depth + 1);
      Node_List_Clone (&n->package_spec.private_decls, &node->package_spec.private_decls, env, depth + 1);
      break;
    case NK_PACKAGE_BODY:
      n->package_body.name = node->package_body.name;
      Node_List_Clone (&n->package_body.declarations, &node->package_body.declarations, env, depth + 1);
      Node_List_Clone (&n->package_body.statements, &node->package_body.statements, env, depth + 1);
      Node_List_Clone (&n->package_body.handlers, &node->package_body.handlers, env, depth + 1);
      break;
    case NK_ASSIGNMENT:
    case NK_CALL_STMT:  // Reuses assignment.target field
      n->assignment.target = Node_Deep_Clone (node->assignment.target, env, depth + 1);
      n->assignment.value = Node_Deep_Clone (node->assignment.value, env, depth + 1);
      break;
    case NK_IF:
      n->if_stmt.condition = Node_Deep_Clone (node->if_stmt.condition, env, depth + 1);
      Node_List_Clone (&n->if_stmt.then_stmts, &node->if_stmt.then_stmts, env, depth + 1);
      Node_List_Clone (&n->if_stmt.elsif_parts, &node->if_stmt.elsif_parts, env, depth + 1);
      Node_List_Clone (&n->if_stmt.else_stmts, &node->if_stmt.else_stmts, env, depth + 1);
      break;
    case NK_LOOP:
      n->loop_stmt.label = node->loop_stmt.label;
      n->loop_stmt.iteration_scheme = Node_Deep_Clone (node->loop_stmt.iteration_scheme, env, depth + 1);
      Node_List_Clone (&n->loop_stmt.statements, &node->loop_stmt.statements, env, depth + 1);
      n->loop_stmt.is_reverse = node->loop_stmt.is_reverse;
      break;
    case NK_RETURN:
      n->return_stmt.expression = Node_Deep_Clone (node->return_stmt.expression, env, depth + 1);
      break;
    case NK_BLOCK:
      n->block_stmt.label = node->block_stmt.label;
      Node_List_Clone (&n->block_stmt.declarations, &node->block_stmt.declarations, env, depth + 1);
      Node_List_Clone (&n->block_stmt.statements, &node->block_stmt.statements, env, depth + 1);
      Node_List_Clone (&n->block_stmt.handlers, &node->block_stmt.handlers, env, depth + 1);
      break;
    case NK_CASE:
      n->case_stmt.expression = Node_Deep_Clone (node->case_stmt.expression, env, depth + 1);
      Node_List_Clone (&n->case_stmt.alternatives, &node->case_stmt.alternatives, env, depth + 1);
      break;
    case NK_EXIT:
      n->exit_stmt.loop_name = node->exit_stmt.loop_name;
      n->exit_stmt.condition = Node_Deep_Clone (node->exit_stmt.condition, env, depth + 1);
      break;
    case NK_NULL_STMT:
    case NK_OTHERS:

      // No fields to copy
      break;
    default:

      // For node kinds not explicitly handled, do shallow copy.
      // This is safer than the original which returned aliased nodes.
      *n = *node;
      n->symbol = NULL;
      break;
  }
  return n;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §16.4 Build_Instantiation_Env - Create mapping from formals to actuals                           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Build_Instantiation_Env (Instantiation_Env *env,
                  Symbol *template_sym,
                  Symbol *instance_sym) {
  (void)sm;  // reserved for future use
  env->count = 0;
  env->template_sym = template_sym;
  env->instance_sym = instance_sym;
  if (not template_sym or not template_sym->declaration) return;
  Syntax_Node *gen_decl = template_sym->declaration;
  if (gen_decl->kind != NK_GENERIC_DECL) return;
  Node_List *formals = &gen_decl->generic_decl.formals;

  // Use pre-resolved actuals from instance symbol
  for (uint32_t i = 0; i < instance_sym->generic_actual_count and i < 32; i++) {
    Generic_Mapping *m = &env->mappings[env->count++];
    m->formal_name = instance_sym->generic_actuals[i].formal_name;
    m->actual_type = instance_sym->generic_actuals[i].actual_type;
    m->actual_symbol = NULL;
    m->actual_expr = NULL;

    // For object/subprogram formals, populate additional fields
    if (i < formals->count) {
      Syntax_Node *formal = formals->items[i];

      // Store expression for object formals
      if (formal->kind == NK_GENERIC_OBJECT_PARAM) {
        m->actual_expr = instance_sym->generic_actuals[i].actual_expr;

      // Store actual subprogram symbol for substitution during clone
      } else if (formal->kind == NK_GENERIC_SUBPROGRAM_PARAM) {
        m->actual_symbol = instance_sym->generic_actuals[i].actual_subprogram;
      }
    }
  }
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §16.5 Expand_Generic_Package - Full instantiation of generic package                             
//                                                                                                  
//   1. Clone the package spec with type substitutions                                              
//   2. Clone the package body (if found)                                                           
//   3. Resolve cloned trees with actual types                                                      
//   4. Store expanded body for code generation                                                     
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Expand_Generic_Package (Symbol *instance_sym) {
  if (not instance_sym or not instance_sym->generic_template) return;
  Symbol *template = instance_sym->generic_template;
  if (not template->generic_unit) return;

  // Build substitution environment
  Instantiation_Env env;
  Build_Instantiation_Env (&env, template, instance_sym);

  // Clone the package spec
  Syntax_Node *spec_clone = Node_Deep_Clone (template->generic_unit, &env, 0);

  // Rename to instance name
  if (spec_clone) {
    if (spec_clone->kind == NK_PACKAGE_SPEC) {
      spec_clone->package_spec.name = instance_sym->name;
    }

    // Store for later processing
    instance_sym->expanded_spec = spec_clone;
  }

  // Try to find and clone the package body
  String_Slice pkg_name = template->generic_unit->kind == NK_PACKAGE_SPEC ?
              template->generic_unit->package_spec.name :
              template->name;
  char *body_src = Lookup_Path_Body (pkg_name);

  // Parse the body - arena-allocate filename for Source_Location stability
  if (body_src) {
    size_t bfn_len = pkg_name.length + 4;
    char *body_filename = Arena_Allocate (bfn_len + 1);
    snprintf (body_filename, bfn_len + 1, "%.*s.adb",
         (int)pkg_name.length, pkg_name.data);
    Parser body_parser = Parser_New (body_src, strlen (body_src), body_filename);
    Syntax_Node *body_cu = Parse_Compilation_Unit (&body_parser);
    if (body_cu and body_cu->compilation_unit.unit and
      body_cu->compilation_unit.unit->kind == NK_PACKAGE_BODY) {

      // Clone with substitutions
      Syntax_Node *body_clone = Node_Deep_Clone (
        body_cu->compilation_unit.unit, &env, 0);

      // Rename to instance name
      if (body_clone) {
        body_clone->package_body.name = instance_sym->name;

        // Store expanded body
        instance_sym->expanded_body = body_clone;
      }
    }
  }
}

char *Read_File(const char *path, size_t *out_size) {
  FILE *f = fopen (path, "rb");
  if (not f) return NULL;
  fseek (f, 0, SEEK_END);
  long fsize = ftell (f);
  if (fsize < 0) { fclose (f); return NULL; }
  size_t size = (size_t)fsize;
  fseek (f, 0, SEEK_SET);
  char *buffer = malloc (size + 1);
  if (not buffer) { fclose (f); return NULL; }
  size_t read = fread (buffer, 1, size, f);
  fclose (f);
  buffer[read] = '\0';
  *out_size = read;
  return buffer;
}

char *Read_File_Simple(const char *path) {
  FILE *f = fopen (path, "rb");
  if (not f) return NULL;
  fseek (f, 0, SEEK_END);
  long fsize = ftell (f);
  if (fsize < 0) { fclose (f); return NULL; }
  size_t size = (size_t)fsize;
  fseek (f, 0, SEEK_SET);
  char *buffer = malloc (size + 1);
  if (not buffer) { fclose (f); return NULL; }
  size_t read_size = fread (buffer, 1, size, f);
  fclose (f);
  buffer[read_size] = '\0';
  return buffer;
}

// Find a package source file in include paths
char *Lookup_Path(String_Slice name) {
  char path[512], full_path[520];  // full_path larger for .ads extension

  // Build lowercase filename
  for (uint32_t i = 0; i < Include_Path_Count; i++) {
    size_t base_len = strlen (Include_Paths[i]);
    snprintf (path, sizeof (path), "%s%s%.*s",
         Include_Paths[i],
         (base_len > 0 and Include_Paths[i][base_len-1] != '/') ? "/" : "",
         (int)name.length, name.data);

    // Lowercase the filename part
    for (char *cursor = path + base_len; *cursor; cursor++) {
      if (*cursor >= 'A' and *cursor <= 'Z') *cursor = *cursor - 'A' + 'a';
    }

    // Try .ads extension
    snprintf (full_path, sizeof (full_path), "%s.ads", path);
    char *src = Read_File_Simple (full_path);
    if (src) return src;

    // Try .ada extension (ACATS naming convention)
    snprintf (full_path, sizeof (full_path), "%s.ada", path);
    src = Read_File_Simple (full_path);
    if (src) return src;
  }
  return NULL;
}

// Check if a precompiled .ll file exists for a package in include paths
bool Has_Precompiled_LL (String_Slice name) {
  char path[512], full_path[520];
  for (uint32_t i = 0; i < Include_Path_Count; i++) {
    size_t base_len = strlen (Include_Paths[i]);
    snprintf (path, sizeof (path), "%s%s%.*s",
         Include_Paths[i],
         (base_len > 0 and Include_Paths[i][base_len-1] != '/') ? "/" : "",
         (int)name.length, name.data);
    for (char *cursor = path + base_len; *cursor; cursor++) {
      if (*cursor >= 'A' and *cursor <= 'Z') *cursor = *cursor - 'A' + 'a';
    }
    snprintf (full_path, sizeof (full_path), "%s.ll", path);
    FILE *f = fopen (full_path, "r");
    if (f) { fclose (f); return true; }
  }
  return false;
}

// Find a package body source file in include paths
char *Lookup_Path_Body (String_Slice name) {
  char path[512], full_path[520];
  for (uint32_t i = 0; i < Include_Path_Count; i++) {
    size_t base_len = strlen (Include_Paths[i]);
    snprintf (path, sizeof (path), "%s%s%.*s",
         Include_Paths[i],
         (base_len > 0 and Include_Paths[i][base_len-1] != '/') ? "/" : "",
         (int)name.length, name.data);

    // Lowercase the filename part
    for (char *cursor = path + base_len; *cursor; cursor++) {
      if (*cursor >= 'A' and *cursor <= 'Z') *cursor = *cursor - 'A' + 'a';
    }

    // Try .adb extension
    snprintf (full_path, sizeof (full_path), "%s.adb", path);
    char *src = Read_File_Simple (full_path);
    if (src) return src;

    // Try .ada extension (ACATS uses .ada for both specs and bodies)
    snprintf (full_path, sizeof (full_path), "%s.ada", path);
    src = Read_File_Simple (full_path);
    if (src) return src;
  }
  return NULL;
}
// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §17. FILE LOADING - Include Path and Unit Loading                                                
//                                                                                                  
// Resolves Ada WITH clauses by searching include paths for source files,                           
// loading package specs/bodies on demand, and tracking which units have                            
// already been loaded to avoid duplicate processing.                                               
// ─────────────────────────────────────────────────────────────────────────────────────────────────

const char     *Include_Paths[32];
uint32_t        Include_Path_Count        = 0;

// Track loaded package bodies for code generation
Syntax_Node    *Loaded_Package_Bodies[128];
int             Loaded_Body_Count          = 0;

// Track which package bodies have already been loaded (to avoid duplicates)
String_Slice    Loaded_Body_Names[128];
int             Loaded_Body_Names_Count    = 0;
bool Body_Already_Loaded (String_Slice name) {
  for (int i = 0; i < Loaded_Body_Names_Count; i++) {
    if (Loaded_Body_Names[i].length == name.length and
      strncasecmp (Loaded_Body_Names[i].data, name.data, name.length) == 0) {
      return true;
    }
  }
  return false;
}
void Mark_Body_Loaded (String_Slice name) {
  if (Loaded_Body_Names_Count < 128) {
    Loaded_Body_Names[Loaded_Body_Names_Count++] = name;
  }
}

Loading_Set Loading_Packages = {0};
bool Loading_Set_Contains (String_Slice name) {
  for (int i = 0; i < Loading_Packages.count; i++) {
    if (Loading_Packages.names[i].length == name.length and
      strncasecmp (Loading_Packages.names[i].data, name.data, name.length) == 0) {
      return true;
    }
  }
  return false;
}
void Loading_Set_Add (String_Slice name) {
  if (Loading_Packages.count < 64) {
    Loading_Packages.names[Loading_Packages.count++] = name;
  }
}
void Loading_Set_Remove (String_Slice name) {
  for (int i = 0; i < Loading_Packages.count; i++) {
    if (Loading_Packages.names[i].length == name.length and
      strncasecmp (Loading_Packages.names[i].data, name.data, name.length) == 0) {

      // Swap with last and decrement count
      Loading_Packages.names[i] = Loading_Packages.names[--Loading_Packages.count];
      return;
    }
  }
}

// Find ALI file for a unit name in include paths
char *ALI_Find (String_Slice unit_name) {
  static char path_buf[512];
  char        file_buf[256];

  // Convert unit name to file name (lowercase, dots to hyphens)
  size_t pos = 0;
  for (size_t i = 0; i < unit_name.length and pos < sizeof (file_buf) - 5; i++) {
    char ch = unit_name.data[i];
    if (ch == '.')                       file_buf[pos++] = '-';
    else if (ch >= 'A' and ch <= 'Z')   file_buf[pos++] = ch - 'A' + 'a';
    else                                 file_buf[pos++] = ch;
  }
  file_buf[pos] = '\0';

  // Try each include path
  for (uint32_t i = 0; i < Include_Path_Count; i++) {
    snprintf (path_buf, sizeof (path_buf), "%s/%s.ali", Include_Paths[i], file_buf);
    if (access (path_buf, R_OK) == 0) {
      return path_buf;
    }
  }
  return NULL;
}

// Load symbols from an ALI file into the symbol manager
void ALI_Load_Symbols (ALI_Cache_Entry *entry) {
  if (not entry or entry->loaded) return;
  entry->loaded = true;

  // Recursively load dependencies first
  for (uint32_t i = 0; i < entry->with_count; i++) {
    char *dep_ali = ALI_Find ((String_Slice){entry->withs[i], strlen (entry->withs[i])});
    if (dep_ali) {
      ALI_Cache_Entry *dep = ALI_Read (dep_ali);
      if (dep) ALI_Load_Symbols (dep);
    }
  }

  // For specs, create package symbol and exports
  if (not entry->is_spec or entry->export_count == 0) return;
  String_Slice pkg_name = {entry->unit_name, strlen (entry->unit_name)};

  // Check if package already exists
  Symbol *pkg_sym = Symbol_Find (pkg_name);
  if (pkg_sym) return;

  // Create package symbol
  Source_Location loc = {entry->source_file, 1, 1};
  pkg_sym = Symbol_New (SYMBOL_PACKAGE, pkg_name, loc);
  pkg_sym->type = Type_New (TYPE_PACKAGE, pkg_name);
  Symbol_Add (pkg_sym);

  // Allocate exported array for qualified access
  if (entry->export_count > 0) {
    pkg_sym->exported = Arena_Allocate (entry->export_count * sizeof (Symbol*));
    pkg_sym->exported_count = 0;
  }

  // Push package scope to add exports
  Symbol_Manager_Push_Scope (pkg_sym);

  // Create symbols from exports                                                                    
  //                                                                                                
  // The mangled_name from ALI becomes external_name on Symbol,                                     
  // enabling direct LLVM references without re-mangling.                                           
  //                                                                                                
  for (uint32_t i = 0; i < entry->export_count; i++) {
    ALI_Export *exp = &entry->exports[i];
    String_Slice name = {exp->name, strlen (exp->name)};
    String_Slice mangled = exp->mangled_name ?
      (String_Slice){exp->mangled_name, strlen (exp->mangled_name)} : (String_Slice){0};
    Source_Location exp_loc = {entry->source_file, exp->line, 1};
    Symbol *sym = NULL;
    switch (exp->kind) {

      // Type: create type symbol with size derived from exported LLVM type.                        
      // Type_New defaults to 4 bytes (i32) which is wrong for smaller                              
      // types like 3-value enumerations (i8).  Use the LLVM type                                   
      // signature from the ALI export to set the correct size so that                              
      // cross-compilation-unit type widths are consistent.                                         
      //                                                                                            
      case 'T': {
        sym = Symbol_New (SYMBOL_TYPE, name, exp_loc);
        Type_Info *exported_type = Type_New (TYPE_INTEGER, name);
        if (exp->llvm_type and exp->llvm_type[0] == 'i') {
          int bits = atoi (exp->llvm_type + 1);
          if (bits > 0) exported_type->size = (bits + 7) / 8;
        }
        sym->type = exported_type;
        break;
      }

      // Subtype: link to base type
      case 'S': {
        sym = Symbol_New (SYMBOL_SUBTYPE, name, exp_loc);
        if (exp->type_name) {
          String_Slice base = {exp->type_name, strlen (exp->type_name)};
          Symbol *base_sym = Symbol_Find (base);
          sym->type = base_sym ? base_sym->type : Type_New (TYPE_INTEGER, base);
        } else {
          sym->type = Type_New (TYPE_INTEGER, name);
        }
        break;
      }

      // Variable: external reference
      case 'V': {
        sym = Symbol_New (SYMBOL_VARIABLE, name, exp_loc);
        sym->is_imported    = true;
        sym->external_name  = mangled;
        if (exp->type_name) {
          String_Slice type_slice = {exp->type_name, strlen (exp->type_name)};
          Symbol      *type_sym  = Symbol_Find (type_slice);
          sym->type = type_sym ? type_sym->type : Type_New (TYPE_INTEGER, type_slice);
        }
        break;
      }

      // Constant: external reference
      case 'C': {
        sym = Symbol_New (SYMBOL_CONSTANT, name, exp_loc);
        sym->is_imported    = true;
        sym->external_name  = mangled;
        if (exp->type_name) {
          String_Slice type_slice = {exp->type_name, strlen (exp->type_name)};
          Symbol      *type_sym  = Symbol_Find (type_slice);
          sym->type = type_sym ? type_sym->type : Type_New (TYPE_INTEGER, type_slice);
        }
        break;
      }

      // Procedure: external subprogram declaration
      case 'P': {
        sym = Symbol_New (SYMBOL_PROCEDURE, name, exp_loc);
        sym->is_imported      = true;
        sym->external_name    = mangled;
        sym->parameter_count  = exp->param_count;
        break;
      }

      // Function: external subprogram declaration
      case 'F': {
        sym = Symbol_New (SYMBOL_FUNCTION, name, exp_loc);
        sym->is_imported      = true;
        sym->external_name    = mangled;
        sym->parameter_count  = exp->param_count;
        if (exp->type_name) {
          String_Slice type_slice = {exp->type_name, strlen (exp->type_name)};
          Symbol      *type_sym  = Symbol_Find (type_slice);
          sym->return_type = type_sym ? type_sym->type : Type_New (TYPE_INTEGER, type_slice);
          sym->type        = sym->return_type;
        }
        break;
      }

      // Exception: external exception identity
      case 'E': {
        sym = Symbol_New (SYMBOL_EXCEPTION, name, exp_loc);
        sym->is_imported    = true;
        sym->external_name  = mangled;
        break;
      }
    }
    if (sym) {
      sym->parent = pkg_sym;  // Set parent for proper scoping
      Symbol_Add (sym);

      // Also add to package exported list for qualified access
      if (pkg_sym->exported_count < 256) {
        pkg_sym->exported[pkg_sym->exported_count++] = sym;
      }
    }
  }
  Symbol_Manager_Pop_Scope ();
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §17.1 ALI-Based Loading                                                                          
//                                                                                                  
// Try_Load_From_ALI is called by Load_Package_Spec to attempt fast loading                         
// from a pre-existing ALI file, bypassing full source parsing when the                             
// source checksum matches.                                                                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Try_Load_From_ALI - Called by Load_Package_Spec to attempt ALI-based loading                     
//                                                                                                  
// This is the entry point for ALI-based separate compilation:                                      
//   1. Look for ALI file in include paths                                                          
//   2. Verify checksum against source                                                              
//   3. Load symbols directly from ALI X lines                                                      
//                                                                                                  
// Returns true if successful (caller should skip parsing).                                         
//                                                                                                  
bool Try_Load_From_ALI (String_Slice name) {
  char *ali_path = ALI_Find (name);
  if (not ali_path) return false;

  // Build source path from unit name
  char source_file[256];
  size_t pos = 0;
  for (size_t i = 0; i < name.length and pos < sizeof (source_file) - 5; i++) {
    char ch = name.data[i];
    if (ch == '.')                       source_file[pos++] = '-';
    else if (ch >= 'A' and ch <= 'Z')   source_file[pos++] = ch - 'A' + 'a';
    else                                 source_file[pos++] = ch;
  }
  strcpy (source_file + pos, ".ads");

  // Find full source path in include paths
  char full_source_path[512] = {0};
  for (uint32_t i = 0; i < Include_Path_Count; i++) {
    snprintf (full_source_path, sizeof (full_source_path), "%s/%s",
         Include_Paths[i], source_file);
    if (access (full_source_path, R_OK) == 0) break;
    full_source_path[0] = '\0';
  }
  if (not full_source_path[0]) return false;

  // Check if ALI is current
  if (not ALI_Is_Current (ali_path, full_source_path)) {
    return false;  // Stale - need to recompile
  }

  // Read ALI and check for exports
  ALI_Cache_Entry *entry = ALI_Read (ali_path);
  if (not entry or entry->export_count == 0) return false;

  // Load symbols from ALI
  ALI_Load_Symbols (entry);
  return true;
}

// Load and resolve a package specification
void Load_Package_Spec (String_Slice name, char *src) {
  if (not src) return;

  // Check if already loaded
  Symbol *existing = Symbol_Find (name);
  if (existing and existing->kind == SYMBOL_PACKAGE and existing->declaration) {
    return;  // Already loaded
  }

  // Check for circular dependency (package currently being loaded)
  if (Loading_Set_Contains (name)) {
    return;  // Break cycle - package will be available when outer load completes
  }

  // Try to load from cached ALI file first.
  // If successful, we skip parsing entirely.
  if (Try_Load_From_ALI (name)) {
    return;
  }

  // Mark package as loading to detect cycles
  Loading_Set_Add (name);

  // Parse the package (ALI not available or stale)
  size_t fn_len = name.length + 4;  // ".ads"
  char *filename = Arena_Allocate (fn_len + 1);
  snprintf (filename, fn_len + 1, "%.*s.ads", (int)name.length, name.data);
  Parser parser = Parser_New (src, strlen (src), filename);
  Syntax_Node *cu = Parse_Compilation_Unit (&parser);
  if (not cu) {
    Loading_Set_Remove (name);
    return;
  }

  // Recursively load WITH'd packages
  if (cu->compilation_unit.context) {
    Node_List *withs = &cu->compilation_unit.context->context.with_clauses;
    for (uint32_t i = 0; i < withs->count; i++) {
      Syntax_Node *with_node = withs->items[i];
      for (uint32_t j = 0; j < with_node->use_clause.names.count; j++) {
        Syntax_Node *pkg_name = with_node->use_clause.names.items[j];
        if (pkg_name->kind == NK_IDENTIFIER) {
          char *pkg_src = Lookup_Path (pkg_name->string_val.text);
          if (pkg_src) {
            Load_Package_Spec (pkg_name->string_val.text, pkg_src);
          }
        }
      }
    }
  }

  // Resolve the package declarations
  if (cu->compilation_unit.unit) {
    Syntax_Node *unit = cu->compilation_unit.unit;
    if (unit->kind == NK_PACKAGE_SPEC) {
      Syntax_Node *pkg = unit;

      // Create package symbol
      Symbol *pkg_sym = Symbol_New (SYMBOL_PACKAGE, pkg->package_spec.name,
                     pkg->location);
      Type_Info *pkg_type = Type_New (TYPE_PACKAGE, pkg->package_spec.name);
      pkg_sym->type = pkg_type;
      pkg_sym->declaration = pkg;
      Symbol_Add (pkg_sym);
      pkg->symbol = pkg_sym;

      // Push package scope
      Symbol_Manager_Push_Scope (pkg_sym);

      // Resolve visible declarations
      Resolve_Declaration_List (&pkg->package_spec.visible_decls);

      // Populate package exports for qualified access (e.g., SYSTEM.MAX_INT)
      Populate_Package_Exports (pkg_sym, pkg);

      // Resolve private declarations
      Resolve_Declaration_List (&pkg->package_spec.private_decls);
      Symbol_Manager_Pop_Scope ();

      // SYSTEM.ADDRESS override (RM 13.7):                                                         
      // The SYSTEM package declares ADDRESS as NEW INTEGER (32-bit),                               
      // but on 64-bit targets, addresses require 64 bits.  Override                                
      // the parsed type to match the compiler's internal type_address.                             
      //                                                                                            
      if (Slice_Equal_Ignore_Case (name, S("SYSTEM")) and sm->type_address) {
        for (uint32_t ei = 0; ei < pkg_sym->exported_count; ei++) {
          Symbol *esym = pkg_sym->exported[ei];
          if (esym and esym->type and
            Slice_Equal_Ignore_Case (esym->name, S("ADDRESS"))) {
            esym->type->size       = sm->type_address->size;
            esym->type->alignment  = sm->type_address->alignment;
            esym->type->low_bound  = sm->type_address->low_bound;
            esym->type->high_bound = sm->type_address->high_bound;
            break;
          }
        }
      }
    }
    else if (unit->kind == NK_GENERIC_DECL) {

      // Generic unit: create SYMBOL_GENERIC with the inner spec
      Syntax_Node *inner = unit->generic_decl.unit;
      String_Slice unit_name = {0};
      if (inner and inner->kind == NK_PACKAGE_SPEC) {
        unit_name = inner->package_spec.name;
      } else if (inner and (inner->kind == NK_PROCEDURE_SPEC or
                 inner->kind == NK_FUNCTION_SPEC)) {
        unit_name = inner->subprogram_spec.name;
      }

      // Create generic symbol
      if (unit_name.data) {
        Symbol *sym = Symbol_New (SYMBOL_GENERIC, unit_name, unit->location);
        sym->declaration = unit;
        sym->generic_unit = inner;

        // Store formals list for later instantiation
        if (unit->generic_decl.formals.count > 0) {
          sym->generic_formals = unit->generic_decl.formals.items[0];
        }
        Symbol_Add (sym);
        unit->symbol = sym;

        // Resolve the generic package spec's declarations so type/exception                        
        // names are available when the body is parsed. Push scope, install                         
        // generic formals, then resolve visible/private declarations.                              
        //                                                                                          
        if (inner and inner->kind == NK_PACKAGE_SPEC) {
          Symbol_Manager_Push_Scope (sym);

          // Install generic formal parameters
          Node_List *formals = &unit->generic_decl.formals;
          for (uint32_t i = 0; i < formals->count; i++) {
            Syntax_Node *formal = formals->items[i];
            if (formal->kind == NK_GENERIC_TYPE_PARAM) {
              Symbol *type_sym = Symbol_New (SYMBOL_TYPE,
                formal->generic_type_param.name, formal->location);

              // Map def_kind to appropriate Type_Kind
              Type_Kind formal_kind = TYPE_PRIVATE;
              switch (formal->generic_type_param.def_kind) {
                case 2:  formal_kind = TYPE_ENUMERATION; break;  // DISCRETE
                case 3:  formal_kind = TYPE_INTEGER;     break;  // INTEGER
                case 4:  formal_kind = TYPE_FLOAT;       break;  // FLOAT
                case 5:  formal_kind = TYPE_FIXED;       break;  // FIXED
                case 6:  formal_kind = TYPE_ARRAY;       break;  // ARRAY
                case 7:  formal_kind = TYPE_ACCESS;      break;  // ACCESS
                default: formal_kind = TYPE_PRIVATE;     break;
              }
              Type_Info *type = Type_New (formal_kind, formal->generic_type_param.name);
              type_sym->type = type;
              formal->symbol = type_sym;
              Symbol_Add (type_sym);
            }
          }

          // Resolve visible declarations
          Resolve_Declaration_List (&inner->package_spec.visible_decls);

          // Populate package exports for qualified access
          Populate_Package_Exports (sym, inner);

          // Resolve private declarations
          Resolve_Declaration_List (&inner->package_spec.private_decls);
          Symbol_Manager_Pop_Scope ();
        }
      }
    }
  }

  // Done loading this package
  Loading_Set_Remove (name);

  // Also try to load the package body if available.
  // Skip if a precompiled .ll file exists (package provided externally).
  if (Body_Already_Loaded (name)) {
    return;  // Body already loaded
  }
  if (Has_Precompiled_LL (name)) {
    return;  // Precompiled version will be linked in
  }
  char *body_src = Lookup_Path_Body (name);
  if (body_src and Loaded_Body_Count < 128) {
    Mark_Body_Loaded (name);

    // Parse the body - arena-allocate filename so AST Source_Locations stay valid
    size_t bfn_len = name.length + 4;
    char *body_filename = Arena_Allocate (bfn_len + 1);
    snprintf (body_filename, bfn_len + 1, "%.*s.adb", (int)name.length, name.data);
    Parser body_parser = Parser_New (body_src, strlen (body_src), body_filename);
    Syntax_Node *body_cu = Parse_Compilation_Unit (&body_parser);
    if (body_cu and body_cu->compilation_unit.unit) {
      Syntax_Node *body_unit = body_cu->compilation_unit.unit;

      // Recursively load WITH'd packages from body
      if (body_cu->compilation_unit.context) {
        Node_List *withs = &body_cu->compilation_unit.context->context.with_clauses;
        for (uint32_t i = 0; i < withs->count; i++) {
          Syntax_Node *with_node = withs->items[i];
          for (uint32_t j = 0; j < with_node->use_clause.names.count; j++) {
            Syntax_Node *pkg_name = with_node->use_clause.names.items[j];
            if (pkg_name->kind == NK_IDENTIFIER) {
              char *pkg_src = Lookup_Path (pkg_name->string_val.text);
              if (pkg_src) {
                Load_Package_Spec (pkg_name->string_val.text, pkg_src);
              }
            }
          }
        }
      }

      // Look up the package symbol
      if (body_unit->kind == NK_PACKAGE_BODY) {
        String_Slice body_name = body_unit->package_body.name;
        Symbol *pkg_sym = Symbol_Find (body_name);

        // Link body to spec
        if (pkg_sym and (pkg_sym->kind == SYMBOL_PACKAGE or pkg_sym->kind == SYMBOL_GENERIC)) {
          body_unit->symbol = pkg_sym;

          // For generic packages, store the body for later instantiation
          if (pkg_sym->kind == SYMBOL_GENERIC) {
            pkg_sym->generic_body = body_unit;
          }

          // Resolve the body within package scope
          Symbol_Manager_Push_Scope (pkg_sym);

          // Install visible and private declarations from package spec
          // into the body's scope (RM 7.1, 7.2)
          Syntax_Node *spec = pkg_sym->declaration;

          // For generics, install formals first, then look at the unit
          if (spec and spec->kind == NK_GENERIC_DECL) {
            Node_List *formals = &spec->generic_decl.formals;
            for (uint32_t i = 0; i < formals->count; i++) {
              Syntax_Node *formal = formals->items[i];
              if (formal->symbol) Symbol_Add (formal->symbol);

              // For generic type parameters, create type symbol if needed
              if (formal->kind == NK_GENERIC_TYPE_PARAM and not formal->symbol) {
                Symbol *type_sym = Symbol_New (SYMBOL_TYPE,
                  formal->generic_type_param.name, formal->location);

                // Map def_kind to appropriate Type_Kind
                Type_Kind formal_kind = TYPE_PRIVATE;
                switch (formal->generic_type_param.def_kind) {
                  case 2:  formal_kind = TYPE_ENUMERATION; break;  // DISCRETE
                  case 3:  formal_kind = TYPE_INTEGER;     break;  // INTEGER
                  case 4:  formal_kind = TYPE_FLOAT;       break;  // FLOAT
                  case 5:  formal_kind = TYPE_FIXED;       break;  // FIXED
                  case 6:  formal_kind = TYPE_ARRAY;       break;  // ARRAY
                  case 7:  formal_kind = TYPE_ACCESS;      break;  // ACCESS
                  default: formal_kind = TYPE_PRIVATE;     break;
                }
                Type_Info *type = Type_New (formal_kind, formal->generic_type_param.name);
                type_sym->type = type;
                formal->symbol = type_sym;
                Symbol_Add (type_sym);
              }
            }
            spec = spec->generic_decl.unit;
          }
          if (spec and spec->kind == NK_PACKAGE_SPEC) {
            #define INSTALL_DECL_SYMBOLS(decl) do { \
              if ((decl)->symbol) Symbol_Add ((decl)->symbol); \
              if ((decl)->kind == NK_OBJECT_DECL) { \
                for (uint32_t k = 0; k < (decl)->object_decl.names.count; k++) { \
                  Syntax_Node *n = (decl)->object_decl.names.items[k]; \
                  if (n->symbol) Symbol_Add (n->symbol); \
                } \
              } \
              if ((decl)->kind == NK_EXCEPTION_DECL) { \
                for (uint32_t k = 0; k < (decl)->exception_decl.names.count; k++) { \
                  Syntax_Node *n = (decl)->exception_decl.names.items[k]; \
                  if (n->symbol) Symbol_Add (n->symbol); \
                } \
              } \
              if ((decl)->kind == NK_TYPE_DECL and (decl)->type_decl.definition and \
                (decl)->type_decl.definition->kind == NK_ENUMERATION_TYPE) { \
                Node_List *lits = &(decl)->type_decl.definition->enum_type.literals; \
                for (uint32_t k = 0; k < lits->count; k++) { \
                  if (lits->items[k]->symbol) Symbol_Add (lits->items[k]->symbol); \
                } \
              } \
            } while (0)

            // Install visible declarations
            for (uint32_t i = 0; i < spec->package_spec.visible_decls.count; i++) {
              Syntax_Node *decl = spec->package_spec.visible_decls.items[i];
              INSTALL_DECL_SYMBOLS (decl);
            }

            // Install private declarations
            for (uint32_t i = 0; i < spec->package_spec.private_decls.count; i++) {
              Syntax_Node *decl = spec->package_spec.private_decls.items[i];
              INSTALL_DECL_SYMBOLS (decl);
            }
            #undef INSTALL_DECL_SYMBOLS
          }
          Resolve_Declaration_List (&body_unit->package_body.declarations);
          Symbol_Manager_Pop_Scope ();

          // Store for code generation
          Loaded_Package_Bodies[Loaded_Body_Count++] = body_cu;
        }
      }
    }
  }
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §18. VECTOR PATHS - SIMD-Accelerated Scanning on x86-64 and ARM64                                
// ═════════════════════════════════════════════════════════════════════════════════════════════════


// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §18.1 Runtime CPU Feature Detection                                                              
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// Runtime CPU feature detection for x86-64.  AVX-512 code paths are only compiled when the
// toolchain is invoked with -mavx512bw; without that flag we fall back to AVX2 or scalar.
#ifdef SIMD_X86_64
#ifdef __AVX512BW__
int Simd_Has_Avx512 = -1;  // -1 = unchecked, 0 = absent, 1 = present
#else
__attribute__ ((unused))
int Simd_Has_Avx512 = 0;  // Permanently disabled: not compiled with AVX-512
#endif
int Simd_Has_Avx2 = -1;
void Simd_Detect_Features (void) {
  if (Simd_Has_Avx2 >= 0) return;  // Already detected, nothing to do
  uint32_t eax, ebx, ecx, edx;

  // CPUID leaf 07H, sub-leaf 0: structured extended feature flags.  AVX2 is EBX bit 5.
  __asm__ volatile (
    "mov $7, %%eax\n\t"
    "xor %%ecx, %%ecx\n\t"
    "cpuid\n\t"
    : "=a" (eax), "=b" (ebx), "=c" (ecx), "=d" (edx)
    :
    : "memory"
  );
  Simd_Has_Avx2 = (ebx >> 5) & 1;
#ifdef __AVX512BW__

  // AVX-512F is EBX bit 16, AVX-512BW is EBX bit 30 - both are required.
  Simd_Has_Avx512 = ((ebx >> 16) & 1) and ((ebx >> 30) & 1);
#endif
}
#endif

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §18.2 SIMD-Accelerated Scanning Functions                                                        
//                                                                                                  
// Four architecture paths: x86-64 (AVX-512/AVX2/SSE4.2), ARM64 (NEON), generic scalar fallback.    
// Each function scans for interesting bytes (whitespace boundaries, end-of-identifier, etc.)       
// without character-by-character loops.                                                            
// ─────────────────────────────────────────────────────────────────────────────────────────────────

#ifdef SIMD_X86_64

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// x86-64 AVX-512/AVX2 SIMD Lexer Acceleration                                                      
//                                                                                                  
//   AVX-512BW - 64-byte vector processing with k-mask registers                                    
//   AVX2      - 32-byte processing with optimised instruction scheduling                           
//   BMI2      - TZCNT for fast "find first non-matching byte" within a mask                        
//                                                                                                  
// Scanning functions:                                                                              
//   Simd_Skip_Whitespace  - skip space (0x20) and C0 controls (0x09–0x0D)                          
//   Simd_Find_Char_X86    - generic single-character search (newline, quotes)                      
//   Simd_Scan_Identifier  - match the [A-Za-z0-9_] character class                                 
//   Simd_Scan_Digits      - match [0-9_] for numeric literals                                      
// ─────────────────────────────────────────────────────────────────────────────────────────────────

// BMI2 TZCNT (trailing zero count) - returns the index of the lowest set bit.
uint32_t Tzcnt32 (uint32_t value) {
  uint32_t index;
  __asm__ ("tzcntl %1, %0" : "=r" (index) : "r" (value));
  return index;
}
uint64_t Tzcnt64 (uint64_t value) {
  uint64_t index;
  __asm__ ("tzcntq %1, %0" : "=r" (index) : "r" (value));
  return index;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// AVX-512 whitespace skip - 64 bytes at a time using k-mask registers.  Matches space (0x20)       
// and the C0 control range 0x09–0x0D (tab, LF, VT, FF, CR).  Only compiled with -mavx512bw.        
// ─────────────────────────────────────────────────────────────────────────────────────────────────

#ifdef __AVX512BW__
const char *Simd_Skip_Whitespace_Avx512 (const char *cursor, const char *limit) {
  while (cursor + 64 <= limit) {
    uint64_t mask;
    __asm__ volatile (
      "prefetcht0 128(%[src])\n\t"  // Prefetch next cache line
      "vmovdqu8 (%[src]), %%zmm0\n\t"  // Load 64 bytes
      "vpbroadcastb %[space], %%zmm1\n\t"  // Broadcast space char
      "vpcmpeqb %%zmm1, %%zmm0, %%k1\n\t"  // k1 = (c == ' ')
      "vpbroadcastb %[lo], %%zmm2\n\t"  // Broadcast 0x08
      "vpbroadcastb %[hi], %%zmm3\n\t"  // Broadcast 0x0E
      "vpcmpgtb %%zmm2, %%zmm0, %%k2\n\t"  // k2 = (c > 0x08)
      "vpcmpgtb %%zmm0, %%zmm3, %%k3\n\t"  // k3 = (0x0E > c)
      "kandd %%k2, %%k3, %%k2\n\t"  // k2 = in range [0x09,0x0D]
      "kord %%k1, %%k2, %%k0\n\t"  // k0 = whitespace mask
      "kmovq %%k0, %[mask]\n\t"  // Extract to GPR
      : [mask] "=r" (mask)
      : [src] "r" (cursor), [space] "r" ((uint32_t) ' '),
        [lo] "r" ((uint32_t) 0x08), [hi] "r" ((uint32_t) 0x0E)
      : "zmm0", "zmm1", "zmm2", "zmm3", "k0", "k1", "k2", "k3", "memory"
    );

    // If any non-whitespace byte was found, return its position.
    if (~mask) {
      uint64_t first_nonwhite = ~mask;
      __asm__ volatile ("tzcntq %1, %0" : "=r" (first_nonwhite) : "r" (first_nonwhite));
      return cursor + first_nonwhite;
    }
    cursor += 64;
  }
  return cursor;
}
#endif  // __AVX512BW__

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// AVX2 Whitespace Skip with 2x Unrolling                                                           
//                                                                                                  
// Processes 64 bytes (two 32-byte YMM loads) per iteration for better throughput on long runs of   
// whitespace.  Falls back to a single 32-byte pass for the remaining tail before returning to the  
// scalar loop in the dispatcher.                                                                   
// ─────────────────────────────────────────────────────────────────────────────────────────────────

const char *Simd_Skip_Whitespace_Avx2 (const char *cursor, const char *limit) {

  // 2x unrolled: process 64 bytes per iteration
  while (cursor + 64 <= limit) {
    uint32_t m0, m1;
    __asm__ volatile (
      "prefetcht0 128(%[src])\n\t"

      // Load two 32-byte chunks in parallel
      "vmovdqu (%[src]), %%ymm0\n\t"
      "vmovdqu 32(%[src]), %%ymm8\n\t"

      // Broadcast constants once, reuse for both chunks
      "vmovd %[space], %%xmm1\n\t"
      "vpbroadcastb %%xmm1, %%ymm1\n\t"
      "vmovd %[lo], %%xmm2\n\t"
      "vpbroadcastb %%xmm2, %%ymm2\n\t"
      "vmovd %[hi], %%xmm3\n\t"
      "vpbroadcastb %%xmm3, %%ymm3\n\t"

      // First chunk: space match, range check 0x09..0x0D
      "vpcmpeqb %%ymm1, %%ymm0, %%ymm5\n\t"
      "vpcmpgtb %%ymm2, %%ymm0, %%ymm6\n\t"
      "vpcmpgtb %%ymm0, %%ymm3, %%ymm7\n\t"

      // Second chunk: same comparisons on the next 32 bytes
      "vpcmpeqb %%ymm1, %%ymm8, %%ymm9\n\t"
      "vpcmpgtb %%ymm2, %%ymm8, %%ymm10\n\t"
      "vpcmpgtb %%ymm8, %%ymm3, %%ymm11\n\t"

      // Combine results: whitespace = space OR (> 0x08 AND < 0x0E)
      "vpand %%ymm6, %%ymm7, %%ymm6\n\t"
      "vpor %%ymm5, %%ymm6, %%ymm0\n\t"
      "vpand %%ymm10, %%ymm11, %%ymm10\n\t"
      "vpor %%ymm9, %%ymm10, %%ymm8\n\t"

      // Extract per-byte masks
      "vpmovmskb %%ymm0, %[m0]\n\t"
      "vpmovmskb %%ymm8, %[m1]\n\t"
      "vzeroupper\n\t"
      : [m0] "=r" (m0), [m1] "=r" (m1)
      : [src] "r" (cursor), [space] "r" ((uint32_t)' '),
        [lo] "r" ((uint32_t)0x08), [hi] "r" ((uint32_t)0x0E)
      : "ymm0", "ymm1", "ymm2", "ymm3", "ymm5", "ymm6", "ymm7",
        "ymm8", "ymm9", "ymm10", "ymm11", "memory"
    );
    if (m0 != 0xFFFFFFFF) return cursor + Tzcnt32 (~m0);
    if (m1 != 0xFFFFFFFF) return cursor + 32 + Tzcnt32 (~m1);
    cursor += 64;
  }

  // Handle remaining 32-byte chunk
  while (cursor + 32 <= limit) {
    uint32_t mask;
    __asm__ volatile (
      "vmovdqu (%[src]), %%ymm0\n\t"
      "vmovd %[space], %%xmm1\n\t"
      "vpbroadcastb %%xmm1, %%ymm1\n\t"
      "vmovd %[lo], %%xmm2\n\t"
      "vpbroadcastb %%xmm2, %%ymm2\n\t"
      "vmovd %[hi], %%xmm3\n\t"
      "vpbroadcastb %%xmm3, %%ymm3\n\t"
      "vpcmpeqb %%ymm1, %%ymm0, %%ymm5\n\t"
      "vpcmpgtb %%ymm2, %%ymm0, %%ymm6\n\t"
      "vpcmpgtb %%ymm0, %%ymm3, %%ymm7\n\t"
      "vpand %%ymm6, %%ymm7, %%ymm6\n\t"
      "vpor %%ymm5, %%ymm6, %%ymm0\n\t"
      "vpmovmskb %%ymm0, %[mask]\n\t"
      "vzeroupper\n\t"
      : [mask] "=r" (mask)
      : [src] "r" (cursor), [space] "r" ((uint32_t)' '),
        [lo] "r" ((uint32_t)0x08), [hi] "r" ((uint32_t)0x0E)
      : "ymm0", "ymm1", "ymm2", "ymm3", "ymm5", "ymm6", "ymm7", "memory"
    );
    if (mask != 0xFFFFFFFF) return cursor + Tzcnt32 (~mask);
    cursor += 32;
  }
  return cursor;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Whitespace Skip Dispatcher                                                                       
//                                                                                                  
// Selects the best available SIMD path at runtime based on CPU features detected by                
// Simd_Detect_Features, then falls through to a scalar tail for the last few bytes.                
// ─────────────────────────────────────────────────────────────────────────────────────────────────

const char *Simd_Skip_Whitespace (const char *cursor, const char *limit) {
  Simd_Detect_Features ();
#ifdef __AVX512BW__
  if (Simd_Has_Avx512 and (limit - cursor) >= 64) {
    cursor = Simd_Skip_Whitespace_Avx512 (cursor, limit);
  }
#endif
  if (Simd_Has_Avx2 and (limit - cursor) >= 32) {
    cursor = Simd_Skip_Whitespace_Avx2 (cursor, limit);
  }

  // Scalar tail for remaining bytes
  while (cursor < limit) {
    unsigned char ch = (unsigned char)*cursor;
    if (ch != ' ' and (ch < 0x09 or ch > 0x0D)) break;
    cursor++;
  }
  return cursor;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Generic Single-Character Search                                                                  
//                                                                                                  
// Searches for the first occurrence of a specific byte (newline, quote, etc.) in the given range.  
// The first 16 bytes are checked with a scalar fast path that covers most short comments and       
// string literals; beyond that the function dispatches to AVX-512 or AVX2 SIMD loops.  Used as     
// the building block for Simd_Find_Newline, Simd_Find_Quote, and Simd_Find_Double_Quote.           
// ─────────────────────────────────────────────────────────────────────────────────────────────────

const char *Simd_Find_Char_X86 (const char *cursor, const char *limit, char target) {

  // Fast path: scalar check for first 16 bytes (covers most short comments/strings)
  for (int i = 0; i < 16 and cursor + i < limit; i++) {
    if (cursor[i] == target) return cursor + i;
  }

  // Short remaining buffer - finish with scalar
  if (cursor + 16 >= limit) {
    cursor += 16;
    while (cursor < limit and *cursor != target) cursor++;
    return cursor;
  }
  cursor += 16;  // Fast path didn't find it, continue with SIMD
  Simd_Detect_Features ();
#ifdef __AVX512BW__

  // AVX-512: 64 bytes at a time
  if (Simd_Has_Avx512) {
    while (cursor + 64 <= limit) {
      uint64_t mask;
      __asm__ volatile (
        "vmovdqu8 (%[src]), %%zmm0\n\t"
        "vpbroadcastb %[c], %%zmm1\n\t"
        "vpcmpeqb %%zmm1, %%zmm0, %%k0\n\t"
        "kmovq %%k0, %[mask]\n\t"
        : [mask] "=r" (mask)
        : [src] "r" (cursor), [c] "r" ((uint32_t)target)
        : "zmm0", "zmm1", "k0", "memory"
      );
      if (mask) {
        __asm__ volatile ("tzcntq %1, %0" : "=r" (mask) : "r" (mask));
        return cursor + mask;
      }
      cursor += 64;
    }
  }
#endif

  // AVX2: 32 bytes at a time
  while (cursor + 32 <= limit) {
    uint32_t mask;
    __asm__ volatile (
      "vmovdqu (%[src]), %%ymm0\n\t"
      "vmovd %[c], %%xmm1\n\t"
      "vpbroadcastb %%xmm1, %%ymm1\n\t"
      "vpcmpeqb %%ymm1, %%ymm0, %%ymm0\n\t"
      "vpmovmskb %%ymm0, %[mask]\n\t"
      "vzeroupper\n\t"
      : [mask] "=r" (mask)
      : [src] "r" (cursor), [c] "r" ((uint32_t)target)
      : "ymm0", "ymm1", "memory"
    );
    if (mask) return cursor + Tzcnt32 (mask);
    cursor += 32;
  }

  // Scalar tail
  while (cursor < limit and *cursor != target) cursor++;
  return cursor;
}

// Convenience wrappers for common character searches
const char *Simd_Find_Newline (const char *cursor, const char *limit) {
  return Simd_Find_Char_X86 (cursor, limit, '\n');
}
const char *Simd_Find_Quote (const char *cursor, const char *limit) {
  return Simd_Find_Char_X86 (cursor, limit, '\'');
}
const char *Simd_Find_Double_Quote (const char *cursor, const char *limit) {
  return Simd_Find_Char_X86 (cursor, limit, '"');
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Identifier Character Class Scanner                                                               
//                                                                                                  
// Uses a fast unrolled table lookup for the first 8 characters, which covers the vast majority of  
// Ada identifiers.  Longer identifiers continue with a scalar table-driven loop - the branch       
// predictor handles this well since identifiers rarely exceed 8 characters.                        
// ─────────────────────────────────────────────────────────────────────────────────────────────────

const char *Simd_Scan_Identifier (const char *cursor, const char *limit) {

  // Fast path: unrolled table lookup for first 8 chars (covers most identifiers)
  if (cursor >= limit or not Is_Id_Char (*cursor)) return cursor;
  if (cursor + 1 >= limit or not Is_Id_Char (cursor[1])) return cursor + 1;
  if (cursor + 2 >= limit or not Is_Id_Char (cursor[2])) return cursor + 2;
  if (cursor + 3 >= limit or not Is_Id_Char (cursor[3])) return cursor + 3;
  if (cursor + 4 >= limit or not Is_Id_Char (cursor[4])) return cursor + 4;
  if (cursor + 5 >= limit or not Is_Id_Char (cursor[5])) return cursor + 5;
  if (cursor + 6 >= limit or not Is_Id_Char (cursor[6])) return cursor + 6;
  if (cursor + 7 >= limit or not Is_Id_Char (cursor[7])) return cursor + 7;

  // Long identifier (> 8 chars) - continue with table lookup
  cursor += 8;
  while (cursor < limit and Is_Id_Char (*cursor)) cursor++;
  return cursor;
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Digit Scanner for Numeric Literals                                                               
//                                                                                                  
// Matches the character class [0-9_] - decimal digits with optional underscores, which is the Ada  
// numeric literal syntax (LRM §2.4).  Returns a pointer to the first character that is neither a   
// digit nor an underscore.                                                                         
// ─────────────────────────────────────────────────────────────────────────────────────────────────

const char *Simd_Scan_Digits (const char *cursor, const char *limit) {
  Simd_Detect_Features ();
#ifdef __AVX512BW__

  // AVX-512 path
  if (Simd_Has_Avx512) {
    while (cursor + 64 <= limit) {
      uint64_t mask;
      __asm__ volatile (
        "vmovdqu8 (%[src]), %%zmm0\n\t"
        "vpbroadcastb %[lo], %%zmm1\n\t"
        "vpbroadcastb %[hi], %%zmm2\n\t"
        "vpbroadcastb %[u], %%zmm3\n\t"
        "vpcmpgtb %%zmm1, %%zmm0, %%k1\n\t"  // byte > '0'-1
        "vpcmpgtb %%zmm0, %%zmm2, %%k2\n\t"  // '9'+1 > byte
        "kandd %%k1, %%k2, %%k3\n\t"  // digit range
        "vpcmpeqb %%zmm3, %%zmm0, %%k4\n\t"  // underscore
        "kord %%k3, %%k4, %%k0\n\t"  // digit OR underscore
        "kmovq %%k0, %[mask]\n\t"
        : [mask] "=r" (mask)
        : [src] "r" (cursor),
          [lo] "r" ((uint32_t)('0' - 1)),
          [hi] "r" ((uint32_t)('9' + 1)),
          [u] "r" ((uint32_t)'_')
        : "zmm0", "zmm1", "zmm2", "zmm3", "k0", "k1", "k2", "k3", "k4", "memory"
      );
      if (~mask) {
        __asm__ volatile ("tzcntq %1, %0" : "=r" (mask) : "r" (~mask));
        return cursor + mask;
      }
      cursor += 64;
    }
  }
#endif

  // AVX2 path
  while (cursor + 32 <= limit) {
    uint32_t mask;
    __asm__ volatile (
      "vmovdqu (%[src]), %%ymm0\n\t"
      "vmovd %[lo], %%xmm1\n\t"
      "vpbroadcastb %%xmm1, %%ymm1\n\t"
      "vmovd %[hi], %%xmm2\n\t"
      "vpbroadcastb %%xmm2, %%ymm2\n\t"
      "vpcmpgtb %%ymm1, %%ymm0, %%ymm3\n\t"
      "vpcmpgtb %%ymm0, %%ymm2, %%ymm4\n\t"
      "vpand %%ymm3, %%ymm4, %%ymm5\n\t"
      "vmovd %[u], %%xmm1\n\t"
      "vpbroadcastb %%xmm1, %%ymm1\n\t"
      "vpcmpeqb %%ymm1, %%ymm0, %%ymm1\n\t"
      "vpor %%ymm5, %%ymm1, %%ymm0\n\t"
      "vpmovmskb %%ymm0, %[mask]\n\t"
      "vzeroupper\n\t"
      : [mask] "=r" (mask)
      : [src] "r" (cursor),
        [lo] "r" ((uint32_t)('0' - 1)),
        [hi] "r" ((uint32_t)('9' + 1)),
        [u] "r" ((uint32_t)'_')
      : "ymm0", "ymm1", "ymm2", "ymm3", "ymm4", "ymm5", "memory"
    );
    if (mask != 0xFFFFFFFF) return cursor + Tzcnt32 (~mask);
    cursor += 32;
  }

  // Scalar tail
  while (cursor < limit and ((*cursor >= '0' and *cursor <= '9') or *cursor == '_')) cursor++;
  return cursor;
}
#elif defined(SIMD_ARM64)

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// ARM64 NEON Implementation (raw inline assembly)                                                  
// ─────────────────────────────────────────────────────────────────────────────────────────────────

const char *Simd_Skip_Whitespace (const char *cursor, const char *limit) {
  while (cursor + 16 <= limit) {
    uint64_t lo, hi;
    __asm__ volatile (
      "ldr q0, [%[src]]\n\t"  // Load 16 bytes

      // Check for space (0x20)
      "movi v1.16b, #0x20\n\t"
      "cmeq v5.16b, v0.16b, v1.16b\n\t"

      // Check range 0x09..0x0D: byte > 0x08 and byte < 0x0E
      "movi v2.16b, #0x08\n\t"
      "movi v3.16b, #0x0E\n\t"
      "cmhi v6.16b, v0.16b, v2.16b\n\t"
      "cmhi v7.16b, v3.16b, v0.16b\n\t"
      "and v6.16b, v6.16b, v7.16b\n\t"  // in range

      // Combine: whitespace = space OR in_range, then invert for non-whitespace
      "orr v0.16b, v5.16b, v6.16b\n\t"
      "mvn v0.16b, v0.16b\n\t"
      "mov %[lo], v0.d[0]\n\t"
      "mov %[hi], v0.d[1]\n\t"
      : [lo] "=r" (lo), [hi] "=r" (hi)
      : [src] "r" (cursor)
      : "v0", "v1", "v2", "v3", "v5", "v6", "v7", "memory"
    );
    if (lo) return cursor + (Tzcnt64 (lo) >> 3);
    if (hi) return cursor + 8 + (Tzcnt64 (hi) >> 3);
    cursor += 16;
  }

  // Scalar tail
  while (cursor < limit) {
    unsigned char ch = (unsigned char)*cursor;
    if (ch != ' ' and (ch < 0x09 or ch > 0x0D)) break;
    cursor++;
  }
  return cursor;
}
const char *Simd_Find_Newline (const char *cursor, const char *limit) {
  while (cursor + 16 <= limit) {
    uint64_t lo, hi;
    __asm__ volatile (
      "ldr q0, [%[src]]\n\t"
      "movi v1.16b, #0x0A\n\t"
      "cmeq v0.16b, v0.16b, v1.16b\n\t"
      "mov %[lo], v0.d[0]\n\t"
      "mov %[hi], v0.d[1]\n\t"
      : [lo] "=r" (lo), [hi] "=r" (hi)
      : [src] "r" (cursor)
      : "v0", "v1", "memory"
    );
    if (lo) return cursor + (Tzcnt64 (lo) >> 3);
    if (hi) return cursor + 8 + (Tzcnt64 (hi) >> 3);
    cursor += 16;
  }
  while (cursor < limit and *cursor != '\n') cursor++;
  return cursor;
}
const char *Simd_Scan_Identifier (const char *cursor, const char *limit) {
  while (cursor + 16 <= limit) {
    uint64_t lo, hi;
    __asm__ volatile (
      "ldr q0, [%[src]]\n\t"

      // Check a-z
      "movi v1.16b, #0x60\n\t"
      "movi v2.16b, #0x7B\n\t"
      "cmhi v3.16b, v0.16b, v1.16b\n\t"
      "cmhi v4.16b, v2.16b, v0.16b\n\t"
      "and v5.16b, v3.16b, v4.16b\n\t"

      // Check A-Z
      "movi v1.16b, #0x40\n\t"
      "movi v2.16b, #0x5B\n\t"
      "cmhi v3.16b, v0.16b, v1.16b\n\t"
      "cmhi v4.16b, v2.16b, v0.16b\n\t"
      "and v6.16b, v3.16b, v4.16b\n\t"

      // Check 0-9
      "movi v1.16b, #0x2F\n\t"
      "movi v2.16b, #0x3A\n\t"
      "cmhi v3.16b, v0.16b, v1.16b\n\t"
      "cmhi v4.16b, v2.16b, v0.16b\n\t"
      "and v7.16b, v3.16b, v4.16b\n\t"

      // Check underscore
      "movi v1.16b, #0x5F\n\t"
      "cmeq v16.16b, v0.16b, v1.16b\n\t"

      // Combine: valid = lower | upper | digit | underscore, then invert
      "orr v5.16b, v5.16b, v6.16b\n\t"
      "orr v7.16b, v7.16b, v16.16b\n\t"
      "orr v0.16b, v5.16b, v7.16b\n\t"
      "mvn v0.16b, v0.16b\n\t"
      "mov %[lo], v0.d[0]\n\t"
      "mov %[hi], v0.d[1]\n\t"
      : [lo] "=r" (lo), [hi] "=r" (hi)
      : [src] "r" (cursor)
      : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "v7", "v16", "memory"
    );
    if (lo) return cursor + (Tzcnt64 (lo) >> 3);
    if (hi) return cursor + 8 + (Tzcnt64 (hi) >> 3);
    cursor += 16;
  }
  while (cursor < limit) {
    char ch = *cursor;
    if (not ((ch >= 'a' and ch <= 'z') or (ch >= 'A' and ch <= 'Z') or
        (ch >= '0' and ch <= '9') or ch == '_'))
      break;
    cursor++;
  }
  return cursor;
}
const char *Simd_Find_Quote (const char *cursor, const char *limit) {
  while (cursor + 16 <= limit) {
    uint64_t lo, hi;
    __asm__ volatile (
      "ldr q0, [%[src]]\n\t"
      "movi v1.16b, #0x27\n\t"
      "cmeq v0.16b, v0.16b, v1.16b\n\t"
      "mov %[lo], v0.d[0]\n\t"
      "mov %[hi], v0.d[1]\n\t"
      : [lo] "=r" (lo), [hi] "=r" (hi)
      : [src] "r" (cursor)
      : "v0", "v1", "memory"
    );
    if (lo) return cursor + (Tzcnt64 (lo) >> 3);
    if (hi) return cursor + 8 + (Tzcnt64 (hi) >> 3);
    cursor += 16;
  }
  while (cursor < limit and *cursor != '\'') cursor++;
  return cursor;
}
const char *Simd_Find_Double_Quote (const char *cursor, const char *limit) {
  while (cursor + 16 <= limit) {
    uint64_t lo, hi;
    __asm__ volatile (
      "ldr q0, [%[src]]\n\t"
      "movi v1.16b, #0x22\n\t"
      "cmeq v0.16b, v0.16b, v1.16b\n\t"
      "mov %[lo], v0.d[0]\n\t"
      "mov %[hi], v0.d[1]\n\t"
      : [lo] "=r" (lo), [hi] "=r" (hi)
      : [src] "r" (cursor)
      : "v0", "v1", "memory"
    );
    if (lo) return cursor + (Tzcnt64 (lo) >> 3);
    if (hi) return cursor + 8 + (Tzcnt64 (hi) >> 3);
    cursor += 16;
  }
  while (cursor < limit and *cursor != '"') cursor++;
  return cursor;
}
const char *Simd_Scan_Digits (const char *cursor, const char *limit) {
  while (cursor + 16 <= limit) {
    uint64_t lo, hi;
    __asm__ volatile (
      "ldr q0, [%[src]]\n\t"

      // Check 0-9
      "movi v1.16b, #0x2F\n\t"
      "movi v2.16b, #0x3A\n\t"
      "cmhi v3.16b, v0.16b, v1.16b\n\t"
      "cmhi v4.16b, v2.16b, v0.16b\n\t"
      "and v5.16b, v3.16b, v4.16b\n\t"

      // Check underscore
      "movi v1.16b, #0x5F\n\t"
      "cmeq v6.16b, v0.16b, v1.16b\n\t"

      // Combine and invert
      "orr v0.16b, v5.16b, v6.16b\n\t"
      "mvn v0.16b, v0.16b\n\t"
      "mov %[lo], v0.d[0]\n\t"
      "mov %[hi], v0.d[1]\n\t"
      : [lo] "=r" (lo), [hi] "=r" (hi)
      : [src] "r" (cursor)
      : "v0", "v1", "v2", "v3", "v4", "v5", "v6", "memory"
    );
    if (lo) return cursor + (Tzcnt64 (lo) >> 3);
    if (hi) return cursor + 8 + (Tzcnt64 (hi) >> 3);
    cursor += 16;
  }
  while (cursor < limit and ((*cursor >= '0' and *cursor <= '9') or *cursor == '_')) cursor++;
  return cursor;
}
#else

// Generic Scalar Implementation (Portable Fallback)                                                
//                                                                                                  
// Reference implementation for platforms without SIMD support.  All SIMD paths must produce        
// identical results to these scalar functions for all possible inputs.                             
//                                                                                                  
const char *Simd_Skip_Whitespace (const char *cursor, const char *limit) {
  while (cursor < limit) {
    unsigned char ch = (unsigned char)*cursor;
    if (ch != ' ' and (ch < 0x09 or ch > 0x0D)) break;
    cursor++;
  }
  return cursor;
}
const char *Simd_Find_Newline (const char *cursor, const char *limit) {
  while (cursor < limit and *cursor != '\n') cursor++;
  return cursor;
}
const char *Simd_Scan_Identifier (const char *cursor, const char *limit) {
  while (cursor < limit) {
    char ch = *cursor;
    if (not ((ch >= 'a' and ch <= 'z') or (ch >= 'A' and ch <= 'Z') or
        (ch >= '0' and ch <= '9') or ch == '_'))
      break;
    cursor++;
  }
  return cursor;
}
const char *Simd_Find_Quote (const char *cursor, const char *limit) {
  while (cursor < limit and *cursor != '\'') cursor++;
  return cursor;
}
const char *Simd_Find_Double_Quote (const char *cursor, const char *limit) {
  while (cursor < limit and *cursor != '"') cursor++;
  return cursor;
}
const char *Simd_Scan_Digits (const char *cursor, const char *limit) {
  while (cursor < limit and ((*cursor >= '0' and *cursor <= '9') or *cursor == '_')) cursor++;
  return cursor;
}
#endif  // SIMD architecture selection

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// §18.3 SIMD-Accelerated Decimal Parsing                                                           
//                                                                                                  
// Instead of processing one digit at a time (multiply by 10, add digit), the SIMD path batches     
// eight digits at once (multiply by 10**8, add the 8-digit value).                                 
// ─────────────────────────────────────────────────────────────────────────────────────────────────

#ifdef SIMD_X86_64

// Parse exactly 8 ASCII digit characters ('0'–'9') into a 32-bit integer in the range
// [0 .. 99_999_999] using AVX2 multiply-add instructions (see §6.1 header for the algorithm).
uint32_t Simd_Parse_8_Digits_Avx2 (const char *digits) {
  uint32_t result;
  __asm__ volatile (

    // Load 8 bytes into low portion of xmm0
    "vmovq (%[src]), %%xmm0\n\t"

    // Broadcast '0' and subtract to get digit values
    "vmovd %[zero], %%xmm1\n\t"
    "vpbroadcastb %%xmm1, %%xmm1\n\t"
    "vpsubb %%xmm1, %%xmm0, %%xmm0\n\t"

    // Multiply-add bytes to words: weights [10,1,10,1,10,1,10,1]
    "vmovq %[w1], %%xmm2\n\t"
    "vpmaddubsw %%xmm2, %%xmm0, %%xmm0\n\t"

    // Multiply-add words to dwords: weights [100,1,100,1]
    "vmovq %[w2], %%xmm2\n\t"
    "vpmaddwd %%xmm2, %%xmm0, %%xmm0\n\t"

    // Extract two dwords and combine: dw0 * 10000 + dw1
    "vmovd %%xmm0, %%eax\n\t"
    "vpextrd $1, %%xmm0, %%edx\n\t"
    "imull $10000, %%eax\n\t"
    "addl %%edx, %%eax\n\t"
    : "=a" (result)
    : [src] "r" (digits),
      [zero] "r" ((uint32_t) '0'),
      [w1] "r" (0x010A010A010A010AULL),  // byte weights [10,1,10,1,10,1,10,1]  (0x0A = 10)
      [w2] "r" (0x0001006400010064ULL)  // word weights [100,1,100,1]
    : "xmm0", "xmm1", "xmm2", "edx", "memory"
  );
  return result;
}

// Parse up to 16 ASCII digits from the buffer [cursor .. limit) using AVX2.  Validates that all    
// bytes are ASCII digits first; returns the number of digits actually consumed and stores the      
// parsed value in *out.                                                                            
//                                                                                                  
int Simd_Parse_Digits_Avx2 (const char *cursor, const char *limit, uint64_t *out) {
  int length = (limit - cursor > 16) ? 16 : (int) (limit - cursor);

  // Fewer than 8 digits available - fall back to scalar.
  if (length < 8) {
    uint64_t value = 0;
    int count = 0;
    while (count < length and cursor[count] >= '0' and cursor[count] <= '9') {
      value = value * 10 + (cursor[count] - '0');
      count++;
    }
    *out = value;
    return count;
  }

  // Validate that the first eight bytes are all ASCII digits using SIMD comparison.
  uint32_t valid_mask;
  __asm__ volatile (
    "vmovq (%[src]), %%xmm0\n\t"
    "vmovd %[lo], %%xmm1\n\t"
    "vpbroadcastb %%xmm1, %%xmm1\n\t"
    "vmovd %[hi], %%xmm2\n\t"
    "vpbroadcastb %%xmm2, %%xmm2\n\t"
    "vpcmpgtb %%xmm1, %%xmm0, %%xmm3\n\t"
    "vpcmpgtb %%xmm0, %%xmm2, %%xmm4\n\t"
    "vpand %%xmm3, %%xmm4, %%xmm0\n\t"
    "vpmovmskb %%xmm0, %[mask]\n\t"
    : [mask] "=r" (valid_mask)
    : [src] "r" (cursor),
      [lo] "r" ((uint32_t) ('0' - 1)),
      [hi] "r" ((uint32_t) ('9' + 1))
    : "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "memory"
  );

  // Not all eight are digits - fall back to scalar.
  if ((valid_mask & 0xFF) != 0xFF) {
    uint64_t value = 0;
    int count = 0;
    while (count < length and cursor[count] >= '0' and cursor[count] <= '9') {
      value = value * 10 + (cursor[count] - '0');
      count++;
    }
    *out = value;
    return count;
  }
  uint32_t high_8 = Simd_Parse_8_Digits_Avx2 (cursor);

  // If 16 digits are available, validate the second group of eight.
  if (length >= 16) {
    __asm__ volatile (
      "vmovq 8(%[src]), %%xmm0\n\t"
      "vmovd %[lo], %%xmm1\n\t"
      "vpbroadcastb %%xmm1, %%xmm1\n\t"
      "vmovd %[hi], %%xmm2\n\t"
      "vpbroadcastb %%xmm2, %%xmm2\n\t"
      "vpcmpgtb %%xmm1, %%xmm0, %%xmm3\n\t"
      "vpcmpgtb %%xmm0, %%xmm2, %%xmm4\n\t"
      "vpand %%xmm3, %%xmm4, %%xmm0\n\t"
      "vpmovmskb %%xmm0, %[mask]\n\t"
      : [mask] "=r" (valid_mask)
      : [src] "r" (cursor),
        [lo] "r" ((uint32_t) ('0' - 1)),
        [hi] "r" ((uint32_t) ('9' + 1))
      : "xmm0", "xmm1", "xmm2", "xmm3", "xmm4", "memory"
    );
    if ((valid_mask & 0xFF) == 0xFF) {
      uint32_t low_8 = Simd_Parse_8_Digits_Avx2 (cursor + 8);
      *out = (uint64_t) high_8 * 100000000ULL + low_8;
      return 16;
    }
  }

  // Only the first eight digits were valid.
  *out = high_8;
  return 8;
}
#endif  // SIMD_X86_64

// Convert a decimal digit string (with optional leading sign) to a Big_Integer.  Uses the AVX2
// SIMD path when available to process eight or sixteen digits at a time.
Big_Integer *Big_Integer_From_Decimal_SIMD (const char *text) {
  Big_Integer *integer = Big_Integer_New (4);
  integer->is_negative = (*text == '-');
  if (*text == '-' or *text == '+') text++;

  // Skip leading zeros.
  while (*text == '0') text++;
  if (*text == '\0' or (*text < '0' or *text > '9')) {
    integer->limbs[0] = 0;
    integer->count = 1;
    Big_Integer_Normalize (integer);
    return integer;
  }

  // Locate the end of the digit run.
  const char *end = text;
  while (*end >= '0' and *end <= '9') end++;
#ifdef SIMD_X86_64
  Simd_Detect_Features ();
  if (Simd_Has_Avx2) {
    integer->limbs[0] = 0;
    integer->count = 1;
    const char *cursor = text;
    while (cursor < end) {
      int remaining = (int) (end - cursor);

      // When eight or more digits remain, try the SIMD batch path.
      if (remaining >= 8) {
        uint64_t chunk;
        int parsed = Simd_Parse_Digits_Avx2 (cursor, end, &chunk);
        if (parsed == 16) {
          Big_Integer_Mul_Add_Small (integer, 10000000000000000ULL, chunk);
          cursor += 16;
        } else if (parsed == 8) {
          Big_Integer_Mul_Add_Small (integer, 100000000ULL, chunk);
          cursor += 8;
        } else {
          Big_Integer_Mul_Add_Small (integer, 10, (uint64_t) (*cursor - '0'));
          cursor++;
        }
      } else {
        Big_Integer_Mul_Add_Small (integer, 10, (uint64_t) (*cursor - '0'));
        cursor++;
      }
    }
    Big_Integer_Normalize (integer);
    return integer;
  }
#endif

  // Scalar fallback: one digit at a time.
  integer->limbs[0] = 0;
  integer->count = 1;
  while (text < end) {
    Big_Integer_Mul_Add_Small (integer, 10, (uint64_t) (*text - '0'));
    text++;
  }
  Big_Integer_Normalize (integer);
  return integer;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// §19. DRIVER                                                                                      
// ═════════════════════════════════════════════════════════════════════════════════════════════════


void Compile_File (const char *input_path, const char *output_path) {

  // Reset loaded bodies for this compilation
  Loaded_Body_Count = 0;
  Loaded_Body_Names_Count = 0;
  size_t source_size;
  char *source = Read_File (input_path, &source_size);
  if (not source) {
    fprintf (stderr, "Error: cannot read file '%s'\n", input_path);
    return;
  }

  // Parse all compilation units in the file
  Parser parser = Parser_New (source, source_size, input_path);
  Syntax_Node *units[64];
  int unit_count = 0;
  while (parser.current_token.kind != TK_EOF and unit_count < 64 and not parser.had_error) {
    units[unit_count++] = Parse_Compilation_Unit (&parser);
  }
  if (parser.had_error) {
    fprintf (stderr, "Parsing failed with %d error(s)\n", Error_Count);
    free (source);
    return;
  }

  // Semantic analysis for all units
  Symbol_Manager_Init ();
  for (int i = 0; i < unit_count; i++) {
    Resolve_Compilation_Unit (units[i]);
  }
  if (Error_Count > 0) {
    fprintf (stderr, "Semantic analysis failed with %d error(s)\n", Error_Count);
    free (source);
    return;
  }

  // Code generation
  FILE *out_file;
  bool close_output = false;
  if (output_path) {
    out_file = fopen (output_path, "w");
    if (not out_file) {
      fprintf (stderr, "Error: cannot open output file '%s'\n", output_path);
      free (source);
      return;
    }
    close_output = true;
  } else {
    out_file = stdout;  // Output to stdout if no -o specified
  }
  Code_Generator_Init (out_file);
  for (int i = 0; i < unit_count; i++) {
    Generate_Compilation_Unit (units[i]);
  }

  // Generate code for loaded package bodies (e.g., TEXT_IO)
  for (int i = 0; i < Loaded_Body_Count; i++) {
    Generate_Compilation_Unit (Loaded_Package_Bodies[i]);
  }

  // Note: Derived type operations (RM 3.4) don't need wrapper functions.                           
  // Derived types have identical representation to parent types in Ada 83,                         
  // so calls to derived operations are emitted directly to the parent's                            
  // implementation (GNAT-style optimization). See call_target handling                             
  // in Generate_Apply ().                                                                          
  //                                                                                                

  // Emit address marker globals for 'ADDRESS attribute on packages/generics
  for (uint32_t i = 0; i < cg->address_marker_count; i++) {
    Symbol *sym = cg->address_markers[i];
    Emit ("@__addr.");
    Emit_Symbol_Name (sym);
    Emit (" = linkonce_odr constant i8 0\n");
  }

  // ═══════════════════════════════════════════════════════════════════════════════════════════════
  // Emit @main() with Standard-style elaboration order (§15)                                       
  //                                                                                                
  // Per Ada RM 10.2, library units must be elaborated in a safe order                              
  // before the main subprogram executes. The elaboration model (§15)                               
  // computes this order using dependency analysis and topological sort.                            
  //                                                                                                
  // The algorithm respects:                                                                        
  //   - WITH clause dependencies                                                                   
  //   - pragma Elaborate / Elaborate_All                                                           
  //   - Spec-before-body ordering                                                                  
  //   - Preelaborate / Pure unit optimizations                                                     
  // ═══════════════════════════════════════════════════════════════════════════════════════════════
  // Compute elaboration order using the dependency graph algorithm                                 
  //                                                                                                
  if (cg->main_candidate) {
    Elab_Order_Status elab_status = Elab_Compute_Order ();
    if (elab_status == ELAB_ORDER_HAS_ELABORATE_ALL_CYCLE) {
      fprintf (stderr, "Error: circular pragma Elaborate_All dependency\n");
    } else if (elab_status == ELAB_ORDER_HAS_CYCLE) {
      fprintf (stderr, "Warning: elaboration cycle detected, using source order\n");
    }
    Emit ("\n; C main entry point\n");
    Emit ("; Elaboration order computed by Standard-style algorithm (S15.7)\n");
    Emit ("define i32 @main() {\n");

    // Call elaboration functions in computed dependency order.
    // Prefer the graph-computed order; fall back to source order.
    uint32_t elab_order_count = Elab_Get_Order_Count ();
    if (elab_order_count > 0 and elab_status == ELAB_ORDER_OK) {
      for (uint32_t i = 0; i < elab_order_count; i++) {
        if (not Elab_Needs_Elab_Call (i)) continue;
        Symbol *sym = Elab_Get_Order_Symbol (i);
        if (not sym) continue;
        Emit ("  call void @");
        Emit_Symbol_Name (sym);
        Emit ("___elab()\n");
      }

    // Fallback to source order (old behavior)
    } else {
      for (uint32_t i = 0; i < cg->elab_func_count; i++) {
        Emit ("  call void @");
        Emit_Symbol_Name (cg->elab_funcs[i]);
        Emit ("___elab()\n");
      }
    }
    Emit ("  call void @");
    Emit_Symbol_Name (cg->main_candidate);
    Emit ("()\n");
    Emit ("  call void @exit (i32 0)\n");
    Emit ("  ret i32 0\n");
    Emit ("}\n");
  }

  // Emit tracked exception globals that weren't defined in the header.                             
  // This handles instance-prefixed exceptions from generic instantiations                          
  // (e.g., @__exc.seq_io__status_error_s0 from SEQ_IO.STATUS_ERROR).                               
  //                                                                                                
  if (cg->exc_ref_count > 0) {
    for (uint32_t i = 0; i < cg->exc_ref_count; i++) {
      const char *name = cg->exc_refs[i];

      // Check if already defined by Generate_Exception_Globals (in header).
      // Compare against each declared exception's mangled name.
      bool already = false;
      for (uint32_t j = 0; j < Exception_Symbol_Count; j++) {
        FILE *real_out = cg->output;
        char buf[256];
        FILE *mem = fmemopen (buf, sizeof (buf) - 1, "w");
        cg->output = mem;
        Emit_Symbol_Name (Exception_Symbols[j]);
        fflush (mem);
        long len = ftell (mem);
        fclose (mem);
        buf[len] = '\0';
        cg->output = real_out;
        if (strcmp (name, buf) == 0) { already = true; break; }
      }

      // Also check standard exceptions
      if (not already) {
        static const char *std_exc[] = {
          "constraint_error", "numeric_error", "program_error",
          "storage_error", "tasking_error", NULL
        };
        for (int j = 0; std_exc[j]; j++) {
          if (strcmp (name, std_exc[j]) == 0) { already = true; break; }
        }
      }
      if (not already) {
        Emit ("@__exc.%s = private constant i8 0\n", name);
      }
    }
  }
  if (close_output) {
    fclose (out_file);
    fprintf (stderr, "Compiled '%s' -> '%s'\n", input_path, output_path);

    // Generate GNAT-compatible .ali file for dependency tracking
    Generate_ALI_File (output_path, units, unit_count, source, source_size);
  }
  free (source);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Derive output .ll path from input path by replacing extension.                                   
// Writes into caller-supplied buffer.                                                              
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void Derive_Output_Path (const char *input, char *out, size_t out_size) {
  strncpy (out, input, out_size - 1);
  out[out_size - 1] = '\0';
  char *dot   = strrchr (out, '.');
  char *slash = strrchr (out, '/');

  // Only replace if the dot is after the last slash (i.e., part of filename)
  if (dot and (not slash or dot > slash))
    strcpy (dot, ".ll");
  else
    strncat (out, ".ll", out_size - strlen (out) - 1);
}

// ─────────────────────────────────────────────────────────────────────────────────────────────────
// Parallel compilation - fork-based worker called from a pthread.                                  
//                                                                                                  
// Each thread forks a child process that compiles one file.  fork() gives                          
// complete isolation of all global state (arena, error count, loaded                               
// packages, etc.) without refactoring Compile_File.                                                
// ─────────────────────────────────────────────────────────────────────────────────────────────────

void *Compile_Worker (void *arg) {
  Compile_Job *job = (Compile_Job *)arg;
  char derived[512];
  const char *out = job->output_path;
  if (not out) {
    Derive_Output_Path (job->input_path, derived, sizeof (derived));
    out = derived;
  }
  pid_t pid = fork ();

  // Child - compile and exit
  if (pid == 0) {
    Compile_File (job->input_path, out);
    _exit (Error_Count > 0 ? 1 : 0);
  } else if (pid > 0) {
    int status;
    waitpid (pid, &status, 0);
    job->exit_status = WIFEXITED (status) ? WEXITSTATUS (status) : 1;
  } else {
    perror ("fork");
    job->exit_status = 1;
  }
  return NULL;
}
int main (int argc, char *argv[]) {
  if (argc < 2) {
    fprintf (stderr,
      "Usage: %s [-I path] <input.ada ...> [-o output.ll]\n", argv[0]);
    return 1;
  }
  const char *inputs[256];
  int input_count = 0;
  const char *output = NULL;  // NULL means derive from input name

  // Parse command-line arguments
  for (int i = 1; i < argc; i++) {
    if (strcmp (argv[i], "-I") == 0 and i + 1 < argc) {
      if (Include_Path_Count < 32)
        Include_Paths[Include_Path_Count++] = argv[++i];
    } else if (strncmp (argv[i], "-I", 2) == 0) {
      if (Include_Path_Count < 32)
        Include_Paths[Include_Path_Count++] = argv[i] + 2;
    } else if (strcmp (argv[i], "-o") == 0 and i + 1 < argc) {
      output = argv[++i];
    } else if (argv[i][0] != '-') {
      if (input_count < 256)
        inputs[input_count++] = argv[i];
    }
  }
  if (input_count == 0) {
    fprintf (stderr, "Error: no input file specified\n");
    return 1;
  }
  if (output and input_count > 1) {
    fprintf (stderr, "Error: -o cannot be used with multiple input files\n");
    return 1;
  }

  // ── Auto-discover rts path from executable location ────────────────────────────────────────────

  {
    char exe_path[PATH_MAX];
    ssize_t len = readlink ("/proc/self/exe", exe_path, sizeof (exe_path) - 1);
    if (len > 0) {
      exe_path[len] = '\0';

    // Fallback: use argv[0]
    } else {
      strncpy (exe_path, argv[0], sizeof (exe_path) - 1);
      exe_path[sizeof (exe_path) - 1] = '\0';
    }
    char *slash = strrchr (exe_path, '/');
    if (slash) {
      *slash = '\0';
      static char rts_path[PATH_MAX + 8];
      snprintf (rts_path, sizeof (rts_path), "%s/rts", exe_path);
      struct stat st;
      if (stat (rts_path, &st) == 0 and S_ISDIR (st.st_mode)) {
        if (Include_Path_Count < 32)
          Include_Paths[Include_Path_Count++] = rts_path;
      }
    }
  }

  // ── Auto-discover input file's directory as include path ───────────────────────────────────────

  {
    const char *slash = strrchr (inputs[0], '/');
    if (slash) {
      static char input_dir[PATH_MAX];
      size_t dir_len = (size_t)(slash - inputs[0]);
      if (dir_len >= sizeof (input_dir)) dir_len = sizeof (input_dir) - 1;
      memcpy (input_dir, inputs[0], dir_len);
      input_dir[dir_len] = '\0';
      if (Include_Path_Count < 32)
        Include_Paths[Include_Path_Count++] = input_dir;
    }
  }

  // Add current directory to include paths by default
  if (Include_Path_Count < 32)
    Include_Paths[Include_Path_Count++] = ".";

  // ── Compile ────────────────────────────────────────────────────────────────────────────────────
  // Single file - existing sequential behaviour
  if (input_count == 1) {
    Compile_File (inputs[0], output);
    Arena_Free_All ();
    return Error_Count > 0 ? 1 : 0;
  }

  // Multiple files - parallel compilation using pthreads + fork
  long nprocs = sysconf(_SC_NPROCESSORS_ONLN);
  if (nprocs < 1) nprocs = 1;
  int nthreads = (input_count < (int)nprocs) ? input_count : (int)nprocs;
  Compile_Job jobs[256];
  pthread_t threads[256];
  for (int base = 0; base < input_count; base += nthreads) {
    int batch = input_count - base;
    if (batch > nthreads) batch = nthreads;
    for (int i = 0; i < batch; i++) {
      jobs[base + i].input_path = inputs[base + i];
      jobs[base + i].output_path = NULL;  // derive from input
      jobs[base + i].exit_status = 0;
      pthread_create (&threads[i], NULL, Compile_Worker,
                     &jobs[base + i]);
    }
    for (int i = 0; i < batch; i++) {
      pthread_join (threads[i], NULL);
    }
  }
  int failed = 0;
  for (int i = 0; i < input_count; i++) {
    if (jobs[i].exit_status != 0) failed++;
  }
  if (failed > 0)
    fprintf (stderr, "%d of %d compilations failed\n", failed, input_count);
  return failed > 0 ? 1 : 0;
}

// ═════════════════════════════════════════════════════════════════════════════════════════════════
// END OF Ada 83                                                                                    
// ═════════════════════════════════════════════════════════════════════════════════════════════════
